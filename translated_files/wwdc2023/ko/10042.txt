10042

♪ ♪

더그: 모두 환영합니다. 저는 더그 데이비슨이고, 자연어 처리에 대해 이야기하기 위해 왔습니다.

이제, 지난 몇 년 동안 자연어 처리에 대한 많은 세션이 있었다.

오늘 우리가 이야기할 것은 흥미진진한 새로운 기능을 추가하여 그 모든 것을 기반으로 합니다.

먼저 나는 NLP와 NLP 모델에 대한 배경을 제공할 것이다.

그런 다음 기존 기능을 요약하겠습니다.

그럼 올해 새로운 것에 대해 얘기할게.

나는 몇 가지 고급 지원서에 대해 논의할 것이다.

그리고 나서 내가 마무리할게.

배경부터 시작합시다.

개략적으로, NLP 모델은 일반적으로 비슷한 흐름을 가지고 있다.

그들은 텍스트 데이터로 시작한 다음, 기계 학습 모델이 작동할 수 있는 수치 기능 표현으로 변환하는 입력 계층을 가지고 있으며, 이는 일부 출력을 생성합니다.

지난 몇 년 동안 이것의 가장 명백한 예는 텍스트 분류와 단어 태그를 지원하는 Create ML 모델이다.

필드로서의 NLP의 발전은 점점 더 정교한 버전의 입력 레이어의 개발만으로도 상당히 밀접하게 추적될 수 있다.

10년 또는 20년 전, 이것들은 간단한 직교 특징이었다.

그리고 약 10년 전, Word2Vec과 GloVe와 같은 정적 단어 임베딩의 사용으로 옮겨갔다.

그런 다음 CNN과 LSTM과 같은 신경망 모델을 기반으로 한 문맥 단어 삽입.

그리고 최근에는 변압기 기반 언어 모델.

나는 임베딩이 무엇인지에 대해 몇 마디 해야 한다.

가장 간단한 형태로, 그것은 언어의 단어에서 일부 추상적인 벡터 공간의 벡터로의 지도일 뿐이지만, 비슷한 의미를 가진 단어가 벡터 공간에서 서로 가까울 수 있도록 기계 학습 모델로 훈련되었다.

이것은 언어 지식을 통합할 수 있게 해준다.

정적 임베딩은 단어에서 벡터로 가는 간단한 지도일 뿐이다.

한 단어를 전달하면, 모델은 테이블에서 그것을 찾고 벡터를 제공한다.

이것들은 비슷한 의미를 가진 단어들이 벡터 공간에서 서로 가까워지도록 훈련되었다.

이것은 개별 단어를 이해하는 데 꽤 유용하다.

더 정교한 임베딩은 문장의 각 단어가 문장에서의 사용에 따라 다른 벡터로 매핑되도록 역동적이고 맥락적이다.

예를 들어, "패스트푸드 조인트"의 "음식"은 "생각을 위한 음식"의 "음식"과 다른 의미를 가지고 있으므로, 그들은 다른 임베딩 벡터를 얻을 것이다.

이제, 입력 계층으로 강력한 임베딩을 갖는 요점은 전송 학습을 허용하는 것이다.

임베딩은 많은 양의 데이터에 대해 훈련되며 엄청난 양의 작업별 교육 데이터를 요구하지 않고도 특정 작업으로 전송할 수 있는 언어에 대한 일반적인 지식을 캡슐화합니다.

현재, Create ML은 ELMo 모델을 사용하여 이러한 종류의 임베딩을 지원합니다.

이 모델들은 임베딩 벡터를 생성하기 위해 출력이 결합된 LSTM을 기반으로 한다.

이것들은 훈련 분류 및 태그 모델을 위해 Create ML을 통해 사용할 수 있습니다.

이제, 지금까지 지원된 모델에 대해 논의하겠습니다.

이것들은 2019년과 2020년의 이전 세션에서 매우 자세하게 논의되어 왔으므로, 여기서 간단히 설명하겠습니다.

자연어는 일반적으로 우리가 NLP 모델에서 본 패턴을 따르는 Create ML을 사용하여 모델 훈련을 지원합니다.

여기에는 텍스트 분류와 단어 태그 지정이라는 두 가지 작업을 위한 모델이 포함됩니다.

텍스트 분류에서, 출력은 일련의 클래스 중 하나를 사용하여 입력 텍스트를 설명한다.

예를 들어, 그것은 주제나 감정일 수 있다.

그리고 단어 태깅에서, 출력은 입력 텍스트의 각 단어에 라벨을 붙인다, 예를 들어, 연설의 일부 또는 역할 라벨.

그리고 지원되는 Create ML 모델은 일반적으로 Maxent 및 CRF 기반 모델로 시작하여 정적 단어 임베딩에 대한 지원을 추가한 다음 ELMo 임베딩을 사용하여 Create ML 모델에 대한 동적 단어 임베딩을 추가하여 NLP 필드의 진화를 따랐습니다.

그리고 이전 세션인 2019년의 "자연어 프레임워크의 발전"과 2020년의 "자연어로 앱을 더 똑똑하게 만들기"에서 이에 대한 자세한 내용을 볼 수 있습니다.

이제 올해 자연어의 새로운 것으로 돌아가겠습니다.

나는 우리가 이제 변압기 기반 문맥 임베딩을 제공한다고 말하게 되어 기쁩니다.

구체적으로, 이것들은 BERT 임베딩이다.

그것은 단지 트랜스포머의 양방향 인코더 표현을 의미한다.

이들은 마스킹된 언어 모델 스타일의 훈련을 사용하여 많은 양의 텍스트에 대해 훈련된 모델을 포함하고 있다.

이것은 모델이 한 단어가 가려진 문장을 받고, 예를 들어 "생각을 위한 음식"에서 "음식"이라는 단어를 제안하라는 요청을 받고, 이것을 더 잘 할 수 있도록 훈련받았다는 것을 의미한다.

그들의 중심에 있는 트랜스포머는 주의 메커니즘, 특히 다중 머리 자기 주의라고 불리는 것을 기반으로 하며, 이는 모델이 한 번에 여러 가지 다른 방식으로 다른 가중치를 가진 텍스트의 다른 부분을 고려할 수 있게 해준다.

다중 머리 자기 주의 메커니즘은 여러 개의 다른 레이어로 싸인 다음 여러 번 반복되며, 이는 모두 많은 양의 텍스트 데이터를 활용할 수 있는 강력하고 유연한 모델을 제공한다.

사실 그것은 한 번에 여러 언어로 훈련될 수 있으며, 다국어 모델로 이어진다.

이것은 몇 가지 장점이 있다.

그것은 많은 언어를 즉시 지원하고 심지어 한 번에 여러 언어를 지원할 수 있게 해준다.

하지만 그 이상으로, 언어 간의 유사성 때문에, 한 언어에 대한 데이터가 다른 언어에 도움이 되는 시너지 효과가 있다.

그래서 우리는 즉시 다양한 언어군에 걸쳐 27개의 다른 언어를 지원하기 위해 갔다.

이것은 세 개의 별도의 모델로 이루어지며, 각각은 관련 쓰기 시스템을 공유하는 언어 그룹을 위한 것이다.

그래서 라틴 문자 언어에 대한 하나의 모델, 키릴 문자를 사용하는 언어에 대한 모델, 그리고 중국어, 일본어, 한국어에 대한 모델이 있습니다.

이러한 임베딩 모델은 앞서 논의한 Create ML 교육과 잘 맞으며, 입력 인코딩 계층 역할을 합니다.

이것은 다양한 모델에 대한 강력한 인코딩이다.

게다가, 당신이 훈련에 사용하는 데이터가 모두 단일 언어로 되어 있을 필요는 없습니다.

이것이 예시로 어떻게 작동하는지 보여드리겠습니다.

메시징 앱을 작성하고 사용자가 받는 메시지를 자동으로 분류하여 사용자를 돕고 싶다고 가정해 봅시다.

그것들을 세 가지 범주로 나누고 싶다고 가정해 봅시다: 친구로부터 받을 수 있는 개인 메시지, 동료로부터 받을 수 있는 비즈니스 메시지, 그리고 상호 작용하는 비즈니스에서 받을 수 있는 상업 메시지.

하지만 사용자는 다양한 언어로 메시지를 받을 수 있으며, 당신은 그것을 처리하고 싶습니다.

이 예를 들어, 저는 영어, 이탈리아어, 독일어, 스페인어 등 여러 언어로 훈련 데이터를 수집했습니다.

저는 json 형식을 사용했지만, 디렉토리나 CSV를 사용할 수도 있습니다.

모델을 훈련시키기 위해, 우리는 Create ML 앱으로 가서 프로젝트를 만듭니다.

그런 다음 우리는 훈련 데이터를 선택해야 합니다.

나는 또한 그것과 함께하기 위해 검증 데이터와 테스트 데이터를 준비했다.

그런 다음 우리는 알고리즘을 선택해야 하며, 여기에 새로운 선택이 있습니다: BERT 임베딩.

일단 우리가 그것들을 선택하면, 우리는 대본을 선택할 수 있다.

이것들은 라틴어 문자 언어이기 때문에, 나는 그것을 라틴어로 남겨둘 것이다.

만약 우리가 단일 언어를 사용하고 있다면, 여기에 지정할 수 있는 옵션이 있을 것이지만, 이것은 다국어이기 때문에, 우리는 그것을 자동으로 놔둘 것이다.

그러면 우리가 해야 할 일은 기차를 누르는 것뿐이고, 모델 훈련이 시작될 것이다.

교육에서 가장 시간이 많이 걸리는 부분은 이러한 강력한 임베딩을 텍스트에 적용하는 것이다.

그런 다음 모델은 높은 수준의 정확도로 상당히 빠르게 훈련한다.

그 시점에서, 우리는 몇 가지 예시 메시지에서 그것을 시도해 볼 수 있다.

영어로... 아니면 스페인어로.

그리고 그 모델은 이것들이 상업적인 메시지라고 꽤 확신한다.

가능한 시너지 효과의 예로, 이 모델은 프랑스어에 대한 교육을 받지 않았지만, 여전히 일부 프랑스어 텍스트도 분류할 수 있다.

나는 당신이 관심 있는 각 언어에 대해 훈련 데이터를 사용하는 것을 추천합니다.

이제, 지금까지 우리는 Create ML로 작업해 왔지만, NLContextualEmbedding이라는 새로운 클래스와 함께 자연어 프레임워크를 사용하여 이러한 임베딩으로 작업할 수도 있습니다.

이를 통해 원하는 임베딩 모델을 식별하고 일부 속성을 찾을 수 있습니다.

예를 들어, 언어나 스크립트로 다양한 방법으로 임베딩 모델을 찾을 수 있습니다.

그러한 모델이 있으면, 벡터의 차원과 같은 속성을 얻을 수 있습니다.

또한, 각 모델에는 모델을 고유하게 식별하는 문자열인 식별자가 있습니다.

예를 들어, 모델로 작업을 시작할 때, 언어별로 찾을 수 있지만, 나중에 정확히 동일한 모델을 사용하고 있는지 확인하고 싶을 것이며, 식별자를 통해 이 작업을 수행할 수 있습니다.

명심해야 할 한 가지는 다른 많은 자연어 기능과 마찬가지로 이러한 임베딩 모델은 필요에 따라 다운로드되는 자산에 의존한다는 것입니다.

NLContextualEmbedding은 사용 전에 다운로드를 요청하는 것과 같이 이에 대한 추가 제어를 제공하는 몇 가지 API를 제공합니다.

주어진 임베딩 모델에 현재 장치에서 사용할 수 있는 자산이 있는지 물어볼 수 있으며, 요청하지 않으면 다운로드될 수 있습니다.

이제, 여러분 중 몇몇은 제가 Create ML을 사용하여 훈련하지 않는 모델이 있지만, 대신 PyTorch 또는 TensorFlow를 사용하여 훈련한다고 말할 수도 있습니다.

이 새로운 BERT 임베딩을 여전히 사용할 수 있나요?

응, 넌 할 수 있어.

우리는 당신이 훈련하고 싶은 거의 모든 모델에 입력 레이어로 사용할 수 있는 사전 훈련된 다국어 임베딩 모델을 제공합니다.

그게 작동하는 방법은 다음과 같습니다.

macOS 장치에서 NLContextualEmbedding을 사용하여 훈련 데이터에 대한 임베딩 벡터를 얻을 수 있습니다.

그런 다음 PyTorch 또는 TensorFlow를 사용하여 교육에 입력으로 공급하고 Core ML 도구를 사용하여 결과를 Core ML 모델로 변환합니다.

그런 다음 장치의 추론 시간에 NLContextualEmbedding을 사용하여 입력 데이터에 대한 임베딩 벡터를 가져오고 Core ML 모델로 전달하여 출력을 얻을 수 있습니다.

이를 지원하기 위해, 모델을 로드하고, 텍스트에 적용하고, 결과 임베딩 벡터를 얻을 수 있는 추가 NLContextualEmbedding API가 있습니다.

이전 모델 식별자를 기억한다면, 그것을 사용하여 훈련에 사용한 것과 동일한 모델을 검색할 수 있습니다.

그런 다음 모델을 텍스트 조각에 적용하여 NLContextualEmbeddingResult 객체를 제공할 수 있습니다.

이 객체가 있으면, 그것을 사용하여 임베딩 벡터를 반복할 수 있습니다.

이제, 이것으로 가능한 것을 맛보기 위해, 우리는 간단한 예시 모델을 준비했습니다.

우리는 기존의 영어 안정적인 확산 모델로 시작한 다음, 일부 다국어 데이터를 사용하여 새로운 BERT 임베딩을 입력 레이어로 사용하기 위해 미세 조정하고, 고정된 것으로 간주하고, 치수를 변환하기 위해 간단한 선형 투영 레이어를 훈련했습니다.

그 결과는 다국어 입력을 받는 안정적인 확산 모델이다.

다음은 모델 출력의 몇 가지 예입니다.

예를 들어, "분홍색 꽃으로 가득 찬 정원을 통과하는 길"과 같은 영어 텍스트를 통과하면, 그 모델은 우리를 분홍색 꽃으로 가득 찬 정원으로 인도한다.

하지만 또한, 같은 문장을 프랑스어, 스페인어, 이탈리아어, 독일어로 번역하면, 그 모델은 각각 분홍색 꽃으로 가득 찬 길과 정원의 이미지를 만들어낸다.

조금 더 복잡한 예를 들어보겠습니다.

"흐린 하늘 아래 나무와 산 앞에 있는 길." 여기 도로, 나무, 산, 구름이 있는 모델의 출력이 있습니다.

하지만 마찬가지로, 나는 같은 문장을 프랑스어, 스페인어, 이탈리아어, 독일어 또는 다른 여러 언어로 번역할 수 있으며, 각각에 대해 도로, 나무, 산, 구름의 이미지를 얻을 수 있다.

이제 이 세션의 교훈을 요약하겠습니다.

Create ML을 사용하여 텍스트 분류 또는 단어 태그 작업을 위한 모델을 쉽게 훈련시킬 수 있으며, 새로운 다국어 BERT 임베딩 모델은 이러한 목적을 위한 강력한 입력 인코딩 레이어를 제공합니다.

이 모델들은 단일 언어 또는 다국어가 될 수 있다.

또한 BERT 임베딩을 PyTorch 또는 TensorFlow로 훈련하고 싶은 모든 모델의 입력 레이어로 사용할 수 있습니다.

고마워.

이제 나가서 몇몇 모델 훈련을 시작하세요.

♪ ♪