10047

♪ ♪

Pulkit: 안녕하세요, 저는 Pulkit이고, Core ML 팀의 엔지니어입니다.

Core ML Tools에 대한 몇 가지 업데이트를 공유하게 되어 기쁩니다.

이 업데이트는 기계 학습 모델의 크기와 성능을 최적화하는 데 도움이 됩니다.

모델의 기능이 크게 향상됨에 따라, 점점 더 많은 기능이 기계 학습에 의해 구동되고 있다.

결과적으로, 단일 앱에 배포된 모델의 수가 증가하고 있다.

그것과 함께, 앱의 각 모델도 점점 커지고 있으며, 앱의 크기에 상승 압력을 가하고 있다.

그래서, 모델 크기를 점검하는 것이 중요하다.

모델 크기를 줄이는 데는 몇 가지 이점이 있다.

각 모델이 더 작으면 동일한 메모리 예산으로 더 많은 모델을 배송할 수 있습니다.

그것은 또한 당신이 더 크고, 더 유능한 모델을 배송할 수 있게 해준다.

그것은 또한 모델을 더 빨리 실행하는 데 도움이 될 수 있다.

이것은 더 작은 모델이 메모리와 프로세서 사이를 이동할 데이터가 적다는 것을 의미하기 때문이다.

그래서, 모델의 크기를 줄이는 것은 좋은 생각인 것 같다.

무엇이 모델을 크게 만드나요?

당신이 이해할 수 있도록 예시를 살펴보겠습니다.

ResNet50은 인기 있는 이미지 분류 모델이다.

그것의 첫 번째 층은 약 9,000개의 매개 변수를 가진 컨벌루션 층이다.

그리고 그것은 다양한 크기의 총 53개의 컨벌루션 층을 가지고 있다.

결국, 그것은 약 210만 개의 매개 변수를 가진 선형 층을 가지고 있다.

이 모든 것은 최대 2,500만 개의 매개 변수를 추가합니다. Float16 정밀도를 사용하여 모델을 저장하면 무게당 2바이트를 사용하며 50메가바이트 크기의 모델을 얻을 수 있습니다.

50메가바이트 모델은 크지만, 안정적인 확산과 같은 최신 모델에 도달하면 훨씬 더 큰 모델로 끝납니다.

이제, 더 작은 모델을 얻기 위한 몇 가지 경로에 대해 이야기해 봅시다.

한 가지 방법은 점점 더 적은 무게로 좋은 성능을 얻을 수 있는 더 효율적인 모델 아키텍처를 설계하는 것이다.

또 다른 방법은 기존 모델의 가중치를 압축하는 것이다.

이 모델 압축 경로는 내가 집중할 것이다.

모델 압축을 위한 세 가지 유용한 기술을 설명하는 것으로 시작하겠습니다.

다음으로, 저는 이러한 모델 압축 기술을 통합하는 두 가지 워크플로우를 시연할 것입니다.

그런 다음 최신 Core ML Tools가 이러한 기술과 워크플로우를 모델에 적용하는 데 어떻게 도움이 되는지 설명하겠습니다.

그리고 마지막으로, Srijan은 모델 압축이 런타임 성능에 미치는 영향에 대해 논의할 것이다.

압축 기술부터 시작합시다.

모델 가중치를 압축하는 몇 가지 방법이 있다.

첫 번째 방법은 희소 행렬 표현을 사용하여 더 효율적으로 포장하는 것이다.

이것은 가지치기라고 불리는 기술을 사용하여 달성할 수 있다.

또 다른 방법은 무게를 저장하는 데 사용되는 정밀도를 줄이는 것이다.

이것은 양자화 또는 palettization에 의해 달성될 수 있다.

이 두 전략 모두 손실이며, 압축된 모델은 일반적으로 압축되지 않은 모델에 비해 약간 덜 정확하다.

이제 이 기술들 각각에 대해 더 자세히 살펴봅시다.

무게 정리는 희소한 표현으로 모델 무게를 효율적으로 포장하는 데 도움이 됩니다.

가중치 매트릭스를 스파스화하거나 정리하는 것은 가중치 값 중 일부를 0으로 설정하는 것을 의미합니다.

나는 웨이트 매트릭스로 시작한다.

그것을 자르기 위해, 나는 가장 작은 크기 가중치를 0으로 설정할 수 있다.

이제, 나는 0이 아닌 값만 저장하면 된다.

나는 결국 도입된 0개당 약 2바이트의 저장 공간을 절약하게 된다.

물론, 나는 또한 나중에 조밀한 매트릭스를 재구성하기 위해 0의 위치를 저장해야 할 것이다.

모델 크기는 도입된 희소성의 수준에 따라 선형적으로 내려간다.

50% 스파스 모델은 무게의 50%가 0이라는 것을 의미하며, ResNet50 모델의 경우 약 28메가바이트의 크기를 가지고 있으며, 이는 Float16 크기의 약 절반이다.

두 번째 가중치 압축 기술은 가중치를 저장하기 위해 8비트 정밀도를 사용하는 양자화이다.

양자화를 수행하기 위해, 당신은 무게 값과 스케일을 취하고, 이동하고, INT8 범위에 있도록 반올림합니다.

이 예에서, 스케일은 2.35이며, 가장 작은 값을 -127로 매핑하고, 편향은 0이다.

모델에 따라, 0이 아닌 바이어스도 사용될 수 있으며, 이는 때때로 양자화 오류를 줄이는 데 도움이 된다.

스케일과 편향은 나중에 가중치를 quatize하여 원래 범위로 되돌리는 데 사용될 수 있다.

무게 정밀도를 8비트 이하로 줄이려면, 무게 클러스터링 또는 팔레트화라는 기술을 사용할 수 있습니다.

이 기술에서, 비슷한 값을 가진 가중치는 함께 그룹화되고 그들이 속한 클러스터 중심체의 값을 사용하여 표현된다.

이 중심들은 룩업 테이블에 보관되어 있다.

그리고 원래 가중치 행렬은 각 요소가 해당 클러스터 중심을 가리키는 인덱스 테이블로 변환됩니다.

이 예에서, 나는 네 개의 클러스터를 가지고 있기 때문에, 2비트를 사용하여 각 무게를 나타낼 수 있으며, Float16보다 8배의 압축을 달성할 수 있다.

가중치를 나타내는 데 사용할 수 있는 고유한 클러스터 센터의 수는 n의 힘에 2와 같으며, 여기서 n은 팔레트화에 사용되는 정밀도이다.

그래서 4비트 팔레트화는 당신이 16개의 클러스터를 가질 수 있다는 것을 의미합니다.

양자화는 모델 크기를 절반으로 줄이는 반면, 팔레트화는 최대 8배 더 작게 만드는 데 도움이 될 수 있습니다.

요약하자면, 체중 압축을 위한 세 가지 다른 기술이 있다.

그들 각각은 가중치를 나타내기 위해 다른 방법을 사용한다.

그들은 가지치기를 위한 희소의 양과 palettization을 위한 비트 수와 같은 각각의 매개 변수에 의해 제어될 수 있는 다양한 수준의 압축을 제공한다.

이제, 이러한 기술을 모델 개발 워크플로우에 어떻게 통합할 수 있는지 설명하겠습니다.

먼저 Core ML 모델 변환을 위한 워크플로우부터 시작합시다.

좋아하는 파이썬 교육 프레임워크로 모델을 훈련한 다음 Core ML 도구를 사용하여 해당 모델을 Core ML로 변환할 수 있습니다.

이 워크플로우는 훈련 후 압축 워크플로우가 되기 위해 한 단계 더 확장될 수 있다.

이를 위해, 전체 크기를 줄이기 위해 이미 훈련되고 변환된 모델 가중치에서 작동하는 압축 단계를 추가합니다.

이 워크플로우는 언제든지 시작할 수 있습니다.

예를 들어, 훈련 데이터나 이미 변환된 코어 ML 모델이 필요 없는 사전 훈련된 모델로 시작할 수 있습니다.

이 워크플로우를 적용할 때, 적용된 압축의 양을 선택할 수 있는 옵션이 있습니다.

더 많은 압축을 적용할수록, 결과 모델은 더 작아지지만, 예상할 수 있듯이, 몇 가지 절충안이 있습니다.

특히, 당신은 특정 정확도를 달성하는 압축되지 않은 모델로 시작할 것입니다.

약간의 압축을 적용하면 모델 크기가 줄어들지만 정확도에도 영향을 미칠 수 있습니다.

더 많은 압축을 적용함에 따라, 이 영향은 더 두드러질 수 있으며 정확도 손실은 용납할 수 없게 될 수 있습니다.

이러한 추세와 허용 가능한 절충안은 각 사용 사례마다 다르며 모델과 데이터 세트에 따라 다릅니다.

실제로 이 절충안을 보려면, 이미지에서 물체를 분할하는 모델을 살펴봅시다.

내 이미지의 경우, 모델은 소파에 속한 각 픽셀의 확률을 반환한다.

기본 Float16 모델은 물체를 매우 잘 구분한다.

10% 정리된 모델의 경우, 출력은 기본 모델과 매우 유사하다.

유물은 30%의 희소성으로 나타나기 시작하고 더 높은 수준으로 증가한다.

일단 40%의 가지치기에 도달하면, 모델은 완전히 고장나고, 확률 맵은 인식할 수 없게 된다.

마찬가지로, 8비트 양자화와 6비트 팔레트화는 기본 모델의 출력을 보존한다.

4비트 팔레트화에서, 당신은 일부 아티팩트를 보기 시작하고, 2비트 팔레트화에서, 모델은 물체를 완전히 분할하지 못합니다.

더 높은 압축 속도로 모델 성능 저하를 극복하려면, 다른 워크플로우를 사용할 수 있습니다.

이 워크플로우는 훈련 시간 압축이라고 불린다.

여기서, 가중치를 압축하는 동안 일부 데이터에 대한 모델을 미세 조정합니다.

압축은 가중치가 그들에게 부과된 새로운 제약에 다시 적응할 수 있도록 점진적으로 그리고 차별화 가능한 방식으로 도입된다.

모델이 만족스러운 정확도를 달성하면, 변환하고 압축된 코어 ML 모델을 얻을 수 있습니다.

기존 모델 교육 워크플로우에 교육 시간 압축을 통합하거나 사전 훈련된 모델로 시작할 수 있습니다.

훈련 시간 압축은 모델 정확도와 압축 양 사이의 절충을 개선하여 더 높은 압축 속도로 동일한 모델 성능을 유지할 수 있습니다.

같은 이미지 세분화 모델을 다시 살펴봅시다.

훈련 시간 정리의 경우, 모델 출력은 최대 40%의 희소성까지 변경되지 않습니다.

이것은 훈련 후 정확성이 무너진 곳이다.

사실, 이제 50%와 75%의 희소성에서도, 그 모델은 기본 모델과 비슷한 확률 맵을 달성한다.

모델 정확도의 상당한 저하를 관찰하기 시작하는 것은 90%의 희소성이다.

마찬가지로, 훈련 시간 양자화와 팔레트화는 또한 이 경우 최대 2비트의 압축까지 기준선 모델의 출력을 보존한다.

요약하자면, 모델 변환이나 모델 훈련 중에 무게 압축을 적용할 수 있습니다.

후자는 더 긴 훈련 시간의 비용으로 더 나은 정확도 트레이드오프를 제공한다.

두 번째 워크플로우는 훈련 중에 압축을 적용하기 때문에, 우리는 또한 압축 알고리즘을 구현하기 위해 차별화 가능한 작업을 사용해야 합니다.

이제 이러한 압축 워크플로우가 Core ML Tools로 어떻게 실행될 수 있는지 살펴봅시다.

훈련 후 모델 압축 API는 압축 유틸리티 하위 모듈에서 가지치기, palettization 및 양자화를 위해 Core ML Tools 6에서 사용할 수 있습니다.

그러나, 훈련 시간 압축을 위한 API는 없었다.

Core ML Tools 7을 통해 훈련 시간 압축 기능도 제공하기 위해 새로운 API가 추가되었습니다.

그리고 우리는 오래된 API와 새로운 API를 coremltools.optimize라는 단일 모듈로 통합했습니다.

교육 후 압축 API는 coremltools.optimize.coreml에서 마이그레이션되었으며, 새로운 교육 시간 API는 coremltools.optimize.torch에서 사용할 수 있습니다.

후자는 파이토치 모델과 함께 작동한다.

먼저 교육 후 API를 자세히 살펴봅시다.

훈련 후 압축 워크플로우에서, 입력은 코어 ML 모델이다.

내가 설명한 세 가지 압축 기술을 각각 적용하는 optimize.coreml 모듈에서 사용할 수 있는 세 가지 방법으로 업데이트할 수 있습니다.

이러한 방법을 사용하려면, 모델을 압축하는 방법을 설명하는 OptimizationConfig 객체를 만드는 것으로 시작합니다.

여기서, 나는 75%의 목표 희소성으로 규모 가지치기를 하고 있다.

구성이 정의되면, prune_weights 메소드를 사용하여 모델을 다듬을 수 있습니다.

압축된 모델을 얻는 것은 간단한 한 단계 과정이다.

이러한 기술과 관련된 구성을 사용하여 가중치를 팔레트화하고 정량화하기 위해 유사한 API를 사용할 수 있습니다.

이제 훈련 시간 압축 워크플로우를 고려해 봅시다.

이 경우, 앞서 설명했듯이, 훈련 가능한 모델과 데이터가 필요합니다.

더 구체적으로, Core ML Tools로 모델을 압축하려면, 사전 훈련된 가중치로 PyTorch 모델로 시작합니다.

그런 다음 optimize.torch 모듈에서 사용 가능한 API 중 하나를 사용하여 업데이트하고 압축 레이어가 삽입된 새로운 PyTorch 모델을 받으세요.

그런 다음 데이터와 원래 PyTorch 훈련 코드를 사용하여 미세 조정하세요.

이것은 가중치가 압축을 허용하도록 조정되는 단계이다.

그리고 MPS PyTorch 백엔드를 사용하여 로컬로 맥북에서 이 단계를 수행할 수 있습니다.

모델이 정확성을 회복하도록 훈련되면, 코어 ML 모델을 얻기 위해 변환하세요.

코드 예시를 통해 이것을 더 자세히 살펴봅시다.

압축하고 싶은 모델을 미세 조정하는 데 필요한 PyTorch 코드로 시작합니다.

몇 줄의 코드만 추가하여 Core ML 도구를 쉽게 활용하여 훈련 시간 정리를 추가할 수 있습니다.

먼저 모델을 어떻게 다듬고 싶은지 설명하는 MagnitudePrunerConfig 객체를 만듭니다.

여기서, 나는 목표 희소성을 75%로 설정하고 있다.

Yaml 파일에 설정을 작성하고 from_yaml 메소드를 사용하여 로드할 수도 있습니다.

그런 다음, 압축하려는 모델과 방금 만든 설정으로 프루너 객체를 만듭니다.

다음으로, 모델에 가지치기 레이어를 삽입하기 위해 준비를 호출합니다.

모델을 미세 조정하는 동안, 단계 API를 호출하여 프루너의 내부 상태를 업데이트합니다.

훈련이 끝나면, 당신은 가지치기 마스크를 무게로 접기 위해 마무리를 요청합니다.

그런 다음 이 모델은 변환 API를 사용하여 Core ML로 변환할 수 있습니다.

동일한 워크플로우가 양자화와 팔레트화에도 사용될 수 있다.

이제, Srijan은 Core ML Tools API를 사용하여 객체 감지 모델을 분석하는 방법을 보여주는 데모를 안내할 것입니다.

스리잔: 고마워, 풀킷.

제 이름은 Srijan이고, Core ML Tools optimize API의 데모를 안내해 드리겠습니다.

나는 이미지에서 사람들을 감지하기 위해 ResNet18 백본이 있는 SSD 모델을 사용할 것이다.

먼저 기본 모델과 교육 유틸리티를 가져오자.

나는 방금 말한 SSD ResNet18 모델의 인스턴스를 얻는 것으로 시작할 것이다.

단순화하기 위해, 나는 그것을 위해 미리 작성된 get_ssd_model 유틸리티라고 부를 것이다.

이제 모델이 로드되었으니, 몇 가지 시대를 위해 훈련합시다.

객체 감지 모델이기 때문에, 훈련의 목표는 감지 작업의 SSD 손실을 줄이는 것이다.

간결함을 위해, train_epoch 유틸리티는 다른 배치를 통해 포워드를 호출하고, 손실을 계산하고, 그라디언트 하강을 수행하는 것과 같이 시대를 위한 모델을 훈련하는 데 필요한 코드를 캡슐화한다.

훈련하는 동안, SSD 손실이 줄어들고 있는 것 같다.

나는 이제 그 모델을 코어 ML 모델로 변환할 것이다.

그렇게 하기 위해, 나는 먼저 모델을 추적한 다음 coremltools.convert API를 호출할 것이다.

모델의 크기를 확인하기 위해 가져온 유틸리티에 전화합시다.

모델의 크기는 23.6메가바이트이다.

이제, 나는 Core ML 모델에 대한 예측을 실행할 것이다.

나는 런던 여행에서 내 자신의 이미지와 탐지를 테스트하기 위해 다른 이미지를 선택했다.

모델이 물체를 감지하기 위한 신뢰 임계값은 30%로 설정되므로 물체가 존재한다고 최소 30% 확신하는 상자만 플로팅할 수 있습니다.

그 탐지는 정확해 보인다.

나는 이제 이 모델의 크기를 줄일 수 있는지 궁금하다.

나는 먼저 훈련 후 palettization을 시도해 볼 것이다.

그것을 위해, 나는 coremltools.optimize.coreml에서 몇 가지 구성 클래스와 메소드를 가져올 것이다.

나는 이제 모델의 무게를 6비트로 팔레트화할 것이다.

그것을 위해, 나는 OpPalettizerConfig 객체를 만들고, 모드를 kmeans로 지정하고 nbits를 6으로 지정할 것이다.

이것은 op 수준에서 매개 변수를 지정할 것이며, 나는 각 op를 다르게 palettize할 수 있다.

하지만, 지금 당장, 나는 모든 작전에 동일한 6비트 모드를 적용할 것이다.

나는 OptimizationConfig를 정의하고 이 op_config를 글로벌 매개 변수로 전달함으로써 그것을 할 것이다.

그런 다음 최적화 설정은 palettized 모델을 얻기 위해 변환된 모델과 함께 palettize_weights 메소드로 전달됩니다.

지금 크기가 어떻게 줄어든지 봅시다.

모델의 크기는 약 9메가바이트로 떨어졌지만, 테스트 이미지의 성능에 영향을 미쳤나요?

알아보자.

와, 탐지는 여전히 잘 작동해.

나는 지금 2비트 훈련 후 palettization을 시도하는 데 내 행운을 밀어붙이게 되어 정말 신나.

그렇게 하는 것은 OpPalettizerConfig에서 nbits를 6에서 2로 변경하고 palettize_weights API를 다시 실행하는 것만큼 간단합니다.

유틸리티를 사용하여 이 Core ML 모델의 크기와 성능을 봅시다.

예상대로, 모델의 크기는 줄어들었고 약 3메가바이트로 줄어들었다.

그러나 모델이 두 이미지 모두에서 사람들을 감지할 수 없기 때문에 성능은 최적이 아니다.

모델이 예측한 상자 중 어느 것도 30% 임계값 이상의 신뢰 확률을 가지고 있지 않기 때문에 예측에 상자가 나타나지 않습니다.

그것이 더 잘 작동하는지 확인하기 위해 2비트 훈련 시간 팔레트를 시도해 봅시다.

나는 그것을 하기 위해 coremltools.optimize.torch에서 DKMPalettizerConfig와 DKMPalettizer를 가져오는 것으로 시작할 것이다.

DKM은 주의 기반 차별화 가능한 kmeans 연산을 수행하여 가중 클러스터를 배우는 알고리즘이다.

이제 palettization 설정을 정의할 때입니다.

Global_config에서 n_bits를 2로 지정하기만 하면, 지원되는 모든 모듈은 2비트 palettized가 될 것입니다.

그리고 여기서, 나는 모델과 구성에서 팔레티저 객체를 만들 것이다.

지금 준비 API를 호출하면 palettization 친화적인 모듈을 모델에 삽입할 것이다.

몇 가지 시대를 위해 모델을 미세 조정할 시간이다.

이제 모델이 미세 조정되었으므로, 나는 팔레트화된 가중치를 모델의 가중치로 복원하여 프로세스를 완료하는 최종 API를 호출할 것이다.

다음 단계는 모델의 크기를 확인하는 것이다.

그것을 위해, 나는 토치 모델을 코어 ML 모델로 변환할 것이다.

Torch.jit.trace를 사용하여 모델을 추적하는 것으로 시작합시다.

이제 변환 API를 호출할 것이며, 이번에는 PassPipeline이라는 추가 플래그를 사용하고 그 값을 DEFAULT_PALETTIZATION으로 설정할 것입니다.

이것은 변환된 가중치에 대한 palettized 표현을 사용하기 위해 변환기를 나타낼 것이다.

테스트 이미지에서 모델의 크기와 성능을 봅시다.

나는 훈련 시간 palettized 모델이 약 3메가바이트라는 것을 알 수 있으며, 이는 8배 압축으로 내려오지만, 훈련 후 palettized 모델과는 달리, 이 모델은 테스트 이미지에 대한 감지를 올바르게 수행하고 있다.

이것이 데모였기 때문에, 나는 방금 두 개의 샘플 이미지에서 모델 성능을 테스트했다.

실제 시나리오에서, 나는 평균 평균 정밀도와 같은 메트릭을 사용하고 검증 데이터 세트에서 평가할 것이다.

요약하자.

나는 훈련된 모델로 시작하여 Float16 가중치를 가진 23.6메가바이트 모델을 얻기 위해 변환했다.

그런 다음, 나는 palettize_weights API를 사용하여 6비트 가중치를 가진 더 작은 모델을 빠르게 얻었는데, 이는 내 데이터에서 잘 수행되었다.

그러나, 내가 그것을 2비트로 더 밀었을 때, 그것은 성능의 명백한 하락을 보였다.

이것을 게시하면, 나는 optimize.torch API로 토치 모델을 업데이트하고 미분 가능한 kmeans 알고리즘을 사용하여 몇 가지 시대를 미세 조정했다.

그것으로, 나는 2비트 압축 옵션으로 좋은 정확도를 얻을 수 있었다.

데모가 특정 모델과 최적화 알고리즘 조합을 사용하는 동안, 이 워크플로우는 사용 사례로 일반화되며 원하는 압축의 양과 모델을 재교육하는 데 필요한 시간과 데이터 사이의 절충점을 알아내는 데 도움이 될 것입니다.

이것은 우리를 마지막 주제인 공연으로 이끈다.

앱에 배포될 때 이러한 모델을 보다 효율적으로 실행하기 위해 Core ML 런타임의 개선 사항에 대해 간략하게 언급하고 싶습니다.

iOS 16과 iOS 17의 런타임 간의 몇 가지 주요 차이점을 살펴봅시다.

iOS 16에서는 무게 전용 압축 모델에 대한 지원이 있었지만, iOS 17에서는 8비트 활성화 양자화 모델도 실행할 수 있습니다.

iOS 16에서 가중치 압축 모델은 플로트 가중치가 있는 해당 모델과 같은 속도로 실행되는 반면, iOS 17에서는 Core ML 런타임이 업데이트되었으며, 이제 압축 모델은 특정 시나리오에서 더 빠르게 실행됩니다.

비슷한 런타임 개선은 최신 버전의 macOS, tvOS 및 watchOS에서도 사용할 수 있습니다.

하지만 이러한 개선은 어떻게 이루어졌나요?

가중치만 압축되는 모델에서, 활성화가 부동 소수점 정밀도에 있기 때문에, 컨벌루션이나 행렬 곱셈과 같은 작업이 발생하기 전에, 가중치 값은 다른 입력의 정밀도와 일치하도록 압축 해제되어야 합니다.

이 감압 단계는 iOS 16 런타임에서 미리 일어난다.

따라서, 이 경우, 모델은 실행 전에 메모리의 완전 플로트 정밀 모델로 변환됩니다.

따라서, 추론 지연에는 변화가 관찰되지 않는다.

그러나, iOS 17에서, 특정 시나리오에서, 가중치는 작업이 실행되기 직전에 제 시간에 압축 해제된다.

이것은 모든 추론 호출에서 감압을 하는 비용으로 메모리에서 더 작은 비트 가중치를 로드하는 이점이 있다.

신경 엔진과 같은 특정 컴퓨팅 단위와 메모리에 바인딩된 특정 유형의 모델의 경우, 이것은 추론 이득으로 이어질 수 있다.

이러한 런타임 이점을 설명하기 위해, 나는 몇 가지 모델을 선택하고 프로파일링하고 Float16 변형에 비해 추론이 빨라지는 상대적인 양을 플로팅했다.

예상대로, 속도 향상의 양은 모델과 하드웨어에 따라 다르다.

이것들은 iPhone 14 Pro Max의 4비트 팔레트 모델의 속도 향상 범위입니다.

개선 사항은 5%에서 30%까지 다양하다.

스파스 모델의 경우, 모델 유형에 따라 다양한 개선 사항이 있으며, 일부 모델은 Float16 변형보다 75% 빠르게 실행됩니다.

이제 질문이 생깁니다: 최고의 대기 시간 성능을 얻기 위한 전략은 무엇입니까?

그것은 플로트 모델로 시작하고 optimize.coreml API를 사용하여 모델의 다양한 표현을 탐색하는 것입니다.

모델을 재교육할 필요가 없기 때문에, 이것은 빠를 것이다.

그런 다음, 관심 있는 장치에 프로필을 설정하세요.

이를 위해 Xcode의 Core ML 성능 보고서는 작업이 실행되는 위치를 포함하여 추론에 대한 많은 가시성을 제공합니다.

그런 다음, 어떤 구성이 당신에게 최고의 이득을 제공하는지에 기반한 최종 후보자 명단.

이 후, 정확성을 평가하고 개선하는 데 집중할 수 있으며, 모델을 마무리하기 전에 토치와 코어 ML 도구로 훈련 시간 압축을 적용해야 할 수도 있습니다.

요약하자면, 모델의 크기를 줄이는 것이 중요하며, 이제 새로운 Core ML Tools API로 그 어느 때보다 쉽게 할 수 있으며 메모리 공간을 줄이고 추론 속도를 높일 수 있습니다.

더 많은 옵션과 벤치마킹 데이터를 확인하려면, 우리의 문서를 방문하세요.

또한 오늘 슬라이드에서 다루지 않은 Core ML 프레임워크의 개선 사항에 대해 이야기하는 "비동기 예측과 Core ML 통합 개선" 비디오를 시청하는 것이 좋습니다.

고마워, 그리고 행복한 압축.