10033

♪ ♪

그랜트: 안녕하세요, 제 이름은 그랜트입니다. 저는 접근성 팀의 엔지니어입니다.

많은 사람들이 애플 플랫폼에서 음성 합성을 사용하고 어떤 사람들은 신디사이저 목소리에 의존한다.

이 목소리들은 그들의 장치에 대한 창이다.

그러므로, 그들이 선택하는 목소리는 종종 매우 개인적인 선택이다.

iOS에서 음성 합성을 사용하는 사람들은 이미 다양한 목소리 중에서 선택할 수 있다.

당신이 어떻게 더 많은 것을 제공할 수 있는지 살펴봅시다.

먼저, 우리는 음성 합성 마크업 언어가 무엇인지, 사용자 지정 음성에 몰입형 음성 출력을 가져올 수 있는 방법, 그리고 음성 제공자가 이를 채택해야 하는 이유에 대해 이야기할 것입니다.

다음으로, 우리는 장치 전반에 걸쳐 신디사이저와 음성 경험을 제공하기 위해 음성 합성 공급자를 구현하는 방법을 살펴볼 것입니다.

그리고 마지막으로, 우리는 개인 목소리로 뛰어들 것이다.

이건 새로운 기능이야.

이제, 사람들은 그들의 목소리를 녹음한 다음 그 녹음에서 합성된 목소리를 생성할 수 있다.

그래서 이제, 당신은 사용자의 개인적인 목소리로 연설을 합성할 수 있습니다.

SSML을 살펴보는 것으로 시작합시다.

SSML은 음성 텍스트를 표현하기 위한 W3C 표준이다.

SSML 연설은 다양한 태그와 속성이 있는 XML 형식을 사용하여 선언적으로 표현된다.

이 태그를 사용하여 속도와 피치와 같은 음성 속성을 제어할 수 있습니다.

SSML은 자사 신디사이저에 사용된다.

여기에는 WebKit의 WebSpeech가 포함되며 음성 신디사이저의 표준 입력입니다.

SSML을 어떻게 사용할 수 있는지 살펴봅시다.

일시 정지가 있는 이 예시 문구를 가져가세요.

우리는 SSML에서 이 일시 정지를 나타낼 수 있다.

우리는 "안녕하세요" 문자열로 시작하고, SSML 브레이크 태그를 사용하여 1초 일시 정지를 추가하고, "만나서 반가워요!"를 가속화하여 마무리할 것입니다.

우리는 SSML 프로소디 태그를 추가하고 속도 속성을 200%로 설정하여 이를 수행합니다.

이제 우리는 이 SSML을 가지고 AVSpeechUtterance를 만들 수 있습니다.

다음으로, 자신의 음성 신디사이저 목소리를 어떻게 구현할 수 있는지 살펴봅시다.

그래서 스피치 신디사이저가 뭐야?

음성 신디사이저는 SSML의 형태로 원하는 음성 속성에 대한 텍스트와 정보를 수신하고 해당 텍스트의 오디오 표현을 제공합니다.

훌륭한 새로운 목소리를 가진 신디사이저가 있고 iOS, macOS 및 iPadOS로 가져오고 싶다고 가정해 봅시다.

음성 합성 공급자를 사용하면 자신의 음성 신디사이저와 목소리를 플랫폼에 구현하여 시스템 음성을 넘어 사용자에게 더 많은 개인화를 제공할 수 있습니다.

이게 어떻게 작동하는지 보자.

음성 합성 공급자 오디오 장치 확장은 호스트 앱에 포함되며 SSML의 형태로 음성 요청을 받게 됩니다.

확장 프로그램은 SSML 입력에 대한 오디오를 렌더링하고 선택적으로 해당 오디오 버퍼 내에서 단어가 발생하는 위치를 나타내는 마커를 반환합니다.

그러면 시스템은 그 음성 요청에 대한 모든 재생을 관리할 것이다.

오디오 세션 관리를 처리할 필요가 없습니다. 음성 합성 공급자 프레임워크에 의해 내부적으로 관리됩니다.

이제 신디사이저가 무엇인지 이해했으므로, 우리는 음성 신디사이저 확장을 구축하기 시작할 수 있다.

Xcode에서 새로운 오디오 유닛 확장 앱 프로젝트를 만든 다음, "음성 신디사이저" 오디오 유닛 유형을 선택하고 신디사이저에 대한 네 문자 하위 유형 식별자와 제조업체로서 당신을 위한 네 문자 식별자를 제공합시다.

오디오 유닛 확장은 음성 신디사이저 확장이 구축된 핵심 아키텍처이다.

그들은 당신의 신디사이저가 호스트 앱 프로세스 대신 확장 프로세스에서 실행될 수 있게 해준다.

우리 앱은 확장 프로그램이 음성을 합성할 음성을 구매하고 선택할 수 있는 간단한 인터페이스를 제공할 것입니다.

우리는 구매 가능한 목소리를 보여주는 목록 보기를 만드는 것으로 시작할 것입니다.

각 음성 셀에는 음성 이름과 구매 버튼이 표시됩니다.

다음으로, 나는 몇 가지 목소리로 내 목록을 채울 것이다.

여기서, WWDCVoice는 음성 이름과 식별자를 보유하는 간단한 구조체이다.

우리는 또한 구매한 목소리를 추적하기 위한 상태 변수와 그것들을 표시하기 위한 새로운 섹션이 필요합니다.

다음으로, 음성을 구매하기 위한 기능을 만들어 봅시다.

여기서 우리는 새로 구매한 음성을 목록에 추가하고 그에 따라 UI를 업데이트할 수 있습니다.

AVSpeechSynthesisProviderVoice 방법 업데이트SpeechVoices에 주목하세요.

그것이 당신의 앱이 신디사이저에 사용 가능한 음성 세트가 변경되었고 시스템 음성 목록을 재구성해야 한다는 신호를 보낼 수 있는 방법입니다.

우리의 예에서, 우리는 음성에 대한 인앱 구매를 완료한 후 이 전화를 걸 수 있습니다.

우리는 또한 음성 신디사이저 확장 프로그램에서 어떤 목소리를 사용할 수 있는지 확인할 수 있는 방법이 필요합니다.

이것은 앱 그룹을 통해 공유될 UserDefaults의 인스턴스를 만들어 수행할 수 있습니다.

앱 그룹을 사용하면 호스트 앱과 확장 프로그램 간에 이 음성 목록을 공유할 수 있습니다.

우리는 앱 그룹을 만들 때 제공한 제품군 이름을 명시적으로 지정하고 있습니다.

이것은 호스트 앱과 확장 프로그램이 동일한 도메인에서 읽도록 보장합니다.

구매 기능을 되돌아보면, 새로운 음성을 구매할 때 사용자 기본값을 업데이트하는 방법을 구현했습니다.

AVSpeechSynthesizer는 또한 사용 가능한 시스템 음성의 변화를 들을 수 있는 새로운 API를 가지고 있다.

시스템 음성 세트는 사용자가 음성을 삭제하거나 새 음성을 다운로드할 때 바뀔 수 있습니다.

사용 가능한 VoicesDidChangeNotification을 구독하여 이러한 변경 사항을 기반으로 음성 목록을 업데이트할 수 있습니다.

이제 호스트 앱이 끝났으니, 네 가지 주요 구성 요소로 구성된 오디오 장치를 채워봅시다.

우리가 가장 먼저 추가해야 할 것은 우리의 신디사이저가 제공할 목소리를 시스템에 알리는 방법이다.

이것은 우리가 이전에 지정한 앱 그룹 UserDefaults 도메인에서 음성과 읽기 목록을 제공하기 위해 speechVoices 게터를 재정의함으로써 달성됩니다.

음성 목록의 각 항목에 대해, 우리는 미국 영어 AVSpeechSynthesisProviderVoice를 구성할 것입니다.

다음으로, 우리는 시스템이 신디사이저에게 어떤 텍스트를 합성할 것인지 말할 수 있는 방법이 필요하다.

synthesizeSpeechRequest 메서드는 시스템이 일부 텍스트를 합성하기 시작해야 한다는 확장 프로그램에 신호를 보내고 싶을 때 호출됩니다.

이 방법에 대한 인수는 SSML과 어떤 음성으로 말할지 포함하는 AVSpeechSynthesisProviderRequest의 인스턴스가 될 것이다.

다음으로, 나는 음성 엔진 구현에서 만든 도우미 방법을 부를 것이다.

이 예에서, 내 getAudioBuffer 방법은 요청과 SSML 입력에 지정된 음성을 기반으로 오디오 데이터를 생성합니다.

우리는 또한 렌더링 블록이 호출될 때 얼마나 많은 프레임을 렌더링했는지 추적하고 버퍼에서 프레임을 복사하기 위해 framePosition이라는 인스턴스 변수를 0으로 설정할 것입니다.

이 시스템은 또한 오디오 합성을 중단하고 현재 음성 요청을 폐기하기 위해 신디사이저에 신호를 보내는 방법이 필요하다.

이것은 cancelSpeechRequest로 이루어지며, 여기서 우리는 단순히 현재 버퍼를 폐기할 것이다.

마지막으로, 우리는 렌더링 블록을 구현해야 한다.

렌더링 블록은 원하는 frameCount를 가진 시스템에 의해 호출됩니다.

그런 다음 오디오 장치는 요청된 프레임 수를 출력 오디오버퍼에 채울 책임이 있다.

다음으로, 우리는 synthesizeSpeechRequest 호출 중에 이전에 생성하고 저장한 대상 버퍼와 버퍼에 대한 참조를 설정할 것입니다.

그런 다음, 우리는 프레임을 대상 버퍼에 복사할 것이다.

그리고 마지막으로, 오디오 장치가 현재 음성 요청에 대한 모든 버퍼를 소진하면, actionFlags 인수를 offlineUnitRenderAction_Complete로 설정하여 렌더링이 완료되었고 더 이상 렌더링할 오디오 버퍼가 없다는 신호를 시스템에 알려야 합니다.

실제로 그것을 보자!

이건 내 스피치 신디사이저 앱이야.

나는 음성을 구매하고 새로운 음성과 음성 엔진을 사용하여 음성을 합성할 수 있는 보기로 이동할 것이다.

먼저, 나는 신디사이저에게 "안녕하세요"를 입력할 것이다.

합성된 목소리: 안녕하세요.

그랜트: 그럼 내가 "안녕"이라고 입력할게.

합성된 목소리: 안녕.

그랜트: 우리는 이제 합성 공급자를 구현하고 VoiceOver에서 자신의 앱에 이르기까지 시스템 전반에 걸쳐 사용할 수 있는 음성을 제공하는 호스팅 앱을 만들었습니다!

우리는 당신이 이 API를 사용하여 어떤 새로운 목소리와 텍스트 음성 변환 경험을 만드는지 빨리 보고 싶습니다.

개인 음성이라는 새로운 기능에 대해 이야기해 봅시다.

사람들은 이제 장치의 힘을 사용하여 iOS와 macOS에서 음성을 녹음하고 다시 만들 수 있습니다.

당신의 개인 음성은 서버가 아닌 장치에서 생성됩니다.

이 목소리는 나머지 시스템 목소리에 나타날 것이며 라이브 스피치라는 새로운 기능과 함께 사용할 수 있습니다.

라이브 스피크는 iOS, iPadOS, macOS 및 watchOS의 유형 말하기 기능으로, 사람이 즉석에서 자신의 목소리로 음성을 합성할 수 있습니다.

개인 음성에 대한 새로운 요청 인증 API를 사용하여 이러한 음성으로 음성을 합성할 수 있는 액세스를 요청할 수 있습니다.

개인 음성의 사용은 민감하며 주로 증강 또는 대체 통신 앱에 사용되어야 한다는 것을 명심하세요.

내가 개인 음성을 사용하기 위해 만든 AAC 앱을 확인해 보자.

내 앱에는 내가 WWDC에서 말하는 일반적인 문구를 말할 수 있는 두 개의 버튼과 개인 음성 사용을 위한 액세스를 요청하는 버튼이 있다.

AVSpeechSynthesizer에서 requestPersonalVoiceAuthorization이라는 새로운 API로 승인을 요청할 수 있습니다.

일단 승인되면, 개인 음성은 AVSpeechSynthesisVoice API speechVoices에서 시스템 음성과 함께 나타나며 isPersonalVoice라는 새로운 음성 특성으로 표시됩니다.

이제 나는 개인 음성에 접근할 수 있게 되었으므로, 나는 그것을 사용하여 대화할 수 있다.

개인 음성의 데모를 확인해 봅시다.

먼저, "개인 음성 사용" 버튼을 탭하여 승인을 요청하고, 일단 승인되면, 기호를 탭하여 내 목소리를 들을 수 있습니다.

개인 목소리: 안녕하세요, 제 이름은 그랜트입니다. WWDC23에 오신 것을 환영합니다.

그랜트: 놀랍지 않아?

그리고 이제 앱에서도 이 목소리를 사용할 수 있습니다.

이제 SSML에 대해 논의했으므로, 이를 사용하여 음성 입력을 표준화하고 앱에서 풍부한 음성 경험을 구축해야 합니다.

우리는 또한 음성 신디사이저를 Apple 플랫폼에 구현하는 방법을 살펴보었으므로, 이제 사람들이 시스템 전체에서 사용할 수 있는 훌륭한 새로운 음성 음성을 제공할 수 있습니다.

그리고 마지막으로, Personal Voice를 사용하면, 특히 자신의 목소리를 잃을 위험에 처한 사람들을 위해 앱의 합성에 더 많은 개인적인 터치를 가져올 수 있습니다.

우리는 당신이 이 API를 사용하여 어떤 경험을 만드는지 보게 되어 매우 기쁩니다.

봐줘서 고마워.