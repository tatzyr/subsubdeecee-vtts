10091

♪ 부드러운 기악 힙합 ♪

♪

오미드 칼릴리: 안녕하세요! 내 이름은 오미드야.

올리버와 저는 ARKit 팀의 엔지니어이며 iOS AR 앱을 새로운 플랫폼으로 가져올 때 알아야 할 친숙하고 새로운 개념을 검토하게 되어 기쁩니다.

ARKit은 2017년에 iOS에 도입되었으며, 이를 통해 증강 현실 애플리케이션을 구축하기 위한 세 가지 핵심 개념을 도입했습니다.

세계 추적을 통해, ARKit은 6개의 자유도로 세계에서 장치의 위치를 추적할 수 있습니다.

이것은 현실 세계에 대한 위치와 방향으로 가상 콘텐츠를 고정할 수 있게 해준다.

장면 이해는 당신 주변의 현실 세계에 대한 통찰력을 제공합니다.

제공된 기하학과 의미론적 지식을 사용하여, 당신의 콘텐츠를 지능적으로 배치하고 주변 환경과 현실적으로 상호 작용할 수 있습니다.

마지막으로, 렌더링 엔진은 ARKit에서 제공하는 카메라 변환과 본질을 활용하여 캡처된 이미지 위에 가상 콘텐츠를 올바르게 등록하고 합성할 수 있습니다.

처음에, 우리는 ARKit의 카메라 변환을 사용하고 iOS에서 3D 콘텐츠를 렌더링하기 위해 SceneKit 보기로 시작했습니다.

그런 다음 RealityKit을 도입하여 주변 환경과 함께 매우 사실적인 물리적 기반 렌더링과 정확한 물체 시뮬레이션을 할 수 있는 엔진의 기반을 마련했습니다.

공간 컴퓨팅을 가능하게 하기 위해, ARKit과 RealityKit은 성숙했으며 운영 체제에 깊이 통합되어 있다.

예를 들어, ARKit의 추적 및 장면 이해는 이제 시스템 서비스로 실행되고 있으며, 창 배치에서 공간 오디오에 이르기까지 모든 것을 뒷받침합니다.

그 시스템은 애플리케이션에 속했던 책임을 진다.

이제 사용자 손의 카메라 패스스루와 매트가 내장되어 있으므로, 애플리케이션은 이러한 기능을 무료로 얻을 수 있습니다.

또 다른 내장 기능은 ARKit 세계 지도가 시스템 서비스에 의해 지속적으로 유지되므로 애플리케이션이 더 이상 수행할 필요가 없다는 것입니다.

우리는 이것이 당신이 이 플랫폼에 가능한 최고의 응용 프로그램과 콘텐츠를 구축하는 데 집중할 수 있다고 믿습니다.

다음은 이 플랫폼에 도입된 새로운 기능과 함께 이러한 기능을 보여주는 예입니다.

예를 들어, ARKit은 이제 앱에 손 추적을 제공하여 사람들이 주변 환경과 상호 작용할 수 있는 가상 콘텐츠와 직접 상호 작용할 수 있습니다.

이 새로운 플랫폼이 제공하는 모든 새로운 기능과 몰입형 경험을 활용하려면 iOS ARKit 기반 경험을 업데이트해야 합니다.

이것은 공간 컴퓨팅을 위한 앱과 AR 경험을 다시 상상할 수 있는 좋은 기회입니다.

이 전환의 일환으로, 당신은 우리가 ARKit과 RealityKit과 함께 도입한 친숙한 개념을 사용할 것입니다.

우리는 이 개념들이 어떻게 전달되었는지, 어떻게 진화했는지, 그리고 어떻게 활용할 수 있는지에 대해 다룰 것입니다.

시작하자!

먼저, 우리는 당신이 공간 컴퓨팅을 위한 앱을 발표할 수 있는 몇 가지 새로운 방법을 탐구하고, 당신이 사용할 수 있는 새로운 콘텐츠 도구를 소개할 것입니다.

다음으로, 우리는 당신의 콘텐츠를 렌더링하고 상호 작용하는 데 사용하는 엔진인 리얼리티 키트에 대해 이야기할 것입니다.

RealityView를 통해 앱이 iOS의 ARView와 유사한 공간 컴퓨팅을 어떻게 활용할 수 있는지 알아보겠습니다.

그런 다음, 우리는 당신의 앱이 사람들의 주변 환경에 콘텐츠를 가져올 수 있는 다양한 방법에 대해 이야기할 것입니다.

레이캐스팅은 많은 iOS 애플리케이션이 콘텐츠를 배치하는 데 사용하는 것이다.

우리는 ARKit 데이터와 RealityKit을 결합하여 공간 컴퓨팅을 위한 레이캐스팅을 활성화하는 방법의 예를 보여줄 것입니다.

그리고 마지막으로, 우리는 ARKit에 대한 업데이트를 검토하고 iOS의 친숙한 개념을 활용하는 새로운 방법을 볼 것입니다.

공간 컴퓨팅을 위한 경험을 마이그레이션할 준비를 합시다.

공간 컴퓨팅을 사용하면 iOS AR 경험을 창 너머로 확장할 수 있습니다.

이 플랫폼은 iOS 경험을 가져올 때 고려해야 할 애플리케이션을 제시하는 새로운 방법을 제공합니다.

여기 Hello World 샘플 앱의 예가 있습니다.

이제 창과 3차원 콘텐츠를 포함한 UI를 주변 어디에서나 표시할 수 있습니다.

기본적으로, 이 플랫폼의 애플리케이션은 공유 공간으로 실행됩니다.

공유 공간은 Mac 데스크톱의 여러 앱과 마찬가지로 앱이 나란히 존재하는 곳이다.

공유 공간 내에서, 당신의 앱은 콘텐츠를 표시하기 위해 하나 이상의 창을 열 수 있습니다.

게다가, 당신의 앱은 3차원 볼륨을 만들 수 있습니다.

예를 들어, 이제 한 창에 사용 가능한 보드 게임 목록, 다른 창에 규칙을 표시하고, 선택한 게임을 자체 볼륨으로 열 수 있습니다.

이 게임은 승리 전략을 읽기 위해 사파리 창을 열어 두면서 플레이할 수 있습니다.

창과 볼륨에 추가하는 콘텐츠는 다른 응용 프로그램과 공간을 공유할 수 있도록 범위 내에 포함되어 있습니다.

어떤 경우에는, 당신은 당신의 앱이 당신의 경험에 대한 몰입 수준을 더 잘 제어하기를 원할 수도 있습니다 - 아마도 당신의 방과 상호 작용하는 게임을 할 수도 있습니다.

이를 위해, 앱은 앱의 창, 볼륨 및 3D 개체만 나타나는 전용 전체 공간을 열 수 있습니다.

풀 스페이스에 들어가면, 당신의 애플리케이션은 더 많은 기능에 접근할 수 있습니다.

RealityKit의 앵커 엔티티를 사용하면 테이블, 바닥, 심지어 손바닥이나 손목과 같은 손 부분과 같은 주변 환경에 물체를 타겟팅하고 부착할 수 있습니다.

앵커 엔티티는 사용자 허가 없이 작동합니다.

ARKit 데이터는 앱이 전체 공간에서만 액세스할 수 있는 다른 것입니다.

허가를 받으면, ARKit은 실제 표면, 장면 기하학 및 골격 손 추적에 대한 데이터를 제공하여 현실적인 물리학과 자연스러운 상호 작용을 위한 앱의 능력을 확장합니다.

윈도우, 볼륨 및 공간은 모두 SwiftUI 장면 유형입니다.

당신이 이것들에 대해 배울 수 있는 것이 훨씬 더 많습니다.

우선, 여기에 언급된 세션으로 갈 수 있습니다.

다음으로, 공간 컴퓨팅으로 가져오기 위해 콘텐츠를 준비하는 데 필요한 주요 단계를 검토해 봅시다.

iOS의 기억에 남는 AR 경험은 훌륭한 3D 콘텐츠로 시작됩니다. 이 플랫폼의 공간 경험도 마찬가지입니다.

그리고 3D 콘텐츠에 관해서는, Universal Scene Description 또는 줄여서 USD와 같은 개방형 표준에 의존하는 것이 좋습니다.

USD는 제작이 입증되었으며, 단일 자산을 만드는 제작자부터 AAA 게임과 영화를 작업하는 대형 스튜디오에 이르기까지 확장된다.

애플은 USD의 얼리 어답터였으며, 2017년에 우리 플랫폼에 추가했고 그 이후로 지원이 증가했다.

오늘날, USD는 공간 컴퓨팅을 위한 3D 콘텐츠의 핵심이다.

USD 자산이 준비되면, 새로운 개발자 도구인 Reality Composer Pro로 가져와 3D 콘텐츠를 구성, 편집 및 미리 볼 수 있습니다.

iOS에서 3D 콘텐츠에 CustomMaterials를 사용하는 경우, 셰이더 그래프를 사용하여 다시 작성해야 합니다.

또한 UI를 통해 RealityKit 구성 요소를 직접 편집할 수 있습니다.

마지막으로, Reality Composer Pro 프로젝트를 Xcode로 직접 가져올 수 있으므로 모든 USD 자산, 재료 및 사용자 지정 구성 요소를 Xcode 프로젝트에 쉽게 묶을 수 있습니다.

Reality Composer Pro와 공간 컴퓨팅을 위한 맞춤형 자료를 만드는 방법에 대해 더 많이 배우는 데 도움이 되는 훌륭한 세션이 있습니다.

이제 우리는 당신의 애플리케이션을 제시하는 다양한 방법을 보았으므로, 당신의 경험을 가져올 때 RealityView가 제공하는 기능에 대해 자세히 알아봅시다.

우리는 방금 공간 컴퓨팅을 통해 앱이 당신의 공간에 콘텐츠를 표시할 수 있는 방법을 보았습니다.

iOS에서 오는 주요 차이점 중 하나는 서로 다른 요소를 나란히 제시할 수 있는 방법이다.

3D 콘텐츠와 2D 요소가 어떻게 나타나고 서로 함께 작동할 수 있는지 주목하세요.

iOS에서 온, 당신은 친숙한 프레임워크를 사용하여 이들 각각을 만들 것입니다.

SwiftUI를 사용하여 최고의 2D UI를 구축하고 iOS와 같은 시스템 제스처 이벤트를 얻을 수 있습니다.

그리고 RealityKit을 사용하여 공간 경험을 위한 3D 콘텐츠를 렌더링할 수 있습니다.

이 두 가지와 동시에 인터페이스하는 방법은 공간 컴퓨팅의 고유한 요구를 충족시키기 위해 도입하는 새로운 SwiftUI 뷰인 RealityView를 통해서입니다.

RealityView는 SwiftUI와 RealityKit을 진정으로 연결하여 2D 및 3D 요소를 결합하여 기억에 남는 공간 경험을 만들 수 있습니다.

RealityView를 사용하여 표시하고 상호 작용하고자 하는 모든 엔티티를 보유하게 됩니다.

제스처 이벤트를 얻고 보기에 있는 엔티티에 연결하여 제어할 수 있습니다.

그리고 ARKit의 장면 이해에 액세스하면 RealityKit의 충돌 구성 요소를 사용하여 사람들의 주변 환경과 심지어 손으로 현실적인 시뮬레이션을 가능하게 할 수 있습니다.

RealityKit을 사용하는 것이 iOS에서 어떻게 전달되는지 살펴보기 전에, RealityKit의 Entity Component System과 함께 작동하는 방법에 대해 빠르게 재교육해 봅시다.

리얼리티 키트 엔티티 구성 요소 시스템에서, 각 엔티티는 3D 콘텐츠를 위한 컨테이너이다.

모양과 행동을 정의하기 위해 다른 구성 요소가 엔티티에 추가됩니다.

여기에는 렌더링 방법에 대한 모델 구성 요소, 다른 엔티티와 충돌할 수 있는 충돌 구성 요소, 그리고 더 많은 것들이 포함될 수 있습니다.

RealityComposer Pro를 사용하여 충돌 구성 요소와 같은 RealityKit 구성 요소를 준비하고 엔티티에 추가할 수 있습니다.

시스템에는 필요한 구성 요소가 있는 엔티티에 대해 작동하는 코드가 포함되어 있습니다.

예를 들어, 제스처 지원에 필요한 시스템은 CollisionComponent와 InputTargetComponent가 있는 엔티티에서만 작동합니다.

공간 컴퓨팅을 위해 RealityView가 사용하는 많은 개념은 iOS의 ARView에서 이월된다.

이 둘이 어떻게 쌓여 있는지 보자.

두 뷰 모두 앱에 표시하려는 엔티티를 보관하는 이벤트 인식 컨테이너입니다.

뷰에 제스처 지원을 추가하여 엔티티와의 선택과 상호 작용을 가능하게 할 수 있습니다.

공간 컴퓨팅을 위한 SwiftUI를 사용하면 엔티티를 선택하거나 드래그할 수 있습니다.

ARView와 RealityView는 모두 엔티티 컬렉션을 제공합니다.

ARView는 이를 위해 장면을 사용한다.

RealityView에는 당신의 엔티티를 추가할 콘텐츠가 있습니다.

AnchorEntities를 추가하여 콘텐츠를 현실 세계에 고정할 수 있습니다.

두 플랫폼 모두에서 콘텐츠 모델을 로드할 엔티티와 이를 배치할 AnchorEntity를 만듭니다.

플랫폼 간의 주요 차이점 중 하나는 앵커 엔티티의 행동에 있다.

iOS의 ARView는 ARSession을 사용하며 앱은 앵커 엔티티가 작동하는 데 필요한 장면 이해 알고리즘을 실행할 수 있는 권한을 받아야 합니다.

RealityView는 anchorEntities를 활성화하기 위해 시스템 서비스를 사용하고 있다.

이것은 공간적 경험이 허가 없이 주변 환경에 콘텐츠를 고정시킬 수 있다는 것을 의미합니다.

이 접근 방식을 사용하는 앱은 기본 장면 이해 데이터나 변환을 받지 않습니다.

앱이 콘텐츠를 배치하기 위해 데이터를 변환하지 않는 것은 올리버가 나중에 그의 섹션에서 이야기할 몇 가지 의미가 있습니다.

우리가 보았듯이, iOS에서 오는 많은 친숙한 개념이 있지만, RealityKit이 공간 컴퓨팅을 위해 제공하는 새로운 기능도 있습니다.

우리는 이 새로운 플랫폼에서 RealityKit으로 가능한 것의 표면만 긁었고, 더 많은 후속 조치를 위해 아래 세션을 확인하고 싶을 수도 있습니다.

이제 올리버에게, 리얼리티뷰와 iOS에서 콘텐츠를 가져오는 방법에 대해 더 이야기할 것입니다.

올리버 덩클리: 고마워, 오미드!

기존 콘텐츠를 공간 컴퓨팅으로 가져올 수 있는 다양한 방법을 계속 탐구해 봅시다.

공유 공간에서 시작합시다.

우리는 창이나 볼륨에 3D 콘텐츠를 추가하고, 시스템 제스처를 사용하여 상호 작용할 수 있습니다.

자산을 표시하려면, RealityView의 콘텐츠에 직접 추가하기만 하면 됩니다.

모델 구성 요소를 보유하고 변환 구성 요소를 설정하여 배치할 엔티티를 만들어 이를 수행하십시오.

변환 구성 요소를 수정하기 위해 제스처 지원을 설정할 수도 있습니다.

뷰의 콘텐츠에 추가된 모든 엔티티는 공간의 원점에 비해 동일한 공간에 존재하므로 서로 상호 작용할 수 있습니다.

공유 공간에서는 콘텐츠가 주변 환경에 고정될 수 없습니다.

앱을 전체 공간으로 전환하면 옵션을 고려해 봅시다.

공유 공간에서 오는 주요 차이점 중 하나는 앱이 이제 사람들의 주변 환경에 콘텐츠를 추가로 고정할 수 있다는 것이다.

여기에 콘텐츠를 고정하는 것은 두 가지 방법으로 이루어질 수 있습니다.

먼저 RealityKit의 AnchorEntity를 사용하여 앱에서 ARKit 데이터를 사용할 수 있는 권한 없이 콘텐츠를 배치하는 것을 살펴봅시다.

RealityKit의 AnchorEntities를 사용하면 시스템이 콘텐츠를 찾고 자동으로 고정할 대상을 지정할 수 있습니다.

예를 들어, 당신 앞에 있는 테이블 표면에 3D 모델을 배치하기 위해, 대상을 테이블로 설정한 RealityKit AnchorEntity를 사용할 수 있습니다.

iOS와 달리, AnchorEntities는 사용자 권한을 요청하지 않고도 사용할 수 있습니다.

사람들의 사생활은 AnchorEntity의 기본 변환을 애플리케이션과 공유하지 않음으로써 보존됩니다.

참고: 이것은 다른 앵커 엔티티의 아이들이 서로를 알지 못한다는 것을 의미합니다.

anchorEntities의 새로운, 당신은 손을 목표로 삼을 수 있으며, 이는 흥미로운 상호 작용 기회의 완전히 새로운 영역을 열어줍니다.

예를 들어, 당신은 사람의 손바닥에 콘텐츠를 고정하고 그들이 움직일 때 그들의 손을 따라가도록 할 수 있습니다.

이것은 모두 앱에 사람의 손이 실제로 어디에 있는지 알려주지 않고 시스템에 의해 이루어집니다.

AnchorEntitys는 앱이 사람들의 주변 환경에 콘텐츠를 고정할 수 있는 빠르고 개인 정보 보호 친화적인 방법을 제공합니다.

전체 공간으로 돌아와서, 우리는 또한 ARKit을 활용하여 사람들의 환경에 대한 시스템 수준의 지식을 통합할 수 있습니다.

이를 통해 자신만의 사용자 지정 배치 논리를 구축할 수 있습니다.

이것이 어떻게 작동하는지 살펴봅시다.

iOS와 마찬가지로, 당신의 애플리케이션은 장면 이해 데이터에 대한 앵커 업데이트를 받습니다.

이 앵커 데이터를 앱 로직에 통합하여 모든 종류의 놀라운 경험을 얻을 수 있습니다.

예를 들어, 비행기의 경계를 사용하여 콘텐츠를 중심에 두고 배포할 수 있습니다.

또는, 비행기와 그 분류를 사용하여 두 벽과 바닥의 교차점을 찾아 방의 모서리를 찾을 수 있습니다.

콘텐츠를 어디에 배치할지 결정하면, ARKit이 추적하고 엔티티의 변환 구성 요소를 업데이트할 수 있는 세계 앵커를 추가합니다.

이것은 기본 세계 지도가 업데이트됨에 따라 당신의 콘텐츠가 현실 세계에 고정되어 있을 뿐만 아니라, 우리가 곧 탐구할 끈기를 고정할 수 있는 문을 열어줍니다.

당신의 공간에 추가된 모든 엔티티는 주변 환경뿐만 아니라 서로 상호 작용할 수 있습니다.

이 모든 것은 장면 이해 앵커가 공간의 기원에 대한 변환으로 전달되기 때문에 작동한다.

ARKit 기능을 사용하려면 사용자 권한이 필요합니다.

ARKit 데이터를 앱 로직에 통합하는 것이 어떻게 고급 기능을 활성화할 수 있는지 방금 보았습니다.

지금까지 우리는 당신의 앱이 콘텐츠를 배치하도록 하는 것에 대해 이야기했습니다.

사람들이 배치를 안내할 수 있는 방법을 살펴봅시다.

iOS에서는 레이캐스팅을 사용하여 2D 입력을 3D 위치로 변환할 수 있습니다.

하지만 이 새로운 플랫폼으로, 우리는 손을 사용하여 자연스럽게 경험과 직접 상호 작용할 수 있기 때문에 더 이상 이 2D-3D 브리지가 필요하지 않습니다.

레이캐스팅은 여전히 강력하다; 그것은 사람들이 팔 길이를 넘어 손을 뻗을 수 있게 해준다.

레이캐스팅을 설정하는 다양한 방법이 있다.

기본적으로, 레이캐스트를 위해 RealityKit의 충돌 구성 요소를 설정해야 합니다.

충돌 구성 요소는 또한 ARKit의 메쉬 앵커에서 사람들의 환경에 대한 레이캐스트로 만들 수 있습니다.

공간 컴퓨팅을 위해 레이캐스트하는 방법에 대한 두 가지 예를 살펴봅시다: 첫 번째는 시스템 제스처를 사용하고, 두 번째는 손 데이터를 사용합니다.

위치를 얻은 후, 우리는 콘텐츠를 고정하기 위해 ARKit worldAnchor를 배치할 수 있습니다.

다음 예시를 고려해 봅시다.

우리의 앱이 모델러를 위한 영감을 주는 3D 자산을 배치하는 것을 중심으로 돌아가고 있다고 상상해 보세요.

아마도 이 특정 시나리오에서, 어떤 사람은 우리의 앱을 사용하여 모델링 프로젝트를 위해 작업대에 가상 선박을 배치하고 싶어할 것이다.

여기 우리가 배를 배치하고 싶은 작업대가 있습니다.

우리는 빈 RealityView로 시작할 것이다.

ARKit의 장면 이해는 주변 환경을 표현하는 데 사용할 메쉬 앵커를 제공합니다.

그것들은 우리가 사용할 수 있는 기하학과 의미론적 정보를 제공한다.

장면 재구성 데이터의 메쉬는 일련의 덩어리로 전달된다는 것을 기억하세요.

우리는 이 메쉬 청크를 나타내는 엔티티를 만들 것이며, 메쉬 앵커의 변환을 사용하여 이 엔티티를 전체 공간에 올바르게 배치할 것입니다.

그러면 우리 단체는 테스트를 하기 위해 충돌 구성 요소가 필요하다.

우리는 RealityKit의 ShapeResources 방법을 사용하여 우리 엔티티의 meshAnchor에서 충돌 모양을 생성할 것입니다.

그런 다음 히트 테스트를 지원하는 올바르게 배치된 엔티티를 추가할 것입니다.

우리는 모든 주변을 나타내기 위해 우리가 받는 각 메쉬 청크에 대한 엔티티와 충돌 구성 요소를 구축할 것입니다.

장면 재구성이 개선됨에 따라, 우리는 메쉬를 업데이트하거나 덩어리를 제거할 수 있습니다.

우리는 또한 이러한 변화에 대해 우리의 단체를 업데이트할 준비가 되어 있어야 한다.

우리는 이제 주변을 대표하는 단체들의 컬렉션을 가지고 있다.

이 모든 엔티티는 충돌 구성 요소를 가지고 있으며 레이캐스트 테스트를 지원할 수 있습니다.

먼저 시스템 제스처를 사용하여 레이캐스팅을 살펴본 다음, 손 데이터를 사용하여 예제를 계속해 봅시다.

우리는 레이캐스트를 하고 시스템 제스처를 사용하여 배를 배치할 위치를 얻을 수 있다.

제스처는 충돌과 InputTarget 구성 요소가 모두 있는 엔티티와만 상호 작용할 수 있으므로, 각 메쉬 엔티티에 하나씩 추가합니다.

RealityView에 SpatialTapGesture를 추가함으로써, 사람들은 엔티티를 보고 탭하여 레이캐스트할 수 있습니다.

이 결과 이벤트는 사람들이 두드릴 때 보았던 장소를 나타내는 세계 공간에서의 위치를 차지하고 있다.

시스템 제스처를 사용하는 대신, 우리는 ARKit의 핸드 앵커를 사용하여 광선을 만들 수도 있었다.

한 걸음 물러서서 이 옵션을 살펴보자.

사람들이 어디를 가리키는지 알기 위해, 우리는 먼저 그 사람의 손을 표현해야 한다.

ARkit의 새로운 핸드 앵커는 우리에게 필요한 모든 것을 제공한다.

우리는 쿼리를 위해 광선의 기원과 방향을 구축하기 위해 손가락 관절 정보를 사용할 수 있습니다.

이제 우리는 광선의 기원과 방향을 가지고 있으므로, 우리 장면의 엔티티에 대한 레이캐스트를 할 수 있다.

그 결과 CollisionCastHit은 위치와 표면 정상과 함께 맞은 엔티티를 제공한다.

우리가 콘텐츠를 배치하기 위해 세계에서 위치를 식별하면, 우리는 ARKit이 우리를 위해 이 위치를 지속적으로 추적할 수 있도록 세계 앵커를 추가할 것입니다.

ARKit은 세계 지도가 개선됨에 따라 이 세계 앵커의 변형을 업데이트할 것이다.

우리는 선박의 모델을 로드하기 위해 새로운 엔티티를 만들고, 세계 앵커 업데이트를 사용하여 변환을 설정하여 사용자가 원하는 곳에 배치할 수 있습니다.

마지막으로, 우리는 작업대를 통해 렌더링하기 위해 엔티티를 콘텐츠에 추가할 수 있습니다.

ARKit이 우리가 추가한 세계 앵커를 업데이트할 때마다, 우리는 선박 엔티티의 변환 구성 요소를 업데이트하여 현실 세계에 고정되도록 합니다.

그리고 그게 다야!

우리는 손을 사용하여 우리 주변의 위치를 가리키고 거기에 콘텐츠를 배치했다.

레이캐스팅은 콘텐츠를 배치할 뿐만 아니라 콘텐츠와 상호 작용하는 데에도 유용하다.

우리의 가상 선박에 대한 레이캐스트에 무엇이 필요한지 봅시다.

RealityKit 충돌 구성 요소는 매우 강력하다.

우리는 Reality Composer Pro가 우리를 도울 수 있는 적절한 충돌 구성 요소를 추가하여 선박 엔티티가 충돌에 참여하도록 할 수 있습니다.

선박의 충돌 구성 요소를 활성화하고 최신 핸드 조인트 위치에서 새로운 광선을 구축한 후, 우리는 또 다른 레이캐스트를 하고 사용자가 선박이나 테이블을 가리키고 있는지 알 수 있습니다.

이전 예는 RealityKit의 기능과 ARKit의 장면 이해를 결합하여 진정으로 매력적인 경험을 구축하는 힘과 다재다능함을 보여주었습니다.

공간 컴퓨팅을 위해 ARkit 사용이 어떻게 바뀌었는지 봅시다.

기본적으로, iOS와 마찬가지로, ARKit은 여전히 앵커 업데이트를 받기 위해 세션을 실행하여 작동합니다.

세션을 구성하고 실행하고, 앵커 업데이트를 받고, 세계 앵커를 유지하는 방법이 이 새로운 플랫폼에서 변경되었습니다.

한 번 보자!

iOS에서 ARKit은 선택할 수 있는 다양한 구성을 제공합니다.

각 구성은 당신의 경험에 필요한 기능을 번들로 제공합니다.

예를 들어, 우리는 ARWorldTrackingConfiguration을 선택했고, 메쉬에 대한 sceneReconstruction과 비행기에 대한 planeDetection을 활성화할 것이다.

그런 다음 ARSession을 만들고 선택한 구성으로 실행할 수 있습니다.

이 새로운 플랫폼에서 ARKit은 이제 각 장면 이해 기능에 대한 데이터 공급자를 노출합니다.

핸드 트래킹은 ARKit이 제공하는 새로운 기능이며 자체 공급자도 제공합니다.

각 데이터 제공자의 이니셜라이저는 해당 제공자 인스턴스를 구성하는 데 필요한 매개 변수를 사용합니다.

이제 사전 설정 구성 카탈로그에서 선택하는 대신, 응용 프로그램에 필요한 공급자를 단품으로 선택할 수 있습니다.

예를 들어, 우리는 메쉬 앵커를 받을 SceneReconstructionProvider와 비행기 앵커를 받을 PlaneDetectionProvider를 선택합니다.

우리는 공급자를 만들고 우리가 받고자 하는 메쉬 분류와 평면 유형을 초기화합니다.

그런 다음 ARKitSession을 만들고 인스턴스화된 공급자와 함께 실행합니다.

이제 세션 구성이 어떻게 단순화되었는지 보았으니, 이 새로운 데이터 공급자가 앱이 실제로 ARKit 데이터를 수신하는 방식을 어떤 방식으로 변경하는지 이해합시다.

iOS에서, 단일 대리인은 앵커와 프레임 업데이트를 받는다.

앵커는 카메라 프레임과 앵커를 동기화하기 위해 ARFrames와 함께 집계되고 전달됩니다.

애플리케이션은 카메라 픽셀 버퍼를 표시하고 카메라 변환을 사용하여 추적된 가상 콘텐츠를 등록하고 렌더링할 책임이 있습니다.

메쉬와 평면 앵커는 기본 앵커로 전달되며, 어느 것이 무엇인지 알아내기 위해 그것들을 명확하게 하는 것은 당신에게 달려 있습니다.

우리의 새로운 플랫폼에서, 앵커 업데이트를 제공하는 것은 데이터 제공자이다.

여기 우리가 이전에 구성한 공급자가 있습니다.

ARKitSession을 실행하면, 각 공급자는 즉시 앵커 업데이트를 비동기적으로 게시하기 시작합니다.

SceneReconstructionProvider는 meshAnchors를 제공하고, planeDetectionProvider는 우리에게 PlaneAnchors를 제공한다.

명확성은 필요 없어!

앵커 업데이트는 사용 가능한 즉시 제공되며, 다른 데이터 제공 업체의 업데이트와 분리됩니다.

ARFrame은 더 이상 제공되지 않는다는 점에 유의하는 것이 중요합니다.

공간 컴퓨팅 애플리케이션은 이제 시스템에 의해 자동으로 수행되기 때문에 콘텐츠를 표시하기 위해 프레임이나 카메라 데이터가 필요하지 않습니다.

ARFrame으로 앵커 업데이트를 패키징할 필요 없이, ARKit은 이제 즉시 제공할 수 있으며, 대기 시간을 줄여 애플리케이션이 사람 주변의 업데이트에 빠르게 반응할 수 있습니다.

다음으로, 세계 앵커의 지속성에 대해 이야기해 봅시다.

넌 이 변화를 좋아할 거야!

레이캐스팅 예시에서, 우리는 세계 앵커를 사용하여 가상 콘텐츠를 실제 위치에 배치하고 고정했습니다.

앱은 이러한 앵커를 유지할 수 있으며, 장치가 같은 환경으로 돌아올 때 자동으로 다시 받을 수 있습니다.

먼저 끈기가 iOS에서 어떻게 작동하는지 빠르게 요약해 봅시다.

iOS에서, 세계 지도와 앵커 지속성을 처리하는 것은 애플리케이션의 책임이다.

여기에는 추가된 앵커로 ARKit의 세계 지도를 요청하고 저장하고, 적시에 올바른 세계 지도를 다시 로드하는 논리를 추가한 다음, 이전에 지속된 앵커를 받고 응용 프로그램 경험을 계속하기 전에 재로컬화가 완료되기를 기다리는 것이 포함되었습니다.

이 새로운 플랫폼에서, 시스템은 지속적으로 배경에서 세계 지도를 유지하며, 사람들이 이동함에 따라 기존 지도로 원활하게 로딩, 언로딩, 생성 및 재배치합니다.

당신의 애플리케이션은 더 이상 지도를 처리할 필요가 없습니다, 시스템은 이제 당신을 위해 그것을 합니다!

당신은 단순히 가상 콘텐츠의 위치를 유지하기 위해 세계 앵커를 사용하는 데 집중합니다.

콘텐츠를 배치할 때, 새로운 WorldTrackingProvider를 사용하여 WorldAnchors를 세계 지도에 추가할 것입니다.

시스템은 당신을 위해 이것들을 자동으로 저장할 것입니다.

WorldTrackingProvider는 이러한 세계 앵커의 추적 상태와 변환을 업데이트할 것이다.

WorldAnchor 식별자를 사용하여 해당 가상 콘텐츠를 로드하거나 언로드할 수 있습니다.

우리는 방금 당신이 iOS에서 알고 있던 ARKit 원칙에 대한 몇 가지 업데이트를 강조했지만, 탐구해야 할 것이 훨씬 더 많습니다!

코드 예제와 함께 더 자세히 알아보려면, "공간 컴퓨팅을 위한 ARKit을 만나세요"를 보는 것이 좋습니다.

이 세션을 끝내자!

이 세션에서, 우리는 ARKit과 RealityKit 개념이 iOS에서 어떻게 진화했는지, 고려해야 할 변경 사항, 그리고 자세한 내용을 보려면 어떤 세션을 봐야 하는지에 대한 높은 수준의 이해를 제공했습니다.

이 플랫폼은 iOS 앱이 처리해야 하는 많은 작업을 수행하므로, 이미 익숙한 프레임워크와 개념을 사용하여 아름다운 콘텐츠와 경험을 만드는 데 집중할 수 있습니다.

우리는 당신이 공간 컴퓨팅과 모든 놀라운 기능을 활용하여 앱을 발전시키는 방법을 보게 되어 기쁩니다!

봐줘서 고마워!

♪