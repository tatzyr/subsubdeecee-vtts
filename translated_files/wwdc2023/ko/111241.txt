111241

♪ ♪

Andrew: 안녕하세요, 저는 Vision 프레임워크의 소프트웨어 엔지니어인 Andrew Rauh입니다.

오늘 저는 인체 포즈, 시력 프레임워크의 깊이 사용, 그리고 인스턴스 마스크로 이미지에서 사람들을 들어올리는 것에 대해 이야기할 것입니다.

사람들을 감지하고 이해하는 것은 항상 비전의 초점이었고, 몇 년 동안 비전 프레임워크는 2D로 인체 포즈를 제공해 왔다.

재교육으로, 2D의 인체 포즈는 입력 이미지에 해당하는 골격에 정의된 랜드마크 포인트의 정규화된 픽셀 좌표로 관찰을 반환합니다.

더 자세히 알아보고 싶다면, 아직 하지 않았다면 "신체와 손 포즈 감지" 세션을 검토하세요.

비전은 VNDetectHumanBodyPose3DRequest라는 새로운 요청으로 환경에서 사람들을 3D로 포착하기 위한 지원을 확대하고 있다.

이 요청은 17개의 관절이 있는 3D 골격을 반환하는 관찰을 생성합니다.

조인트는 조인트 이름이나 조인트 그룹 이름을 제공함으로써 컬렉션으로 접근할 수 있다.

왼쪽 하단 원점으로 정규화된 비전에 의해 반환된 다른 인식 지점과 달리, 3D 관절의 위치는 뿌리 관절에 기원이 있는 현실 세계에서 캡처된 장면에 비해 미터 단위로 반환됩니다.

이 초기 개정판은 프레임에서 감지된 가장 눈에 띄는 사람을 위해 하나의 골격을 반환합니다.

피트니스 앱을 만들고 체육관에서 운동 수업의 이미지에 대한 요청을 실행했다면, 관찰은 카메라에 가장 가까운 앞에 있는 여성과 일치할 것입니다.

문맥으로 3D 골격의 구조를 더 잘 보여주기 위해, 이 요가 포즈를 분해해 봅시다.

놀랍지 않게도, 3D 인체 골격은 머리의 중앙과 상단의 지점을 포함하는 머리 그룹으로 시작한다.

다음으로 왼쪽과 오른쪽 어깨 관절, 척추, 엉덩이 중앙에 있는 뿌리 관절, 그리고 고관절을 포함하는 몸통 그룹이 있습니다.

일부 관절은 여러 그룹으로 반환된다는 것을 명심하세요.

팔의 경우, 왼쪽과 오른쪽 팔 그룹이 있으며, 각각 손목, 어깨, 팔꿈치가 있다.

왼쪽과 오른쪽은 이미지의 왼쪽이나 오른쪽이 아니라 항상 사람에 상대적이다.

마지막으로, 우리의 골격에는 왼쪽과 오른쪽 다리 그룹이 있으며, 각각 상응하는 엉덩이, 무릎, 발목 관절이 있다.

이 새로운 요청을 사용하려면 다른 요청과 동일한 워크플로우를 따르므로, 이전에 코드에서 Vision을 사용한 적이 있다면 이 흐름이 친숙해야 합니다.

새로운 DetectHumanBodyPose3DRequest의 인스턴스를 만든 다음, 탐지를 실행하려는 자산으로 이미지 요청 핸들러를 초기화합니다.

요청을 실행하려면, 요청 인스턴스를 수행으로 전달하세요.

그리고 요청이 성공하면, VNHumanBodyPose3DObservation이 오류 없이 반환됩니다.

모든 사진은 3D 세계에 있는 사람들의 2D 표현이다.

Vision을 사용하면 이제 ARKit 또는 ARSession 없이 이미지에서 3D 위치를 검색할 수 있습니다.

이것은 3D 공간에서 주제를 이해하기 위한 강력하고 가벼운 옵션이며 앱에서 완전히 새로운 범위의 기능을 잠금 해제합니다.

나는 이것을 이해하고 시각화하는 데 도움이 되는 샘플 앱을 만들었다.

내가 그것을 열면, 내 사진 라이브러리에서 어떤 이미지든 선택할 수 있다.

내 동료들과 나는 이전에 요가 강사의 침착함에 영감을 받았기 때문에, 우리는 휴식을 취하고, 밖으로 나갔고, 스스로 몇 가지 포즈를 취했다.

이제, 나는 그 선생님만큼 유연하지는 않지만, 이 포즈로 꽤 잘했고, 3D로 멋지게 보일 것이다.

요청을 실행하고 나를 3차원으로 다시 데려오자.

요청이 성공했고, 3D 골격은 내가 입력 이미지에 있는 곳과 정렬되어 있다.

내가 그 장면을 회전시키면, 내 팔은 뻗어 있고 다리는 내가 서 있는 방식에 따라 엉덩이에 비해 정확해 보인다.

이 피라미드 모양은 이미지가 캡처되었을 때 카메라가 어디에 있었는지를 나타낸다.

전환 원근법 버튼을 탭하면, 이제 카메라의 위치에서 볼 수 있습니다.

앱에서 3D 인체 포즈를 사용하여 멋진 경험을 만들기 위해 알아야 할 코드와 개념을 안내해 드리겠습니다.

앱 구축은 관찰에서 반환된 포인트를 사용하는 것으로 시작됩니다.

그것들을 검색할 수 있는 두 가지 주요 API가 있습니다. 특정 조인트의 위치에 액세스하는 recognizedPoint 또는 지정된 그룹 이름으로 조인트 컬렉션에 액세스하는 recognizedPoints입니다.

이러한 핵심 방법 외에도, 관찰은 몇 가지 추가적인 유용한 정보를 제공한다.

첫째, 신체 높이는 피사체의 예상 높이를 미터 단위로 제공합니다.

사용 가능한 깊이 메타데이터에 따라, 이 높이는 더 정확하게 측정된 높이 또는 1.8미터의 기준 높이가 될 것이다.

나는 1분 안에 깊이와 비전에 대해 할 말이 많다.

heightEstimation 속성으로 높이를 계산하는 데 사용되는 기술을 결정할 수 있습니다.

다음으로, 카메라 위치는 cameraOriginMatrix를 통해 사용할 수 있습니다.

실생활에서 카메라가 당신의 피사체를 정확히 마주하고 있지 않을 수 있기 때문에, 이것은 프레임이 캡처되었을 때 카메라가 그 사람과 상대적인 위치를 이해하는 데 유용합니다.

그 관찰은 또한 공동 좌표를 2D로 다시 투사하기 위한 API를 제공한다.

이것은 반환된 점을 입력 이미지와 오버레이하거나 정렬하려는 경우에 유용합니다.

그리고 마지막으로, 사람이 두 개의 유사한 이미지를 가로질러 어떻게 이동했는지 이해하기 위해, 카메라에 대한 주어진 관절의 위치를 얻기 위해 API를 사용할 수 있다.

3D 인체 포인트를 사용하는 방법을 보여주기 전에, 비전에서 물려받은 새로운 기하학 클래스를 소개하고 싶습니다.

VNPoint3D는 3D 위치를 저장하기 위한 simd_float 4x4 행렬을 정의하는 기본 클래스입니다.

이 표현은 ARKit과 같은 다른 Apple 프레임워크와 일치하며 사용 가능한 모든 회전 및 번역 정보를 포함합니다.

다음으로, 이 위치를 상속하지만 식별자를 추가하는 VNRecognizedPoint3D가 있습니다.

이것은 공동 이름과 같은 해당 정보를 저장하는 데 사용됩니다.

마지막으로, 오늘의 초점은 지역 위치와 부모 관절을 추가하는 VNHumanBodyRecognizedPoint3D이다.

그 지점의 속성으로 작업하는 방법에 대해 좀 더 자세히 알아보자.

recognizedPoint API를 사용하여, 나는 왼쪽 손목의 위치를 검색했다.

관절의 모델 위치 또는 점의 위치 속성은 항상 엉덩이 중앙에 있는 골격의 뿌리 관절에 상대적이다.

우리가 위치 매트릭스의 세 번째 열에 초점을 맞추면, 번역을 위한 값이 있다.

왼쪽 손목의 y 값은 이 그림의 엉덩이보다 0.9미터 높으며, 이 포즈에 맞는 것 같다.

다음으로, 반환된 포인트의 localPosition 속성이 있는데, 이는 부모 관절에 상대적인 위치이다.

그래서 이 경우, 왼쪽 팔꿈치는 왼쪽 손목의 부모 관절이 될 것이다.

여기 마지막 열은 x축에 대해 -0.1미터의 값을 보여주며, 이는 또한 맞는 것 같다.

음수 또는 양수 값은 기준점에 의해 결정되며, 이 자세에서 손목은 팔꿈치의 왼쪽에 있다.

localPosition은 앱이 신체의 한 영역에서만 작동하는 경우 유용합니다.

그것은 또한 아이와 부모 관절 사이의 각도를 결정하는 것을 단순화한다.

몇 초 안에 코드에서 이 각도를 계산하는 방법을 보여줄게.

반환된 3D 포인트로 작업할 때, 앱을 빌드할 때 도움이 될 수 있는 몇 가지 개념이 있습니다.

먼저, 당신은 종종 아이와 부모 관절 사이의 각도를 결정해야 합니다.

LocalAngleToParent를 계산하는 방법에서, 부모 관절에 상대적인 위치는 그 각도를 찾는 데 사용된다.

노드의 회전은 x, y 및 z 축 또는 피치, 요 및 롤에 대한 회전으로 구성됩니다.

피치의 경우, 90도 회전은 SceneKit 노드 지오메트리를 기본 방향에서 스켈레톤에 더 적합한 방향으로 배치하는 데 사용됩니다.

요의 경우, 우리는 적절한 각도를 얻기 위해 벡터 길이로 나눈 z 좌표의 호 코사인을 사용합니다.

그리고 롤의 경우, 각도 측정은 y와 x 좌표의 아크 탄젠트로 얻어진다.

다음으로, 앱은 내 샘플 앱과 같이 반환된 3D 위치를 원본 이미지와 연관시켜야 할 수도 있습니다.

시각화에서, 나는 이미지 평면, 스케일과 번역에 대한 두 가지 변환을 위해 point-in-image API를 사용한다.

먼저 이미지 평면을 반환점에 비례하여 조정해야 합니다.

나는 3D와 2D 모두에 대해 중앙 어깨와 척추와 같은 두 개의 알려진 관절 사이의 거리를 가져오고, 비례적으로 연관시키고, 내 이미지 평면을 이 양만큼 조정한다.

번역 구성 요소의 경우, pointInImage API를 사용하여 2D 이미지에서 루트 조인트의 위치를 가져옵니다.

이 방법은 그 위치를 사용하여 x축과 y축의 이미지 평면의 이동을 결정하는 동시에 VNPoint 좌표의 왼쪽 하단 원점과 이미지 중앙의 렌더링 환경 원점 사이를 변환합니다.

마지막으로, 카메라의 관점에서 장면을 보거나 그 위치에서 지점을 렌더링할 수 있으며, cameraOriginMatrix에서 이것을 검색할 수 있습니다.

올바른 방향은 렌더링 환경에 따라 다르지만, 이것이 이 노드의 로컬 좌표계를 나머지 장면과 연관시키는 피벗 변환을 사용하여 이 변환 정보로 노드를 배치한 방법입니다.

나는 또한 cameraOriginMatrix의 회전 정보를 사용하여 역변환을 사용하여 이 코드로 카메라에 직면하도록 이미지 평면을 올바르게 회전시켰다.

여기서는 회전 정보만 필요하기 때문에, 마지막 열의 번역 정보는 무시됩니다.

이 모든 조각들을 합치면 내 샘플 앱에 표시된 장면이 허용되었다.

이제, 저는 몇 분 동안 Depth in Vision과 관련된 몇 가지 흥미로운 추가 사항에 대해 논의하고 싶습니다.

비전 프레임워크는 이제 이미지 또는 프레임 버퍼와 함께 깊이를 입력으로 받아들인다.

VNImageRequestHandler는 AVDepthData의 새로운 매개 변수를 취하는 cvPixelBuffer 및 cmSampleBuffer용 이니셜라이저 API를 추가했습니다.

또한, 파일에 이미 깊이 데이터가 포함되어 있다면, 수정 없이 기존 API를 사용할 수 있습니다.

비전은 당신을 위해 파일에서 깊이를 자동으로 가져올 것입니다.

Apple SDK에서 Depth로 작업할 때, AVDepthData는 모든 Depth 메타데이터와 인터페이스하기 위한 컨테이너 클래스 역할을 합니다.

카메라 센서에 의해 캡처된 깊이 메타데이터에는 불균형 또는 깊이 형식으로 표시된 깊이 지도가 포함되어 있습니다.

이러한 형식은 상호 교환 가능하며 AVFoundation을 사용하여 서로 변환할 수 있습니다.

깊이 메타데이터에는 3D 장면을 재구성하는 데 필요한 본질, 외부 및 렌즈 왜곡과 같은 카메라 보정 데이터도 포함되어 있습니다.

더 자세히 알아야 한다면, 2022년부터 "iOS 카메라 캡처의 발전 발견" 세션을 검토하십시오.

깊이는 카메라 캡처 세션이나 이전에 캡처한 파일을 통해 얻을 수 있습니다.

사진의 초상화 이미지와 같이 카메라 앱으로 캡처한 이미지는 항상 카메라 보정 메타데이터와 함께 격차 맵으로 깊이를 저장합니다.

라이브 캡처 세션에서 깊이를 캡처할 때, 장치가 지원하는 경우 LiDAR를 사용할 세션을 지정하는 추가 이점이 있습니다.

LiDAR는 장면의 정확한 스케일과 측정을 허용하기 때문에 강력하다.

비전은 또한 이미지에서 한 명 이상의 사람과 상호 작용하기 위해 API를 도입하고 있다.

비전은 현재 GeneratePersonSegmentation 요청으로 주변 장면에서 사람들을 분리할 수 있는 기능을 제공합니다.

이 요청은 프레임에 있는 모든 사람들을 포함하는 단일 마스크를 반환합니다.

비전은 이제 새로운 사람 인스턴스 마스크 요청으로 좀 더 선택적일 수 있게 해준다.

이 새로운 API는 각각 신뢰 점수가 있는 최대 4개의 개인 마스크를 출력합니다.

그래서 이제 당신은 이미지와 별도로 친구를 선택하고 들어 올릴 수 있습니다.

사람 이외의 피사체를 선택하고 들어 올려야 하는 경우, VisionKit의 피사체 리프팅 API 또는 Vision 프레임워크의 전경 인스턴스 마스크 요청을 사용할 수 있습니다.

자세한 내용은 "앱의 이미지에서 주제 들어 올리기" 세션을 확인하세요.

다음은 이미지에서 원하는 사람의 특정 인스턴스를 선택하는 방법을 보여주는 샘플 코드입니다.

현재 모든 인스턴스를 반환하도록 지정하고 있지만, 이미지에서 집중하고 싶은 친구에 따라 인스턴스 1 또는 2를 선택하거나 인스턴스 0을 사용하여 배경을 얻을 수 있습니다.

이 새로운 요청은 최대 4명까지 분할되므로, 이미지에 4명 이상이 있는 경우, 코드에서 처리해야 할 몇 가지 추가 조건이 있습니다.

장면에 많은 사람들이 포함될 때, 반환된 관찰은 사람들을 놓치거나 결합할 수 있다.

일반적으로, 이것은 배경에 있는 사람들과 함께 발생한다.

앱이 붐비는 장면을 처리해야 한다면, 가능한 최고의 경험을 구축하기 위해 사용할 수 있는 전략이 있습니다.

비전의 얼굴 감지 API는 이미지의 얼굴 수를 계산하는 데 사용할 수 있으며, 네 명 이상의 이미지를 건너뛰거나 기존 사람 세분화 요청을 사용하여 모든 사람을 위해 하나의 마스크로 작업하도록 선택할 수 있습니다.

요약하자면, 비전은 이제 깊이, 3D 인체 포즈 및 사람 인스턴스 마스크를 지원하여 사람과 환경을 이해하는 강력한 새로운 방법을 제공합니다.

하지만 그것이 비전이 올해 출시하는 전부는 아니다.

"비전에서 동물 포즈 감지" 세션에서 사람들을 넘어 털복숭이 친구들의 이미지로 놀라운 경험을 할 수 있습니다.

고마워, 그리고 네가 어떤 놀라운 기능을 만드는지 빨리 보고 싶어.

♪ ♪