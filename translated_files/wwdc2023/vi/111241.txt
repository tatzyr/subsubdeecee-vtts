111241

♪ ♪

Andrew: Xin chào, tôi là Andrew Rauh, một kỹ sư phần mềm về khung Vision.

Hôm nay tôi sẽ nói về Tư thế cơ thể con người, sử dụng khung Depth in Vision và nâng mọi người lên khỏi hình ảnh bằng mặt nạ ví dụ.

Phát hiện và hiểu con người luôn là trọng tâm của Tầm nhìn, và trong một vài năm, khung Tầm nhìn đã cung cấp Tư thế Cơ thể Con người ở dạng 2D.

Để bồi dưỡng, Tư thế cơ thể con người trong 2D trả về một quan sát với tọa độ điểm ảnh chuẩn hóa của các điểm mốc được xác định trên bộ xương tương ứng với hình ảnh đầu vào.

Nếu bạn muốn đi sâu vào chi tiết cụ thể hơn, vui lòng xem lại phiên "Khử kiến tư thế cơ thể và tay" nếu bạn chưa có.

Vision đang mở rộng hỗ trợ chụp mọi người trong môi trường của họ thành 3D với một yêu cầu mới có tên VNDetectHumanBodyPose3DRequest.

Yêu cầu này tạo ra một quan sát trả về một bộ xương 3D với 17 khớp.

Các khớp có thể được truy cập bằng tên chung hoặc dưới dạng một bộ sưu tập bằng cách cung cấp tên nhóm chung.

Không giống như các điểm được công nhận khác được trả về bởi Vision được chuẩn hóa thành gốc thấp hơn bên trái, vị trí của các khớp 3D được trả về bằng mét so với cảnh được chụp trong thế giới thực với nguồn gốc tại khớp gốc.

Bản sửa đổi ban đầu này trả về một bộ xương cho người nổi bật nhất được phát hiện trong khung.

Nếu bạn đang xây dựng một ứng dụng thể dục và chạy yêu cầu trên hình ảnh này của một lớp tập luyện trong phòng tập thể dục, quan sát sẽ tương ứng với người phụ nữ ở phía trước gần máy ảnh nhất.

Để chứng minh tốt hơn cấu trúc của bộ xương 3D với một số bối cảnh, hãy chia nhỏ tư thế yoga này.

Không có gì đáng ngạc nhiên, bộ xương 3D Human Body bắt đầu với một nhóm đầu chứa các điểm ở trung tâm và đỉnh đầu.

Tiếp theo là nhóm thân, chứa khớp vai trái và phải, cột sống, khớp rễ, nằm ở trung tâm của hông và khớp hông.

Hãy nhớ rằng một số khớp được trả về theo nhiều nhóm.

Đối với cánh tay, có các nhóm cánh tay trái và phải, mỗi nhóm có cổ tay, vai và khuỷu tay.

Trái và phải luôn liên quan đến con người, không phải bên trái hoặc bên phải của hình ảnh.

Cuối cùng, bộ xương của chúng tôi chứa một nhóm chân trái và chân phải, mỗi nhóm có khớp hông, đầu gối và mắt cá chân tương ứng.

Để sử dụng yêu cầu mới này, bạn tuân theo quy trình làm việc giống như các yêu cầu khác, vì vậy quy trình này sẽ quen thuộc với bạn nếu bạn đã sử dụng Vision trong mã của mình trước đây.

Bạn sẽ bắt đầu bằng cách tạo một phiên bản của DetectHumanBodyPose3DRequest mới, sau đó khởi tạo trình xử lý yêu cầu hình ảnh với nội dung bạn muốn chạy tính năng phát hiện của mình.

Để chạy yêu cầu của bạn, hãy chuyển phiên bản yêu cầu của bạn để thực hiện.

Và nếu yêu cầu thành công, VNHumanBodyPose3DObservation sẽ được trả về mà không có lỗi.

Tất cả các bức ảnh đều là đại diện 2D của con người trong thế giới 3D.

Vision hiện cho phép bạn lấy vị trí 3D đó từ hình ảnh mà không cần ARKit hoặc ARSession.

Đây là một lựa chọn mạnh mẽ, nhẹ nhàng để hiểu một chủ đề trong không gian 3D và mở ra một loạt các tính năng hoàn toàn mới trong ứng dụng.

Tôi đã xây dựng một ứng dụng mẫu để giúp hiểu và hình dung điều này.

Khi tôi mở nó ra, tôi có thể chọn bất kỳ hình ảnh nào từ thư viện ảnh của mình.

Đồng nghiệp của tôi và tôi đã được truyền cảm hứng từ sự bình tĩnh của người hướng dẫn yoga trước đó, vì vậy chúng tôi đã nghỉ ngơi, đi ra ngoài và tự mình thử một vài tư thế.

Bây giờ, tôi không linh hoạt như giáo viên đó, nhưng tôi đã làm khá tốt với tư thế này, và nó sẽ trông tuyệt vời ở dạng 3D.

Hãy chạy yêu cầu và đưa tôi trở lại chiều không gian thứ ba.

Yêu cầu đã thành công và một bộ xương 3D được căn chỉnh với vị trí của tôi trong hình ảnh đầu vào.

Nếu tôi xoay cảnh, cánh tay của tôi mở rộng ra và chân trông chính xác so với hông dựa trên cách tôi đứng.

Hình dạng kim tự tháp này đại diện cho vị trí của máy ảnh khi hình ảnh được chụp.

Nếu tôi nhấn vào nút Chuyển đổi phối cảnh, chế độ xem bây giờ là từ vị trí của máy ảnh.

Tôi sẽ hướng dẫn bạn thông qua mã và khái niệm bạn cần biết để tạo ra những trải nghiệm tuyệt vời bằng cách sử dụng 3D Human Body Pose trong ứng dụng của bạn.

Xây dựng một ứng dụng bắt đầu bằng việc sử dụng các điểm được trả về trong quan sát.

Có hai API chính để truy xuất chúng, recognizedPoint để truy cập vị trí của một khớp cụ thể hoặc recognizedPoints để truy cập một tập hợp các khớp với tên nhóm được chỉ định.

Bên cạnh những phương pháp cốt lõi này, quan sát cung cấp một số thông tin hữu ích bổ sung.

Đầu tiên, bodyHeight cho chiều cao ước tính của đối tượng của bạn tính bằng mét.

Tùy thuộc vào siêu dữ liệu độ sâu có sẵn, chiều cao này sẽ là chiều cao đo chính xác hơn hoặc chiều cao tham chiếu là 1,8 mét.

Tôi có nhiều điều để nói về Độ sâu và Tầm nhìn trong một phút.

Bạn có thể xác định kỹ thuật được sử dụng để tính chiều cao với thuộc tính heightEstimation.

Tiếp theo, vị trí máy ảnh có sẵn thông qua cameraOriginMatrix.

Vì trong cuộc sống thực, máy ảnh có thể không đối diện chính xác với đối tượng của bạn, điều này rất hữu ích để hiểu được vị trí của máy ảnh so với người đó khi chụp khung hình.

Quan sát cũng cung cấp một API để chiếu tọa độ chung trở lại 2D.

Điều này rất hữu ích nếu bạn muốn phủ hoặc căn chỉnh các điểm trả về với hình ảnh đầu vào.

Và cuối cùng, để hiểu cách một người di chuyển qua hai hình ảnh tương tự nhau, một API có sẵn để có được vị trí của một khớp nhất định so với máy ảnh.

Trước khi tôi chỉ ra cách sử dụng các điểm 3D Human Body, tôi muốn giới thiệu các lớp hình học mới trong Vision mà nó kế thừa.

VNPoint3D là lớp cơ sở xác định ma trận simd_float 4x4 để lưu trữ vị trí 3D.

Biểu diễn này phù hợp với các khuôn khổ khác của Apple như ARKit và chứa tất cả thông tin dịch và xoay vòng có sẵn.

Tiếp theo, có VNRecognizedPoint3D, kế thừa vị trí này nhưng cũng thêm một mã định danh.

Cái này được sử dụng để lưu trữ thông tin tương ứng như một tên chung.

Cuối cùng, trọng tâm của ngày hôm nay là VNHumanBodyRecognizedPoint3D, bổ sung một vị trí cục bộ và khớp mẹ.

Hãy đi sâu vào một số chi tiết cụ thể hơn về cách làm việc với các thuộc tính của điểm.

Sử dụng recognizedPoint API, tôi đã lấy lại vị trí cho cổ tay trái.

Vị trí mô hình của khớp, hoặc thuộc tính vị trí của điểm, luôn liên quan đến khớp gốc của bộ xương ở trung tâm hông.

Nếu chúng ta tập trung vào cột thứ ba trong ma trận vị trí, sẽ có các giá trị để dịch.

Giá trị cho y cho cổ tay trái cao hơn hông của con số này 0,9 mét, có vẻ phù hợp với tư thế này.

Tiếp theo, có thuộc tính localPosition của điểm trả về, là vị trí liên quan đến khớp mẹ.

Vì vậy, trong trường hợp này, khuỷu tay trái sẽ là khớp mẹ của cổ tay trái.

Cột cuối cùng ở đây hiển thị giá trị là -0,1 mét cho trục x, có vẻ cũng đúng.

Các giá trị âm hoặc dương được xác định bởi điểm tham chiếu, và trong tư thế này, cổ tay nằm ở phía bên trái của khuỷu tay.

localPosition rất hữu ích nếu ứng dụng của bạn chỉ hoạt động với một vùng trên cơ thể.

Nó cũng đơn giản hóa việc xác định góc giữa khớp của con và khớp mẹ.

Tôi sẽ chỉ ra cách tính góc này trong mã trong một giây.

Khi làm việc với các điểm 3D được trả về, có một số khái niệm có thể hữu ích khi xây dựng ứng dụng của bạn.

Đầu tiên, bạn thường cần xác định góc giữa khớp của con và khớp mẹ.

Trong phương pháp calculateLocalAngleToParent, vị trí liên quan đến khớp mẹ được sử dụng để tìm góc đó.

Xoay cho một nút bao gồm xoay đối với trục x, y và z, hoặc cao độ, ngáp và cuộn.

Đối với cao độ, một vòng quay 90 độ được sử dụng để định vị hình học nút SceneKit từ hướng mặc định của nó hướng thẳng xuống đến một hình học phù hợp hơn cho bộ xương của chúng ta.

Đối với ngáp, chúng ta sử dụng cosin cung của tọa độ z chia cho độ dài vectơ để có được góc thích hợp.

Và đối với cuộn, phép đo góc thu được với tiếp tuyến cung của tọa độ y và x.

Tiếp theo, ứng dụng của bạn có thể cần liên kết các vị trí 3D được trả về với hình ảnh gốc, như trong ứng dụng mẫu của tôi.

Trong hình ảnh trực quan của mình, tôi sử dụng API điểm trong hình ảnh cho hai phép biến đổi đối với mặt phẳng hình ảnh của mình, tỷ lệ và bản dịch.

Đầu tiên tôi cần chia tỷ lệ mặt phẳng hình ảnh của mình theo tỷ lệ thuận với các điểm trả về.

Tôi lấy khoảng cách giữa hai khớp đã biết, như vai trung tâm và cột sống, cho cả 3D và 2D, liên hệ chúng theo tỷ lệ và chia tỷ lệ mặt phẳng hình ảnh của tôi theo số tiền này.

Đối với thành phần dịch, tôi sử dụng pointInImage API để tìm nạp vị trí của khớp gốc trong hình ảnh 2D.

Phương pháp này sử dụng vị trí đó để xác định sự dịch chuyển cho mặt phẳng hình ảnh cho trục x và y đồng thời chuyển đổi giữa gốc dưới bên trái của tọa độ VNPoint và gốc môi trường kết xuất ở trung tâm của hình ảnh.

Cuối cùng, bạn có thể muốn xem cảnh từ góc nhìn của máy ảnh hoặc hiển thị một điểm tại vị trí của nó và bạn có thể truy xuất cảnh này từ cameraOriginMatrix.

Hướng chính xác sẽ phụ thuộc vào môi trường kết xuất của bạn, nhưng đây là cách tôi định vị các nút của mình với thông tin chuyển đổi này bằng cách sử dụng biến đổi trục, liên quan đến hệ tọa độ cục bộ của nút này với phần còn lại của cảnh.

Tôi cũng đã sử dụng thông tin xoay trong cameraOriginMatrix để xoay chính xác mặt phẳng hình ảnh của mình để đối mặt với máy ảnh bằng mã này bằng cách sử dụng biến đổi nghịch đảo.

Vì chỉ cần thông tin xoay vòng ở đây, thông tin dịch thuật ở cột cuối cùng sẽ bị bỏ qua.

Đặt tất cả các mảnh này lại với nhau cho phép cảnh được hiển thị trong ứng dụng mẫu của tôi.

Bây giờ, tôi muốn dành vài phút để thảo luận về một số bổ sung thú vị liên quan đến Độ sâu trong Tầm nhìn.

Khung tầm nhìn hiện chấp nhận Độ sâu làm đầu vào cùng với bộ đệm hình ảnh hoặc khung.

VNImageRequestHandler đã thêm API khởi tạo cho cvPixelBuffer và cmSampleBuffer lấy một tham số mới cho AVDepthData.

Ngoài ra, nếu tệp của bạn đã chứa dữ liệu Độ sâu, bạn có thể sử dụng các API hiện có mà không cần sửa đổi.

Vision sẽ tự động tìm nạp Depth từ tệp cho bạn.

Khi làm việc với Depth trong Apple SDKs, AVDepthData đóng vai trò là lớp vùng chứa để giao tiếp với tất cả siêu dữ liệu Depth.

Siêu dữ liệu Độ sâu được chụp bởi cảm biến máy ảnh chứa bản đồ Độ sâu được biểu thị dưới dạng Định dạng Chênh lệch hoặc Độ sâu.

Các định dạng này có thể hoán đổi cho nhau và có thể được chuyển đổi với nhau bằng cách sử dụng AVFoundation.

Siêu dữ liệu độ sâu cũng chứa dữ liệu hiệu chuẩn máy ảnh, như nội tại, bên ngoài và biến dạng ống kính, cần thiết để tái tạo lại cảnh 3D.

Nếu bạn cần tìm hiểu thêm chi tiết cụ thể, vui lòng xem lại phiên "Khám phá những tiến bộ trong chụp ảnh iOS" từ năm 2022.

Độ sâu có thể đạt được thông qua các phiên chụp máy ảnh hoặc từ các tệp đã chụp trước đó.

Hình ảnh được chụp bởi ứng dụng Máy ảnh, như hình ảnh Chân dung trong ảnh, luôn lưu trữ Độ sâu dưới dạng bản đồ chênh lệch với siêu dữ liệu hiệu chuẩn máy ảnh.

Khi chụp Độ sâu trong phiên chụp trực tiếp, bạn có thêm lợi ích là chỉ định phiên sử dụng LiDAR nếu thiết bị hỗ trợ nó.

LiDAR mạnh mẽ vì nó cho phép đo và đo chính xác cảnh.

Vision cũng đang giới thiệu các API để tương tác với nhiều hơn một người trong một hình ảnh.

Vision hiện đang cung cấp khả năng tách mọi người khỏi cảnh xung quanh với yêu cầu GeneratePersonSegmentation.

Yêu cầu này trả về một mặt nạ duy nhất chứa tất cả mọi người trong khung.

Vision hiện đang cho phép bạn chọn lọc hơn một chút với yêu cầu mặt nạ phiên bản người mới.

API mới này xuất ra tối đa bốn mặt nạ cá nhân, mỗi mặt nạ có điểm tin cậy.

Vì vậy, bây giờ bạn có thể chọn và nâng bạn bè của mình riêng biệt với một hình ảnh.

Nếu bạn cần chọn và nâng các đối tượng khác ngoài con người, bạn có thể sử dụng API nâng chủ đề trong VisionKit hoặc yêu cầu mặt nạ phiên bản tiền cảnh trong khung Vision.

Vui lòng xem phiên "Nâng đối tượng từ hình ảnh trong ứng dụng của bạn" để biết thêm thông tin.

Đây là một số mã mẫu cho thấy cách chọn một ví dụ cụ thể của một người bạn muốn từ một hình ảnh.

Hiện tại nó chỉ định để trả về tất cả các phiên bản, nhưng bạn có thể chọn phiên bản 1 hoặc 2, tùy thuộc vào người bạn bạn mà bạn muốn tập trung vào trong hình ảnh hoặc sử dụng phiên bản 0 để lấy nền.

Yêu cầu mới này phân đoạn tối đa bốn người, vì vậy nếu có nhiều hơn bốn người trong hình ảnh của bạn, có một số điều kiện bổ sung để xử lý trong mã của bạn.

Khi các cảnh chứa nhiều người, các quan sát được trả lại có thể bỏ lỡ mọi người hoặc kết hợp chúng.

Thông thường, điều này xảy ra với những người có mặt trong nền.

Nếu ứng dụng của bạn phải đối phó với những cảnh đông đúc, có những chiến lược bạn có thể sử dụng để xây dựng trải nghiệm tốt nhất có thể.

API phát hiện khuôn mặt trong Vision có thể được sử dụng để đếm số lượng khuôn mặt trong hình ảnh và bạn có thể chọn bỏ qua hình ảnh với hơn bốn người hoặc sử dụng yêu cầu Phân đoạn người hiện có và làm việc với một mặt nạ cho mọi người.

Tóm lại, Vision hiện cung cấp những cách mới mạnh mẽ để hiểu con người và môi trường của họ với sự hỗ trợ về chiều sâu, Tư thế cơ thể người 3D và mặt nạ người.

Nhưng đó không phải là tất cả những gì Vision sẽ phát hành trong năm nay.

Bạn có thể vượt xa con người và tạo ra những trải nghiệm tuyệt vời với hình ảnh của những người bạn lông lá trong phiên "Khử kiến tư thế động vật trong Tầm nhìn".

Cảm ơn bạn, và tôi nóng lòng muốn xem bạn xây dựng những tính năng đáng kinh ngạc nào.

♪ ♪