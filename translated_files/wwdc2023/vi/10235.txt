10235

♪ ♪

Julian: Xin chào, chào mừng đến với "Có gì mới trong xử lý giọng nói." Tôi là Julian từ nhóm Core Audio.

Các ứng dụng Voice over IP đã trở nên cần thiết hơn bao giờ hết, để giúp mọi người duy trì kết nối với đồng nghiệp, bạn bè và gia đình của họ.

Chất lượng âm thanh trong trò chuyện thoại đóng một vai trò quan trọng trong việc mang lại trải nghiệm người dùng tuyệt vời.

Điều quan trọng nhưng đầy thách thức là thực hiện xử lý âm thanh nghe có vẻ tuyệt vời trong mọi trường hợp.

Đó là lý do tại sao Apple cung cấp API xử lý giọng nói, để mọi người luôn có thể tận hưởng trải nghiệm âm thanh tốt nhất có thể khi họ trò chuyện trong ứng dụng của bạn, bất kể họ đang ở trong môi trường âm thanh nào, họ đang sử dụng sản phẩm Apple nào và phụ kiện âm thanh nào đang được kết nối.

API xử lý giọng nói của Apple được sử dụng rộng rãi bởi nhiều ứng dụng, bao gồm ứng dụng FaceTime và Điện thoại của riêng chúng tôi.

Nó cung cấp khả năng xử lý tín hiệu âm thanh tốt nhất trong lớp, bao gồm khử tiếng vang, khử tiếng ồn, kiểm soát độ lợi tự động, trong số những thứ khác để tăng cường âm thanh trò chuyện bằng giọng nói.

Hiệu suất của nó được điều chỉnh bởi các kỹ sư âm thanh cho từng mẫu sản phẩm của Apple kết hợp với từng loại thiết bị âm thanh, để tính đến các đặc tính âm thanh độc đáo của chúng.

Chọn API xử lý giọng nói của Apple cũng cho phép người dùng toàn quyền kiểm soát cài đặt chế độ micrô cho ứng dụng của bạn, bao gồm Tiêu chuẩn, Cách ly giọng nói và Phổ rộng.

Chúng tôi thực sự khuyên bạn nên sử dụng API xử lý giọng nói của Apple cho các ứng dụng Thoại qua IP của bạn.

API xử lý giọng nói của Apple có sẵn trong hai tùy chọn.

Tùy chọn đầu tiên là một đơn vị âm thanh I/O được gọi là AUVoiceIO, còn được gọi là AUVoiceProcessingIO.

Tùy chọn này dành cho các ứng dụng cần tương tác trực tiếp với đơn vị âm thanh I/O.

Tùy chọn thứ hai là AVAudioEngine, cụ thể hơn bằng cách bật chế độ "xử lý giọng nói" của AVAudioEngine.

AVAudioEngine là API cấp cao hơn.

Nó thường dễ sử dụng hơn và nó làm giảm lượng mã bạn phải viết khi làm việc với âm thanh.

Cả hai tùy chọn đều cung cấp khả năng xử lý giọng nói giống nhau.

Bây giờ, có gì mới?

Lần đầu tiên chúng tôi làm cho các API xử lý giọng nói có sẵn trên tvOS.

Để biết thêm chi tiết về điều đó, vui lòng xem phiên "Khám phá Camera liên tục cho tvOS".

Chúng tôi cũng đang thêm một vài API mới vào AUVoiceIO và AVAudioEngine để cung cấp cho bạn nhiều quyền kiểm soát hơn đối với xử lý giọng nói và giúp bạn triển khai các tính năng mới.

API đầu tiên là giúp bạn kiểm soát hành vi né tránh của các âm thanh khác-- và tôi sẽ giải thích điều đó có nghĩa là gì trong một phút.

API thứ hai là giúp bạn triển khai tính năng phát hiện người nói chuyện bị tắt tiếng cho ứng dụng của mình.

Trong phiên này, tôi sẽ tập trung vào các chi tiết của hai API mới này.

API đầu tiên tôi muốn nói đến là "Nuống âm thanh khác." Trước khi chúng ta đi sâu vào API này, trước tiên hãy để tôi giải thích âm thanh khác là gì và tại sao việc né tránh lại quan trọng.

Khi bạn sử dụng API xử lý giọng nói của Apple, hãy xem điều gì đang xảy ra với âm thanh phát lại.

Ứng dụng của bạn đang cung cấp luồng âm thanh trò chuyện thoại được xử lý bằng xử lý giọng nói của Apple và phát đến thiết bị đầu ra.

Tuy nhiên, có thể có các luồng âm thanh khác phát cùng một lúc.

Ví dụ, ứng dụng của bạn có thể đang phát một luồng âm thanh khác không được hiển thị thông qua API xử lý giọng nói.

Cũng có thể có các ứng dụng khác phát âm thanh cùng lúc với ứng dụng của bạn.

Tất cả các luồng âm thanh khác với luồng âm thanh giọng nói từ ứng dụng của bạn được coi là "âm thanh khác" bởi bộ xử lý giọng nói của Apple và âm thanh giọng nói của bạn được trộn lẫn với âm thanh khác trước khi phát đến thiết bị đầu ra.

Đối với các ứng dụng trò chuyện thoại, thông thường trọng tâm chính của âm thanh phát lại là âm thanh trò chuyện thoại.

Đó là lý do tại sao chúng tôi tránh mức âm lượng của các âm thanh khác, để cải thiện tính dễ hiểu của âm thanh giọng nói.

Trong quá khứ, chúng tôi đã áp dụng một lượng cố định cho các âm thanh khác.

Điều này đã hoạt động tốt cho hầu hết các ứng dụng và nếu ứng dụng của bạn hài lòng với hành vi né tránh hiện tại, thì bạn không cần phải làm gì cả.

Tuy nhiên, chúng tôi nhận ra rằng một số ứng dụng muốn có nhiều quyền kiểm soát hơn đối với hành vi né tránh và API này sẽ giúp bạn thực hiện điều đó.

Hãy kiểm tra API này cho AUVoiceIO trước, và chúng ta sẽ đến AVAudioEngine sau.

Đối với AUVoiceIO, đây là cấu trúc của cấu hình vịt âm thanh khác.

Nó cung cấp sự kiểm soát hai khía cạnh độc lập của việc né tránh - phong cách né tránh; đó là, mEnableAdvancedDucking, và số lượng vịt, đó là mDuckingLevel.

Đối với mEnableAdvancedDucking, theo mặc định, điều này bị vô hiệu hóa.

Sau khi được bật, nó sẽ điều chỉnh mức độ né tránh động dựa trên sự hiện diện của hoạt động bằng giọng nói từ hai bên của những người tham gia trò chuyện.

Nói cách khác, nó áp dụng nhiều hơn khi người dùng từ hai bên đang nói chuyện và nó làm giảm việc né tránh khi không bên nào đang nói.

Điều này rất giống với việc né tránh trong FaceTime SharePlay, nơi âm lượng phát lại phương tiện cao khi không bên nào trong FaceTime đang nói chuyện, nhưng ngay khi ai đó bắt đầu nói chuyện, âm lượng phát lại phương tiện sẽ giảm.

Tiếp theo, mDuckingLevel.

Có bốn cấp độ điều khiển: mặc định (Mặc định), tối thiểu (tối thiểu), trung bình (Trung bình) và tối đa (Tối đa).

Mức ducking mặc định (Mặc định) áp dụng cùng một lượng ducking mà chúng tôi đã áp dụng và đây sẽ tiếp tục là cài đặt mặc định của chúng tôi.

Mức độ ducking tối thiểu (Min) giảm thiểu số lượng ducking mà chúng tôi áp dụng.

Nói cách khác, đây là cài đặt để sử dụng nếu bạn muốn âm lượng âm thanh khác càng lớn càng tốt.

Ngược lại, mức độ vịt tối đa (Tối đa) tối đa hóa số lượng vịt mà chúng tôi áp dụng.

Nói chung, việc chọn mức độ né tránh cao hơn giúp cải thiện tính dễ hiểu của trò chuyện thoại.

Hai điều khiển có thể được sử dụng độc lập.

Khi được sử dụng kết hợp, nó mang lại cho bạn sự linh hoạt hoàn toàn trong việc kiểm soát hành vi né tránh.

Bây giờ chúng tôi đã đề cập đến những gì cấu hình ducking làm, bạn có thể tạo một cấu hình phù hợp với ứng dụng của mình.

Ví dụ, ở đây tôi sẽ kích hoạt vịt vịt nâng cao và chọn cấp độ vịt là tối thiểu.

Sau đó, tôi sẽ đặt cấu hình ducking này thành AUVoiceIO, thông qua kAUVoiceIOProperty_ OtherAudioDuckingConfiguration.

Đối với khách hàng AVAudioEngine, API trông rất giống nhau.

Đây là định nghĩa cấu trúc của cấu hình ducking âm thanh khác, và đây là định nghĩa enum của mức độ ducking.

Để sử dụng API này với AVAudioEngine, trước tiên bạn sẽ bật xử lý giọng nói trên nút đầu vào của công cụ sau đó thiết lập cấu hình ducking.

Và cuối cùng, thiết lập cấu hình trên nút đầu vào.

Tiếp theo, hãy nói về một API khác giúp bạn triển khai một tính năng rất hữu ích trong ứng dụng của mình.

Bạn đã bao giờ ở trong tình huống trong một cuộc họp trực tuyến mà bạn nghĩ rằng bạn đang trò chuyện với đồng nghiệp hoặc bạn bè, nhưng một lúc sau, bạn nhận ra mình bị tắt tiếng và không ai thực sự nghe thấy những điểm tuyệt vời hoặc những câu chuyện hài hước của bạn?

Vâng, thật khó xử.

Nó khá hữu ích để cung cấp tính năng phát hiện người nói bị tắt tiếng trong ứng dụng của bạn, giống như những gì FaceTime đang làm ở đây.

Đó là lý do tại sao chúng tôi đang cung cấp một API để bạn phát hiện sự hiện diện của một người nói chuyện bị tắt tiếng.

Nó được giới thiệu lần đầu tiên trong iOS 15, và bây giờ chúng tôi đang cung cấp nó cho macOS 14 và tvOS 17.

Đây là tổng quan cấp cao về cách sử dụng API này.

Đầu tiên, bạn cần cung cấp một khối người nghe cho AUVoiceIO hoặc AVAudioEngine để nhận thông báo khi phát hiện người nói bị tắt tiếng.

Khối người nghe bạn cung cấp được gọi bất cứ khi nào người nói bị tắt tiếng bắt đầu nói hoặc ngừng nói sau đó triển khai mã xử lý của bạn cho thông báo đó.

Ví dụ, bạn có thể muốn nhắc người dùng tự bật tiếng nếu thông báo cho biết người dùng bắt đầu nói chuyện trong khi tắt tiếng.

Cuối cùng nhưng không kém phần quan trọng, cần phải thực hiện tắt tiếng thông qua API tắt tiếng của AUVoiceIO hoặc AVAudioEngine.

Hãy để tôi hướng dẫn bạn một số ví dụ mã với AUVoiceIO.

Chúng ta sẽ đến ví dụ AVAudioEngine sau.

Đầu tiên, chuẩn bị một khối người nghe xử lý thông báo.

Khối có một tham số thuộc loại AUVoiceIOSpeechActivityEvent, có thể là một trong hai giá trị-- SpeechActivityHasStarted hoặc SpeechActivityHasEnded.

Khối người nghe sẽ được gọi bất cứ khi nào sự kiện hoạt động lời nói thay đổi trong quá trình tắt tiếng.

Bên trong khối, đây là nơi bạn thực hiện cách bạn muốn xử lý sự kiện này, Ví dụ: khi nhận được sự kiện SpeechActivityHasStarted, bạn có thể muốn nhắc người dùng tự bật tiếng.

Khi bạn đã sẵn khối trình nghe này, hãy đăng ký khối với AUVoiceIO thông qua kAUVoiceIOProperty_MutedSpeechActivityEventListener.

Khi người dùng tắt tiếng, hãy triển khai tắt tiếng thông qua API tắt tiếng kAUVoiceIOProperty_MuteOutput.

Khối người nghe của bạn chỉ được gọi nếu A, người dùng bị tắt tiếng và B, khi trạng thái hoạt động lời nói thay đổi.

Sự hiện diện liên tục hoặc thiếu hoạt động lời nói sẽ không gây ra thông báo dư thừa.

Đối với khách hàng AVAudioEngine, việc triển khai rất giống nhau.

Sau khi bạn bật xử lý giọng nói trên nút nhập của công cụ, hãy chuẩn bị một khối người nghe nơi bạn xử lý thông báo.

Sau đó đăng ký khối người nghe bằng nút nhập.

Khi người dùng tắt tiếng, hãy sử dụng API tắt tiếng xử lý giọng nói của AVAudioEngine để tắt tiếng.

Bây giờ, chúng tôi đã đề cập đến việc triển khai tính năng phát hiện người nói bị tắt tiếng với AUVoiceIO và AVAudioEngine.

Đối với những người bạn chưa sẵn sàng áp dụng API xử lý giọng nói của Apple, chúng tôi đang cung cấp một giải pháp thay thế để giúp bạn triển khai tính năng này.

Giải pháp thay thế này chỉ có sẵn trên macOS thông qua CoreAudio HAL APIs, tức là API Lớp Trừu tượng Phần cứng.

Có hai thuộc tính HAL mới để giúp bạn phát hiện hoạt động bằng giọng nói khi được sử dụng kết hợp.

Đầu tiên, cho phép phát hiện hoạt động bằng giọng nói trên thiết bị đầu vào thông qua kAudioDevicePropertyVoiceActivityDetectionEnable.

Sau đó đăng ký trình nghe thuộc tính HAL trên kAudioDevicePropertyVoiceActivityDetectionState.

Trình nghe thuộc tính HAL này được gọi bất cứ khi nào có thay đổi trong trạng thái hoạt động bằng giọng nói.

Khi ứng dụng của bạn được thông báo bởi người nghe tài sản, hãy truy vấn tài sản để có được giá trị hiện tại của nó.

Bây giờ hãy để tôi hướng dẫn bạn điều này với một số ví dụ mã.

Để cho phép phát hiện hoạt động bằng giọng nói trên thiết bị đầu vào, trước tiên hãy xây dựng địa chỉ thuộc tính HAL.

Sau đó đặt thuộc tính vào thiết bị đầu vào để kích hoạt nó.

Tiếp theo, để đăng ký trình nghe trên thuộc tính trạng thái phát hiện hoạt động bằng giọng nói, hãy xây dựng địa chỉ thuộc tính HAL, sau đó cung cấp trình nghe thuộc tính của bạn.

Ở đây "listener_callback" là tên của hàm nghe của bạn.

Đây là một ví dụ về cách triển khai trình nghe thuộc tính.

Người nghe phù hợp với chữ ký chức năng này.

Trong ví dụ này, chúng tôi giả định rằng trình nghe này chỉ được đăng ký cho một thuộc tính HAL, có nghĩa là khi nó được gọi, không có sự mơ hồ về thuộc tính HAL nào đã thay đổi.

Nếu bạn đăng ký cùng một trình nghe để thông báo về nhiều thuộc tính HAL, thì trước tiên bạn phải xem qua mảng inAddresses để xem chính xác những gì đã thay đổi.

Khi xử lý thông báo này, hãy truy vấn thuộc tính VoiceActivityDetectionState để lấy giá trị hiện tại của nó sau đó thực hiện logic của riêng bạn trong việc xử lý giá trị đó.

Có một số chi tiết quan trọng về các API HAL phát hiện hoạt động bằng giọng nói này.

Trước hết, nó phát hiện hoạt động của giọng nói từ đầu vào micrô bị khử tiếng vang nên nó lý tưởng cho các ứng dụng trò chuyện thoại.

Thứ hai, phát hiện này hoạt động bất kể trạng thái tắt tiếng của quá trình.

Để triển khai tính năng phát hiện người nói chuyện bị tắt tiếng với nó, tùy thuộc vào ứng dụng của bạn để triển khai logic bổ sung kết hợp trạng thái hoạt động bằng giọng nói với trạng thái tắt tiếng.

Đối với các máy khách HAL API triển khai tắt tiếng, chúng tôi thực sự khuyên bạn nên sử dụng API tắt tiếng quy trình của HAL.

Nó ngăn chặn đèn báo ghi âm trong thanh menu và mang lại cho người dùng sự tự tin rằng quyền riêng tư của họ được bảo vệ dưới chế độ tắt tiếng.

Hãy tóm tắt lại những gì đã nói hôm nay.

Chúng tôi đã thảo luận về API xử lý giọng nói của Apple và lý do tại sao chúng tôi đề xuất nó cho các ứng dụng thoại qua IP.

Chúng tôi đã nói về việc né tránh các âm thanh khác và API để kiểm soát hành vi né tránh với các ví dụ mã về cách sử dụng nó với AUVoiceIO và AVAudioEngine.

Chúng tôi cũng đã nói về cách triển khai phát hiện người nói bị tắt tiếng với các ví dụ mã của AUVoiceIO và AVAudioEngine.

Và đối với những khách hàng chưa áp dụng API xử lý giọng nói của Apple, chúng tôi cũng đã chỉ ra một tùy chọn thay thế để thực hiện điều đó trên macOS với Core Audio HAL APIs.

Chúng tôi đang mong chờ những ứng dụng tuyệt vời mà bạn sẽ xây dựng với các API xử lý giọng nói của Apple.

Cảm ơn vì đã xem!

♪ ♪