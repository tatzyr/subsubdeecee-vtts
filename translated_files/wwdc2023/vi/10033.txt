10033

♪ ♪

Grant: Xin chào, tên tôi là Grant. Tôi là một Kỹ sư trong Nhóm Tiếp cận.

Nhiều người sử dụng tổng hợp giọng nói trên các nền tảng của Apple và một số người dựa vào giọng nói tổng hợp.

Những giọng nói này là một cửa sổ vào thiết bị của họ.

Do đó, tiếng nói họ chọn thường là một lựa chọn rất cá nhân.

Những người sử dụng tổng hợp giọng nói trên iOS đã có thể chọn từ nhiều giọng nói khác nhau.

Hãy xem cách bạn có thể cung cấp nhiều hơn nữa.

Đầu tiên, chúng ta sẽ nói về Ngôn ngữ đánh dấu tổng hợp giọng nói là gì, cách nó có thể mang lại đầu ra giọng nói nhập vai cho giọng nói tùy chỉnh của bạn và lý do tại sao nhà cung cấp giọng nói của bạn nên áp dụng nó.

Tiếp theo, chúng tôi sẽ hướng dẫn cách bạn có thể triển khai nhà cung cấp tổng hợp giọng nói để mang lại trải nghiệm tổng hợp và giọng nói của bạn trên thiết bị.

Và cuối cùng, chúng ta sẽ đi sâu vào Giọng nói Cá nhân.

Đây là một tính năng mới.

Giờ đây, mọi người có thể ghi lại giọng nói của họ và sau đó tạo ra một giọng nói tổng hợp từ những bản ghi âm đó.

Vì vậy, bây giờ, bạn có thể tổng hợp lời nói bằng giọng nói cá nhân của chính người dùng.

Hãy bắt đầu bằng cách xem SSML.

SSML là một tiêu chuẩn W3C để đại diện cho văn bản nói.

SSML Speech được thể hiện một cách khai báo bằng cách sử dụng định dạng XML với nhiều thẻ và thuộc tính khác nhau.

Bạn có thể sử dụng các thẻ này để kiểm soát các thuộc tính giọng nói như tốc độ và cao độ.

SSML được sử dụng trong các bộ tổng hợp của bên thứ nhất.

Điều này bao gồm WebSpeech trong WebKit và là đầu vào tiêu chuẩn cho bộ tổng hợp giọng nói.

Hãy xem cách bạn có thể sử dụng SSML.

Lấy cụm từ ví dụ này có tạm dừng trong đó.

Chúng tôi có thể đại diện cho khoảng dừng này trong SSML.

Chúng ta sẽ bắt đầu với chuỗi "xin chào" của mình, thêm một giây tạm dừng của chúng ta bằng cách sử dụng thẻ ngắt SSML và kết thúc bằng cách tăng tốc "rất vui được gặp bạn!"

Chúng tôi làm điều này bằng cách thêm thẻ prosody SSML và đặt thuộc tính tỷ lệ thành 200%.

Bây giờ chúng ta có thể lấy SSML này và tạo một AVSpeechUtterance để nói chuyện.

Tiếp theo, chúng ta hãy xem cách bạn có thể triển khai giọng nói tổng hợp giọng nói của riêng mình.

Vậy bộ tổng hợp giọng nói là gì?

Bộ tổng hợp giọng nói nhận được một số văn bản và thông tin về các thuộc tính giọng nói mong muốn dưới dạng SSML và cung cấp biểu diễn âm thanh của văn bản đó.

Giả sử bạn có một bộ tổng hợp với những giọng nói mới tuyệt vời và bạn muốn đưa nó lên iOS, macOS và iPadOS.

Các nhà cung cấp tổng hợp giọng nói cho phép bạn triển khai bộ tổng hợp giọng nói và giọng nói của riêng mình vào nền tảng của chúng tôi để mang lại nhiều cá nhân hóa hơn cho người dùng ngoài giọng nói của hệ thống.

Hãy xem cái này hoạt động như thế nào.

Các tiện ích mở rộng đơn vị âm thanh của nhà cung cấp tổng hợp giọng nói sẽ được nhúng vào ứng dụng máy chủ và sẽ nhận được các yêu cầu giọng nói dưới dạng SSML.

Tiện ích mở rộng sẽ chịu trách nhiệm hiển thị âm thanh cho đầu vào SSML và tùy chọn trả về các điểm đánh dấu cho biết nơi các từ xuất hiện trong các bộ đệm âm thanh đó.

Hệ thống sau đó sẽ quản lý tất cả phát lại cho yêu cầu giọng nói đó.

Bạn không cần phải xử lý bất kỳ quản lý phiên âm thanh nào; nó được quản lý nội bộ bởi khuôn khổ Nhà cung cấp tổng hợp giọng nói.

Bây giờ chúng ta đã hiểu bộ tổng hợp là gì, chúng ta có thể bắt đầu xây dựng một phần mở rộng bộ tổng hợp giọng nói.

Hãy bắt đầu bằng cách tạo một dự án ứng dụng Mở rộng Đơn vị Âm thanh mới trong Xcode, sau đó chọn Loại Đơn vị Âm thanh "Speech Synthesizer" và cung cấp mã định danh loại phụ bốn ký tự cho bộ tổng hợp của bạn, cũng như mã định danh bốn ký tự cho bạn với tư cách là nhà sản xuất.

Phần mở rộng đơn vị âm thanh là kiến trúc cốt lõi mà trên đó các phần mở rộng bộ tổng hợp giọng nói đã được xây dựng.

Chúng cho phép bộ tổng hợp của bạn chạy trong quy trình mở rộng thay vì trong quy trình ứng dụng máy chủ của bạn.

Ứng dụng của chúng tôi sẽ cung cấp một giao diện đơn giản để mua và chọn một giọng nói mà tiện ích mở rộng của chúng tôi sẽ tổng hợp giọng nói.

Chúng tôi sẽ bắt đầu bằng cách tạo một chế độ xem danh sách hiển thị các giọng nói có sẵn của chúng tôi để mua.

Mỗi ô thoại sẽ hiển thị tên giọng nói và nút mua.

Tiếp theo, tôi sẽ điền vào danh sách của mình với một số giọng nói.

Ở đây, WWDCVoice là một cấu trúc đơn giản giữ tên giọng nói và số nhận dạng.

Chúng tôi cũng cần một biến trạng thái để theo dõi các giọng nói đã mua và một phần mới để hiển thị chúng.

Tiếp theo, hãy tạo một chức năng để mua một giọng nói.

Tại đây chúng tôi có thể thêm giọng nói mới mua vào danh sách của mình và cập nhật giao diện người dùng của chúng tôi cho phù hợp.

Lưu ý về phương pháp AVSpeechSynthesisProviderVoice updateSpeechVoices.

Đó là cách ứng dụng của bạn có thể báo hiệu rằng tập hợp các giọng nói có sẵn cho bộ tổng hợp của bạn đã thay đổi và danh sách giọng nói hệ thống nên được xây dựng lại.

Trong ví dụ của chúng tôi, chúng tôi có thể thực hiện cuộc gọi này sau khi hoàn tất giao dịch mua trong ứng dụng cho một giọng nói.

Chúng tôi cũng cần một cách để theo dõi giọng nói nào có sẵn trong tiện ích mở rộng tổng hợp giọng nói của chúng tôi.

Điều này có thể được thực hiện bằng cách tạo một phiên bản UserDefaults sẽ được chia sẻ thông qua một nhóm ứng dụng.

Một nhóm ứng dụng sẽ cho phép chúng tôi chia sẻ danh sách thoại này giữa ứng dụng chủ và tiện ích mở rộng của chúng tôi.

Chúng tôi đang chỉ định rõ ràng một tên bộ mà chúng tôi đã cung cấp khi tạo nhóm ứng dụng.

Điều này đảm bảo ứng dụng máy chủ và tiện ích mở rộng được đọc từ cùng một miền.

Nhìn lại chức năng mua hàng, tôi đã triển khai một phương pháp để cập nhật mặc định của người dùng khi mua một giọng nói mới.

AVSpeechSynthesizer cũng có API mới để lắng nghe sự thay đổi trong giọng nói hệ thống có sẵn.

Bộ giọng nói hệ thống có thể thay đổi khi người dùng xóa giọng nói hoặc tải xuống giọng nói mới.

Bạn có thể đăng ký availableVoicesDidChangeNotification để cập nhật danh sách giọng nói của mình dựa trên những thay đổi này.

Bây giờ ứng dụng máy chủ của chúng tôi đã hoàn tất, hãy điền vào đơn vị âm thanh, bao gồm bốn thành phần chính.

Điều đầu tiên chúng tôi cần bổ sung là một số cách để thông báo cho hệ thống về những giọng nói mà bộ tổng hợp của chúng tôi sẽ cung cấp.

Điều này được thực hiện bằng cách ghi đè trình nhận speechVoices để cung cấp danh sách giọng nói và đọc từ nhóm ứng dụng UserDefaults miền mà chúng tôi đã chỉ định trước đó.

Đối với mỗi mục trong danh sách giọng nói của chúng tôi, chúng tôi sẽ xây dựng một AVSpeechSynthesisProviderVoice tiếng Anh Hoa Kỳ.

Tiếp theo, chúng ta cần một số cách để hệ thống cho bộ tổng hợp biết văn bản nào cần tổng hợp.

Phương thức synthesizeSpeechRequest sẽ được gọi khi hệ thống muốn báo hiệu cho một tiện ích mở rộng rằng nó sẽ bắt đầu tổng hợp một số văn bản.

Đối số của phương pháp này sẽ là một ví dụ của AVSpeechSynthesisProviderRequest chứa SSML và giọng nói nào để nói chuyện.

Tiếp theo, tôi sẽ gọi một phương thức trợ giúp mà tôi đã tạo trong việc triển khai công cụ giọng nói của mình.

Trong ví dụ này, phương thức getAudioBuffer của tôi sẽ tạo dữ liệu âm thanh dựa trên giọng nói được chỉ định trong yêu cầu và đầu vào SSML.

Chúng tôi cũng sẽ đặt một biến thể hiện, được gọi là framePosition, thành 0 để theo dõi số lượng khung hình chúng tôi đã hiển thị khi khối kết xuất được gọi và chúng tôi sao chép các khung ra khỏi bộ đệm.

Hệ thống cũng cần một cách để báo hiệu cho bộ tổng hợp để ngừng tổng hợp âm thanh và loại bỏ yêu cầu giọng nói hiện tại.

Điều này được thực hiện với cancelSpeechRequest, nơi chúng tôi sẽ đơn giản loại bỏ bộ đệm hiện tại.

Cuối cùng, chúng ta cần triển khai khối kết xuất.

Khối kết xuất được gọi bởi hệ thống với frameCount mong muốn.

Đơn vị âm thanh sau đó chịu trách nhiệm điền số lượng khung hình được yêu cầu vào đầu ra AudioBuffer.

Tiếp theo, chúng tôi sẽ tự thiết lập một tham chiếu đến bộ đệm đích và bộ đệm mà chúng tôi đã tạo và lưu trữ trước đó trong cuộc gọi synthesizeSpeechRequest.

Sau đó, chúng tôi sẽ sao chép các khung vào bộ đệm mục tiêu.

Và cuối cùng, một khi đơn vị âm thanh đã cạn kiệt tất cả các bộ đệm cho yêu cầu giọng nói hiện tại, đối số actionFlags nên được đặt thành offlineUnitRenderAction_Complete để báo hiệu cho hệ thống rằng kết xuất đã hoàn tất và không còn bộ đệm âm thanh nào được hiển thị nữa.

Hãy xem nó hoạt động!

Đây là ứng dụng tổng hợp giọng nói của tôi.

Tôi sẽ mua một giọng nói và điều hướng đến chế độ xem nơi tôi có thể tổng hợp giọng nói bằng cách sử dụng công cụ giọng nói và giọng nói mới của mình.

Đầu tiên, tôi sẽ cung cấp cho bộ tổng hợp một đầu vào là "xin chào".

Giọng nói tổng hợp: Xin chào.

Grant: Sau đó tôi sẽ đưa ra đầu vào "tạm biệt".

Giọng tổng hợp: Tạm biệt.

Grant: Chúng tôi hiện đã triển khai một nhà cung cấp tổng hợp và tạo ra một ứng dụng lưu trữ cung cấp giọng nói mà bạn có thể sử dụng trên toàn hệ thống, từ VoiceOver đến các ứng dụng của riêng bạn!

Chúng tôi nóng lòng muốn xem những giọng nói và trải nghiệm chuyển văn bản thành giọng nói mới mà bạn tạo ra bằng cách sử dụng các API này.

Hãy tiếp tục và nói về một tính năng mới có tên là Personal Voice.

Giờ đây mọi người có thể ghi âm và tạo lại giọng nói của họ trên iOS và macOS bằng sức mạnh của thiết bị.

Giọng nói cá nhân của bạn được tạo trên thiết bị chứ không phải trên máy chủ.

Giọng nói này sẽ xuất hiện giữa các giọng nói còn lại của Hệ thống và có thể được sử dụng với một tính năng mới gọi là Live Speech.

Live Speech là một tính năng gõ để nói trên iOS, iPadOS, macOS và watchOS cho phép một người tổng hợp lời nói bằng giọng nói của chính họ một cách nhanh chóng.

Bạn có thể yêu cầu quyền truy cập để tổng hợp lời nói với những giọng nói này bằng cách sử dụng API ủy quyền yêu cầu mới cho Giọng nói cá nhân.

Hãy nhớ rằng việc sử dụng Giọng nói Cá nhân rất nhạy cảm và nên được sử dụng chủ yếu cho các ứng dụng giao tiếp tăng cường hoặc thay thế.

Hãy kiểm tra một ứng dụng AAC mà tôi đã tạo để sử dụng Personal Voice.

Ứng dụng của tôi có hai nút sẽ nói các cụm từ phổ biến mà tôi thấy mình đang nói tại WWDC và một nút để yêu cầu quyền truy cập để sử dụng Giọng nói Cá nhân.

Ủy quyền có thể được yêu cầu với một API mới được gọi là requestPersonalVoiceAuthorization trên AVSpeechSynthesizer.

Sau khi được ủy quyền, Personal Voices sẽ xuất hiện cùng với System voices trong AVSpeechSynthesisVoice API speechVoices và sẽ được biểu thị bằng một voiceTrait mới được gọi làPersonalVoice.

Bây giờ tôi đã có quyền truy cập vào Personal Voice, tôi có thể sử dụng nó để nói chuyện.

Hãy cùng xem bản demo của Personal Voice đang hoạt động.

Đầu tiên, tôi sẽ nhấn vào nút "Sử dụng giọng nói cá nhân" để yêu cầu ủy quyền và sau khi được ủy quyền, tôi có thể nhấn vào biểu tượng để nghe giọng nói của mình.

Giọng nói cá nhân: Xin chào, tên tôi là Grant. Chào mừng đến với WWDC23.

Grant: Điều đó thật tuyệt vời phải không?

Và bây giờ bạn cũng có thể sử dụng những giọng nói này trong ứng dụng của mình.

Bây giờ chúng ta đã thảo luận về SSML, bạn nên sử dụng nó để chuẩn hóa đầu vào giọng nói và xây dựng trải nghiệm giọng nói phong phú trong các ứng dụng của mình.

Chúng tôi cũng đã hướng dẫn cách triển khai Speech Synthesizer của bạn vào các nền tảng của Apple, vì vậy bây giờ bạn có thể cung cấp giọng nói giọng nói mới tuyệt vời mà mọi người có thể sử dụng trên toàn hệ thống.

Và cuối cùng, với Personal Voice, bạn có thể mang lại nhiều nét cá nhân hơn để tổng hợp trong các ứng dụng của mình, đặc biệt là đối với những người có thể có nguy cơ mất giọng nói của chính họ.

Chúng tôi rất vui mừng khi thấy những trải nghiệm bạn tạo ra khi sử dụng các API này.

Cảm ơn vì đã xem.