10042

♪ ♪

Doug: Chào mừng mọi người. Tôi là Doug Davidson, và tôi ở đây để nói chuyện với bạn về Xử lý Ngôn ngữ Tự nhiên.

Bây giờ, đã có một số phiên về Xử lý Ngôn ngữ Tự nhiên trong những năm qua.

Những gì chúng ta sẽ nói về ngày hôm nay được xây dựng dựa trên tất cả những điều đó, với việc bổ sung một số chức năng mới thú vị.

Đầu tiên tôi sẽ đưa ra một số thông tin cơ bản về các mô hình NLP và NLP.

Sau đó tôi sẽ tóm tắt chức năng hiện có.

Sau đó tôi sẽ nói về những gì mới trong năm nay.

Tôi sẽ thảo luận về một số ứng dụng nâng cao.

Và sau đó tôi sẽ gói nó lại.

Hãy bắt đầu với một số thông tin cơ bản.

Về mặt sơ đồ, các mô hình NLP thường có quy trình tương tự.

Họ bắt đầu với dữ liệu văn bản, sau đó có một lớp đầu vào chuyển đổi nó thành biểu diễn tính năng số, trên đó một mô hình học máy có thể hoạt động và tạo ra một số đầu ra.

Các ví dụ rõ ràng nhất về điều này từ những năm trước là các mô hình Tạo ML được hỗ trợ để phân loại văn bản và gắn thẻ từ.

Sự phát triển của NLP như một lĩnh vực có thể được truy tìm khá chặt chẽ chỉ bằng sự phát triển của các phiên bản ngày càng tinh vi của các lớp đầu vào.

Mười hoặc hai mươi năm trước, đây là những đặc điểm chính tả đơn giản.

Sau đó khoảng một thập kỷ trước, mọi thứ chuyển sang sử dụng nhúng từ tĩnh, chẳng hạn như Word2Vec và GloVe.

Sau đó là nhúng từ theo ngữ cảnh dựa trên các mô hình mạng thần kinh, chẳng hạn như CNN và LSTM.

Và gần đây hơn, các mô hình ngôn ngữ dựa trên máy biến áp.

Tôi nên nói một vài từ về việc nhúng là gì.

Ở dạng đơn giản nhất, nó chỉ là một bản đồ từ các từ trong một ngôn ngữ đến các vectơ trong một số không gian vectơ trừu tượng, nhưng được đào tạo như một mô hình học máy sao cho các từ có ý nghĩa tương tự gần nhau trong không gian vectơ.

Điều này cho phép nó kết hợp kiến thức ngôn ngữ.

Nhúng tĩnh chỉ là một bản đồ đơn giản từ từ đến vectơ.

Chuyển một từ, mô hình tra cứu nó trong một bảng và cung cấp một vectơ.

Những thứ này được đào tạo sao cho các từ có ý nghĩa tương tự gần nhau trong không gian vectơ.

Điều này khá hữu ích cho việc hiểu từng từ riêng lẻ.

Các nhúng phức tạp hơn là động và theo ngữ cảnh sao cho mỗi từ trong câu được ánh xạ đến một vectơ khác nhau tùy thuộc vào việc sử dụng nó trong câu.

Ví dụ, "thực phẩm" trong "căn ăn nhanh" có ý nghĩa khác với "thực phẩm" trong "thực phẩm để suy nghĩ", vì vậy chúng sẽ nhận được các vectơ nhúng khác nhau.

Bây giờ, mục đích của việc nhúng mạnh mẽ làm lớp đầu vào là cho phép học chuyển giao.

Việc nhúng được đào tạo trên một lượng lớn dữ liệu và gói gọn kiến thức chung về ngôn ngữ, có thể được chuyển sang nhiệm vụ cụ thể của bạn mà không yêu cầu một lượng lớn dữ liệu đào tạo dành riêng cho nhiệm vụ.

Hiện tại, Create ML hỗ trợ nhúng loại này bằng cách sử dụng các mô hình ELMo.

Các mô hình này dựa trên LSTM có đầu ra được kết hợp để tạo ra vectơ nhúng.

Chúng có thể được sử dụng thông qua Create ML để phân loại đào tạo và gắn thẻ các mô hình.

Bây giờ, hãy để tôi thảo luận về các mô hình đã được hỗ trợ cho đến nay.

Những điều này đã được thảo luận rất chi tiết trong các phiên trước vào năm 2019 và 2020, vì vậy tôi sẽ chỉ mô tả ngắn gọn chúng ở đây.

Ngôn ngữ tự nhiên hỗ trợ đào tạo mô hình bằng cách sử dụng Tạo ML thường tuân theo mô hình mà chúng tôi đã thấy cho các mô hình NLP.

Điều này liên quan đến các mô hình cho hai nhiệm vụ khác nhau: phân loại văn bản và gắn thẻ từ.

Trong phân loại văn bản, đầu ra mô tả văn bản đầu vào bằng cách sử dụng một trong một tập hợp các lớp.

Ví dụ, nó có thể là một chủ đề hoặc một tình cảm.

Và trong gắn thẻ từ, đầu ra đặt nhãn trên mỗi từ trong văn bản đầu vào, ví dụ, một phần của bài phát biểu hoặc nhãn vai trò.

Và các mô hình Create ML được hỗ trợ thường theo dõi sự phát triển của trường NLP, bắt đầu với các mô hình tối đa và dựa trên CRF, sau đó thêm hỗ trợ cho nhúng từ tĩnh, và sau đó nhúng từ động cho các mô hình Tạo ML bằng cách sử dụng nhúng ELMo.

Và bạn có thể xem chi tiết về điều này trong các phiên trước, "Những tiến bộ trong Khung ngôn ngữ tự nhiên" từ năm 2019 và "Làm cho ứng dụng thông minh hơn với ngôn ngữ tự nhiên" từ năm 2020.

Bây giờ hãy để tôi chuyển sang những gì mới trong năm nay trong Ngôn ngữ Tự nhiên.

Tôi rất vui khi nói rằng bây giờ chúng tôi cung cấp các nhúng theo ngữ cảnh dựa trên máy biến áp.

Cụ thể, đây là những nhúng BERT.

Điều đó chỉ là viết tắt của Biểu diễn Bộ mã hóa Hai chiều từ Transformers.

Đây là những mô hình nhúng được đào tạo trên một lượng lớn văn bản bằng cách sử dụng phong cách đào tạo mô hình ngôn ngữ đeo mặt nạ.

Điều này có nghĩa là mô hình được đưa ra một câu với một từ được che giấu và yêu cầu đề xuất từ đó, ví dụ, "thực phẩm" trong "thực phẩm để suy nghĩ" và được đào tạo để làm tốt hơn và tốt hơn trong việc này.

Các máy biến áp trong trái tim của chúng dựa trên cái được gọi là cơ chế chú ý, cụ thể là sự tự chú ý nhiều đầu, cho phép mô hình tính đến các phần khác nhau của văn bản với trọng lượng khác nhau, theo nhiều cách khác nhau cùng một lúc.

Cơ chế tự chú ý nhiều đầu được bao bọc với nhiều lớp khác, sau đó được lặp lại nhiều lần, điều này hoàn toàn cung cấp một mô hình mạnh mẽ và linh hoạt có thể tận dụng một lượng lớn dữ liệu văn bản.

Trên thực tế, nó có thể được đào tạo trên nhiều ngôn ngữ cùng một lúc, dẫn đến một mô hình đa ngôn ngữ.

Điều này có một số lợi thế.

Nó cho phép hỗ trợ nhiều ngôn ngữ ngay lập tức và thậm chí nhiều ngôn ngữ cùng một lúc.

Nhưng thậm chí còn hơn thế nữa, vì sự tương đồng giữa các ngôn ngữ, có một số sức mạnh tổng hợp như dữ liệu cho một ngôn ngữ giúp ích cho các ngôn ngữ khác.

Vì vậy, chúng tôi đã ngay lập tức hỗ trợ 27 ngôn ngữ khác nhau trên nhiều họ ngôn ngữ khác nhau.

Điều này được thực hiện với ba mô hình riêng biệt, mỗi mô hình dành cho các nhóm ngôn ngữ có chung hệ thống chữ viết liên quan.

Vì vậy, có một mô hình cho các ngôn ngữ viết Latinh, một cho các ngôn ngữ sử dụng Cyrillic và một cho tiếng Trung, tiếng Nhật và tiếng Hàn.

Các mô hình nhúng này phù hợp với khóa đào tạo Create ML mà chúng ta đã thảo luận trước đó, đóng vai trò là lớp mã hóa đầu vào.

Đây là một mã hóa mạnh mẽ cho nhiều mô hình khác nhau.

Ngoài ra, dữ liệu mà bạn sử dụng để đào tạo không nhất thiết phải bằng một ngôn ngữ duy nhất.

Hãy để tôi chỉ cho bạn cách hoạt động của nó với một ví dụ.

Giả sử bạn đang viết một ứng dụng nhắn tin và muốn hỗ trợ người dùng bằng cách tự động phân loại các tin nhắn mà họ nhận được.

Giả sử bạn muốn chia chúng thành ba loại: tin nhắn cá nhân, chẳng hạn như bạn có thể nhận được từ bạn bè, tin nhắn kinh doanh, chẳng hạn như bạn có thể nhận được từ đồng nghiệp và tin nhắn thương mại, chẳng hạn như bạn có thể nhận được từ các doanh nghiệp mà bạn tương tác.

Nhưng người dùng có thể nhận được tin nhắn bằng nhiều ngôn ngữ khác nhau và bạn muốn xử lý điều đó.

Đối với ví dụ này, tôi đã tập hợp một số dữ liệu đào tạo bằng nhiều ngôn ngữ, tiếng Anh, tiếng Ý, tiếng Đức và tiếng Tây Ban Nha.

Tôi đã sử dụng định dạng json, nhưng bạn cũng có thể sử dụng thư mục hoặc CSV.

Để đào tạo mô hình của chúng tôi, chúng tôi vào ứng dụng Tạo ML và tạo một dự án.

Sau đó chúng ta cần chọn dữ liệu đào tạo của mình.

Tôi cũng đã chuẩn bị dữ liệu xác thực và dữ liệu thử nghiệm để đi cùng với nó.

Sau đó, chúng ta cần chọn thuật toán của mình và chúng ta có một lựa chọn mới ở đây: nhúng BERT.

Một khi chúng ta đã chọn những thứ đó, chúng ta có thể chọn kịch bản.

Vì đây là những ngôn ngữ viết bằng tiếng Latinh, tôi sẽ để nó bằng tiếng Latinh.

Nếu chúng tôi đang sử dụng một ngôn ngữ duy nhất, chúng tôi sẽ có tùy chọn chỉ định nó ở đây, nhưng đây là đa ngôn ngữ, vì vậy chúng tôi sẽ để nó ở chế độ tự động.

Sau đó, tất cả những gì chúng ta cần làm là nhấn Train, và đào tạo mô hình sẽ bắt đầu.

Phần tốn nhiều thời gian nhất của khóa đào tạo là áp dụng những nhúng mạnh mẽ này vào văn bản.

Sau đó, mô hình đào tạo khá nhanh với độ chính xác cao.

Tại thời điểm đó, chúng ta có thể thử nó trên một số tin nhắn ví dụ.

Bằng Tiếng Anh... Hoặc Tiếng Tây Ban Nha.

Và người mẫu khá tự tin rằng đây là những thông điệp thương mại.

Như một ví dụ về sự phối hợp có thể xảy ra, mô hình này chưa được đào tạo về tiếng Pháp, nhưng nó vẫn có thể phân loại một số văn bản tiếng Pháp.

Tuy nhiên, tôi khuyên bạn nên sử dụng dữ liệu đào tạo cho từng ngôn ngữ mà bạn quan tâm.

Bây giờ, cho đến nay chúng tôi vừa làm việc với Create ML, nhưng cũng có thể làm việc với các nhúng này bằng cách sử dụng khung Ngôn ngữ Tự nhiên với một lớp mới gọi là NLContextualEmbedding.

Điều này cho phép bạn xác định mô hình nhúng mà bạn muốn và tìm ra một số thuộc tính của nó.

Bạn có thể tìm kiếm một mô hình nhúng theo nhiều cách khác nhau, ví dụ, theo ngôn ngữ hoặc theo kịch bản.

Một khi bạn có một mô hình như vậy, bạn có thể nhận được các thuộc tính như kích thước của các vectơ.

Ngoài ra, mỗi mô hình có một mã định danh, chỉ là một chuỗi xác định duy nhất mô hình.

Ví dụ: khi bạn bắt đầu làm việc với một mô hình, bạn có thể định vị nó theo ngôn ngữ, nhưng sau này bạn sẽ muốn đảm bảo rằng bạn đang sử dụng cùng một mô hình và mã định danh sẽ cho phép bạn thực hiện việc này.

Một điều cần lưu ý là, giống như nhiều tính năng Ngôn ngữ Tự nhiên khác, các mô hình nhúng này dựa vào các nội dung được tải xuống khi cần thiết.

NLContextualEmbedding cung cấp một số API để cung cấp cho bạn quyền kiểm soát bổ sung đối với điều này, ví dụ, để yêu cầu tải xuống trước khi sử dụng.

Bạn có thể hỏi liệu một mô hình nhúng nhất định hiện có tài sản có sẵn trên thiết bị hay không và nếu không đưa vào yêu cầu, điều này sẽ dẫn đến việc chúng được tải xuống.

Bây giờ, một số bạn có thể nói, tôi có một số mô hình mà tôi không đào tạo bằng Create ML, nhưng thay vào đó tôi đào tạo bằng PyTorch hoặc TensorFlow.

Tôi vẫn có thể sử dụng những nhúng BERT mới này chứ?

Vâng, bạn có thể.

Chúng tôi cung cấp các mô hình nhúng đa ngôn ngữ được đào tạo trước này có sẵn cho bạn, bạn có thể sử dụng làm lớp đầu vào cho bất kỳ mô hình nào bạn muốn đào tạo.

Đây là cách nó sẽ hoạt động.

Trên thiết bị macOS của bạn, bạn sẽ sử dụng NLContextualEmbedding để lấy các vectơ nhúng cho dữ liệu đào tạo của mình.

Sau đó, bạn sẽ cung cấp những thứ này làm đầu vào cho khóa đào tạo của mình bằng cách sử dụng PyTorch hoặc TensorFlow và chuyển đổi kết quả thành mô hình Core ML bằng các công cụ Core ML.

Sau đó, tại thời điểm suy luận trên thiết bị, bạn sẽ sử dụng NLContextualEmbedding để lấy các vectơ nhúng cho dữ liệu đầu vào của mình, chuyển chúng vào mô hình Core ML của bạn để có được đầu ra.

Để hỗ trợ điều này, có các API NLContextualEmbedding bổ sung cho phép bạn tải một mô hình, áp dụng nó vào một đoạn văn bản và nhận các vectơ nhúng kết quả.

Nếu bạn nhớ mã định danh mô hình từ trước đó, bạn có thể sử dụng nó để truy xuất cùng một mô hình mà bạn đã sử dụng để đào tạo.

Sau đó, bạn có thể áp dụng mô hình cho một đoạn văn bản, tạo ra một đối tượng NLContextualEmbeddingResult.

Khi bạn có đối tượng này, bạn có thể sử dụng nó để lặp lại các vectơ nhúng.

Bây giờ, để cung cấp cho bạn một hương vị về những gì có thể với điều này, chúng tôi đã chuẩn bị một mô hình ví dụ đơn giản.

Chúng tôi bắt đầu với một mô hình khuếch tán ổn định bằng tiếng Anh hiện có, sau đó sử dụng một số dữ liệu đa ngôn ngữ để tinh chỉnh nó để sử dụng các nhúng BERT mới làm lớp đầu vào, lấy chúng làm cố định và cũng đào tạo một lớp chiếu tuyến tính đơn giản để chuyển đổi chiều.

Kết quả sau đó là một mô hình Khuếch tán Ổn định lấy đầu vào đa ngôn ngữ.

Đây là một số ví dụ về đầu ra từ mô hình.

Nếu tôi đi qua một số văn bản tiếng Anh, ví dụ, "Một con đường xuyên qua một khu vườn đầy hoa màu hồng", mô hình sẽ dẫn chúng ta xuống một con đường vào một khu vườn đầy hoa màu hồng.

Ngoài ra, nếu tôi dịch cùng một câu sang tiếng Pháp, tiếng Tây Ban Nha, tiếng Ý và tiếng Đức, mô hình sẽ tạo ra hình ảnh của những con đường và khu vườn đầy hoa màu hồng cho mỗi người.

Hãy để tôi lấy một ví dụ phức tạp hơn một chút.

"Một con đường trước cây cối và núi non dưới bầu trời nhiều mây." Đây là một số đầu ra từ mô hình, với đường, cây cối, núi và mây.

Nhưng tương tự như vậy, tôi có thể dịch cùng một câu sang tiếng Pháp, tiếng Tây Ban Nha, tiếng Ý và tiếng Đức, hoặc bất kỳ ngôn ngữ nào khác, và đối với mỗi ngôn ngữ có được hình ảnh đường, cây cối, núi và mây.

Bây giờ hãy để tôi tóm tắt các bài học từ phiên này.

Bạn có thể sử dụng Create ML để dễ dàng đào tạo các mô hình cho các nhiệm vụ phân loại văn bản hoặc gắn thẻ từ và các mô hình nhúng BERT đa ngôn ngữ mới cung cấp một lớp mã hóa đầu vào mạnh mẽ cho mục đích này.

Những mô hình này có thể là một ngôn ngữ hoặc đa ngôn ngữ.

Bạn cũng có thể sử dụng nhúng BERT làm lớp đầu vào cho bất kỳ mô hình nào bạn muốn đào tạo với PyTorch hoặc TensorFlow.

Cảm ơn bạn.

Bây giờ hãy ra ngoài và bắt đầu đào tạo một số người mẫu.

♪ ♪