10049

♪ ♪

Ben: Xin chào, tôi là Ben Levine, một kỹ sư trong đội Core ML.

Hôm nay, tôi sẽ nói về những gì mới khi tích hợp Core ML vào ứng dụng của bạn.

Xây dựng trải nghiệm thông minh trong ứng dụng của bạn chưa bao giờ dễ dàng hơn thế.

Xcode SDK cung cấp một nền tảng vững chắc để tận dụng và triển khai các tính năng hỗ trợ học máy.

Một tập hợp các khung tên miền cụ thể cung cấp cho bạn quyền truy cập vào trí thông minh tích hợp thông qua các API đơn giản.

Các khả năng mà họ cung cấp được cung cấp bởi các mô hình được đào tạo và tối ưu hóa bởi Apple.

Những mô hình này được thực thi thông qua Core ML.

Khung Core ML cung cấp công cụ để chạy các mô hình học máy trên thiết bị.

Nó cho phép bạn dễ dàng triển khai các mô hình tùy chỉnh cho ứng dụng của mình.

Nó tóm tắt các chi tiết phần cứng trong khi tận dụng khả năng tính toán hiệu suất cao của Apple silicon với sự trợ giúp từ dòng khung Accelerate và Metal.

Nhiệm vụ của Core ML là giúp bạn tích hợp các mô hình học máy vào ứng dụng của mình.

Năm nay, trọng tâm của chúng tôi đối với Core ML là hiệu suất và tính linh hoạt.

Chúng tôi đã cải thiện quy trình làm việc, bề mặt API và cả công cụ suy luận cơ bản của chúng tôi.

Trước khi nhảy vào quy trình làm việc và làm nổi bật các cơ hội mới để bạn tối ưu hóa tích hợp Core ML của mình, đây là ý tưởng về các lợi ích hiệu suất tiềm năng mà bạn có thể nhận được tự động chỉ bằng cách cập nhật lên hệ điều hành mới nhất.

Khi so sánh thời gian dự đoán tương đối giữa iOS 16 và 17, bạn sẽ thấy rằng iOS 17 đơn giản là nhanh hơn đối với nhiều kiểu máy của bạn.

Sự tăng tốc này trong công cụ suy luận đi kèm với hệ điều hành và không yêu cầu biên dịch lại các mô hình của bạn hoặc thực hiện bất kỳ thay đổi nào đối với mã của bạn.

Điều tương tự cũng đúng với các nền tảng khác.

Đương nhiên, số lượng tăng tốc phụ thuộc vào mô hình và phần cứng.

Chuyển sang chương trình nghị sự, tôi sẽ bắt đầu với tổng quan về quy trình làm việc khi tích hợp Core ML vào ứng dụng của bạn.

Trên đường đi, tôi sẽ làm nổi bật các cơ hội tối ưu hóa cho các phần khác nhau của quy trình làm việc.

Sau đó, tôi sẽ tập trung vào tích hợp mô hình và thảo luận về các API và hành vi mới để tính khả dụng của tính toán, vòng đời mô hình và dự đoán không đồng bộ. Tôi sẽ bắt đầu với tổng quan về quy trình làm việc Core ML.

Có hai giai đoạn để tích hợp Core ML vào ứng dụng của bạn.

Đầu tiên là phát triển mô hình của bạn, và thứ hai là sử dụng mô hình đó trong ứng dụng của bạn.

Để phát triển mô hình, bạn có một số lựa chọn.

Một trong những cách thuận tiện nhất để phát triển mô hình của riêng bạn là sử dụng Create ML.

Tạo ML cung cấp các mẫu khác nhau cho các tác vụ học máy phổ biến và có thể tận dụng các mô hình được tối ưu hóa cao được tích hợp trong hệ điều hành.

Nó hướng dẫn bạn qua quy trình phát triển mô hình và cho phép bạn đánh giá kết quả một cách tương tác.

Nếu bạn muốn tìm hiểu thêm, hãy xem video Tạo ML năm nay.

Một cách khác để phát triển một mô hình là đào tạo một mô hình bằng cách sử dụng một trong một số khung học máy python.

Sau đó, sử dụng gói python CoreMLTools để chuyển đổi sang định dạng mô hình Core ML.

Cuối cùng, điều quan trọng là bạn đánh giá mô hình của mình cả về độ chính xác và hiệu suất trên phần cứng Apple.

Sử dụng phản hồi từ đánh giá thường dẫn đến việc xem xét lại một số bước này để tối ưu hóa hơn nữa mô hình của bạn.

Có rất nhiều cơ hội để tối ưu hóa trong các bước này.

Đối với đào tạo, cách bạn thu thập và chọn dữ liệu đào tạo của mình rất quan trọng.

Nó phải phù hợp với dữ liệu được chuyển đến mô hình khi nó được triển khai và trong tay người dùng của bạn.

Kiến trúc mô hình bạn chọn cũng rất quan trọng.

Bạn có thể đang khám phá nhiều lựa chọn, mỗi lựa chọn có sự cân bằng riêng giữa các yêu cầu dữ liệu đào tạo, độ chính xác, kích thước và hiệu suất.

Nhiều sự đánh đổi trong số này có thể không được nhìn thấy đầy đủ tại thời điểm đào tạo và yêu cầu một vài lần lặp lại thông qua toàn bộ quá trình phát triển.

Tiếp theo là chuyển đổi mô hình.

Các công cụ Core ML cung cấp các tùy chọn khác nhau để giúp tối ưu hóa độ chính xác, dấu chân và chi phí tính toán của mô hình được chuyển đổi.

Bạn có thể chọn các định dạng đầu vào và đầu ra phù hợp nhất với luồng dữ liệu của ứng dụng để tránh các bản sao không cần thiết.

Nếu hình dạng đầu vào của bạn có thể thay đổi, bạn có thể chỉ định biến thể đó, thay vì chỉ chọn một hình dạng hoặc chuyển đổi giữa nhiều mô hình hình dạng cụ thể.

Độ chính xác tính toán cũng có thể được thiết lập rõ ràng cho toàn bộ mô hình hoặc các hoạt động riêng lẻ.

Cả float32 và float16 đều có sẵn.

Ngoài độ chính xác của tính toán, bạn cũng có một số quyền kiểm soát cách các tham số mô hình của bạn được biểu diễn.

CoreMLTools đi kèm với một bộ tiện ích để định lượng và nén trọng lượng sau tập luyện.

Những tiện ích này có thể giúp bạn giảm đáng kể dấu chân của mô hình và cải thiện hiệu suất trên thiết bị.

Tuy nhiên, để đạt được những lợi ích này, có một số đánh đổi về độ chính xác.

Có một số công cụ mới để giúp bạn trong không gian này Có một mô-đun con tối ưu hóa mới trong gói CoreMLTools.

Nó thống nhất và cập nhật các tiện ích nén sau đào tạo và thêm các tiện ích mở rộng đào tạo nhận biết lượng tử hóa mới cho PyTorch.

Điều này cho phép bạn truy cập vào tối ưu hóa dựa trên dữ liệu để giúp duy trì độ chính xác cho các mô hình lượng tử hóa trong quá trình đào tạo.

Điều này được kết hợp với các hoạt động mới hỗ trợ lượng tử hóa kích hoạt trong loại mô hình Chương trình ML của Core ML.

Kiểm tra phiên năm nay về nén các mô hình học máy với Core ML để tìm hiểu thêm.

Tiếp theo là đánh giá.

Một lựa chọn để đánh giá mô hình của bạn là chạy dự đoán trên mô hình đã chuyển đổi trực tiếp từ mã python của bạn với CoreMLTools.

Nó sẽ sử dụng cùng một ngăn xếp suy luận Core ML mà mã ứng dụng của bạn sẽ sử dụng và cho phép bạn nhanh chóng kiểm tra xem các lựa chọn của mình trong quá trình chuyển đổi mô hình ảnh hưởng như thế nào đến độ chính xác và hiệu suất của mô hình.

Xcode cũng cung cấp một số công cụ hữu ích khi đánh giá và khám phá các mô hình của bạn.

Xem trước mô hình có sẵn cho nhiều loại mô hình phổ biến.

Điều này cho phép bạn cung cấp một số đầu vào mẫu cho mô hình và xem trước đầu ra dự đoán mà không cần phải viết bất kỳ mã nào.

Báo cáo hiệu suất ML cốt lõi cung cấp cho bạn bảng phân tích hiệu suất tính toán mô hình cho thời gian tải, dự đoán và biên dịch trên bất kỳ thiết bị đính kèm nào.

Lưu ý rằng điều này có thể hữu ích để đánh giá kiến trúc mô hình ngay cả trước khi bạn đã đào tạo chúng.

Bây giờ, quay trở lại quy trình làm việc tổng thể, chủ đề tiếp theo là tích hợp mô hình.

Tích hợp mô hình là một phần trong việc phát triển ứng dụng của bạn.

Cũng giống như bất kỳ tài nguyên nào khác mà bạn sử dụng trong ứng dụng của mình, bạn muốn quản lý cẩn thận và tối ưu hóa cách bạn sử dụng mô hình Core ML của mình.

Có ba bước trong tích hợp mô hình.

Đầu tiên bạn viết mã ứng dụng để sử dụng mô hình.

Bạn có mã cho nơi và thời điểm tải mô hình, cách chuẩn bị dữ liệu đầu vào của mô hình, đưa ra dự đoán và sử dụng kết quả.

Sau đó bạn biên dịch mã này cùng với mô hình.

Và thứ ba, bạn kiểm tra, chạy và lập hồ sơ mô hình đang chạy trong ứng dụng của mình.

Khi nói đến hồ sơ, bạn có thể thấy các công cụ Core ML và Neural Engine hữu ích.

Đây cũng là một quá trình thiết kế và tối ưu hóa lặp đi lặp lại cho đến khi bạn sẵn sàng giao hàng.

Có một số bổ sung mới trong năm nay để tối ưu hóa việc tích hợp mô hình của bạn.

Đầu tiên là tính toán tính khả dụng.

Core ML được hỗ trợ trên tất cả các nền tảng của Apple và theo mặc định xem xét tất cả các tính toán có sẵn để tối ưu hóa việc thực thi của nó.

Điều này bao gồm CPU, GPU và Neural Engine khi có sẵn.

Tuy nhiên, các đặc tính hiệu suất và tính khả dụng của các thiết bị tính toán này khác nhau giữa phần cứng được hỗ trợ mà ứng dụng của bạn có thể chạy.

Điều này có thể ảnh hưởng đến trải nghiệm của người dùng với các tính năng được hỗ trợ ML của bạn hoặc ảnh hưởng đến sự lựa chọn của bạn trong các mô hình và cấu hình.

Ví dụ, một số kinh nghiệm có thể yêu cầu các mô hình chạy trên Động cơ thần kinh để đáp ứng các yêu cầu về hiệu suất hoặc công suất.

Hiện tại có một API mới để kiểm tra thời gian chạy về tính khả dụng của thiết bị tính toán.

MLComputeDevice enum nắm bắt loại thiết bị tính toán và các thuộc tính của thiết bị tính toán cụ thể trong giá trị liên quan của nó.

Với thuộc tính availableComputeDevices trên MLModel, bạn có thể kiểm tra những thiết bị nào có sẵn cho Core ML.

Ví dụ, mã này kiểm tra xem có sẵn Neural Engine hay không.

Cụ thể hơn, nó kiểm tra xem bộ sưu tập của tất cả các thiết bị tính toán có sẵn có chứa một thiết bị có loại là Neural Engine hay không.

Chủ đề tiếp theo để tích hợp mô hình là hiểu vòng đời mô hình.

Tôi sẽ bắt đầu bằng cách xem xét các loại tài sản mô hình khác nhau.

Có hai loại: mô hình nguồn và mô hình biên dịch.

Mô hình nguồn có phần mở rộng tệp của MLModel hoặc MLPackage.

Đó là một định dạng mở được thiết kế để xây dựng và chỉnh sửa.

Mô hình được biên dịch có phần mở rộng tệp của MLModelC.

Nó được thiết kế để truy cập thời gian chạy.

Trong hầu hết các trường hợp, bạn thêm một mô hình nguồn vào mục tiêu ứng dụng của mình, sau đó Xcode biên dịch mô hình và đưa nó vào tài nguyên của ứng dụng.

Trong thời gian chạy, để sử dụng mô hình của bạn, bạn khởi tạo một MLModel.

Instantiation lấy một URL đến dạng biên dịch của nó và một cấu hình tùy chọn.

MLModel kết quả đã tải tất cả các tài nguyên cần thiết để suy luận tối ưu dựa trên cấu hình được chỉ định và khả năng phần cứng dành riêng cho thiết bị.

Đây là cái nhìn sâu hơn về những gì xảy ra trong quá trình tải này.

Đầu tiên, Core ML kiểm tra bộ nhớ cache để xem liệu nó đã chuyên biệt hóa mô hình dựa trên cấu hình và thiết bị chưa.

Nếu có, nó sẽ tải các tài nguyên cần thiết từ bộ nhớ cache và trả về.

Đây được gọi là tải được lưu trong bộ nhớ cache.

Nếu cấu hình không được tìm thấy trong bộ nhớ cache, nó sẽ kích hoạt một bản biên dịch chuyên dụng cho thiết bị cho nó.

Khi quá trình này hoàn tất, nó sẽ thêm đầu ra vào bộ nhớ cache và kết thúc quá trình tải từ đó.

Đây được gọi là tải không có bộ nhớ cache.

Đối với một số mô hình nhất định, tải trọng không được lưu trong bộ nhớ cache có thể mất một khoảng thời gian đáng kể.

Tuy nhiên, nó tập trung vào việc tối ưu hóa mô hình cho thiết bị và thực hiện các lần tải tiếp theo nhanh nhất có thể.

Trong quá trình chuyên môn hóa thiết bị, Core ML trước tiên phân tích cú pháp mô hình và áp dụng các đường chuyền tối ưu hóa chung cho nó.

Sau đó, nó phân đoạn chuỗi hoạt động cho các thiết bị tính toán cụ thể dựa trên hiệu suất ước tính và tính khả dụng của phần cứng.

Phân đoạn này sau đó được lưu vào bộ nhớ cache.

Bước cuối cùng là để mỗi phân đoạn trải qua quá trình biên dịch cụ thể của thiết bị tính toán cho thiết bị tính toán mà chúng được chỉ định.

Bản tổng hợp này bao gồm các tối ưu hóa hơn nữa cho thiết bị tính toán cụ thể và xuất ra một tạo tác mà thiết bị tính toán có thể chạy.

Sau khi hoàn thành, Core ML lưu trữ các tạo tác này để sử dụng cho các lần tải mô hình tiếp theo.

Core ML lưu trữ các tài sản chuyên biệt trên đĩa.

Chúng được gắn với đường dẫn và cấu hình của mô hình.

Những tài sản này có nghĩa là tồn tại trong quá trình khởi chạy ứng dụng và khởi động lại thiết bị của bạn.

Khi dung lượng đĩa trống của thiết bị bị cạn kiệt, đã có bản cập nhật hệ thống hoặc mô hình được biên dịch đã bị xóa hoặc sửa đổi, hệ điều hành sẽ xóa bộ nhớ cache.

Nếu điều này xảy ra, lần tải mô hình tiếp theo sẽ thực hiện chuyên môn hóa thiết bị một lần nữa.

Để tìm hiểu xem tải mô hình của bạn có va vào bộ nhớ cache hay không, bạn có thể theo dõi ứng dụng của mình bằng Core ML Instrument và xem sự kiện tải.

Nếu nó có nhãn "chuẩn bị và lưu trữ", thì đó là một tải không được lưu trong bộ nhớ cache, vì vậy Core ML đã thực hiện chuyên môn hóa thiết bị và lưu trữ kết quả.

Nếu sự kiện tải có nhãn "được lưu trong bộ nhớ cache", thì đó là một tải được lưu trong bộ nhớ cache và không phát sinh chuyên môn hóa thiết bị.

Đây là sản phẩm mới đặc biệt dành cho các mô hình MLProgram.

Các báo cáo hiệu suất ML cốt lõi cũng có thể cung cấp cho bạn khả năng hiển thị chi phí tải.

Theo mặc định, nó hiển thị tải trung bình được lưu trong bộ nhớ cache.

Bây giờ nó cũng có tùy chọn hiển thị thời gian tải không được lưu trong bộ nhớ cache.

Vì việc tải một mô hình có thể tốn kém về độ trễ và bộ nhớ, đây là một số phương pháp hay nhất chung.

Đầu tiên, không tải các mô hình trong quá trình khởi chạy ứng dụng của bạn trên chuỗi giao diện người dùng.

Thay vào đó, hãy cân nhắc sử dụng API tải không đồng bộ hoặc lười biếng tải mô hình.

Tiếp theo, giữ cho mô hình được tải nếu ứng dụng có thể sẽ chạy nhiều dự đoán liên tiếp, thay vì tải lại mô hình cho mỗi dự đoán trong chuỗi.

Cuối cùng, bạn có thể dỡ mô hình nếu ứng dụng của bạn không sử dụng nó trong một thời gian.

Điều này có thể giúp giảm bớt áp lực bộ nhớ và nhờ bộ nhớ đệm, các lần tải tiếp theo sẽ nhanh hơn.

Khi mô hình của bạn được tải, đã đến lúc suy nghĩ về việc chạy các dự đoán với mô hình.

Tôi sẽ nhảy vào bản demo để hiển thị các tùy chọn không đồng bộ mới.

Để hiển thị API dự đoán không đồng bộ mới, tôi sẽ sử dụng một ứng dụng hiển thị thư viện hình ảnh và cho phép áp dụng các bộ lọc cho hình ảnh.

Tôi sẽ tập trung vào bộ lọc tô màu sử dụng mô hình Core ML lấy hình ảnh thang độ xám làm đầu vào và xuất ra phiên bản màu của hình ảnh.

Đây là một ví dụ về ứng dụng đang hoạt động.

Nó bắt đầu bằng cách tải các hình ảnh gốc, ở thang độ xám, và sau đó khi tôi chọn chế độ Hình ảnh được tô màu, nó sẽ tô màu các hình ảnh bằng Core ML.

Khi tôi cuộn xuống, mô hình chắc chắn đang hoạt động, nhưng nó chậm hơn một chút so với tôi mong đợi.

Ngoài ra, nếu tôi cuộn xuống xa, tôi nhận thấy rằng phải mất khá nhiều thời gian để các hình ảnh được tô màu.

Khi tôi cuộn lại, có vẻ như nó đang dành thời gian tô màu tất cả các hình ảnh trên đường đi.

Nhưng trong mã SwiftUI của tôi, tôi đang sử dụng LazyVGrid để giữ hình ảnh, vì vậy nó sẽ hủy các tác vụ khi chế độ xem tắt màn hình.

Hãy để tôi xem xét việc triển khai hiện tại của mình để cố gắng hiểu tại sao hiệu suất lại thiếu và cũng như tại sao nó không tôn trọng các nhiệm vụ bị hủy bỏ.

Đây là việc thực hiện.

Vì API dự đoán đồng bộ không an toàn cho luồng, ứng dụng phải đảm bảo rằng các dự đoán được chạy nối tiếp trên mô hình.

Điều này đạt được bằng cách biến ColorizingService thành một diễn viên, điều này sẽ chỉ cho phép một cuộc gọi đến phương thức colorize tại một thời điểm.

Diễn viên này sở hữu colorizerModel, là giao diện được tạo tự động được tạo ra cho mô hình đi kèm với ứng dụng.

Phương pháp tô màu hiện đang thực hiện hai thao tác.

Đầu tiên, nó chuẩn bị đầu vào cho mô hình, bao gồm thay đổi kích thước hình ảnh để phù hợp với kích thước đầu vào của mô hình.

Sau đó, nó chạy đầu vào thông qua mô hình và nhận được đầu ra được tô màu.

Tôi đã tiếp tục và ghi lại dấu vết Instruments của ứng dụng đang chạy với mẫu Core ML Instruments.

Khi nhìn vào dấu vết của Dụng cụ, nó cho thấy rằng các dự đoán được chạy nối tiếp, điều này được đảm bảo bởi sự cô lập của diễn viên.

Tuy nhiên, có những khoảng trống xung quanh mỗi dự đoán trước khi dự đoán tiếp theo được chạy, điều này góp phần vào việc thiếu hiệu suất.

Đây là kết quả của việc cô lập diễn viên được bao bọc xung quanh không chỉ dự đoán mô hình mà còn cả việc chuẩn bị đầu vào.

Một cải tiến sẽ là đánh dấu việc chuẩn bị đầu vào là một phương pháp không cô lập, vì vậy nó sẽ không chặn yêu cầu tô màu tiếp theo nhập vào diễn viên.

Mặc dù điều này sẽ hữu ích, nhưng bản thân các dự đoán Core ML vẫn sẽ được tuần tự hóa, đó là nút cổ chai trong quá trình xử lý của tôi.

Để tận dụng lợi thế của tính đồng thời cho bản thân các dự đoán Core ML, một tùy chọn tôi có thể xem xét là API dự đoán hàng loạt.

Nó cần một loạt các đầu vào và chạy chúng qua mô hình.

Dưới mui xe, Core ML sẽ tận dụng lợi thế của sự đồng thời khi có thể.

Tạo một phiên bản hàng loạt của phương pháp tô màu khá đơn giản.

Tuy nhiên, phần thách thức là tìm ra cách tôi sẽ thu thập các đầu vào thành một lô và chuyển chúng sang phương pháp này.

Thực tế có nhiều khía cạnh của trường hợp sử dụng này khiến việc sử dụng API dự đoán hàng loạt trở nên khó khăn.

API hàng loạt được sử dụng tốt nhất khi có một số lượng công việc đã biết phải được thực hiện.

Trong trường hợp này, số lượng hình ảnh được xử lý không cố định mà là một chức năng của kích thước màn hình và số lượng cuộn được thực hiện.

Tôi có thể tự chọn kích thước lô, nhưng tôi sẽ phải xử lý các trường hợp kích thước lô không được đáp ứng nhưng vẫn cần được xử lý.

Ngoài ra, tôi sẽ có một trải nghiệm giao diện người dùng khác, nơi hình ảnh được tô màu theo lô.

Cuối cùng, tôi sẽ không thể hủy một lô ngay cả khi người dùng cuộn ra khỏi nó.

Vì những thách thức này, tôi muốn gắn bó với một API xử lý từng dự đoán một.

Đây là nơi mà API dự đoán không đồng bộ mới có thể rất hữu ích.

Nó an toàn với luồng và hoạt động tốt khi sử dụng Core ML cùng với Swift concurrency.

Để chuyển sang thiết kế không đồng bộ cho mã, trước tiên tôi đã thay đổi phương pháp tô màu thành không đồng bộ.

Sau đó tôi đã thêm từ khóa await trước cuộc gọi dự đoán, được yêu cầu để sử dụng phiên bản API không đồng bộ mới.

Sau đó, tôi đã thay đổi ColorizingService thành một lớp học hơn là một diễn viên.

Bằng cách đó, nhiều hình ảnh có thể được tô màu đồng thời.

Cuối cùng, tôi đã thêm kiểm tra hủy vào phần đầu của phương pháp.

API dự đoán không đồng bộ sẽ cố gắng hết sức để phản hồi việc hủy bỏ, đặc biệt là khi nhiều dự đoán được yêu cầu đồng thời, nhưng tốt nhất là bao gồm một kiểm tra bổ sung khi bắt đầu trong trường hợp này.

Bằng cách đó, nó cũng tránh chuẩn bị đầu vào nếu nhiệm vụ bị hủy trước khi phương thức tô màu thậm chí được nhập.

Bây giờ tôi sẽ thực hiện những thay đổi này và chạy lại ứng dụng.

Cũng như trước đây, tôi sẽ đặt nó ở chế độ Tô màu.

Tôi đã có thể thấy những hình ảnh đang được tô màu nhanh hơn nhiều.

Và nếu tôi cuộn nhanh xuống dưới cùng, hình ảnh sẽ tải gần như ngay lập tức.

Cuộn lên một chút, tôi có thể xác minh các hình ảnh đang được tô màu khi tôi cuộn lại, điều đó có nghĩa là các cuộc gọi tô màu đã bị hủy thành công lần đầu tiên khi tôi vuốt nhanh xuống dưới cùng.

Khi nhìn vào một dấu vết sử dụng thiết kế không đồng bộ mới này, nó cho thấy các dự đoán đang được chạy đồng thời trên nhiều hình ảnh.

Điều này được biểu thị bằng nhiều khoảng dự đoán xếp chồng lên nhau theo chiều dọc.

Vì mô hình này chạy một phần trên Động cơ thần kinh, nó cũng có thể được quan sát thấy trong Dụng cụ Động cơ Thần kinh.

Với việc triển khai ban đầu, tô màu các hình ảnh nối tiếp, việc tô màu chế độ xem ban đầu của hình ảnh mà không cần cuộn mất khoảng hai giây.

Sau khi chuyển sang triển khai không đồng bộ, điều này tô màu đồng thời cho hình ảnh, thời gian đó đã bị cắt giảm một nửa xuống còn khoảng một giây.

Vì vậy, nhìn chung, tôi đã có thể đạt được sự cải thiện gấp 2 lần trong tổng thông lượng bằng cách tận dụng API dự đoán không đồng bộ và tính đồng thời với mô hình Colorizer của mình.

Tuy nhiên, điều quan trọng cần lưu ý là số tiền mà một mô hình và trường hợp sử dụng nhất định có thể được hưởng lợi từ thiết kế đồng thời phụ thuộc rất nhiều vào một số yếu tố, bao gồm hoạt động của mô hình, đơn vị tính toán và kết hợp phần cứng và các công việc khác mà các thiết bị tính toán có thể đang bận rộn.

Ngoài ra, chương trình ML và các loại mô hình Đường ống sẽ cung cấp những cải tiến hiệu suất tốt nhất từ việc chạy dự đoán đồng thời.

Nhìn chung, khi thêm đồng thời vào ứng dụng của bạn, bạn nên cẩn thận lập hồ sơ khối lượng công việc để đảm bảo rằng nó thực sự mang lại lợi ích cho trường hợp sử dụng của bạn.

Một điều quan trọng khác cần lưu ý khi thêm đồng thời vào ứng dụng của bạn là việc sử dụng bộ nhớ.

Có nhiều bộ đầu vào và đầu ra mô hình được tải đồng thời trong bộ nhớ có thể làm tăng đáng kể mức sử dụng bộ nhớ cao nhất của ứng dụng của bạn.

Bạn có thể lập hồ sơ này bằng cách kết hợp Công cụ Core ML với Công cụ Phân bổ.

Dấu vết cho thấy việc sử dụng bộ nhớ của ứng dụng của tôi đang tăng lên nhanh chóng khi tôi tải nhiều đầu vào vào bộ nhớ để chạy qua mô hình tạo màu.

Một vấn đề tiềm ẩn là phương thức tô màu từ mã của tôi không có kiểm soát luồng, vì vậy số lượng hình ảnh được tô màu đồng thời không có giới hạn cố định.

Đây có thể không phải là vấn đề nếu đầu vào và đầu ra của mô hình nhỏ.

Tuy nhiên, nếu chúng lớn, thì việc có nhiều bộ đầu vào và đầu ra này trong bộ nhớ cùng một lúc có thể làm tăng đáng kể mức sử dụng bộ nhớ cao nhất của ứng dụng.

Một cách để cải thiện điều này là thêm logic giới hạn số lượng dự đoán tối đa trên chuyến bay.

Điều này sẽ dẫn đến ít đầu vào và đầu ra được tải đồng thời trong bộ nhớ hơn, điều này sẽ làm giảm mức sử dụng bộ nhớ cao nhất trong khi chạy dự đoán.

Trong ví dụ này, nếu đã có hai mục đang được xử lý, nó sẽ trì hoãn các mục công việc mới cho đến khi mục trước đó hoàn thành.

Chiến lược tốt nhất sẽ phụ thuộc vào trường hợp sử dụng của bạn.

Ví dụ, khi truyền dữ liệu từ máy ảnh, bạn có thể chỉ muốn bỏ công việc thay vì trì hoãn nó.

Bằng cách này, bạn tránh tích lũy khung hình và thực hiện công việc không còn phù hợp về mặt thời gian.

Lùi lại một chút, đây là một số hướng dẫn chung về thời điểm sử dụng các API dự đoán khác nhau.

Nếu bạn đang ở trong một ngữ cảnh đồng bộ và thời gian giữa mỗi đầu vào có sẵn là lớn so với độ trễ của mô hình, thì API dự đoán đồng bộ hóa hoạt động tốt.

Nếu đầu vào của bạn có sẵn theo lô, thì API dự đoán hàng loạt là một sự phù hợp tự nhiên.

Nếu bạn đang ở trong một ngữ cảnh không đồng bộ và có một lượng lớn đầu vào trở nên có sẵn riêng lẻ theo thời gian, đó là khi API không đồng bộ có thể hữu ích nhất.

Để kết thúc, khi bạn chuyển qua quy trình làm việc Core ML, có rất nhiều cơ hội để tối ưu hóa trong cả quá trình phát triển mô hình và tích hợp mô hình.

Các API tính khả dụng tính toán mới có thể giúp bạn đưa ra quyết định trong thời gian chạy dựa trên phần cứng nào có sẵn trên thiết bị.

Hiểu được vòng đời của mô hình và hành vi bộ nhớ đệm có thể giúp bạn quyết định tốt nhất khi nào và ở đâu để tải và dỡ mô hình của mình.

Và cuối cùng, API dự đoán không đồng bộ có thể giúp bạn tích hợp Core ML với mã Swift không đồng bộ khác và cũng cải thiện thông lượng bằng cách hỗ trợ các dự đoán đồng thời.

Đây là Ben từ đội Core ML, và tôi không phải là AI.