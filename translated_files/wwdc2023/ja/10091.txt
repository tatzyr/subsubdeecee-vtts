10091

♪まろやかなインストゥルメンタルヒップホップ♪

♪

オミド・カリリ:こんにちは!私の名前はオミッドです。

オリバーと私はARKitチームのエンジニアであり、iOS ARアプリを新しいプラットフォームに持ち込む際に知っておく必要がある概念（馴染みのあるものと新しいもの）を見直すことに興奮しています。

ARKitは2017年にiOSで導入され、拡張現実アプリケーションを構築するための3つの重要な概念を導入しました。

ワールドトラッキングにより、ARKitは6つの自由度で世界におけるデバイスの位置を追跡することができます。

これにより、現実世界への位置と向きで仮想コンテンツを固定することができます。

シーンの理解は、あなたの周りの現実世界についての洞察を提供します。

提供されたジオメトリとセマンティック知識を使用して、コンテンツをインテリジェントに配置し、環境と現実的に対話することができます。

最後に、レンダリングエンジンは、ARKitが提供するカメラ変換と本質を利用して、キャプチャされた画像上に仮想コンテンツを正しく登録して合成することができます。

当初は、iOSでARKitのカメラ変換とレンダリング3Dコンテンツを使用するためのSceneKitビューから始めました。

その後、RealityKitを導入し、非常に現実的な物理ベースのレンダリングと周囲との正確なオブジェクトシミュレーションが可能なエンジンの基礎をレイアウトしました。

空間コンピューティングを可能にするために、ARKitとRealityKitは成熟し、オペレーティングシステムに深く統合されています。

たとえば、ARKitのトラッキングとシーン理解は現在、システムサービスとして実行されており、ウィンドウの配置から空間オーディオまですべてをバックアップしています。

このシステムは、以前はアプリケーションに属していた責任を負います。

ユーザーの手のカメラパススルーとマットが組み込まれているため、アプリケーションはこれらの機能を無料で利用できます。

もう1つの組み込み機能は、ARKitワールドマップがシステムサービスによって継続的に永続化されるため、アプリケーションがもうそれを行う必要がないことです。

これにより、このプラットフォームで可能な限り最高のアプリケーションとコンテンツの構築に集中できると信じています。

以下は、このプラットフォームで導入された新しい機能とともに、これらの機能を実証する例です。

たとえば、ARKitはアプリにハンドトラッキングを提供するようになりました。これにより、人々は周囲と対話できる仮想コンテンツに手を差し伸べて直接対話することができます。

この新しいプラットフォームが提供するすべての新機能と没入型体験を活用するには、iOS ARKitベースのエクスペリエンスを更新する必要があります。

これは、空間コンピューティングのためのアプリとAR体験を再考する絶好の機会です。

この移行の一環として、ARKitとRealityKitで導入した使い慣れた概念を使用します。

これらの概念がどのように引き継がれたか、どのように進化したか、そしてそれらをどのように活用できるかについて説明します。

始めましょう！

まず、空間コンピューティングのためにアプリを提示するいくつかの新しい方法を探り、利用可能な新しいコンテンツツールを紹介します。

次に、コンテンツをレンダリングして操作するために使用するエンジンであるリアリティキットについて説明します。

RealityViewが、iOSのARViewと同様に、アプリが空間コンピューティングをどのように活用できるかを見ていきます。

次に、アプリがコンテンツを人々の環境に持ち込むさまざまな方法について説明します。

レイキャスティングは、多くのiOSアプリケーションがコンテンツを配置するために使用するものです。

ARKitデータとRealityKitを組み合わせて、空間コンピューティングのレイキャスティングを有効にする方法の例を紹介します。

そして最後に、ARKitのアップデートを確認し、iOSから使い慣れた概念を活用する新しい方法を見ていきます。

空間コンピューティングのためにあなたの経験を移行する準備をしましょう。

空間コンピューティングを使用すると、iOS ARエクスペリエンスをウィンドウを超えて拡張できます。

このプラットフォームは、iOS体験をもたらす際に検討したいアプリケーションを提示する新しい方法を提供します。

これは、Hello Worldのサンプルアプリの例です。

ウィンドウや3次元コンテンツを含むUIを周りのどこにでも表示できるようになりました。

デフォルトでは、このプラットフォーム上のアプリケーションは共有スペースに起動します。

共有スペースは、Macデスクトップ上の複数のアプリのように、アプリが並んで存在する場所です。

共有スペース内では、アプリは1つ以上のウィンドウを開いてコンテンツを表示できます。

さらに、アプリは3次元ボリュームを作成できます。

たとえば、利用可能なボードゲームのリストを1つのウィンドウに表示し、ルールを別のウィンドウに表示し、選択したゲームを独自のボリュームで開くことができます。

このゲームは、サファリのウィンドウを開いたままプレイして、勝利戦略を読むことができます。

ウィンドウとボリュームに追加するコンテンツは、他のアプリケーションとスペースを共有できるように、その範囲内に含まれるままです。

場合によっては、あなたのアプリがあなたの経験への没入のレベルをよりコントロールしたいかもしれません - 多分あなたの部屋と相互作用するゲームをプレイするために。

このため、アプリは、アプリのウィンドウ、ボリューム、および3Dオブジェクトのみが表示される専用のフルスペースを開くことができます。

フルスペースに入ると、アプリケーションはより多くの機能にアクセスできます。

RealityKitのアンカーエンティティを使用すると、テーブル、床、さらには手のひらや手首などの手の一部などの周囲にオブジェクトをターゲットにして取り付けることができます。

アンカーエンティティは、ユーザーの許可を必要とせずに動作します。

ARKitデータは、アプリがフルスペースでしかアクセスできない他のものです。

許可を得て、ARKitは現実世界の表面、シーンジオメトリ、骨格ハンドトラッキングに関するデータを提供し、現実的な物理学と自然な相互作用のためのアプリの能力を拡大します。

Windows、ボリューム、スペースはすべてSwiftUIシーンタイプです。

これらについて学ぶべきことはもっとたくさんあります。

まず第一に、ここで言及されているセッションに行くことができます。

次に、空間コンピューティングに持ち込むためのコンテンツを準備するために必要な主なステップを確認しましょう。

iOSでの思い出に残るAR体験は、素晴らしい3Dコンテンツから始まります。このプラットフォームでの空間体験にも同じことが言えます。

そして、3Dコンテンツに関しては、Universal Scene Description、略してUSDのようなオープンスタンダードに頼るのは素晴らしいことです。

USDは生産が証明されており、単一の資産を作るクリエイターからAAAゲームや映画に取り組む大規模なスタジオまで拡大しています。

Appleは米ドルを早期に採用し、2017年に当社のプラットフォームに追加し、それ以来サポートが増加しています。

今日、USDは空間コンピューティングの3Dコンテンツの中心です。

USDアセットの準備ができたら、新しい開発者ツールであるReality Composer Proに持ち込んで、3Dコンテンツを構成、編集、プレビューすることができます。

iOSで3DコンテンツにCustomMaterialsを使用している場合は、シェーダーグラフを使用して再構築する必要があります。

また、UIから直接RealityKitコンポーネントを編集することもできます。

そして最後に、Reality Composer ProプロジェクトをXcodeに直接インポートして、すべてのUSDアセット、マテリアル、カスタムコンポーネントをXcodeプロジェクトに簡単にバンドルできます。

Reality Composer Proの詳細と、空間コンピューティング用の独自のカスタム資料を構築する方法を学ぶのに役立つ素晴らしいセッションがいくつかあります。

アプリケーションを提示するさまざまな方法を見たので、体験をもたらす際にRealityViewが提供する機能について詳しく学びましょう。

空間コンピューティングにより、アプリがあなたのスペースにコンテンツを表示する方法を見ました。

iOSから生じる重要な違いの1つは、異なる要素を並べて提示する方法です。

3Dコンテンツと2D要素がどのように表示され、互いに連携するかに注目してください。

iOSから来て、あなたはこれらのそれぞれを作成するために使い慣れたフレームワークを使用します。

SwiftUIを使用して、最高の2D UIを構築し、iOSのようなシステムジェスチャーイベントを取得します。

また、RealityKitを使用して、空間体験のために3Dコンテンツをレンダリングします。

これら両方と同時にインターフェースする方法は、空間コンピューティングのユニークなニーズに応えるために導入している新しいSwiftUIビューであるRealityViewです。

RealityViewはSwiftUIとRealityKitを真に橋渡しし、2Dと3Dの要素を組み合わせて、思い出に残る空間体験を作り出すことができます。

RealityViewを使用して、表示および対話するすべてのエンティティを保持します。

ジェスチャーイベントを取得し、ビューのエンティティに接続して制御できます。

また、ARKitのシーン理解にアクセスすることで、RealityKitの衝突コンポーネントを使用して、人々の周囲や手との現実的なシミュレーションを有効にすることができます。

RealityKitの使用がiOSからどのように引き継がれているかを見る前に、RealityKitのエンティティコンポーネントシステムの操作方法について簡単に復習しましょう。

リアリティキットエンティティコンポーネントシステムでは、各エンティティは3Dコンテンツのコンテナです。

さまざまなコンポーネントがエンティティに追加され、その外観と動作が定義されます。

これには、レンダリング方法のモデルコンポーネント、他のエンティティと衝突する方法の衝突コンポーネントなどが含まれます。

RealityComposer Proを使用して、衝突コンポーネントなどのRealityKitコンポーネントを準備し、エンティティに追加することができます。

システムには、必要なコンポーネントを持つエンティティに作用するコードが含まれています。

たとえば、ジェスチャーサポートに必要なシステムは、CollisionComponentとInputTargetComponentを持つエンティティでのみ動作します。

RealityViewが空間コンピューティングに使用する概念の多くは、iOSのARViewの概念から引き継がれています。

この2つがどのように積み重なるか見てみましょう。

どちらのビューも、アプリに表示したいエンティティを保持するためのイベント対応コンテナです。

ビューにジェスチャーサポートを追加して、エンティティとの選択とインタラクションを有効にすることができます。

空間コンピューティング用のSwiftUIを使用すると、エンティティを選択またはドラッグすることができます。

ARViewとRealityViewの両方が、エンティティのコレクションを提供します。

ARViewはこれのためにシーンを使用します。

RealityViewには、エンティティを追加するコンテンツがあります。

AnchorEntitiesを追加して、コンテンツを現実世界に固定することができます。

どちらのプラットフォームでも、コンテンツモデルをロードするエンティティと、それを配置するAnchorEntityを作成します。

プラットフォームの主な違いの1つは、アンカーエンティティの動作です。

iOS上のARViewはARSessionを使用しており、アプリはアンカーエンティティが機能するために必要なシーン理解アルゴリズムを実行する許可を受ける必要があります。

RealityViewは、システムサービスを使用してanchorEntitiesを有効にしています。

これは、空間体験が許可を必要とせずに周囲にコンテンツを固定できることを意味します。

このアプローチを使用するアプリは、基礎となるシーンの理解データや変換を受信しません。

アプリがコンテンツを配置するための変換データを持っていないことは、オリバーが後で彼のセクションで話すいくつかの意味合いがあります。

ご覧のように、iOSから引き継がされる多くのおなじみの概念がありますが、RealityKitが空間コンピューティングに提供する新機能もあります。

私たちは、この新しいプラットフォームでRealityKitで何が可能かの表面を引っ掻いただけであり、あなたはより多くのフォローアップのために以下のセッションをチェックしたいかもしれません。

さて、オリバーがRealityViewとiOSからコンテンツを持ち込む方法について詳しく話します。

オリバー・ダンクレー:ありがとう、オミッド!

既存のコンテンツを空間コンピューティングに持ち込むさまざまな方法を探り続けましょう。

共有スペースから始めましょう。

ウィンドウやボリュームに3Dコンテンツを追加し、システムジェスチャーを使用して操作することができます。

アセットを表示するには、RealityViewのコンテンツに直接追加するだけです。

これを行うには、モデルコンポーネントを保持するエンティティを作成し、変換コンポーネントを設定して配置します。

ジェスチャーサポートを設定して、変換コンポーネントを変更することもできます。

ビューのコンテンツに追加されたすべてのエンティティは、スペースの原点に対して同じスペースに存在するため、互いに対話できることに注意してください。

共有スペースでは、コンテンツを周囲に固定することはできません。

アプリをフルスペースに移行する場合は、オプションを検討しましょう。

共有スペースからの主な違いの1つは、アプリが人々の周囲にコンテンツをさらに固定できるようになったことです。

ここでコンテンツを固定することは、2つの方法で行うことができます。

まず、RealityKitのAnchorEntityを使用して、アプリでARKitデータを使用する許可を必要とせずにコンテンツを配置することを見てみましょう。

RealityKitのAnchorEntitiesを使用すると、システムがコンテンツを見つけて自動的にアンカーするターゲットを指定できます。

たとえば、目の前のテーブル面に3Dモデルを配置するには、ターゲットをテーブルに設定したRealityKit AnchorEntityを使用できます。

iOSとは異なり、AnchorEntitiesはユーザー許可を求めることなく使用できます。

人々のプライバシーは、AnchorEntityの根底にある変換をアプリケーションと共有しないことによって保護されます。

注：これは、異なるアンカーエンティティの子がお互いを認識していないことを意味します。

anchorEntitiesの初心者は、手をターゲットにすることができ、興味深い相互作用の機会の全く新しい領域を開きます。

たとえば、コンテンツを手のひらに固定して、手を動かすときに手を追うことができます。

これはすべて、人の手が実際にどこにあるかをアプリに伝えることなく、システムによって行われます。

AnchorEntitysは、アプリがコンテンツを人々の周囲に固定するための迅速でプライバシーに優しい方法を提供します。

フルスペースに戻ると、ARKitを活用して、人々の環境に関するシステムレベルの知識を組み込むこともできます。

これにより、独自のカスタム配置ロジックを構築できます。

これがどのように機能するかを見てみましょう。

iOSと同様に、アプリケーションはシーン理解データのアンカー更新を受け取ります。

このアンカーデータをアプリのロジックに統合して、あらゆる種類の素晴らしい体験を実現できます。

たとえば、平面の境界を使用して、コンテンツを中央に配置して配布することができます。

または、飛行機とその分類を使用して、2つの壁と床の交差点を探して部屋の隅を見つけることもできます。

コンテンツをどこに配置するかを決めたら、ARKitが追跡するためのワールドアンカーを追加し、それを使用してエンティティの変換コンポーネントを更新します。

これにより、基礎となる世界地図が更新されるにつれて、コンテンツが現実世界に固定されたままになるだけでなく、アンカーの永続性への扉も開きます。

あなたのスペースに追加されたすべてのエンティティは、周囲だけでなく、互いに相互作用することができます。

シーン理解アンカーは、空間の原点に対する変換で配信されるため、これはすべて機能します。

ARKit機能を使用するには、ユーザーの許可が必要です。

ARKitデータをアプリロジックに統合することで、より高度な機能を有効にする方法を見ました。

これまでのところ、あなたのアプリにコンテンツを配置させることについて話しました。

人々に配置を指導させる方法を探りましょう。

iOSでは、レイキャストを使用して2D入力を3D位置に変換できます。

しかし、この新しいプラットフォームでは、手を使って経験と直接対話できるため、この2D-3Dブリッジはもう必要ありません。

レイキャスティングは依然として強力です。それは人々が腕の長さを超えて手を差し伸べることを可能にします。

レイキャスティングを設定するにはさまざまな方法があります。

基本的に、RealityKitの衝突コンポーネントをレイキャストに設定する必要があります。

衝突コンポーネントは、ARKitのメッシュアンカーから人々の周囲に対してレイキャストするために作成することもできます。

空間コンピューティングのためにレイキャストする方法の2つの例を探りましょう。1つ目はシステムジェスチャーを使用し、2つ目は針データを使用します。

ポジションを取得した後、コンテンツを固定したままにするためにARKit worldAnchorを配置することができます。

次の例を考えてみましょう。

私たちのアプリがモデラーのための感動的な3Dアセットを配置することを中心に展開していると想像してみてください。

たぶん、この特定のシナリオでは、ある人が私たちのアプリを使用して、モデリングプロジェクトのためにワークベンチに仮想船を配置したいと考えています。

これが私たちの船を置きたい作業台です。

空のRealityViewから始めます。

ARKitのシーン理解は、周囲を表現するために使用するメッシュアンカーを提供します。

それらは私たちが使用できる幾何学と意味情報を提供します。

シーン再構築データのメッシュは、一連のチャンクとして配信されることを忘れないでください。

このメッシュチャンクを表すエンティティを作成し、メッシュアンカーの変換を使用して、このエンティティを完全なスペースに正しく配置します。

その後、私たちのエンティティは、テストを打つために衝突コンポーネントを必要とします。

RealityKitのShapeResourcesメソッドを使用して、エンティティのmeshAnchorから衝突形状を生成します。

次に、ヒットテストをサポートする正しく配置されたエンティティを追加します。

受け取った各メッシュチャンクのエンティティと衝突コンポーネントを構築し、すべての環境を表します。

シーンの再構築が洗練されるにつれて、メッシュを更新したり、チャンクを削除したりすることがあります。

これらの変更についても、エンティティを更新する準備ができているはずです。

私たちは今、周囲を代表するエンティティのコレクションを持っています。

これらのエンティティはすべて衝突コンポーネントを持ち、レイキャストテストをサポートできます。

まず、システムジェスチャーを使用してレイキャストを探索し、その後、手のデータを使用して例を続けましょう。

レイキャストし、システムのジェスチャーを使用して船を配置する位置を得ることができます。

ジェスチャーは、CollisionコンポーネントとInputTargetコンポーネントの両方を持つエンティティとのみ対話できるため、各メッシュエンティティに1つを追加します。

RealityViewにSpatialTapGestureを追加することで、人々はエンティティを見てタップすることでレイキャストすることができます。

この結果として生じるイベントは、人々がタップしたときに見た場所を表す世界空間での地位を保持しています。

システムのジェスチャーを使用する代わりに、ARKitのハンドアンカーを使用してレイを構築することもできます。

一歩下がって、このオプションを探りましょう。

人々がどこを指しているのかを知るには、まずその人の手の表現が必要です。

ARkitの新しいハンドアンカーは、私たちが必要とするすべてを提供します。

フィンガージョイント情報を使用して、クエリの光線の起源と方向を構築できます。

光線の起源と方向がわかったので、シーンのエンティティに対してレイキャストを行うことができます。

結果として得られるCollisionCastHitは、ヒットしたエンティティとその位置と表面法線を提供します。

コンテンツを配置するための世界の位置を特定したら、ARKitのワールドアンカーを追加して、この位置を継続的に追跡します。

ARKitは、世界地図が洗練されるにつれて、この世界アンカーの変換を更新します。

船のモデルをロードするための新しいエンティティを作成し、ワールドアンカーアップデートを使用してその変換を設定し、ユーザーが望む場所に配置することができます。

最後に、エンティティをコンテンツに追加して、ワークベンチ上でレンダリングすることができます。

ARKitが追加したワールドアンカーを更新するたびに、船舶エンティティの変換コンポーネントを更新し、現実世界に固定されたままであることを確認します。

そして、それだけです!

私たちは手を使って周囲の場所を指し、そこにコンテンツを配置しました。

レイキャスティングは、コンテンツを配置するだけでなく、コンテンツとのやり取りにも役立ちます。

私たちの仮想船に対してレイキャストするのに何が必要か見てみましょう。

RealityKitの衝突コンポーネントは非常に強力です。

Reality Composer Proが私たちを助けることができる適切な衝突コンポーネントを追加するだけで、船のエンティティを衝突に参加させることができます。

船の衝突コンポーネントを有効にし、最新のハンドジョイント位置から新しい光線を構築した後、別のレイキャストを行い、ユーザーが船とテーブルのどちらを指しているかを知ることができます。

前の例では、RealityKitの機能とARKitのシーン理解を組み合わせて、真に魅力的な体験を構築するパワーと汎用性を示しました。

ARkitの使用が空間コンピューティングでどのように変化したかを見てみましょう。

基本的に、iOSと同様に、ARKitはアンカーの更新を受信するセッションを実行することで機能します。

セッションの設定と実行、アンカーの更新の受信、ワールドアンカーの永続化方法は、この新しいプラットフォームで変更されました。

見てみましょう！ 

iOSでは、ARKitはさまざまな構成から選択できます。

各構成には、あなたの経験に必要な機能がバンドルされています。

たとえば、ここではARWorldTrackingConfigurationを選択し、メッシュのsceneReconstructionと平面のplaneDetectionを有効にします。

その後、ARSessionを作成し、選択した構成で実行できます。

この新しいプラットフォームでは、ARKitは各シーン理解機能のデータプロバイダーを公開するようになりました。

ハンドトラッキングは、ARKitが提供する新しい機能であり、独自のプロバイダーも取得します。

各データプロバイダーの初期化子は、そのプロバイダーインスタンスを設定するために必要なパラメータを取ります。

プリセット構成のカタログから選択する代わりに、アプリケーションに必要なプロバイダーをアラカルトで選択できます。

たとえば、メッシュアンカーを受信するSceneReconstructionProviderと、平面アンカーを受信するPlaneDetectionProviderを選択します。

プロバイダーを作成し、受信したいメッシュ分類と平面タイプを初期化します。

次に、ARKitSessionを作成し、インスタンス化されたプロバイダーで実行します。

セッションの設定がどのように簡素化されたかを見たので、これらの新しいデータプロバイダーがアプリが実際にARKitデータを受信する方法をどの方法で変更するかを理解しましょう。

iOSでは、1人のデリゲートがアンカーとフレームの更新を受け取ります。

アンカーは、カメラフレームとアンカーを同期させるためにARFramesで集約され、配信されます。

アプリケーションは、カメラのピクセルバッファを表示し、カメラ変換を使用して追跡された仮想コンテンツを登録およびレンダリングする責任があります。

メッシュアンカーとプレーンアンカーはベースアンカーとして配信され、どれがどれであるかを把握するためにそれらを曖昧にするのはあなた次第です。

私たちの新しいプラットフォームでは、アンカーアップデートを提供するのはデータプロバイダーです。

以前に設定したプロバイダーは次のとおりです。

ARKitSessionを実行すると、各プロバイダーはすぐにアンカーアップデートの非同期公開を開始します。

SceneReconstructionProviderはmeshAnchorsを提供し、planeDetectionProviderはPlaneAnchorsを提供します。

曖昧さ回避は必要ありません!

アンカーの更新は、利用可能になるとすぐに提供され、他のデータプロバイダーの更新から切り離されます。

ARFrameはもはや提供されていないことに注意することが重要です。

空間コンピューティングアプリケーションは、システムによって自動的に行われるため、コンテンツを表示するためにフレームやカメラデータを必要としません。

ARFrameでアンカーアップデートをパッケージ化することなく、ARKitはそれらをすぐに配信し、レイテンシを削減し、アプリケーションが周囲のアップデートにすばやく反応できるようにします。

次に、ワールドアンカーの永続性について話しましょう。

あなたはこれらの変化を気に入るはずです!

レイキャスティングの例では、ワールドアンカーを使用して、仮想コンテンツを現実世界の位置に配置およびアンカーしました。

アプリはこれらのアンカーを永続化することができ、デバイスが同じ環境に戻ったときに自動的に再び受信できるようにします。

まず、iOSで永続性がどのように機能したかを簡単に要約しましょう。

iOSでは、世界地図とアンカーの永続性を処理するのはアプリケーションの責任です。

これには、追加されたアンカーを使用してARKitの世界地図を要求して保存し、適切なタイミングで正しい世界地図をリロードするロジックを追加し、以前に持続したアンカーを受け取り、アプリケーションエクスペリエンスを継続する前に再ローカリゼーションが完了するのを待つことが含まれます。

この新しいプラットフォームでは、システムは世界地図をバックグラウンドで継続的に保持し、人々が移動するにつれて既存の地図にシームレスにロード、アンロード、作成、再ローカライズします。

あなたのアプリケーションはもうマップを処理する必要はありません、システムは今あなたのためにそれを行います!

ワールドアンカーを使用して仮想コンテンツの場所を永続化することに集中するだけです。

コンテンツを配置するときは、新しいWorldTrackingProviderを使用してWorldAnchorsを世界地図に追加します。

システムはこれらを自動的に保存します。

WorldTrackingProviderは、これらの世界アンカーの追跡ステータスと変換を更新します。

WorldAnchor識別子を使用して、対応する仮想コンテンツをロードまたはアンロードできます。

私たちは、あなたがiOSから知っていたARKitの原則のいくつかのアップデートを強調しましたが、探求すべきことはもっとたくさんあります!

コード例など、より深く掘り下げるには、「空間コンピューティングのためのARKitの出会い」を見ることをお勧めします。

このセッションを締めくくりましょう!

このセッションでは、ARKitとRealityKitの概念がiOSからどのように進化したか、考慮する必要がある変更点、および詳細についてはどのセッションを見るべきかについて、高レベルの理解を提供しました。

このプラットフォームは、iOSアプリが処理しなければならなかった多くのタスクを引き受け、すでに慣れ親しんでいるフレームワークやコンセプトを使用して、美しいコンテンツや体験を構築することに本当に集中することができます。

私たちは、あなたがあなたのアプリを進化させるために空間コンピューティングとそのすべての驚くべき機能をどのように活用するかを見て興奮しています!

見てくれてありがとう!

♪