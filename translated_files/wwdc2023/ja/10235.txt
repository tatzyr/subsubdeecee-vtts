10235

♪ ♪

ジュリアン:こんにちは、「音声処理の新機能」へようこそ。私はコアオーディオチームのジュリアンです。

Voice over IPアプリケーションは、人々が同僚、友人、家族とのつながりを維持するために、これまで以上に不可欠になっています。

ボイスチャットのオーディオ品質は、優れたユーザーエクスペリエンスを提供する上で重要な役割を果たします。

あらゆる状況下で素晴らしく聞こえるオーディオ処理を実装することは重要ですが、困難です。

そのため、Appleは音声処理APIを提供しているため、どのような音響環境にいるか、どのApple製品を使用しているか、どのオーディオアクセサリに接続されているかに関係なく、アプリでチャットするとき、誰もが常に可能な限り最高のオーディオ体験を楽しむことができます。

Appleの音声処理APIは、独自のFaceTimeや電話アプリなど、多くのアプリで広く使用されています。

ボイスチャットオーディオを強化するために、エコーキャンセル、ノイズ抑制、自動ゲイン制御など、クラス最高のオーディオ信号処理を提供します。

その性能は、独自の音響特性を考慮して、各タイプのオーディオデバイスと組み合わせて、各Apple製品モデルの音響エンジニアによって調整されています。

Appleの音声処理APIを選択すると、ユーザーはStandard、Voice Isolation、Wide Spectrumなど、アプリのマイクモード設定を完全に制御できます。

Voice over IPアプリケーションには、Appleの音声処理APIを使用することを強くお勧めします。

Appleの音声処理APIには2つのオプションがあります。

最初のオプションは、AUVoiceProcessingIOとも呼ばれるAUVoiceIOと呼ばれるI/Oオーディオユニットです。

このオプションは、I/Oオーディオユニットと直接対話する必要があるアプリ向けです。

2番目のオプションはAVAudioEngineで、より具体的にはAVAudioEngineの「音声処理」モードを有効にします。

AVAudioEngineはより高いレベルのAPIです。

一般的に使いやすく、オーディオで作業するときに書かなければならないコードの量を減らします。

どちらのオプションも同じ音声処理機能を提供します。

さて、何が新しいのですか?

音声処理APIをtvOSで初めて利用できるようにしています。

詳細については、セッション「Discover Continuity Camera for tvOS」をご覧ください。

また、AUVoiceIOとAVAudioEngineにいくつかの新しいAPIを追加して、音声処理をより詳細に制御し、新機能を実装するのに役立ちます。

最初のAPIは、他のオーディオのダッキング動作を制御するのに役立ちます。それが何を意味するのかをすぐに説明します。

2番目のAPIは、アプリにミュートされたトーカー検出機能を実装するのに役立ちます。

このセッションでは、これら2つの新しいAPIの詳細に焦点を当てます。

私が話したい最初のAPIは「その他のオーディオダッキング」です。このAPIに飛び込む前に、まず他のオーディオとは何か、なぜダッキングが重要なのかを説明しましょう。

Appleの音声処理APIを使用する場合は、再生オーディオに何が起こっているのかを見てみましょう。

あなたのアプリは、Appleの音声処理で処理され、出力デバイスで再生されるボイスチャットオーディオストリームを提供しています。

ただし、他のオーディオストリームが同時に再生される可能性があります。

たとえば、アプリは音声処理APIを介してレンダリングされていない別のオーディオストリームを再生している可能性があります。

また、アプリと同時にオーディオを再生する他のアプリがあるかもしれません。

アプリからの音声オーディオストリーム以外のすべてのオーディオストリームは、Appleの音声処理によって「他のオーディオ」と見なされ、音声オーディオは出力デバイスに再生される前に他のオーディオと混在します。

ボイスチャットアプリの場合、通常、再生オーディオの主な焦点はボイスチャットオーディオです。

そのため、音声オーディオの明瞭度を向上させるために、他のオーディオの音量レベルを回避します。

過去には、他のオーディオに一定量のダッキングを適用しました。

これはほとんどのアプリでうまく機能しており、アプリが現在のダッキング動作に満足している場合は、何もする必要はありません。

しかし、一部のアプリはダッキング動作をより詳細に制御したいと思っていることを認識しており、このAPIはそれを達成するのに役立ちます。

最初にAUVoiceIOのこのAPIを調べて、後でAVAudioEngineにたどり着きます。

AUVoiceIOの場合、これは他のオーディオダッキング構成の構造です。

ダッキングの2つの独立した側面、つまりダッキングのスタイル、つまりmEnableAdvancedDuckingとダッキングの量、つまりmDuckingLevelのコントロールを提供します。

mEnableAdvancedDuckingの場合、デフォルトでは、これは無効になっています。

有効にすると、チャット参加者の両側からの音声アクティビティの存在に基づいて、ダッキングレベルを動的に調整します。

言い換えれば、どちらかの側からユーザーが話しているときにより多くのダッキングを適用し、どちらの側も話していないときにダッキングを減らします。

これは、FaceTime SharePlayのダッキングと非常によく似ています。FaceTimeのどちらの当事者も話していないときにメディア再生音量が高くなるが、誰かが話し始めるとすぐに、メディア再生音量が減少します。

次に、mDuckingLevel。

コントロールには、デフォルト(デフォルト)、最小(最小)、中(中)、最大(最大)の4つのレベルがあります。

デフォルト(デフォルト)ダッキングレベルは、私たちが適用してきたのと同じ量のダッキングを適用し、これは引き続きデフォルト設定になります。

最小(最小)ダッキングレベルは、私たちが適用するダッキングの量を最小限に抑えます。

言い換えれば、これは他のオーディオボリュームをできるだけ大きくしたい場合に使用する設定です。

逆に、最大(最大)ダッキングレベルは、私たちが適用するダッキングの量を最大化します。

一般的に言えば、より高いダッキングレベルを選択することは、ボイスチャットの明瞭度を向上させるのに役立ちます。

2つのコントロールは独立して使用できます。

組み合わせて使用すると、ダッキング動作を制御する完全な柔軟性が得られます。

ダッキング設定が何をするかを説明したので、アプリに適したものを作成できます。

たとえば、ここでは高度なダッキングを有効にし、ダッキングレベルを最小値に選択します。

次に、kAUVoiceIOProperty_ OtherAudioDuckingConfigurationを介して、このダッキング設定をAUVoiceIOに設定します。

AVAudioEngineクライアントの場合、APIは非常によく似ています。

これは他のオーディオダッキング構成の構造体定義であり、これはダッキングレベルの列挙型定義です。

AVAudioEngineでこのAPIを使用するには、まずエンジンの入力ノードで音声処理を有効にしてから、ダッキング設定を設定します。

そして最後に、入力ノードに設定を設定します。

次に、アプリに非常に便利な機能を実装するのに役立つ別のAPIについて話しましょう。

オンライン会議で、同僚や友人とチャットしていると思っていたが、すぐにミュートされていて、誰もあなたの素晴らしいポイントや面白い話を実際に聞いていないことに気づいたことがありますか?

はい、気まずい。

FaceTimeがここで何をしているかなど、アプリでミュートされたトーカー検出機能を提供することは非常に便利です。

そのため、ミュートされたトーカーの存在を検出するためのAPIを提供しています。

iOS 15で最初に導入され、現在はmacOS 14とtvOS 17で利用可能にしています。

これは、このAPIの使用方法の概要です。

まず、ミュートされたトーカーが検出されたときに通知を受け取るには、AUVoiceIOまたはAVAudioEngineにリスナーブロックを提供する必要があります。

提供するリスナーブロックは、ミュートされたトーカーが話し始めるか、話すのをやめるたびに呼び出され、そのような通知の処理コードを実装します。

たとえば、通知がユーザーがミュート中に会話を開始したことを示した場合、ユーザーにミュートを解除するように促したい場合があります。

最後になりましたが、AUVoiceIOまたはAVAudioEngineのミュートAPIを介してミュートを実装する必要があります。

AUVoiceIOでいくつかのコード例をご案内します。

AVAudioEngineの例は後で説明します。

まず、通知を処理するリスナーブロックを準備します。

このブロックにはAUVoiceIOSpeechActivityEventタイプのパラメータがあり、SpeechActivityHasStartedまたはSpeechActivityHasEndedの2つの値のいずれかになります。

リスナーブロックは、ミュート中に音声アクティビティイベントが変更されるたびに呼び出されます。

ブロック内では、このイベントの処理方法を実装する場所です。たとえば、SpeechActivityHasStartedイベントを受信すると、ユーザーにミュートを解除するように促すことをお勧めします。

このリスナーブロックの準備ができたら、kAUVoiceIOProperty_MutedSpeechActivityEventListenerを介してAUVoiceIOにブロックを登録します。

ユーザーがミュートするときは、ミュートAPI kAUVoiceIOProperty_MuteOutputを介してミュートを実装します。

リスナーブロックは、A、ユーザーがミュート、およびBが音声アクティビティ状態が変更されたときにのみ呼び出されます。

音声活動の継続的な存在または欠如は、冗長な通知を引き起こすことはありません。

AVAudioEngineクライアントの場合、実装は非常によく似ています。

エンジンの入力ノードで音声処理を有効にした後、通知を処理するリスナーブロックを準備します。

次に、入力ノードにリスナーブロックを登録します。

ユーザーがミュートするときは、AVAudioEngineの音声処理ミュートAPIを使用してミュートします。

さて、AUVoiceIOとAVAudioEngineでミュートされたトーカー検出機能の実装について説明しました。

Appleの音声処理APIをまだ採用する準備ができていない方には、この機能を実装するのに役立つ代替手段を提供しています。

この代替案は、CoreAudio HAL API、つまりハードウェア抽象化レイヤーAPIを介してmacOSでのみ利用可能です。

組み合わせて使用すると音声アクティビティを検出するのに役立つ2つの新しいHALプロパティがあります。

まず、kAudioDevicePropertyVoiceActivityDetectionEnableを介して入力デバイスで音声アクティビティ検出を有効にします。

次に、kAudioDevicePropertyVoiceActivityDetectionStateにHALプロパティリスナーを登録します。

このHALプロパティリスナーは、音声アクティビティ状態が変更されるたびに呼び出されます。

アプリがプロパティリスナーから通知されたら、プロパティを照会して現在の値を取得します。

さて、いくつかのコード例でこれを説明しましょう。

入力デバイスで音声アクティビティ検出を有効にするには、まずHALプロパティアドレスを構築します。

次に、入力デバイスにプロパティを設定して有効にします。

次に、音声アクティビティ検出状態プロパティにリスナーを登録するには、HALプロパティアドレスを構築し、プロパティリスナーを指定します。

ここで「listener_callback」はリスナー関数の名前です。

これは、プロパティリスナーを実装する方法の例です。

リスナーはこの関数のシグネチャに準拠しています。

この例では、このリスナーが1つのHALプロパティにのみ登録されていることを前提としています。つまり、呼び出されたときに、どのHALプロパティが変更されたかに曖昧さはありません。

複数のHALプロパティの通知に同じリスナーを登録する場合は、まずinAddressesの配列を調べて、何が正確に変更されたかを確認する必要があります。

この通知を処理する際に、VoiceActivityDetectionStateプロパティを照会して現在の値を取得し、その値を処理する際に独自のロジックを実装します。

これらの音声アクティビティ検出HAL APIに関する重要な詳細がいくつかあります。

まず第一に、エコーキャンセルマイク入力から音声アクティビティを検出しているため、ボイスチャットアプリケーションに最適です。

第二に、この検出はプロセスのミュート状態に関係なく機能します。

ミュートされたトーカー検出機能を実装するには、音声アクティビティ状態とミュート状態を組み合わせた追加のロジックを実装するのはアプリ次第です。

HAL APIクライアントがミュートを実装するには、HALのプロセスミュートAPIを使用することを強くお勧めします。

メニューバーの録音インジケータライトを抑制し、プライバシーがミュートで保護されているという自信をユーザーに提供します。

今日話したことをまとめましょう。 

Appleの音声処理APIと、音声オーバーIPアプリケーションに推奨する理由について議論しました。

私たちは、他のオーディオのダッキングと、AUVoiceIOとAVAudioEngineでそれを使用する方法のコード例でダッキング動作を制御するAPIについて話しました。

また、AUVoiceIOとAVAudioEngineのコード例を使用して、ミュートされたトーカー検出を実装する方法についても説明しました。

また、Appleの音声処理APIを採用していないクライアントのために、Core Audio HAL APIを使用してmacOSでそれを行うための代替オプションも示しました。

Appleの音声処理APIで構築する素晴らしいアプリを楽しみにしています。

見てくれてありがとう!

♪ ♪