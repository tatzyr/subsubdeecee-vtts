10048

♪ ♪

アダム：ようこそ、私はVisionKitのエンジニアであるアダムです。

今日は、チームが今年取り組んでいる新機能やAPIについてお話しできることを嬉しく思います。

要約すると、昨年、Live TextサポートがVisionKitに追加され、テキスト選択、翻訳、QRサポートなどのインタラクションがアプリ内の画像に対応しました。

VisionKitはまた、DataScannerViewControllerを導入しました。データスキャナーはライブカメラフィードを使用して、特定のテキストタイプと機械読み取り可能なコードの多くのバリアントをキャプチャするためのシンプルでフル機能の方法を提供します。

これらのAPIに関する情報は、これらのWWDC22セッションに含まれています。

開発者からの反応は素晴らしく、今年はVisionKitがSubject LiftingとVisual Look Upの両方のサポートを追加したことを発表できることを嬉しく思います。

また、テキスト選択のための新しいLive Text API、Catalystの拡張プラットフォームサポート、ネイティブmacOSアプリのコンテキストメニュー統合もあります。

そして今、私はサブジェクトリフティングを始めるつもりです。

画像の被写体を長押しするだけで、周囲から持ち上げ、この美しいアニメーションの輝きで強調表示され、それを共有したり、ビジュアルルックアップを呼び出すためのいくつかのオプションが表示されます。

iOS 17の新機能で、持ち上げられた被写体を使用してステッカーを作成し、光沢のある、ふくらんでいるなどの楽しい効果で、友人や家族と共有できるようになりました。

さて、良いニュースは、サブジェクトリフティングの統合は非常に簡単だということです。

実際、あなたはすでに終わっている可能性が高いです。

これは、画像を分析してインタラクションに設定した昨年のビデオと同じコードスニペットです。

しかし、今では、コードを変更することなく、サブジェクトリフティングをサポートしています。

さらに探検しましょう。

アナライザの構成に特別なことは何も渡していないことに注意してください。

これは、パワーとパフォーマンスを維持するために、サブジェクトリフティング分析は、初期分析が完了した後、相互作用によって別々に処理されるためです。

iOSの場合、このプロセスは数秒間画面に表示された後に発生し、macOSの場合、メニューが初めて表示されたときに発生します。

これは、ユーザーが多くの写真をスワイプするケースを処理する必要がないことを意味します。

インタラクションはあなたのためにこれを処理します。

必要なのは、適切なインタラクションタイプ（この場合は自動）が設定されていることを確認するだけで、残りはインタラクションによって処理されます。

サブジェクトリフティングの互換性のあるインタラクションタイプをもう少し詳しく調べてみましょう。

自動は、テキストインタラクション、サブジェクトリフティングなどを組み合わせて、箱から出してデフォルトのエクスペリエンスを提供します。

テキスト選択やデータ検出器ではなく、サブジェクトリフティングのみが必要な場合は、インタラクションタイプを.imageSegmentationに設定するか、他のタイプと組み合わせることができます。

そして最後に、サブジェクトリフティングがアプリにとって意味をなさないが、iOS 16からの以前の自動動作が必要な場合は、問題ありません。新しいタイプの.automaticTextOnlyを使用できます。

これは、テキスト選択やデータ検出器などの機能を提供しますが、サブジェクトリフティングは提供しません。

VisionKitとVisionの両方でこの驚くべき新技術に関する高度なトピックを学びたい場合は、特にサブジェクトリフティングに関する詳細なセッションがあります。

今年はVisionKitがVisual Look Upもサポートしています。

ビジュアルルックアップを使用すると、ユーザーはペット、自然、ランドマーク、アート、メディアについて簡単に識別して学ぶことができます。

また、iOS 17では、Visual Look Upは、食品、製品、記号や記号などの追加ドメインをサポートします。

さて、最後に、洗濯タグのこれらのシンボルが何を意味するのかを簡単に調べることができます。

つまり、あなたが私に尋ねるなら、それはかなりクールです。

ビジュアルルックアップの可用性は言語に基づいており、これらの言語で利用できます。

ボンネットの下をちょっと覗いて、Visual Look Upの仕組みを探りましょう。

それは実際には2部構成のプロセスです。

初期処理は、分析時にデバイス上で完全に行われます。

.visualLookUpタイプがアナライザ構成に存在する場合、Visual Look Upは結果の境界ボックスとそのトップレベルドメインを見つけます。

例えば、それが猫、本、または植物の場合。

このステップには、特徴の抽出も含まれます。

ユーザーがオブジェクトの検索をリクエストすると、機能抽出からのドメインと画像の埋め込みが追加の処理のためにサーバーに送信されます。

ビジュアルルックアップの仕組みがわかったので、その使用方法と、アプリに追加するために必要なアクションをすばやく探りましょう。

ビジュアルルックアップは2つの異なる方法で呼び出すことができます。

1つ目は、サブジェクトリフティングと組み合わせて、現在持ち上げられたサブジェクトに1つだけ相関するビジュアルルックアップ結果が含まれている場合、ルックアップオプションがメニューで提供され、それを選択すると、完全なルックアップ結果が表示されます。

VisionKitは、このインタラクションを自動的に処理します。

採用担当者として必要なのは、分析時にアナライザ構成に.visualLookUpを追加することだけです。

第二に、各視覚的な検索結果の上にバッジが配置されるモーダルインタラクションがあります。

ビューポートを離れると、バッジがどのようにコーナーに移動するかに注目してください。ユーザーはこれらのバッジをタップして検索結果を表示できます。

これは、たとえば、写真アプリの情報ボタンやクイックルックをクリックするのと同じインタラクションです。

このモードは、インタラクションの優先InteractionTypeとして.visualLookUpを設定することによって呼び出されます。

ご注意：このタイプは他のインタラクションタイプよりも優先されます。

たとえば、visualLookupモードが設定されていると同時にテキストやデータ検出器を選択することはできません。

そのため、これは通常、ボタン、またはこのモードに出入りするための他のオーダーメイドの方法と組み合わせて使用されます。

たとえば、クイックルックは情報ボタンを使用してビジュアルルックアップモードに入ります。

では、ギアをシフトして、データスキャナーとライブテキストの新しいAPIと機能について話し合いましょう。

iOS 16で導入されたDataScannerViewControllerは、ライブカメラビューファインダーでOCRを使用する最も簡単な方法であるように設計されています。

iOS 17では、光フロートラッキングと通貨サポートで強化されています。

オプティカルフロートラッキングは、ライブカメラ体験のためのテキストトラッキングを強化することができます。

これがiOS 16にあるものです。

highFrameRateTrackingを有効にした状態でテキストをスキャンしています。

そして、これは光学フロートラッキングで得られるものです。

今、ハイライトは以前よりもはるかに安定し、接地されていると感じています。

光学フロートラッキングは、DataScannerViewControllerを使用するたびに無料で提供されますが、テキストを認識する場合にのみ使用でき、機械読み取り可能なコードは使用できません。

また、特定のテキストコンテンツタイプが設定されていないテキストをスキャンする必要があります。

そして最後に、もう一度、高いフレームレートの追跡が有効になっていることを確認してください。

これは、便利なことに、デフォルトです。

どのように設定しても、データスキャナは優れたテキストトラッキングを提供します。しかし、ユースケースがこの構成を可能にする場合、新しい光フロートラッキングはそれをさらに強化することができます。

次に、データスキャナには、ユーザーが金銭的価値を見つけて操作できる新しいオプションがあります。

有効にするのは信じられないほど簡単です。

データスキャナの初期化子でテキスト認識を指定するときは、電子メールアドレスや電話番号などの他のコンテンツタイプと同様に、テキストコンテンツタイプを通貨に設定するだけです。

次に、この新しいタイプを簡単な例でより詳細に探求します。

データスキャナがテキスト内の通貨を認識すると、境界とトランスクリプトの両方が含まれます。

トランスクリプトには通貨記号と金額の両方があります。

これは、領収書のようなもののすべての値の合計を見つける例です。

まず、現在のロケールを使用して通貨記号を取得します。

認識されたアイテムストリームでデータスキャナの結果を待っている間、私は認識された各アイテムをループし、そのトランスクリプトをつかむことができます。

トランスクリプトに興味のある通貨記号が含まれている場合は、先に進んで合計値を更新します。

そして、ちょうどそのように、今、あなたはすべての値の合計を持つことになります。

これは単なる簡単な例ですが、これは非常に強力です、私はあなたがこれで何を構築できるかを見て興奮しています。

そして今、私はライブテキストの強化について話します。

まず、ライブテキストは、サポートされている言語をタイ語とベトナム語を含むように拡大することで、より多くの地域に来ています。

ライブテキストには、今年もドキュメント構造検出のための機能強化が含まれています。

文書構造の検出?それはどういう意味ですか? 

たとえば、iOS 16のライブテキストでは、リスト検出がサポートされています。

これにより、メモなどのリストを理解するアプリにリストを簡単にコピーして貼り付けることができ、リストの書式設定が維持されます。

ライブテキストは、数字や箇条書きなど、いくつかのリストスタイルを処理します。

そして今、Live Textはテーブルに同じサポートを提供しているため、画像からNotesやNumbersなどのアプリケーションに構造化されたテーブルデータをはるかに簡単に取得できます。

これで、この表を選択、コピー、Numbersに貼り付けることができ、構造が維持されます。

必要に応じて、セルを自動的にマージする方法に注目してください。

そして、ちょうどそのように、私は今、グラフでこの情報を視覚化することから数回クリックするだけです。

いいね。

そして、それだけではありません。

また、ライブテキストにコンテキスト認識データ検出器を追加しています。

この機能では、連絡先を追加するときにデータ検出器とその視覚的関係が使用されます。

メールアドレスからこの連絡先を追加すると、周囲のデータ検出器からの追加情報が含まれ、このすべての情報を一度に簡単に追加できるようになったことに注意してください。

名刺やチラシから連絡先を追加することは、かつてないほど簡単になりました。

無料で入手できるこれらの素晴らしい機能に加えて、VisionKitにはテキスト専用の新しいAPIもあります。

昨年は、画像分析のトランスクリプトプロパティにアクセスすることで、テキストコンテンツ全体を取得できました。

フィードバックに基づいて、プレーンテキストと帰属テキスト、選択した範囲へのフルアクセス、および選択したテキストへの簡単なアクセスが可能になりました。

新しいデリゲートメソッドもあるので、テキストの選択が変更されたときに気づき、必要に応じてUIを更新することができます。

ユーザーが選択したものに依存する機能を簡単に追加できるようになりました。

たとえば、メニュービルダーAPIを使用すると、現在のテキスト選択に基づいてリマインダーを作成するメニュー項目を挿入できます。

画像分析インタラクションを所有するビューコントローラーから始めます。

まず、選択したテキストをつかみ、空でないことを確認します。

次に、選択したときにハンドラを呼び出すコマンドを作成し、コマンドを保持するメニューオブジェクトを作成します。

そして最後に、共有メニューオプションの後にそのメニューを兄弟として挿入します。

これで、コピーや共有などのシステム項目の横にカスタムメニューがあります。

次に、拡張されたプラットフォームのサポートについて話します。

そして今年、それはすべてMacについてです。

iOSアプリからライブテキストをMacに簡単に持ち込むために、Catalystサポートを展開しています。

また、ネイティブのmacOS APIとImageAnalysisOverlayViewを初めて使用する場合は、いくつかの詳細と、それらを採用するためのヒントについて説明しますので、お楽しみに。

最後に、コンテキストメニューへのVisionKitのシンプルでシームレスな統合を提供する、メニューの新しいシステムについて説明します。

触媒の採用は非常に簡単です。

Catalystで画像分析のインタラクションを動作させるには、簡単な再コンパイルが必要です。

ライブテキスト、サブジェクトリフティング、ビジュアルルックアップをサポートしていますが、残念ながらQRコードのサポートは、Catalyst環境またはVisionKitのネイティブmacOS APIでは利用できません。

しかし、共有実装がある場合、Catalystのアナライザ構成に.machineReadableCodesを残すことは完全に安全であり、ノーオペになることをお知らせしたいと思います。

また、Macでこの機能が必要な場合は、Vision FrameworkでQR検出サポートが利用可能であることに注意してください。

今、私はネイティブのmacOS APIに移行しています。

iOSと同様に、VisionKitを採用する際に注意する必要がある2つの主要なクラスがあります。ImageAnalyzerとImageAnalysisOverlayViewです。

まず、簡単な部分です。

Macのイメージアナライザーと分析プロセスはiOSと同じです。

先に述べたように、機械読み取り可能なコードがno-opであることを除いて、すべてが同じであり、同じ方法で使用されます。

iOS ImageAnalysisInteractionとmacOSのImageAnalysisOverlayViewの主な違いは、インタラクションがアプリケーションにどのように追加されるかです。

iOSの場合、ImageAnalysisInteractionは、アプリのビュー階層にすでに存在するビューに追加されるUIInteractionです。

しかし、UIInteractionはMacには存在しません。

それで、あなたは何をしますか?

この場合、名前が示すように、画像分析オーバーレイビューはNSViewのサブクラスです。

画像コンテンツの上にあるビュー階層にオーバーレイビューを追加するだけです。

例えば、ここに追加できます。

あるいはここでも。

しかし、最も簡単な方法は、それを私のコンテンツビューのサブビューとして追加することです。

選択した方法は完全に問題ありませんが、サブビューとして追加することは、コンテンツビューの位置が変更されたときにオーバーレイビューの再配置を処理する必要がないため、一般的に簡単で管理が簡単であることがわかりました。

そして今、あなたはそれをあなたのアプリに追加する方法と場所を知っています、長方形について話しましょう。

OverlayViewはコンテンツをホストまたはレンダリングしないため、その境界に関連してコンテンツが存在する場所を正確に知る必要があります。

これは、左上の原点を持つ単位座標空間にあるcontentsRectによって記述されます。

うわー、それは一口でした。

簡単な例は、これを明確にするのに役立つはずです。

オーバーレイビューはimageViewの上に直接配置されるため、同じ境界があります。

この長方形で境界を表示します。 境界を表示します。

そして、一致する内容のrectも追加します。

最も簡単なケースは、コンテンツが境界に一致する場合です。

ここでは、単にユニットの長方形です。

さて、ここにアスペクトフィットがあります。

imageViewのこの部分には、その下にコンテンツがないことに注意してください。

そして、内容のrectに反映されます。

そして、ここにアスペクトフィルがあります。

画像のこの部分はユーザーには表示されなくなりました。コンテンツがここでどのように変化するかに注目してください。

さて、良いニュースがあります。

iOSのUIImageViewと同様に、NSImageViewを使用している場合は、オーバーレイビューでtrackingImageViewプロパティを設定するだけで、これらすべてが自動的に計算されます。

NSImageViewを使用していない場合は、心配しないでください。

デリゲートメソッドcontentsRect(overlayView用:)を実装することで、コンテンツrectを提供できます。オーバーレイビューは、境界が変更されたときにレイアウト中にこれを要求します。

ただし、overlayViewでsetContentsRectNeedsUpdateを呼び出すことで、この更新を手動で要求できます。

さて、コンテキストメニューに移りましょう。

ご存知のように、コンテクストメニューはMac体験の大きな部分を占めています。

ライブテキスト、ルックアップ、サブジェクトリフティングなどの機能のために、VisionKitが提供する機能をメニューに直接簡単に追加できるようになりました。

あなたが持つかもしれない1つの質問は、なぜですか?

macOSの写真アプリを調べてみましょう。

この象徴的な道路標識のテキストを右クリックすると、VisionKitのテキストメニューのみが表示されます。

テキストオーバーでなければ、テキスト項目なしで、代わりにアプリメニューが提供されます。

これは理想的ではありません。

macOS Sonomaでは、アイテムを同じメニューにまとめることができます。

メニューイベントがどこで開始されたかに関係なく、テキストと画像の両方の機能に簡単にアクセスできます。

これはユーザーにとってはるかに優れた体験であり、実装が簡単です。

これを自分のアプリでどのように達成できるかを探りましょう。

これで、overlayview:updatedmenu:forevent:atpointという新しいデリゲートメソッドが利用可能になりました。

引数には、メニューをトリガーしたイベントと、オーバーレイビュー境界のポイントがスペースを調整するため、必要なメニューを作成できます。

そこから、表示したいメニューを返すだけです。

デフォルトの実装は、VisionKitメニューを返します。

ただし、そのメニューに独自のアイテムを追加したり、そのメニューからアイテムを取り出すこともできます。

VisionKitのメニュー項目はタグで識別され、これらのタグを含む構造体があります。

画像と被写体をコピーして共有できるアイテムと、ルックアップ用のアイテムがいくつかあります。

また、VisionKitが提供するメニューにアイテムを追加するための推奨インデックスを見つけるために使用できる特別なアイテムもありますが、それについては後で詳しく説明します。

これがどのように使用されるかの例をいくつか紹介します。

既存のメニューがあり、私が興味を持っていたのはcopySubjectアイテムを追加することだけであれば、このように簡単に追加できます。

まず、アプリのメニューを入手してください。

その後、興味のあるアイテムを入手してください。

この場合、copySubject。

そして、それをあなたのメニューに挿入してください。

さて、アイテムは実際に有効な場合にのみ利用可能であることを覚えておくことが重要です。

たとえば、サブジェクトインタラクション可能なタイプが存在しない場合、copySubject項目はメニューにありません。

また、システムが提供するテキスト項目については、該当する場合に含まれていますが、すべてがタグで識別できるわけではありません。

これらのアイテムを好きなようにカスタマイズすることもできます。

たとえば、アイテムをコピー画像からコピー写真に変更しました。

そして、これらのプロパティを変更することを心配しないでください。

これらのアイテムは毎回再作成され、好きなように変更できます。

既存のメニューにアイテムを追加することを説明したので、VisionKitメニューにアイテムを追加する方法の例を探ってみようと思います。

前述のように、overlayViewは、推奨インデックスにタグが付いたアイテムを提供し、sendedAppItemsというアイテムを挿入します。

このアイテムのインデックスを尋ねて、そのインデックスにアイテムを挿入するだけです。

このインデックスの使用はオプションであり、必須ではありません。

しかし、それはあなたのユーザーのために物事の一貫性を保つための良い方法です。

これらのメニュー項目の一部には特別な特性があることに気付くでしょう。

たとえば、件名に関連するメニュー項目が強調表示されると、私の猫のKiKiを取り巻く領域が暗くなり、グローアニメーションが始まり、コピーまたは共有される前にユーザーに被写体を示します。

VisionKitは、まだ開始されていない場合は、トリガーとして表示されるメニューを使用して主題分析を開始します。

これはすべて自動的に処理されます。

これらの機能を提供するために、VisionKitは、更新メニューメソッドから返されるメニューのデリゲートとして設定します。

以前にこれらのNSMenuDelegateコールバックに依存していた場合、VisionKitは独自のデリゲートコールバックを提供し、以前に使用していた場合はメニューアイテムの機能を保持できるようになりました。

そして、ここに簡単なヒントがあります。

このような状況にある場合、メニューがどこから開始されたかによっては、VisionKitからではない可能性があります。

したがって、既存の実装を維持したいと思うでしょう。

一般的に、これをすべて同期させる最も簡単な方法は、OverlayViewDelegateの実装が一致するNSMenuDelegateの実装を呼び出し、必要に応じて調整することです。

もちろん、これがあなたのアプリにとって理にかなっていることを確認してくださいが、一般的に、これは通常トリックを行います。

そして、それはVisionKitの新機能の簡単な概要です。

今日は、Subject LiftingとVisual Look Up、新しいmacOS APIと関連情報についてお話しできて光栄です。

これらの新機能を使用して、顧客を喜ばせ、驚かせる方法を楽しみにしています。

そして真面目な話、いつものように、楽しんでください!

ありがとう！