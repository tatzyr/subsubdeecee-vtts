10033

♪ ♪

グラント:こんにちは、私の名前はグラントです。私はアクセシビリティチームのエンジニアです。

多くの人がAppleプラットフォームで音声合成を使用しており、シンセサイザーの声に頼っている人もいます。

これらの声は、彼らのデバイスへの窓です。

したがって、彼らが選ぶ声はしばしば非常に個人的な選択です。

iOSで音声合成を使用している人は、すでに多くの異なる声から選択できます。

さらに多くのものを提供する方法を見てみましょう。 

まず、音声合成マークアップ言語とは何か、カスタムボイスに没入型音声出力をもたらす方法、および音声プロバイダーがそれを採用すべき理由について説明します。

次に、デバイス全体にシンセサイザーと音声体験をもたらすために、音声合成プロバイダーを実装する方法を説明します。

そして最後に、パーソナルボイスに飛び込みます。

これは新機能です。

今、人々は自分の声を録音し、それらの録音から合成された声を生成することができます。

だから今、あなたはユーザー自身の個人的な声でスピーチを合成することができます。

SSMLを見ることから始めましょう。

SSMLは、音声テキストを表すためのW3C標準です。

SSMLスピーチは、さまざまなタグと属性を持つXML形式を使用して宣言的に表現されます。

これらのタグを使用して、レートやピッチなどの音声プロパティを制御できます。

SSMLはファーストパーティシンセサイザーで使用されます。

これにはWebKitのWebSpeechが含まれており、音声シンセサイザーの標準入力です。

SSMLの使い方を見てみましょう。 見てみましょう。

一時停止があるこの例のフレーズを見てみましょう。

この一時停止はSSMLで表すことができます。

「こんにちは」の文字列から始めて、SSMLブレークタグを使用して1秒の一時停止を追加し、「はじめまして！」をスピードアップして終了します。

これを行うには、SSMLプロソディタグを追加し、レート属性を200%に設定します。

今、私たちはこのSSMLを取り、話すためのAVSpeechUtteranceを作成することができます。

次に、独自のスピーチシンセサイザーボイスを実装する方法を見てみましょう。

では、スピーチシンセサイザーとは何ですか?

音声シンセサイザーは、SSMLの形式で目的の音声プロパティに関するテキストと情報を受け取り、そのテキストの音声表現を提供します。

素晴らしい新しい声を持つシンセサイザーを持っていて、それをiOS、macOS、iPadOSに持ち込みたいとします。

音声合成プロバイダーを使用すると、独自の音声シンセサイザーと音声を当社のプラットフォームに実装して、システム音声を超えてユーザーにさらにパーソナライズできます。

これがどのように機能するか見てみましょう。

音声合成プロバイダーのオーディオユニット拡張機能は、ホストアプリに埋め込まれ、SSMLの形で音声要求を受け取ります。

拡張機能は、SSML入力のオーディオをレンダリングし、オプションでそれらのオーディオバッファ内の単語が発生する場所を示すマーカーを返す責任があります。

その後、システムはその音声要求のすべての再生を管理します。

オーディオセッション管理を処理する必要はありません。音声合成プロバイダーフレームワークによって内部的に管理されます。

シンセサイザーが何であるかを理解したので、スピーチシンセサイザー拡張機能の構築を開始できます。

Xcodeで新しいAudio Unit Extensionアプリプロジェクトを作成し、「Speech Synthesizer」Audio Unit Typeを選択し、シンセサイザーに4文字のサブタイプ識別子と、製造元として4文字の識別子を提供しましょう。

オーディオユニット拡張機能は、音声シンセサイザー拡張機能が構築されたコアアーキテクチャです。

シンセサイザーは、ホストアプリプロセスではなく、拡張プロセスで実行できます。

私たちのアプリは、拡張機能が音声を合成する音声を購入して選択するためのシンプルなインターフェースを提供します。

まず、購入可能な声を示すリストビューを作成します。

各音声セルには、音声名と購入ボタンが表示されます。

次に、いくつかの声をリストに入力します。

ここでは、WWDCVoiceは音声名と識別子を保持するシンプルな構造体です。

また、購入した音声を追跡するための状態変数と、それらを表示するための新しいセクションも必要です。

次に、音声を購入する機能を作成しましょう。

ここでは、新しく購入した音声をリストに追加し、それに応じてUIを更新することができます。

AVSpeechSynthesisProviderVoiceメソッドupdateSpeechVoicesに注意してください。

このようにして、アプリは、シンセサイザーで利用可能なボイスのセットが変更され、システムのボイスリストを再構築する必要があることを通知することができます。

この例では、音声のアプリ内購入を完了した後にこの電話をかけることができます。

また、スピーチシンセサイザー拡張機能で利用可能な音声を監視する方法も必要です。

これは、アプリグループを通じて共有されるUserDefaultsのインスタンスを作成することで実行できます。

アプリグループでは、ホストアプリと拡張機能の間でこの音声リストを共有できます。

アプリグループを作成するときに提供したスイート名を明示的に指定しています。

これにより、ホストアプリと拡張機能が同じドメインから確実に読み込まれます。

購入機能を振り返ってみると、新しい音声が購入されたときにユーザーのデフォルトを更新する方法を実装しました。

AVSpeechSynthesizerには、利用可能なシステム音声の変更をリッスンするための新しいAPIもあります。

システムボイスのセットは、ユーザーがボイスを削除するか、新しいボイスをダウンロードすると変更される可能性があります。

availableVoicesDidChangeNotificationを購読して、これらの変更に基づいてボイスのリストを更新できます。

ホストアプリが完成したので、4つの主要コンポーネントで構成されるオーディオユニットを埋めましょう。

最初に追加する必要があるのは、シンセサイザーが提供する声をシステムに知らせる方法です。

これは、speechVoicesゲッターをオーバーライドして、先ほど指定したアプリグループUserDefaultsドメインから音声と読み取りのリストを提供することによって達成されます。

音声リストの各項目について、米国英語AVSpeechSynthesisProviderVoiceを構築します。

次に、システムがシンセサイザーにどのテキストを合成するかを伝える方法が必要です。

synthesizeSpeechRequestメソッドは、システムがテキストの合成を開始する必要があることを拡張機能に通知したいときに呼び出されます。

このメソッドの引数は、SSMLを含むAVSpeechSynthesisProviderRequestのインスタンスであり、どの音声で話すかです。

次に、スピーチエンジンの実装で作成したヘルパーメソッドを呼び出します。

この例では、getAudioBufferメソッドは、リクエストで指定された音声とSSML入力に基づいてオーディオデータを生成します。

また、レンダリングブロックが呼び出されたときにレンダリングしたフレーム数を追跡し、バッファからフレームをコピーするために、framePositionと呼ばれるインスタンス変数を0に設定します。

システムには、オーディオの合成を停止し、現在の音声要求を破棄するためにシンセサイザーに信号を送る方法も必要です。

これは、現在のバッファを単に破棄するcancelSpeechRequestで達成されます。

最後に、レンダリングブロックを実装する必要があります。

レンダリングブロックは、目的のframeCountでシステムによって呼び出されます。

その後、オーディオユニットは、要求されたフレーム数をoutputAudioBufferに入力する責任があります。

次に、ターゲットバッファと、synthesizeSpeechRequest呼び出し中に以前に生成および保存したバッファへの参照を設定します。

次に、フレームをターゲットバッファにコピーします。

そして最後に、オーディオユニットが現在の音声要求のすべてのバッファを使い果たしたら、actionFlags引数をofflineUnitRenderAction_Completeに設定して、レンダリングが完了し、レンダリングするオーディオバッファがないことをシステムに通知する必要があります。

実際に見てみましょう! 

これは私のスピーチシンセサイザーアプリです。

音声を購入し、新しい音声と音声エンジンを使用して音声を合成できるビューに移動します。

まず、シンセサイザーに「こんにちは」と入力します。

合成された声:こんにちは。

グラント：それから私は「さようなら」と入力します。

合成された声:さようなら。

グラント:私たちは今、合成プロバイダーを実装し、VoiceOverから独自のアプリまで、システム全体で使用できる音声を提供するホスティングアプリを作成しました!

これらのAPIを使用して作成する新しい音声とテキスト読み上げ体験を見るのが待ちきれません。

先に進んで、パーソナルボイスと呼ばれる新機能について話しましょう。

人々は今、自分のデバイスの力を使って、iOSとmacOSで自分の声を録音し、再作成することができます。

パーソナルボイスは、サーバー上ではなくデバイス上で生成されます。

この音声は、残りのシステム音声の中に表示され、ライブスピーチと呼ばれる新機能で使用できます。

ライブスピーチは、iOS、iPadOS、macOS、watchOSで話すタイプ機能で、その場で自分の声でスピーチを合成することができます。

パーソナルボイスの新しいリクエスト承認APIを使用して、これらの音声で音声を合成するためのアクセスをリクエストできます。

パーソナルボイスの使用は敏感であり、主に増強または代替コミュニケーションアプリに使用される必要があることを覚えておいてください。

パーソナルボイスを使うために作ったAACアプリをチェックアウトしましょう。

私のアプリには、WWDCで言っている一般的なフレーズを話す2つのボタンと、パーソナルボイスを使用するためのアクセスを要求するボタンがあります。

承認は、AVSpeechSynthesizerのrequestPersonalVoiceAuthorizationという新しいAPIで要求できます。

承認されると、Personal VoicesはAVSpeechSynthesisVoice API speechVoicesのシステムボイスと一緒に表示され、isPersonalVoiceと呼ばれる新しいvoiceTraitで示されます。

パーソナルボイスにアクセスできたので、それを使って話すことができます。

動作中のパーソナルボイスのデモをチェックしてみましょう。

まず、「Personal Voiceを使用」ボタンをタップして承認をリクエストし、承認したら、シンボルをタップして自分の声を聞くことができます。

個人的な声:こんにちは、私の名前はグラントです。WWDC23へようこそ。

グラント：それは素晴らしいことではありませんか？

そして今、あなたはこれらの声をあなたのアプリでも使うことができます。

SSMLについて議論したので、それを使用して音声入力を標準化し、アプリで豊富な音声体験を構築する必要があります。

また、スピーチシンセサイザーをAppleプラットフォームに実装する方法も説明しましたので、人々がシステム全体で使用できる素晴らしい新しいスピーチボイスを提供できるようになりました。

そして最後に、パーソナルボイスを使用すると、特に自分の声を失う危険性がある人々のために、アプリの合成にさらに多くの個人的なタッチをもたらすことができます。

私たちは、あなたがこれらのAPIを使用してどのような経験を作成するかを見て非常に興奮しています。

見てくれてありがとう。