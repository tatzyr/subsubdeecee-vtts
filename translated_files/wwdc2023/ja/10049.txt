10049

♪ ♪

ベン：こんにちは、私はCore MLチームのエンジニアであるBen Levineです。

今日は、Core MLをアプリに統合することに関する新機能について話します。

アプリでインテリジェントな体験を構築することは、かつてないほど簡単になりました。

Xcode SDKは、機械学習を活用し、展開するための強固な基盤を提供します。

ドメイン固有のフレームワークのセットにより、シンプルなAPIを通じて組み込みのインテリジェンスにアクセスできます。

彼らが提供する機能は、Appleによって訓練され、最適化されたモデルによって強化されています。

これらのモデルはCore MLを介して実行されます。

Core MLフレームワークは、デバイス上で機械学習モデルを実行するためのエンジンを提供します。

アプリに合わせてカスタマイズされたモデルを簡単に展開できます。

AccelerateとMetalファミリーのフレームワークの助けを借りて、Appleシリコンの高性能コンピューティング機能を活用しながら、ハードウェアの詳細を抽象化します。

Core MLの使命は、機械学習モデルをアプリに統合できるようにすることです。

今年、Core MLの焦点はパフォーマンスと柔軟性でした。

ワークフロー、APIサーフェス、および基礎となる推論エンジンを改善しました。

ワークフローに飛び込み、Core ML統合を最適化するための新しい機会を強調する前に、最新のOSにアップデートするだけで自動的に得られる潜在的なパフォーマンスのメリットのアイデアがあります。

iOS 16と17の間の相対的な予測時間を比較すると、iOS 17は多くのモデルで単に高速であることがわかります。

推論エンジンのこのスピードアップはOSに付属しており、モデルの再コンパイルやコードの変更は必要ありません。

同じことが他のプラットフォームにも当てはまります。

当然のことながら、スピードアップの量はモデルとハードウェアに依存します。

議題に移り、Core MLをアプリに統合する際のワークフローの概要から始めます。

その過程で、ワークフローのさまざまな部分の最適化の機会を強調します。

次に、モデル統合に焦点を当て、コンピューティングの可用性、モデルライフサイクル、非同期予測に関する新しいAPIと動作について説明します。Core MLワークフローの概要から始めます。

Core MLをアプリに統合するには2つの段階があります。

1つ目はモデルの開発で、2つ目はアプリ内でそのモデルを使用することです。

モデル開発には、いくつかの選択肢があります。

独自のモデルを開発する最も便利な方法の1つは、Create MLを使用することです。

Create MLは、一般的な機械学習タスクにさまざまなテンプレートを提供し、OSに組み込まれた高度に最適化されたモデルを活用できます。

モデル開発ワークフローを案内し、結果をインタラクティブに評価できます。

もっと詳しく知りたい場合は、今年のCreate MLビデオをチェックしてください。

モデルを開発するもう1つの方法は、いくつかのPython機械学習フレームワークの1つを使用してモデルを訓練することです。

次に、CoreMLTools pythonパッケージを使用して、Core MLモデル形式に変換します。

最後に、Appleハードウェアの精度とパフォーマンスの両方の観点からモデルを評価することが重要です。

評価からのフィードバックを使用すると、モデルをさらに最適化するために、これらのステップのいくつかを再検討することがよくあります。

これらのステップでは、最適化の機会がたくさんあります。

トレーニングでは、トレーニングデータをどのように収集して選択するかが重要です。

展開時にモデルに渡されるデータと一致し、ユーザーの手にある必要があります。

あなたが選んだモデルアーキテクチャも重要です。

複数のオプションを検討している可能性があり、それぞれがトレーニングデータの要件、精度、サイズ、およびパフォーマンスの間で独自のトレードオフがあります。

これらのトレードオフの多くは、トレーニング時に完全に表示されない可能性があり、完全な開発フローを通じていくつかの反復が必要です。

次はモデル変換です。

Core MLツールは、変換されたモデルの精度、フットプリント、および計算コストを最適化するのに役立つさまざまなオプションを提供します。

不要なコピーを避けるために、アプリのデータフローに最も適した入出力フォーマットを選択できます。

入力形状が異なる場合は、1つの形状を選択したり、複数の形状固有のモデルを切り替えたりするのではなく、そのバリエーションを指定できます。

計算精度は、モデル全体または個々の操作に対して明示的に設定することもできます。

Float32とfloat16の両方が利用可能です。

計算の精度に加えて、モデルパラメータの表現方法もある程度制御できます。

CoreMLToolsには、トレーニング後の重量の量子化と圧縮のための一連のユーティリティが付属しています。

これらのユーティリティは、モデルのフットプリントを大幅に削減し、デバイス上のパフォーマンスを向上させるのに役立ちます。

しかし、これらの利点を達成するためには、正確性にはいくつかのトレードオフがあります。

このスペースに役立つ新しいツールがいくつかあります。CoreMLToolsパッケージに新しい最適化サブモジュールがあります。

トレーニング後の圧縮ユーティリティを統一および更新し、PyTorchの新しい定量化対応トレーニング拡張機能を追加します。

これにより、データ駆動型の最適化にアクセスして、トレーニング中に量子化されたモデルの精度を維持できます。

これは、Core MLのMLプログラムモデルタイプのアクティベーション量子化をサポートする新しい操作と組み合わされています。

詳細については、Core MLを使用した機械学習モデルの圧縮に関する今年のセッションをご覧ください。

次は評価です。

モデルを評価するオプションの1つは、CoreMLToolsを使用してPythonコードから直接変換されたモデルの予測を実行することです。

アプリコードが使用するのと同じCore ML推論スタックを使用し、モデル変換中の選択がモデルの精度とパフォーマンスにどのように影響するかをすばやく確認できます。

Xcodeは、モデルの評価と探索に関して役立つツールも提供しています。

モデルプレビューは、多くの一般的なモデルタイプで利用できます。

これにより、モデルにいくつかのサンプル入力を提供し、コードを書くことなく予測された出力をプレビューできます。

Core MLパフォーマンスレポートは、接続されたデバイスの負荷、予測、およびコンパイル時間のモデル計算パフォーマンスの内訳を提供します。

これは、モデルアーキテクチャをトレーニングする前であっても、モデルアーキテクチャを評価するのに役立つことに注意してください。

さて、全体的なワークフローに戻ると、次のトピックはモデル統合です。

モデル統合は、アプリの開発の一部です。

アプリで使用する他のリソースと同様に、Core MLモデルの使用方法を慎重に管理し、最適化したいと考えています。

モデル統合には3つのステップがあります。

まず、モデルを使用するためのアプリケーションコードを作成します。

モデルをいつロードするか、モデルの入力データを準備する方法、予測を行う方法、結果を使用する方法に関するコードがあります。

次に、このコードをモデルと一緒にコンパイルします。

そして第三に、アプリ内で実行されているモデルをテスト、実行、プロファイリングします。

プロファイリングに関しては、Core MLとNeural Engineの機器が役に立つかもしれません。

これはまた、出荷の準備が整うまでの設計と最適化の反復プロセスです。

今年は、モデル統合を最適化するための新しい追加がいくつかあります。

まず、コンピューティングの可用性です。

Core MLはすべてのAppleプラットフォームでサポートされており、デフォルトでは実行を最適化するために利用可能なすべてのコンピューティングを考慮します。

これには、利用可能な場合はCPU、GPU、およびニューラルエンジンが含まれます。

ただし、これらのコンピューティングデバイスのパフォーマンス特性と可用性は、アプリが実行される可能性のあるサポートされているハードウェアによって異なります。

これは、MLを搭載した機能に関するユーザーエクスペリエンスに影響を与えたり、モデルや構成の選択に影響を与える可能性があります。

たとえば、一部の経験では、パフォーマンスや電力要件を満たすためにニューラルエンジンで実行されているモデルが必要になる場合があります。

コンピューティングデバイスの可用性のランタイム検査のための新しいAPIができるようになりました。

MLComputeDevice列挙型は、関連する値内のコンピューティングデバイスのタイプと特定のコンピューティングデバイスのプロパティをキャプチャします。

MLModelのavailableComputeDevicesプロパティを使用すると、Core MLで利用可能なデバイスを検査できます。

たとえば、このコードは、利用可能なニューラルエンジンがあるかどうかをチェックします。

具体的には、利用可能なすべてのコンピューティングデバイスのコレクションに、タイプがNeural Engineであるものが含まれているかどうかをチェックします。

モデル統合の次のトピックは、モデルのライフサイクルを理解することです。

さまざまなモデル資産タイプを確認することから始めます。

ソースモデルとコンパイルされたモデルの2種類があります。

ソースモデルのファイル拡張子はMLModelまたはMLPackageです。

これは、構築と編集のために設計されたオープンフォーマットです。

コンパイルされたモデルのファイル拡張子はMLModelCです。

ランタイムアクセス用に設計されています。

ほとんどの場合、ソースモデルをアプリのターゲットに追加し、Xcodeがモデルをコンパイルしてアプリのリソースに入れます。

実行時に、モデルを使用するには、MLModelをインスタンス化します。

インスタンス化は、コンパイルされた形式のURLとオプションの設定を取ります。

結果のMLModelは、指定された構成とデバイス固有のハードウェア機能に基づいて、最適な推論に必要なすべてのリソースをロードしました。

この負荷中に何が起こるかを詳しく見てみましょう。

まず、Core MLはキャッシュをチェックして、構成とデバイスに基づいてモデルをすでに専門化しているかどうかを確認します。

あれば、キャッシュから必要なリソースをロードして返します。

これはキャッシュされたロードと呼ばれます。

設定がキャッシュに見つからなかった場合、デバイスに特化したコンパイルがトリガーされます。

このプロセスが完了すると、キャッシュに出力を追加し、そこからロードを終了します。

これはキャッシュされていない負荷と呼ばれます。

特定のモデルでは、キャッシュされていない負荷にかなりの時間がかかることがあります。

しかし、それはデバイスのモデルを最適化し、その後の負荷をできるだけ速くすることに重点を置いています。

デバイスの特殊化中、Core MLは最初にモデルを解析し、一般的な最適化パスを適用します。

次に、推定パフォーマンスとハードウェアの可用性に基づいて、特定のコンピューティングデバイスのオペレーションチェーンをセグメントします。

その後、このセグメンテーションはキャッシュされます。

最後のステップは、各セグメントが割り当てられたコンピューティングデバイスのコンピューティングデバイス固有のコンパイルを通過することです。

このコンパイルには、特定のコンピューティングデバイスのさらなる最適化が含まれており、コンピューティングデバイスが実行できるアーティファクトを出力します。

完了すると、Core MLはこれらのアーティファクトをキャッシュして、後続のモデルロードに使用します。

Core MLは、特殊なアセットをディスクにキャッシュします。

それらはモデルのパスと構成に結びついています。

これらのアセットは、アプリの起動とデバイスの再起動にわたって持続することを目的としています。

デバイスの空きディスク容量が不足している場合、システムアップデートがあった場合、またはコンパイルされたモデルが削除または変更された場合、オペレーティングシステムはキャッシュを削除します。

これが発生すると、次のモデルロードはデバイスの特殊化を再び実行します。

モデルのロードがキャッシュに当たっているかどうかを調べるには、Core ML Instrumentでアプリを追跡し、ロードイベントを見ることができます。

「準備とキャッシュ」というラベルがある場合、それはキャッシュされていないロードだったので、Core MLはデバイスの特殊化を実行し、結果をキャッシュしました。

ロードイベントに「キャッシュ」というラベルがある場合、それはキャッシュされたロードであり、デバイスの特殊化は発生しませんでした。

これは特にMLProgramモデル向けの新しいものです。

コアMLパフォーマンスレポートは、負荷のコストを可視化することもできます。

デフォルトでは、キャッシュされた負荷の中央値が表示されます。

キャッシュされていないロード時間も表示するオプションが追加されました。

モデルの読み込みはレイテンシとメモリの面で費用がかかる可能性があるため、一般的なベストプラクティスをいくつか紹介します。

まず、アプリの起動時にUIスレッドでモデルをロードしないでください。

代わりに、非同期ロードAPIを使用するか、モデルを怠惰にロードすることを検討してください。

次に、アプリケーションがシーケンス内の各予測のモデルをリロードするのではなく、多くの予測を連続して実行する可能性が高い場合は、モデルをロードしたままにします。

最後に、アプリがしばらく使用しない場合は、モデルをアンロードできます。

これはメモリの圧力を軽減するのに役立ち、キャッシングのおかげで、その後の負荷はより速くなるはずです。

モデルがロードされたら、モデルで予測を実行することを考える時です。

デモに飛び込んで、新しい非同期オプションを表示します。

新しい非同期予測APIを表示するには、画像のギャラリーを表示し、画像にフィルターを適用できるアプリを使用します。

グレースケール画像を入力として取り、画像のカラーバージョンを出力するCore MLモデルを使用するカラーリングフィルターに焦点を当てます。

これは、動作中のアプリの例です。

グレースケールの元の画像をロードすることから始まり、カラー化画像モードを選択すると、Core MLを使用して画像をカラー化します。

下にスクロールすると、モデルは間違いなく機能していますが、予想より少し遅いです。

また、遠く下にスクロールすると、画像が色付けされるのにかなり時間がかかることに気づきます。

上にスクロールすると、途中ですべての画像を着色するのに時間を費やしていたように見えます。

しかし、私のSwiftUIコードでは、画像を保持するためにLazyVGridを使用しているので、ビューが画面から外れたときにタスクをキャンセルする必要があります。

現在の実装を見て、パフォーマンスが不足している理由と、タスクがキャンセルされることを尊重しない理由を理解してみましょう。

これが実装です。

同期予測APIはスレッドセーフではないため、アプリは予測がモデル上で連続して実行されるようにしなければならない。

これは、ColorizingServiceをアクターにすることで達成され、一度にcolorizeメソッドへの1回の呼び出ししか許可されません。

このアクターは、アプリにバンドルされているモデル用に生成される自動生成されたインターフェイスであるcolorizerModelを所有しています。

Colorizeメソッドは現在、2つの操作を実行します。

最初にモデルの入力を準備します。これには、モデルの入力サイズに合わせて画像のサイズを変更する必要があります。

その後、モデルを介して入力を実行し、色付きの出力を取得します。

私は先に進み、Core ML Instrumentsテンプレートで実行されているアプリのInstrumentsトレースをキャプチャしました。

インスツルメントのトレースを見ると、予測が連続して実行されていることが示され、これはアクターの分離によって保証されます。

しかし、次の予測が実行される前に、各予測の周りにギャップがあり、パフォーマンスの欠如に寄与しています。

これらは、アクターの分離がモデル予測だけでなく、入力準備にも巻き付けられた結果です。

1つの改善点は、入力準備を非分離方法としてマークすることで、次の着色要求がアクターに入るのをブロックしないようにします。

これは役立ちますが、Core MLの予測自体はまだシリアル化され、これは私の処理のボトルネックです。

コアML予測自体の並行性を利用するために、私が検討できるオプションはバッチ予測APIです。

入力のバッチを取り込み、モデルを介して実行します。

内部では、Core MLは可能な限り並行性を利用します。

着色方法のバッチバージョンを作成するのはとても簡単です。

しかし、難しい部分は、入力をバッチに収集し、この方法に渡す方法を理解することです。

このユースケースには、バッチ予測APIの使用を困難にする複数の側面があります。

バッチAPIは、完了すべき作業の既知の量がある場合に最もよく使用されます。

この場合、処理する画像の量は固定ではなく、画面サイズとスクロール量の機能です。

バッチサイズは自分で選ぶことができますが、バッチサイズが満たされていないが、まだ処理する必要があるケースを処理する必要があります。

また、画像がバッチで色付けされる別のUI体験ができます。

最後に、ユーザーがそこから離れてスクロールしても、バッチをキャンセルすることはできません。

これらの課題のために、私はむしろ一度に1つの予測を処理するAPIに固執したいと思います。

これは、新しい非同期予測APIが非常に役立つ場所です。

スレッドセーフで、Swiftの並行性と一緒にCore MLを使用するのに適しています。

コードの非同期デザインに切り替えるには、まずcolorizeメソッドを非同期に変更しました。

次に、APIの新しい非同期バージョンを使用するために必要な予測呼び出しの前にawaitキーワードを追加しました。

その後、ColorizingServiceを俳優ではなくクラスに変更しました。

そうすれば、複数の画像を同時に色付けすることができます。

最後に、方法の開始にキャンセルチェックを追加しました。

非同期予測APIは、特に複数の予測が同時に要求された場合、キャンセルに対応するために最善を尽くしますが、この場合は開始時に追加のチェックを含めるのが最善です。

そうすれば、colorizeメソッドが入力される前にタスクがキャンセルされた場合、入力の準備も回避できます。

次に、これらの変更を行い、アプリを再実行します。

以前と同じように、カラーモードに設定します。

私はすでに画像がはるかに速く色付けされているのを見ることができます。

そして、一番下まですばやくスクロールすると、画像はほぼすぐに読み込まれます。

少し上にスクロールすると、上にスクロールすると画像が色付けされていることを確認できます。つまり、最初に一番下まですばやくスワイプしたときに、色分けの呼び出しが正常にキャンセルされました。

この新しい非同期設計を使用してトレースを見ると、予測が複数の画像で同時に実行されていることがわかります。

これは、垂直に積み重ねられた複数の予測間隔で示されます。

このモデルはニューラルエンジンで部分的に動作するため、ニューラルエンジン機器でも観察できます。

画像を連続的に色付けした最初の実装では、スクロールせずに画像の初期ビューを色付けするのに約2秒かかりました。

画像を同時に色付けする非同期実装に切り替えた後、その時間は約1秒に半分に短縮されました。

全体として、非同期予測APIとColorizerモデルとの並行性を利用することで、総スループットの約2倍の改善を達成することができました。

ただし、特定のモデルとユースケースが同時設計の恩恵を受ける可能性がある金額は、モデルの操作、コンピューティングユニットとハードウェアの組み合わせ、およびコンピューティングデバイスが忙しい可能性のあるその他の作業など、いくつかの要因に大きく依存していることに注意することが重要です。

また、MLプログラムとパイプラインモデルタイプは、予測を同時に実行することから最高のパフォーマンス改善を提供します。

全体として、アプリに並行性を追加するときは、ワークロードを慎重にプロファイリングして、実際にユースケースに利益をもたらしていることを確認する必要があります。

アプリに並行性を追加するときに覚えておくべきもう1つの重要なことは、メモリ使用量です。

多くのモデル入力と出力が同時にメモリにロードされると、アプリケーションのピークメモリ使用量が大幅に増加します。

コアMLインストゥルメントと割り当てインストゥルメントを組み合わせることで、これをプロファイリングできます。

トレースは、カラーライザーモデルを実行するために多くの入力をメモリにロードするにつれて、私のアプリのメモリ使用量が急速に増加していることを示しています。

潜在的な問題は、私のコードのカラーライズメソッドにはフロー制御がないため、同時にカラー化される画像の量には固定された制限がないことです。

モデルの入力と出力が小さい場合、これは問題にならないかもしれません。

しかし、それらが大きい場合、これらの入力と出力の多くのセットを同時にメモリに持つと、アプリのピークメモリ使用量を大幅に増加させることができます。

これを改善する方法は、飛行中の予測の最大量を制限するロジックを追加することです。

これにより、メモリに同時にロードされる入力と出力が少なくなり、予測の実行中にピークメモリ使用量が減少します。

この例では、すでに2つの項目が作業されている場合、以前の作業項目が完了するまで新しい作業項目を延期します。

最善の戦略は、あなたのユースケースに依存します。

たとえば、カメラからデータをストリーミングするときは、作業を延期するのではなく、単に作業をドロップしたいと思うかもしれません。

このようにして、フレームを蓄積したり、もはや一時的に関連性のない作業をしたりすることを避けることができます。

少し下がって、さまざまな予測APIを使用するタイミングに関する一般的なガイダンスをいくつか紹介します。

同期コンテキストにあり、利用可能な各入力間の時間がモデルのレイテンシに比べて大きい場合、同期予測APIはうまく機能します。

入力がバッチで利用可能になった場合、バッチ予測APIは自然に適合します。

非同期コンテキストにあり、大量の入力が時間の経過とともに個別に利用可能になる場合、非同期APIが最も有用です。

最後に、Core MLワークフローを移動すると、モデル開発とモデル統合の両方で最適化の機会がたくさんあります。

新しいコンピューティング可用性APIは、デバイスで利用可能なハードウェアに基づいて、実行時に決定を下すのに役立ちます。

モデルのライフサイクルとキャッシングの動作を理解することは、モデルをいつ、どこでロードおよびアンロードするかを最適に決定するのに役立ちます。

そして最後に、非同期予測APIは、Core MLを他の非同期Swiftコードと統合し、同時予測をサポートすることでスループットを向上させるのに役立ちます。

これはCore MLチームのBenで、私はAIではありません。