111241

♪ ♪

アンドリュー：こんにちは、ビジョンフレームワークのソフトウェアエンジニアのアンドリュー・ラウです。

今日は、人体のポーズ、ビジョンフレームワークの深さの使用、インスタンスマスクで画像から人々を持ち上げることについて話します。

人々の検出と理解は、常にビジョンの焦点であり、数年間、ビジョンフレームワークは2Dで人体ポーズを提供してきました。

復習として、2Dの人体ポーズは、入力画像に対応するスケルトンに定義されたランドマークポイントの正規化されたピクセル座標で観察を返します。

より詳細に飛び込みたい場合は、まだ見ていない場合は、「体と手のポーズを検出する」セッションを確認してください。

ビジョンは、VNDetectHumanBodyPose3DRequestという名前の新しいリクエストで、環境内の人々を3Dにキャプチャするためのサポートを拡大しています。

この要求は、17の関節を持つ3Dスケルトンを返す観測を生成します。

ジョイントは、ジョイント名で、またはジョイントグループ名を提供することでコレクションとしてアクセスできます。

左下の原点に正規化されたビジョンによって返される他の認識されたポイントとは異なり、3D関節の位置は、根関節の原点を持つ現実世界でキャプチャされたシーンに対してメートル単位で返されます。

この最初のリビジョンは、フレーム内で検出された最も著名な人物の1つのスケルトンを返します。

フィットネスアプリを構築していて、ジムでのワークアウトクラスのこの画像でリクエストを実行した場合、観察はカメラに最も近い正面の女性に対応します。

いくつかの文脈で3Dスケルトンの構造をよりよく実証するために、このヨガのポーズを分解しましょう。

当然のことながら、3D人体の骨格は、頭の中央と上部にポイントを含むヘッドグループから始まります。

次に、左右の肩関節、背骨、股関節の中央にある根関節、股関節を含む胴体グループがあります。

一部の関節は複数のグループで返されることを覚えておいてください。

腕には、左右の腕のグループがあり、それぞれに手首、肩、肘があります。

左右は常に人に関連しており、画像の左側や右側ではありません。

最後に、私たちのスケルトンには左右の脚のグループが含まれており、それぞれに対応する股関節、膝、足首の関節があります。

この新しいリクエストを使用するには、他のリクエストと同じワークフローに従うため、以前にコードでVisionを使用したことがある場合は、このフローに馴染みがあるはずです。

まず、新しいDetectHumanBodyPose3DRequestのインスタンスを作成し、検出を実行するアセットで画像要求ハンドラを初期化します。

リクエストを実行するには、リクエストインスタンスをperformに渡します。

また、リクエストが成功すると、VNHumanBodyPose3DObservationがエラーなく返されます。

すべての写真は、3D世界の人々の2D表現です。

ビジョンにより、ARKitやARSessionなしで画像からその3D位置を取得できるようになりました。

これは、3D空間で主題を理解するための強力で軽量なオプションであり、アプリのまったく新しい機能のロックを解除します。

これを理解し、視覚化するのに役立つサンプルアプリを構築しました。

開くと、フォトライブラリから任意の画像を選択できます。

同僚と私は以前のヨガインストラクターの落ち着きに触発されたので、休憩を取り、外に出て、自分でいくつかのポーズを試しました。

今、私はその先生ほど柔軟ではありませんが、私はこのポーズでかなり良い仕事をしました、そしてそれは3Dで素晴らしく見えるはずです。

リクエストを実行して、私を3次元に戻しましょう。

リクエストは成功し、3Dスケルトンは入力画像のどこにいるかと一致しています。

シーンを回転させると、私の腕は伸び、足は私がどのように立っていたかに基づいて腰に比べて正しく見えます。

このピラミッド形状は、画像がキャプチャされたときにカメラが配置された場所を表しています。

パースペクティブの切り替えボタンをタップすると、カメラの位置からのビューになります。

アプリで3D Human Body Poseを使用して素晴らしい体験を生み出すために必要なコードとコンセプトをご案内します。

アプリの構築は、オブザベーションで返されたポイントを使用することから始まります。

それらを取得するための2つの主要なAPIがあります。特定のジョイントの位置にアクセスするための認識されたポイントと、指定されたグループ名を持つジョイントのコレクションにアクセスするための認識されたポイントです。

これらのコア方法に加えて、観察はいくつかの追加の有用な情報を提供します。

まず、bodyHeightは被写体の推定身長をメートル単位で与えます。

利用可能な深さのメタデータに応じて、この高さはより正確に測定された高さまたは1.8メートルの基準高さのいずれかになります。

深さとビジョンについて1分で言いたいことがたくさんあります。

heightEstimationプロパティを使用して、高さを計算するために使用されるテクニックを決定できます。

次に、カメラの位置はcameraOriginMatrixで利用できます。

実生活では、カメラは被写体に正確に直面していない可能性があるため、これは、フレームがキャプチャされたときにカメラが人に関連していた場所を理解するのに役立ちます。

この観測では、共同座標を2Dに戻すためのAPIも提供しています。

これは、返されたポイントを入力画像にオーバーレイまたは整列したい場合に役立ちます。

そして最後に、人が2つの同様の画像を横切ってどのように移動したかを理解するために、カメラに対する特定の関節の位置を取得するためのAPIが利用可能です。

3D人体ポイントの使い方を示す前に、それが継承するビジョンの新しい幾何学クラスを紹介したいと思います。

VNPoint3Dは、3D位置を格納するためのsimd_float 4x4行列を定義する基本クラスです。

この表現は、ARKitのような他のAppleフレームワークと一致しており、利用可能なすべてのローテーションと翻訳情報が含まれています。

次に、この位置を継承するだけでなく、識別子も追加するVNRecognizedPoint3Dがあります。

これは、共同名などの対応する情報を格納するために使用されます。

最後に、今日の焦点は、ローカルポジションと親ジョイントを追加するVNHumanBodyRecognizedPoint3Dです。

ポイントのプロパティを操作する方法について、もう少し詳しく見てみましょう。

recognizedPoint APIを使用して、左手首の位置を取得しました。

関節のモデル位置、またはポイントの位置特性は、常に股関節の中心にある骨格の根関節に相対的です。

位置行列の3番目の列に焦点を合わせると、翻訳の値があります。

左手首のyの値は、この図の腰から0.9メートル上にあり、このポーズに適しているようです。

次に、返されたポイントのlocalPositionプロパティがあります。これは、親ジョイントに対する相対的な位置です。

したがって、この場合、左肘は左手首の親関節になります。

ここの最後の列は、x軸の-0.1メートルの値を示していますが、これは正しいようです。

負の値または正の値は基準点によって決定され、このポーズでは、手首は肘の左側にあります。

localPositionは、アプリが体の1つの領域でのみ動作している場合に便利です。

また、子関節と親関節の間の角度の決定も簡素化されます。

この角度をコードで計算する方法を1秒で示します。

返された3Dポイントを扱う場合、アプリを構築する際に役立つ概念がいくつかあります。

まず、多くの場合、子供と親の関節の間の角度を決定する必要があります。

calculateLocalAngleToParentメソッドでは、親関節に対する相対的な位置を使用してその角度を見つけます。

ノードの回転は、x、y、z軸、またはピッチ、ヨー、ロールに対する回転で構成されています。

ピッチの場合、90度の回転を使用して、SceneKitノードジオメトリをデフォルトの向きからまっすぐ下向きにして、スケルトンに適したものを配置します。

ヨーには、適切な角度を得るために、z座標のアークコサインをベクトルの長さで割った値を使用します。

そして、ロールの場合、角度測定はy座標とx座標のアークタンジェントで得られます。

次に、あなたのアプリは、私のサンプルアプリのように、返された3D位置を元の画像に関連付ける必要があるかもしれません。

ビジュアライゼーションでは、ポイントインイメージAPIを使用して、イメージプレーン、スケールと翻訳の2つの変換に使用します。

まず、画像平面を戻り値に比例して拡大縮小する必要があります。

私は、3Dと2Dの両方で、中央の肩と背骨のような2つの既知の関節の間の距離を取得し、それらを比例して関連付け、この量で画像面を拡大します。

翻訳コンポーネントでは、pointInImage APIを使用して、2D画像内のルートジョイントの位置を取得します。

この方法は、その場所を使用して、x軸とy軸の画像平面のシフトを決定し、VNPoint座標の左下原点と画像の中央にあるレンダリング環境原点の間で変換します。

最後に、カメラの視点からシーンを見たり、その場所でポイントをレンダリングしたりして、cameraOriginMatrixからこれを取得することができます。

正しい向きはレンダリング環境によって異なりますが、これは、このノードのローカル座標系をシーンの残りの部分に関連付けるピボット変換を使用して、この変換情報を使用してノードを配置した方法です。

また、cameraOriginMatrixの回転情報を使用して、逆変換を使用してこのコードでカメラに直面するように画像平面を正しく回転させました。

ここではローテーション情報のみが必要なため、最後の列の翻訳情報は無視されます。

これらすべてのピースをまとめると、私のサンプルアプリに表示されるシーンが可能になりました。

さて、私は数分かけて、ビジョンの深さを含むいくつかのエキサイティングな追加について話し合いたいと思います。

ビジョンフレームワークは、画像またはフレームバッファと一緒に入力として深度を受け入れるようになりました。

VNImageRequestHandlerは、AVDepthDataの新しいパラメータを取るcvPixelBufferとcmSampleBufferの初期化APIを追加しました。

さらに、ファイルにすでに深度データが含まれている場合は、変更せずに既存のAPIを使用できます。

ビジョンは自動的にファイルから深度を取得します。

Apple SDKでDepthを操作する場合、AVDepthDataはすべてのDepthメタデータとインタフェースするためのコンテナクラスとして機能します。

カメラセンサーによってキャプチャされた深度メタデータには、格差または深度形式として表される深度マップが含まれています。

これらのフォーマットは交換可能で、AVFoundationを使用して互いに変換できます。

深度メタデータには、3Dシーンを再構築するために必要な本質的、外因性、レンズの歪みなどのカメラキャリブレーションデータも含まれています。

詳細を知る必要がある場合は、2022年からの「iOSカメラキャプチャの進歩を発見する」セッションを確認してください。

深さは、カメラキャプチャセッションまたは以前にキャプチャしたファイルから取得できます。

カメラアプリでキャプチャされた画像は、写真のポートレート画像と同様に、常にカメラのキャリブレーションメタデータと格差マップとして深度を保存します。

ライブキャプチャセッションで深度をキャプチャする場合、デバイスがLiDARをサポートしている場合は、LiDARを使用するセッションを指定するという利点があります。

LiDARは、シーンの正確なスケールと測定を可能にするため、強力です。

ビジョンはまた、画像内の複数の人と対話するためのAPIを導入しています。

ビジョンは現在、GeneratePersonSegmentationリクエストで周囲のシーンから人々を分離する機能を提供しています。

このリクエストは、フレーム内のすべての人を含む単一のマスクを返します。

ビジョンにより、新しい人インスタンスマスクリクエストでもう少し選択できるようになりました。

この新しいAPIは、最大4つの個人マスクを出力し、それぞれに信頼スコアがあります。

だから今、あなたは画像とは別にあなたの友人を選択して持ち上げることができます。

人以外の被写体を選択して持ち上げる必要がある場合は、VisionKitの被写体リフティングAPIまたはVisionフレームワークのフォアグラウンドインスタンスマスクリクエストを使用できます。

詳細については、「アプリ内の画像から被写体を持ち上げる」セッションをチェックしてください。

以下は、画像から必要な人の特定のインスタンスを選択する方法を示すサンプルコードです。

現在、すべてのインスタンスを返すように指定されていますが、画像内のどの友人に焦点を合わせるかに応じてインスタンス1または2を選択するか、インスタンス0を使用して背景を取得できます。

この新しいリクエストは最大4人までセグメント化されるため、画像に4人以上いる場合は、コードで処理する追加の条件がいくつかあります。

シーンに多くの人々が含まれている場合、返された観察は人々を見逃したり、それらを組み合わせたりする可能性があります。

通常、これはバックグラウンドにいる人々で発生します。

アプリが混雑したシーンに対処しなければならない場合は、可能な限り最高の体験を構築するために使用できる戦略があります。

ビジョンの顔検出APIを使用して、画像の顔の数を数えることができ、4人以上の画像をスキップするか、既存の人物セグメンテーションリクエストを使用して、全員に1つのマスクで作業するかを選択できます。

要約すると、ビジョンは現在、深さ、3D人体ポーズ、および人のインスタンスマスクをサポートして、人々とその環境を理解するための強力な新しい方法を提供しています。

しかし、ビジョンが今年リリースするのはそれだけではありません。

「ビジョンで動物のポーズを検出する」セッションでは、人を超えて、毛むくじゃらの友人の画像で素晴らしい体験を生み出すことができます。

ありがとう、そしてあなたがどんな素晴らしい機能を構築するかを見るのが待ちきれません。

♪ ♪