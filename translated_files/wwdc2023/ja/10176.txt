10176

♪ ♪

リジー：こんにちは！私はリジーで、ここアップルでVisionKitに取り組んでいるエンジニアです。

私は今日、あなたのアプリにサブジェクトリフティングを持ち込む方法についてあなたと話すことに興奮しています。

サブジェクトリフティングはiOS 16で導入され、ユーザーは画像の被写体を選択、持ち上げ、共有できるようになりました。

まず、サブジェクトリフティングの基本について確認します。

次に、新しいVisionKit APIを使用してサブジェクトリフティングを追加する方法を説明します。

最後に、同僚のSaumitroはより深く掘り下げ、新しい基盤となるVision APIを紹介します。

では、正確には主題とは何ですか?

被写体は、写真の前景オブジェクト、またはオブジェクトです。

これは必ずしも人やペットではありません。

それは建物、食べ物のプレート、または靴のペアから何でもかまいません。

画像は、ここでこれらの3杯のコーヒーのように、複数の被写体を持つことができます。

被験者は必ずしも個々のオブジェクトではないことに注意することが重要です。

この例では、男と彼の犬が一緒に画像の焦点であり、それらを1つの組み合わせた被写体になります。

では、どうやってこれをアプリに入れるのですか?

アプリにサブジェクトリフティングを追加するのに役立つ2つの別々のAPIがあります。

VisionKit。

そしてビジョン。

VisionKitを使用すると、箱から出してすぐにシステムのような被写体持ち上げ動作を非常に簡単に採用できます。

ほんの数行のコードで、私たち全員が知っていて愛している被写体リフティングUIを簡単に再現できます。

VisionKitはまた、これらの被写体に関するいくつかの基本的な情報を公開するので、人々に画像の被写体と対話する新しい方法を与えることができます。

これはすべてプロセス外で発生し、パフォーマンス上の利点がありますが、画像サイズが制限されていることを意味します。

ビジョンは低レベルのフレームワークであり、すぐに使えるUIはありません。

これは、ビューに縛られていないことを意味し、より多くの柔軟性を提供します。

画像分析はプロセス内で行われ、VisionKitほど画像の解像度に制限はありません。

最後に、このAPIは、CoreImageを使用するような、より高度な画像編集パイプラインの一部になることができます。

まず、VisionKitのサブジェクトリフティングAPIに飛び込みましょう。

VisionKitで被写体リフティングを追加するには、ImageAnalysisInteractionを初期化し、画像を含むビューに追加するだけです。

これはUIImageViewにすることができますが、そうである必要はありません。

それはとても簡単です。

今、あなたの画像はシステム被写体リフティングインタラクションを持つことになります。

同様に、macOSでは、ImageAnalysisOverlayViewを作成し、画像を含むNSViewのサブビューとして追加します。

ImageAnalysisInteractionまたはImageAnalysisOverlayViewの優先インタラクションタイプを設定して、サポートするVisionKitインタラクションの種類を選択できます。

デフォルトのインタラクションタイプは.automaticで、システムの動作を反映します。

サブジェクトリフティング、ライブテキスト、データ検出器が必要な場合は、このタイプを使用してください。

新しい画像サブジェクトタイプには、テキストをインタラクティブにしたくない場合に、サブジェクトリフティングのみが含まれます。

これらのUIインタラクションに加えて、VisionKitでは、ImageAnalysisを使用して画像の被写体にプログラムでアクセスすることもできます。

画像分析を生成するには、ImageAnalyzerを作成し、分析関数を呼び出すだけです。

目的の画像とアナライザの設定を渡します。

ImageAnalysisの被写体プロパティを使用して、すべての画像の被写体リストに非同期にアクセスできます。

これは、画像とその境界を含む新しいサブジェクト構造体を使用します。

強調表示された主題プロパティは、強調表示された主題のセットを返します。

この例では、下の2つの科目が強調されています。

ユーザーは、それを長押しすることで主題を強調表示できますが、コードで設定された強調表示された主題を更新することで、選択状態を変更することもできます。

非同期サブジェクト(at:)メソッドを使用して、ポイントごとにサブジェクトを検索できます。

この例では、ここをタップすると真ん中の主語が返されます。

その時点で主題がない場合、このメソッドはnilを返します。

最後に、2つの方法で被写画像を生成できます。

単一の被写体については、被写体の画像プロパティにアクセスするだけです。

複数の被写体で構成される画像が必要な場合は、非同期画像(for:)メソッドを使用し、含めたい被写体を渡します。

この例では、下の2つの被写体の画像が必要な場合は、この方法を使用してこの画像を生成できます。

このすべてがデモで一緒になるのを見てみましょう。

私はパズルアプリに取り組んでいます。

ピースをパズルにドラッグしたいのですが、まだ持ち上げられません。

それを直しましょう。

まず、ピースと対話できるように、この画像で被写体リフティングインタラクションを有効にする必要があります。

ImageAnalysisInteractionを作成することでこれを行うことができます...

...そして、単に私の見解に追加するだけです。

ライブテキストを含める必要がないので、ここではimageSubjectインタラクションタイプを使用しました。

すごい！

今、私はパズルのピースを選択し、このようにそれらと対話することができます。

この画像はいかなる方法でも前処理されていません。

これは、被写体を持ち上げるだけで行われます。

私はパズルのピースをパズルに落とすのを処理するためにいくつかのコードを追加し、さらにそれらを所定の位置に調整しました。

かなりクールに見えますが、アプリをさらに魅力的に感じさせたいです。

わずかな3D効果を与えるために、その上にカーソルを合わせると、各パズルピースの下にドロップシャドウを追加することを考えています。

ここにはすでにホバージェスチャーハンドラーがあります。影を追加するだけです。

画像を簡単に編集できないので、代わりに画像レイヤーのトリックでやります。

まず、imageAnalysis.subject(at point:)を呼び出して、被写体の上にカーソルを合わせていることを確認します。

被写体画像のコピーを挿入し、それを灰色にし、元の被写体位置からわずかにオフセットするaddShadow(被写体:)メソッドがあります。

次に、影の上に被写体画像のコピーを追加して、3次元に見えるようにします。

最後に、ホバーポイントが被写体と交差しなかった場合、私は影をクリアします。

試してみましょう。

すごい。作品の上にカーソルを合わせると、影の効果が得られます。

VisionKitを使用すると、アプリでサブジェストリフティングを設定し、わずか数行のコードで楽しいサブジェクト効果を追加することさえできました。

次に、同僚のSaumitroに伝えます。Saumitroは、新しいVision APIとそれをアプリに統合する方法について話します。

サウミトロ:ありがとう、リジー!

こんにちは、私はサウミトロで、ビジョンチームのエンジニアです。

VisionKitのAPIは、サブジェクトリフティングを始める最も簡単な方法です。

より高度な機能を必要とするアプリケーションについては、ビジョンがカバーします。

サブジェクトリフティングは、顕著性や人物セグメンテーションなどのビジョンの既存のセグメンテーションAPIのコレクションに加わります。

それぞれの強みをすばやく見直し、被写体リフティングがどのように適合するかを見てみましょう。

注意や客観性のためのもののような顕著な要求は、粗い地域ベースの分析に最適です。

生成された顕著性マップはかなり低い解像度であるため、セグメンテーションには適していないことに注意してください。

代わりに、画像の自動トリミングなどのタスクに顕著な領域を使用できます。

人物セグメンテーションAPIは、現場の人々のために詳細なセグメンテーションマスクを生産することに輝いています。

人々をセグメント化することに特に集中したい場合は、これを使用してください。

新しい人物インスタンスセグメンテーションAPIは、シーン内の各人に個別のマスクを提供することで、物事をさらに進めます。

詳細については、個人セグメンテーションに関するこのセッションをチェックしてください。

人のセグメンテーションとは対照的に、新しく導入されたサブジェクトリフティングAPIは「クラス不可知論者」です。

フォアグラウンドオブジェクトは、そのセマンティッククラスに関係なく、潜在的にセグメント化される可能性があります。

例えば、この画像の人々に加えて、それがどのように車を拾うかに注目してください。

それでは、関連する重要な概念のいくつかを見てみましょう。

入力画像から始めます。

被写体リフティングリクエストは、この画像を処理し、同じ解像度でソフトセグメンテーションマスクを生成します。

このマスクを取ってソース画像に適用すると、マスクされた画像になります。

それぞれの異なるセグメント化されたオブジェクトはインスタンスと呼ばれます。

ビジョンは、これらのインスタンスに関するピクセル単位の情報も提供します。

このインスタンスマスクは、ソース画像のピクセルをインスタンスインデックスにマッピングします。

ゼロインデックスはバックグラウンド用に予約されており、各フォアグラウンドインスタンスは1から順次ラベル付けされます。

連続してラベル付けされる以外に、これらのIDの順序は保証されません。

これらのインデックスを使用して、ソース画像内のフォアグラウンドオブジェクトのサブセットをセグメント化できます。

インタラクティブなアプリを設計する場合、このインスタンスマスクはヒットテストにも役立ちます。

これらのタスクの両方を実行する方法を少し実演します。

APIに飛び込みましょう。

被写体リフティングは、ビジョンにおける画像ベースのリクエストの使い慣れたパターンに従います。

まず、フォアグラウンドインスタンスマスク要求をインスタンス化し、続いて入力画像で画像要求ハンドラをインスタンス化します。

次に、リクエストを実行します。

ボンネットの下で、これはビジョンが画像を分析して被写体を把握するときです。

効率のためにAppleのハードウェアを利用するように最適化されていますが、それでもリソースを大量に消費する作業であり、UIをブロックしないようにバックグラウンドスレッドに延期するのが最善です。

これを行う一般的な方法の1つは、このステップを別のDispatchQueueで非同期に実行することです。

入力画像で1つ以上の被写体が検出された場合、結果配列には単一のオブザベーションが入力されます。

ここから、マスクとセグメント化された画像の観察を照会することができます。

どのインスタンスがセグメント化され、結果がどのようにトリミングされるかを制御する2つのパラメータを詳しく見てみましょう。

instancesパラメータは、最終的なセグメント化された画像またはマスクで抽出されるオブジェクトを制御するIndexSetです。

例として、この画像には、バックグラウンドインスタンスを含まない2つのフォアグラウンドインスタンスが含まれています。

検出されたすべてのフォアグラウンドインスタンスをセグメント化することは非常に一般的な操作であるため、Visionは、すべてのフォアグラウンドインスタンスインデックスを含むIndexSetを返す便利なallInstancesプロパティを提供します。

ここの画像には、インデックス1と2が含まれます。

バックグラウンドインスタンス0は含まれていないことに注意してください。

また、これらのインデックスのサブセットのみを提供することもできます。

これはちょうど例1です。

そして、ちょうど例2。

また、最終的なマスクされた画像がどのようにトリミングされるかを制御することもできます。

このパラメータがfalseに設定されている場合、出力画像の解像度は入力画像と一致します。

これは、下流の合成操作のために、セグメント化されたオブジェクトの相対的な位置を保持したい場合にいいです。

Trueに設定すると、選択したインスタンスのタイトなクロップになります。

これまでの例では、私は完全にマスクされた画像出力で作業してきました。

ただし、マスク効果を適用するなど、一部の操作では、代わりにセグメンテーションマスクだけで作業する方が便利です。

オブザベーションでcreateScaledMaskメソッドを呼び出すことで、これらのマスクを生成できます。

パラメータは以前と同じように動作します。

出力は、ソフトセグメンテーションマスクを含むシングルチャンネル浮動小数点ピクセルバッファです。

先ほど生成したマスクは、CoreImageでの使用に最適です。

Visionは、VisionKitと同様に、SDR出力を生成します。

ただし、CoreImageでマスキングを実行すると、入力の高いダイナミックレンジが維持されます。

これの詳細については、アプリにHDRを追加するためのセッションをチェックすることを検討してください。

このマスキングを実行する1つの方法は、CIBlendWithMaskフィルターを使用することです。

マスクする必要があるソース画像から始めます。

これは通常、ビジョンに渡したのと同じ画像になります。

ビジョンのcreateScaledMaskコールから得られたマスク。

そして最後に、被写体が上に合成される新しい背景画像。

これに空の画像を使用すると、背景が透明になります。

あるいは、新しい背景の上に結果を合成する予定がある場合は、ここに直接渡すことができます。

そして、それはほとんどそれです。

出力は、HDRで保存されたマスクされた合成された画像になります。

では、すべてをまとめ、クールな被写体リフティングビジュアルエフェクトアプリを構築しましょう。

背景を削除してその下にビューを表示したり、何か他のものに置き換えたりできます。

さらに、プリセットエフェクトの1つを適用できます。

そして、エフェクトは選択した背景で構成されています。

フォアグラウンドインスタンスをタップして、選択的に持ち上げることもできます。

このアプリの作成にどのようにアプローチするかの概要を見てみましょう。 見てみましょう。

私たちのアプリのコアは、UIからの入力を受け入れ、最終的な出力を生成するために必要なすべての作業を実行するエフェクトパイプラインに依存しています。

ソース画像で被写体リフティングを行うことから始めます。

オプションのタップで、個々のインスタンスを選択できます。

結果のマスクはソース画像に適用されます。

そして最後に、選択した背景と視覚効果が適用され、合成され、最終的な出力画像が生成されます。

これらの最後の2つのステップは、CoreImageを使用して達成されます。

私たちのトップレベル機能は、入力画像、選択した背景画像と効果、および潜在的にインスタンスの1つを選択するためのユーザーからのタップ位置を取ります。

ここのエフェクトタイプは、プリセットの単純な列挙型です。

その出力は最終的な合成画像であり、UIに表示する準備ができています。

このタスクは2つの段階に分解できます。

まず、選択したインスタンスのサブジェクトマスクを生成します。

そして第二に、そのマスクを使用して選択した効果を適用します。

最初の段階から始めましょう。

この段階への入力は、ソース画像とオプションのタップ位置です。

私たちはすでにここで、単にビジョン要求を実行し、マスクを返すコードのほとんどに遭遇しました。

興味深いのは、ラベルマスクを使用してタップ位置を一連のインデックスにマッピングするこの行です。

詳しく見てみましょう。 

タップが見つからない場合は、デフォルトですべてのインスタンスを使用します。

タップ位置をインスタンスマスクのピクセルにマッピングしたい。

ここには関連する情報が2つあります。

まず、UIは、それを渡す前に、タップ位置を0、1に正規化します。

ディスプレイの解像度やスケーリング要因などの詳細を心配する必要がないので、これはいいですね。

第二に、左上に原点を持つデフォルトのUIKit座標系を使用します。

これは、ピクセルバッファの画像空間座標と一致しています。

そのため、この既存のビジョンヘルパー機能を使用して、この変換を実行できます。

私は今、タップされたインスタンスラベルを検索するために必要なすべての情報を持っています。

これには、ピクセルバッファのデータに直接アクセスすることが含まれており、次にその方法を紹介します。

ラベルを手に入れたら、ゼロかどうかを確認します。 

ゼロラベルは、ユーザーが背景ピクセルをタップしたことを意味することを思い出してください。

この場合、すべてのインスタンスを選択することにフォールバックします。

それ以外の場合は、選択したラベルだけでシングルトンセットを返します。

このコードは、インスタンスラベルのルックアップの実行方法を入力します。

他のピクセルバッファと同様に、データにアクセスする前にまずロックする必要があります。

読み取り専用アクセスは、私たちの目的には十分です。

ピクセルバッファの行はアライメントのためにパディングされる可能性があるため、ピクセルのバイトオフセットを計算する最も堅牢な方法は、そのbytesPerRow値を使用することです。

instanceMaskはシングルチャネルのUInt8バッファなので、それ以上のスケーリングを心配する必要はありません。

インスタンスマスクから読み終わったので、バッファのロックを解除できます。

そして、それを包んで、私は選択したインスタンスを分離したマスクを持っています。

私は今、効果の適用に進むことができます。

ここでの最初のステップは、選択したエフェクトを背景に適用することです。

それが完了したら、CoreImageを使用して、変換された背景の上にマスクされた被写体を合成します。

最初のいくつかの効果は、既存のCoreImageフィルターの非常に簡単で直接的なアプリケーションです。

たとえば、被写体を強調表示するために、露出調整フィルターを使用して背景を暗くしました。

ボケ効果は少し複雑です。

背景をぼかすことに加えて、選択した被写体を強調するハローが欲しいです。

ぼやける前に被写体のための白い切り抜きがトリックを行います。

これを達成するための迅速な方法は、現在の機能を再利用し、被写体に真っ白な画像を渡すことです。

そして、それで、私は合成のためのベースレイヤーを持っています。

最後に、先ほどのCoreImageブレンドスニペットをドロップします。

これは、新しく変換された背景の上に持ち上げられた被写体を合成します。

そして、エフェクトパイプラインの最後の部分が整ったことで、アプリは完成しました。

新しいサブジェクトリフティングAPIで何が可能かを味わうことができることを願っています。

要約すると、VisionKitは、被写体リフティングをアプリに組み込む最速の方法です。

より高度なアプリケーションについては、VisionのAPIにドロップダウンできます。

そして最後に、CoreImageは、被写体リフティングでHDR対応の画像処理を実行するための完璧なコンパニオンです。

リジーと私はあなたがこのビデオを楽しんだことを願っています、そして私たちはあなたが作るものを見ることにとても興奮しています。

♪ ♪