10101

♪ ♪

イーサン:こんにちは、私はイーサンです。

私はSiri Understandingチーム出身で、音声認識のエキサイティングな開発についてお話しします。

iOS 10では、Speechフレームワークを導入しました。

これにより、Siriとキーボードディクテーションを強化するのと同じ技術を活用して、シンプルで直感的なインターフェイスを使用して音声対応アプリを作成することができました。

ただし、音声認識器クラスは、箱から出して、すべてのアプリに適しているわけではありません。

理由を説明するために、音声認識がどのように機能するかについて話しましょう。

音声認識システムは、まず音声データを音響モデルに供給し、音声表現を生成します。

その後、音声表現は書面形式または転写に変換されます。

場合によっては、複数の音声表現がオーディオデータに適合したり、単一の音声表現が複数の転写に対応する場合があります。

このような場合、複数の候補者の転写が終わり、曖昧さを解消する方法が必要です。

これを行うには、言語モデルと呼ばれるものを採用しています。

言語モデルは、特定の単語が一連の単語で次に来る可能性を予測します。

文全体に適用すると、その文がおそらくナンセンスであるかどうかの感触を与えることができます。

言語モデルは、モデルがトレーニング中にさらされた使用パターンに基づいて、ありそうもない候補者を拒否するのに役立ちます。

iOS 10以降、Speechフレームワークは、使いやすいインターフェイスを提示するために、このプロセス全体をカプセル化してきました。

それが理想的ではないかもしれない理由を理解するために、例を考えてみましょう。

私はチェスをするのが大好きで、ユーザーが個々の動きだけでなく、一般的な開口部や防御を指示できるチェスアプリに取り組んできました。

ここで、私の対戦相手は古典的なクイーンズギャンビットをプレイしました。

私は勉強していて、反応E5、アルビンのカウンターギャンビットが好きです。

アルビンのカウンターギャンビットをプレイする。

ああ、問題があります。お困りします

認識者は、私のチェスの動きを音楽リクエストとして誤って認識しています。

リコグナイザが使用する言語モデルは、トレーニングプロセス中に多くの音楽リクエストにさらされたため、「アルバムを再生」の後にアルバム名が続くなどのクエリが用意されています。

逆に、それはおそらく私の好きな転写に遭遇したことがないでしょう。

言語モデルの動作を抽象化することで、Speechフレームワークは、異なるドメインが異なる動作を必要とするにもかかわらず、すべてのアプリに同じモデルを使用することを強制します。

iOS 17では、SFSpeechRecognizerの言語モデルの動作をカスタマイズし、アプリケーションに合わせて調整し、その精度を向上させることができます。

言語モデルのカスタマイズを開始するには、まずトレーニングデータのコレクションを作成します。

開発プロセス中にこれを行うことができます。

次に、アプリでデータを準備し、認識要求を設定してから実行します。

トレーニングデータの収集を構築するプロセスについて話しましょう。

高いレベルでは、トレーニングデータは、アプリのユーザーが話す可能性が高いフレーズを表すテキストのビットで構成されます。

これらは、モデルにこれらのフレーズを期待し、正しく認識される可能性を高めることを教えます。

認識器がどれほど有能であるか、そして時間の経過とともにどれだけ改善されるかを見るのは驚くべきことなので、頻繁に実験してください。

スピーチフレームワークは、トレーニングデータのコンテナとして機能する新しいクラスを導入します。

結果ビルダーDSLを使用して構築されています。

PhraseCountオブジェクトを使用して、正確なフレーズまたはフレーズの一部を提供できます。

PhraseCountは、サンプルを最終的なデータセットで何回表現する必要があるかも記述します。

これは、特定のフレーズを他のフレーズよりも重み付けするために使用できます。

システムで受け入れることができるのはそれほど多くのデータだけなので、全体的なトレーニングデータ予算に対してフレーズをブーストする必要性のバランスを取ってください。

テンプレートを活用して、通常のパターンに合った多数のサンプルを生成することもできます。

ここでは、一緒にチェスの動きを構成する単語の3つのクラスを定義しました。

私がターゲットとしているファイルとして倍増する移動するピース、ボードのどちら側でプレイするかを定義するロイヤルピース、および移動するランク。

それらをパターンにまとめることで、すべての可能な動きを表すデータサンプルを簡単に生成できます。

ここでは、カウントがテンプレート全体に適用されるので、チェスの動きを表す10,000のサンプルを取得し、結果のすべてのデータサンプルに均等に分割します。

データオブジェクトの構築が完了したら、それをファイルにエクスポートし、他のアセットと同様にアプリにデプロイします。

アプリが専門用語、例えば医薬品の名前を含む医療アプリを使用している場合は、それらの用語のスペルと発音の両方を定義し、その使用法を示すフレーズ数を提供できます。

発音はX-SAMPA文字列の形で受け入れられます。

各ロケールは、発音記号の一意のサブセットをサポートしています。

ロケールとサポートされているシンボルの完全なセットについては、ドキュメントを参照してください。

私のアプリでは、リコグナイザがフランスの防衛の一般的な変種であるWinawerのバリエーションを理解できるようにしたい。

このロケールでサポートされているX-SAMPAシンボルのサブセットを使用して発音を説明します。

同じAPIを使用して、アプリが実行時にアクセスできるデータをトレーニングできます。

ユーザーが学ぼうとしているチェスの開口部や防御に焦点を当てるなど、ユーザーに固有の使用パターンをサポートするためにこれを行うことができます。

また、名前付きエンティティでトレーニングすることもできます。

たぶん、あなたのアプリは、ユーザーの連絡先に対するネットワークプレイをサポートしています。

そして、いつものように、ユーザーのプライバシーを尊重することが最も重要です。

たとえば、通信アプリは、それらの連絡先が通話履歴に表示される頻度に基づいて、連絡先を呼び出すコマンドをブーストしたい場合があります。

この種の情報は常にデバイスにとどまるべきです。

アプリ内から同じメソッドを呼び出して、データオブジェクトを生成し、ファイルに書き込み、前述のように取り込むだけです。

トレーニングデータが生成されると、単一のロケールにバインドされます。

1つのスクリプト内で複数のロケールをサポートしたい場合は、NSLocalizedStringなどの標準ローカリゼーション機能を使用してサポートできます。

では、モデルをアプリにデプロイすることについて話しましょう。

まず、前のステップで生成したファイルを受け入れ、後で使用する2つの新しいファイルを生成する新しいメソッドprepareCustomLanguageModelを呼び出す必要があります。

このメソッド呼び出しは、大量の関連するレイテンシを持つ可能性があるため、メインスレッドから呼び出し、ロード画面などのUIの背後にレイテンシーを隠すのが最善です。

場合によっては、ユーザーのプライバシーを尊重するために、生成されたデバイスにデータを保持する必要があります。

LMカスタマイズは、ネットワーク経由でカスタマイズデータを送信しないことで、これをサポートします。

すべてのカスタマイズされた要求は、デバイス上で厳密にサービスされます。

アプリが音声認識要求を作成するときは、まず認識がデバイスで実行されることを強制します。

そうしないと、カスタマイズなしでリクエストが処理されます。

次に、リクエストオブジェクトに言語モデルを添付します。

今、私のアプリでLMカスタマイズをオンにして...

アルビンのカウンターギャンビットをプレイする。

私のカスタム用語も機能します。

Winawerのバリエーションを再生します。

言語モデルをカスタマイズすることで、リコグナイザをアプリケーションのドメインに調整し、その動作をある程度制御できるようになりました。

最も重要なことは、アプリの音声認識精度を向上させたことです。

スピーチフレームワークは、より多くのアプリとより多くのユーザーに適応できるようになったので、さらに強力になり、より良い体験を生み出すために使用できます。

言語モデルのカスタマイズは、音声認識器を強化し、アプリに合わせてカスタマイズする方法を提供します。

私はあなたがそれで達成するすべての素晴らしいことを見てとても興奮しています。

ありがとう、そしてセンターのためにプレーすることを忘れないでください。

♪ ♪