10042

♪ ♪

ダグ:皆さん、ようこそ。私はダグ・デビッドソンです。自然言語処理についてお話しします。

現在、過去数年間、自然言語処理に関する多くのセッションがありました。

今日私たちが話すことは、いくつかのエキサイティングな新機能を追加して、そのすべてに基づいています。

まず、NLPとNLPモデルの背景をいくつかお伝えします。

次に、既存の機能をまとめます。

では、今年の新機能について話します。

高度なアプリケーションについて話し合います。

そして、私はそれを包みます。

いくつかの背景から始めましょう。

模式的には、NLPモデルは一般的に同様の流れを持っています。

彼らはテキストデータから始まり、それを数値特徴表現に変換する入力レイヤーを持ち、その上で機械学習モデルが行動し、いくつかの出力を生成します。

過去数年間の最も明白な例は、テキストの分類と単語のタグ付けでサポートされているCreate MLモデルです。

フィールドとしてのNLPの開発は、入力層のますます洗練されたバージョンの開発だけで、かなり密接に追跡できます。

10年か20年前、これらは単純な正書法の特徴でした。

それから約10年前、Word2VecやGloVeなどの静的な単語埋め込みの使用に移行しました。

次に、CNNやLSTMなどのニューラルネットワークモデルに基づくコンテキストワード埋め込み。

そして最近では、トランスフォーマーベースの言語モデル。

埋め込みとは何かについて一言言うべきです。

最も単純な形式では、言語内の単語から抽象的なベクトル空間のベクトルへの単なるマップですが、同様の意味を持つ単語がベクトル空間で互いに近いように機械学習モデルとして訓練されています。

これにより、言語知識を組み込むことができます。

静的埋め込みは、単語からベクトルへの単純なマップにすぎません。

単語を渡すと、モデルはそれをテーブルでルックアップし、ベクトルを提供します。

これらは、似たような意味を持つ単語がベクトル空間で互いに近いように訓練されています。

これは個々の単語を理解するのに非常に役立ちます。

より洗練された埋め込みは、文内の各単語が文での使用に応じて異なるベクトルにマッピングされるように、動的で文脈的です。

たとえば、「ファーストフードジョイント」の「食べ物」は、「思考のための食べ物」の「食べ物」とは異なる意味を持つため、異なる埋め込みベクトルを取得します。

さて、入力レイヤーとして強力な埋め込みを持つことのポイントは、転送学習を可能にすることです。

埋め込みは大量のデータで訓練され、言語の一般的な知識をカプセル化し、大量のタスク固有のトレーニングデータを必要とせずに特定のタスクに転送できます。

現在、Create MLはELMoモデルを使用してこの種の埋め込みをサポートしています。

これらのモデルは、出力を組み合わせて埋め込みベクトルを生成するLSTMに基づいています。

これらは、Create MLを介してトレーニング分類とタグ付けモデルに使用できます。

さて、これまでにサポートされてきたモデルについて説明しましょう。

これらは2019年と2020年の以前のセッションで非常に詳細に議論されたので、ここで簡単に説明します。

自然言語は、一般的にNLPモデルで見てきたパターンに従うCreate MLを使用したモデルトレーニングをサポートしています。

これには、テキスト分類と単語タグ付けの2つの異なるタスクのモデルが含まれます。

テキスト分類では、出力は一連のクラスの1つを使用して入力テキストを記述します。

例えば、それはトピックや感情かもしれません。

また、単語のタグ付けでは、入力テキストの各単語にラベルを付けます。たとえば、スピーチの一部やロールラベルです。

また、サポートされているCreate MLモデルは、一般的にNLPフィールドの進化に従い、maxentおよびCRFベースのモデルから始まり、静的単語埋め込みのサポートを追加し、ELMo埋め込みを使用してCreate MLモデルの動的単語埋め込みを追加しました。

また、以前のセッション、2019年の「自然言語フレームワークの進歩」と2020年の「自然言語でアプリをよりスマートにする」で、この詳細を見ることができます。

では、今年の自然言語の新機能に目を向けてみましょう。

私たちは今、トランスフォーマーベースのコンテキスト埋め込みを提供していることを嬉しく思います。

具体的には、これらはBERT埋め込みです。

これは、トランスフォーマーからの双方向エンコーダ表現の略です。

これらは、マスクされた言語モデルスタイルのトレーニングを使用して、大量のテキストでトレーニングされた埋め込みモデルです。

これは、モデルに1つの単語がマスクされた文を与えられ、例えば「思考のための食べ物」の「食べ物」という単語を提案するように求められ、これをより良くするために訓練されることを意味します。

彼らの中心にあるトランスフォーマーは、注意メカニズムと呼ばれるもの、特にマルチヘッドの自己注意に基づいており、モデルは一度に複数の異なる方法で、異なる重みを持つテキストの異なる部分を考慮することができます。

多頭の自己注意メカニズムは、他の複数のレイヤーで包まれ、数回繰り返され、大量のテキストデータを活用できる強力で柔軟なモデルを提供します。

実際、一度に複数の言語で訓練することができ、多言語モデルにつながります。

これにはいくつかの利点があります。

これにより、多くの言語をすぐにサポートし、一度に複数の言語をサポートすることさえできます。

しかし、それ以上に、言語間の類似性のために、ある言語のデータが他の言語に役立つような相乗効果があります。

そのため、さまざまな言語ファミリーで27の異なる言語をすぐにサポートしました。

これは、関連するライティングシステムを共有する言語のグループのためにそれぞれ1つずつ、3つの別々のモデルで行われます。

したがって、ラテン文字言語のモデルが1つ、キリル文字を使用する言語のモデルが1つ、中国語、日本語、韓国語のモデルが1つあります。

これらの埋め込みモデルは、先ほど説明したCreate MLトレーニングにぴったりフィットし、入力エンコーディングレイヤーとして機能します。

これは、多くの異なるモデルのための強力なエンコーディングです。

さらに、トレーニングに使用するデータはすべて1つの言語である必要はありません。

これがどのように機能するかを例で示しましょう。

メッセージングアプリを書いていて、受信したメッセージを自動的に分類してユーザーを支援したいとします。

それらを3つのカテゴリに分けたいとします。友人から受け取る可能性のある個人的なメッセージ、同僚から受け取る可能性のあるビジネスメッセージ、およびやり取りする企業から受け取る可能性のある商業メッセージです。

しかし、ユーザーは多くの異なる言語でメッセージを受け取る可能性があり、あなたはそれを処理したい。

この例では、英語、イタリア語、ドイツ語、スペイン語の複数の言語でトレーニングデータをまとめました。

json形式を使用しましたが、ディレクトリやCSVを使用することもできます。

モデルを訓練するために、Create MLアプリに入り、プロジェクトを作成します。

次に、トレーニングデータを選択する必要があります。

また、それに合わせて検証データとテストデータも準備しました。

次に、アルゴリズムを選択する必要があり、ここに新しい選択肢があります。BERT埋め込みです。

それらを選択したら、スクリプトを選択できます。

これらはラテン文字の言語なので、ラテン語に残しておきます。

単一の言語を使用している場合は、ここで指定するオプションがありますが、これは多言語なので、自動のままにします。

その後、トレーニングを押すだけで、モデルトレーニングが開始されます。

トレーニングの最も時間のかかる部分は、これらの強力な埋め込みをテキストに適用することです。

その後、モデルは高い精度でかなり迅速に訓練されます。

その時点で、いくつかのメッセージの例で試してみることができます。

英語...またはスペイン語で。

そして、モデルはこれらが商業的なメッセージであるとかなり確信しています。

可能な相乗効果の例として、このモデルはフランス語で訓練されていませんが、フランス語のテキストも分類できます。

ただし、興味のある言語ごとにトレーニングデータを使用することをお勧めします。

これまでのところ、Create MLで作業しているばかりですが、NLContextualEmbeddingと呼ばれる新しいクラスで自然言語フレームワークを使用してこれらの埋め込みを操作することも可能です。

これにより、必要な埋め込みモデルを識別し、そのプロパティのいくつかを見つけることができます。

埋め込みモデルは、言語やスクリプトなど、さまざまな方法で検索できます。

そのようなモデルを取得したら、ベクトルの次元などのプロパティを取得できます。

また、各モデルには識別子があり、これはモデルを一意に識別する文字列にすぎません。

たとえば、モデルで作業を開始すると、言語によってそれを見つけるかもしれませんが、後でまったく同じモデルを使用していることを確認し、識別子でこれを行うことができます。

覚えておくべきことの1つは、他の多くの自然言語機能と同様に、これらの埋め込みモデルは必要に応じてダウンロードされるアセットに依存しているということです。

NLContextualEmbeddingは、使用前にダウンロードを要求するなど、これに対する追加の制御を与えるためにいくつかのAPIを提供します。

特定の埋め込みモデルに現在デバイスで利用可能なアセットがあるかどうかを尋ねることができ、リクエストを入れないと、ダウンロードされます。

さて、あなた方の何人かは、私はCreate MLを使用してトレーニングしないいくつかのモデルを持っていますが、代わりにPyTorchまたはTensorFlowを使用してトレーニングしていると言っているかもしれません。

これらの新しいBERT埋め込みを引き続き使用できますか?

はい、できます。

私たちは、あなたが訓練したいほぼすべてのモデルの入力レイヤーとして使用できる、利用可能なこれらの事前に訓練された多言語埋め込みモデルを提供します。

これがその仕組みです。

macOSデバイスでは、NLContextualEmbeddingを使用して、トレーニングデータの埋め込みベクトルを取得します。

次に、これらを入力としてPyTorchまたはTensorFlowを使用してトレーニングにフィードし、Core MLツールを使用して結果をCore MLモデルに変換します。

次に、デバイス上の推論時に、NLContextualEmbeddingを使用して入力データの埋め込みベクトルを取得し、それらをCore MLモデルに渡して出力を取得します。

これをサポートするために、モデルをロードし、テキストに適用し、結果の埋め込みベクトルを取得できる追加のNLContextualEmbedding APIがあります。

以前のモデル識別子を覚えている場合は、それを使用して、トレーニングに使用したのと同じモデルを取得できます。

その後、モデルをテキストに適用して、NLContextualEmbeddingResultオブジェクトを与えることができます。

このオブジェクトを取得したら、それを使用して埋め込みベクトルを反復できます。

さて、これで何が可能かを味わうために、簡単なサンプルモデルを用意しました。

既存の英語の安定した拡散モデルから始めて、いくつかの多言語データを使用して微調整し、新しいBERT埋め込みを入力レイヤーとして使用し、それらを固定として取り、寸法を変換するための単純な線形投影レイヤーをトレーニングしました。

結果は、多言語入力を取る安定した拡散モデルです。

モデルからの出力の例をいくつか紹介します。

「ピンクの花でいっぱいの庭を通る道」など、英語のテキストを通り過ぎると、モデルは私たちをピンクの花でいっぱいの庭に導きます。

しかし、また、同じ文章をフランス語、スペイン語、イタリア語、ドイツ語に翻訳すると、モデルはそれぞれにピンクの花でいっぱいの小道や庭の画像を生成します。

もう少し複雑な例を挙げてみましょう。

「曇り空の下の木々や山の前の道路」これは、道路、木、山、雲を含むモデルからの出力です。

しかし、同様に、私は同じ文をフランス語、スペイン語、イタリア語、ドイツ語、または他の多くの言語のいずれかに翻訳することができ、それぞれについて道路、木、山、雲のイメージを得ることができます。

では、このセッションの教訓をまとめましょう。

Create MLを使用すると、テキスト分類や単語タグ付けタスクのモデルを簡単にトレーニングでき、新しい多言語BERT埋め込みモデルは、この目的のために強力な入力エンコーディングレイヤーを提供します。

これらのモデルは、単一言語または多言語にすることができます。

また、BERT埋め込みを、PyTorchまたはTensorFlowでトレーニングするモデルの入力レイヤーとして使用することもできます。

ありがとうございます。

さあ、外に出て、いくつかのモデルのトレーニングを始めましょう。

♪ ♪