10050

♪ ♪

Denis: こんにちは、私の名前はDenis Vieriuで、AppleのGPU、グラフィックス、ディスプレイソフトウェアグループのソフトウェアエンジニアです。

今日は、メタルで今年機械学習に導入されたすべての新機能と機能強化を紹介します。

まず、既存の機械学習バックエンドを要約します。

Metal機械学習APIは、Metal Performance Shadersフレームワークを通じて公開されます。

MPSは、画像処理、線形代数、機械学習など、さまざまな分野向けの高性能GPUプリミティブのコレクションです。

MPSGraphは、MPSフレームワークの上にあり、多次元テンソルへのサポートを拡張する汎用計算グラフです。

CoreMLのような機械学習推論フレームワークは、MPSGraphバックエンドの上に構築されています。

MPSGraphは、TensorFlowやPyTorchなどのトレーニングフレームワークもサポートしています。

MPSGraphとMLフレームワークの詳細については、ここにリストされている以前のMetal WWDCトークを参照してください。

このセッションでは、PyTorchとTensorFlow Metalバックエンドに追加されたアップデートと機能強化、JAXの新しいGPUアクセラレーション、およびMPSGraph for ML Inferenceに今年追加された機能に焦点を当てます。

PyTorchとTensorFlow Metalアクセラレーションを使用すると、MPSの高効率カーネルを使用して、Macで最高のパフォーマンスを得ることができます。

PyTorch Metalアクセラレーションは、MPSバックエンドを通じてバージョン1.12から利用可能になりました。

これは昨年PyTorchエコシステムに導入され、それ以来、メモリ使用量とビューテンソルを最適化するために複数の改善が行われました。

今年、PyTorch 2.0 MPSバックエンドは大きな飛躍を遂げ、ベータステージの資格を取得しました。

しかし、これらはすべての改善ではありませんでした。

最新のPyTorchビルドには、MPS操作プロファイリング、カスタムカーネル、自動混合精度サポートなど、多くの新しいアップデートが含まれています。

毎晩のビルド機能をすべてカバーする前に、PyTorch 2.0の新機能から始めます。

グリッドサンプラー、三角ソルプト、topkなどの操作を含む、最も使用されているトップ60のトーチ演算子のサポートがあります。

テストのカバレッジは大幅に改善されました。

これには、ほとんどのトーチ演算子のテスト、勾配テスト、およびModuleInfoベースのテストが含まれます。

リリース以来、複数の人気モデルがmacOSの公式バックエンドとしてMPSを採用したため、ネットワークカバレッジは拡大しました。

これには、WhisperAIなどの基礎モデル、YOLOなどの物体検出モデル、安定した拡散モデルなどが含まれます。

最新のPyTorch 2.0を使用して、これらのモデルの1つを実際に確認しましょう。

この例では、M2 Maxで実行されている物体検出ネットワークであるYoloV5を使用しています。

左側には、ネットワークが実行され、PyTorch MPSバックエンドを使用してライブイメージを生成していますが、右側にはまったく同じモデルがありますが、CPUで実行されています。

左側は、MPSバックエンドを使用して、著しく高いフレームレートで実行されています。

さらに、開発者は外部ネットワークでPyTorch MPSバックエンドを採用しただけでなく、ヒストグラム、group_norm、signbitなど、複数の新しい演算子のコードを貢献しました。

次に、MPS操作のプロファイリングサポートから始めて、最新のPyTorchビルドで利用可能な新機能について説明します。

PyTorchの夜間ビルドには、OSの標識を使用して、操作実行の正確な実行時間、CPUとGPU間のコピー、およびサポートされていない演算子によって引き起こされるCPUへのフォールバックを示すプロファイリングサポートがあります。

インスツルメンツの一部である非常に使い慣れたツールであるメタルシステムトレースでプロファイリングデータを視覚化することができます。

Metal System Traceを使用したMLアプリケーションのプロファイリングの詳細については、昨年のセッション「Metalで機械学習を加速する」を見ることをお勧めします。

プロファイラを使用するのはとても簡単なプロセスです。

MPSプロファイラパッケージのstartメソッドを呼び出してトレースを有効にし、スクリプトの最後にstopメソッドを使用してプロファイリングを終了します。

次に、プロファイラを見て、例をデバッグします。

このサンプルネットワークは、モデル内の合計7つのレイヤーを持つ線形変換とソフトシュリンク活性化機能で構成されるシーケンシャルモデルを使用します。

このモデルの現在の性能は満足のいくものではありません。

この場合、プロファイラを使用してボトルネックを見つけることができます。

メタルシステムトレースで、まず、os_signpostを有効にしてください。

これにより、PyTorchオペレーター情報をキャプチャできます。

次に、デバイスと適切な実行可能ファイル、この場合はPythonバイナリが設定されていることを確認します。

次に、録画ボタンをクリックします。

Instrumentsは現在、PyTorchの実行を記録しています。

十分なデータをキャプチャするために、数秒間実行します。

次に、[停止]をクリックします。

os_signpostタブで、PyTorch Intervalsのタイムラインを開示します。

このタイムラインは、文字列識別子、データ型、コピー長などのPyTorchメタデータとともに、演算子の実行時間を表示します。

タイムラインにズームインすると、この例で使用されるPyTorch演算子が明らかになります。

このトレースのパターンは、7つのレイヤーで作られたカスタムシーケンシャルモデルに簡単に識別できます。

トレースから、ボトルネックがCPUへのSoftshrinkフォールバックにあることは明らかです。

このプロセスは非常に非効率的です。

このモデルは、GPUが飢えている間、Softshrink演算子のCPU実行と追加コピーからのオーバーヘッドを負担します。

GPUタイムラインのギャップのほとんどは、CPUにフォールバックするSoftshrinkアクティベーション機能から来ています。

これを修正するために、パフォーマンスを向上させるためのカスタムカーネルを書きます。

カスタム操作を書くには4つのステップがあります。

まず、Objective-CとMetalで操作を実装します。

次に、Objective-CコードのPythonバインディングを作成し、拡張機能をコンパイルします。

最後に、拡張機能が構築されたら、操作をトレーニングスクリプトにインポートして使用を開始します。

操作の実装から始めます。

トーチ拡張ヘッダーをインポートすることから始めます。

これには、C++拡張機能を書くために必要なすべてのPyTorchビットが含まれます。

次に、計算関数を定義し、get_command_buffer MPSバックエンドAPIを使用して、MPSStreamコマンドバッファへの参照を取得します。

同様に、get_dispatch_queue APIを使用して、シリアルキューへの参照を取得します。

次に、コマンドバッファを使用してエンコーダを作成し、カスタムGPUカーネルを定義します。

ディスパッチキュー内のカーネルをエンコードして、複数のスレッドからの送信がシリアル化されるようにします。

すべての作業がエンコードされた後、同期APIを使用して、現在のコマンドバッファの実行が完了するまで待機し、シリアル化された送信を観察できます。

または、シリアル化が不要な場合は、コミットAPIを使用してください。

次に、カスタム関数をバインドします。

PYBIND11を使用して、非常に簡単な方法でObjective-C関数をPythonにバインドできます。

この拡張機能では、必要なバインディングコードは2行にしか及びません。

バインド後、拡張機能をコンパイルします。

最初にtorch.utils.cpp_extensionをインポートします。

これは、拡張機能をコンパイルするために使用できるロード機能を提供します。

次に、ビルドする拡張機能の名前を渡してから、ソースコードファイルへの相対パスまたは絶対パスのリストを渡します。

オプションで、ビルドに転送する追加のコンパイラフラグを一覧表示できます。

ロード関数は、ソースファイルを共有ライブラリにコンパイルし、その後、現在のPythonプロセスにモジュールとしてロードされます。

最後に、演算子をスクリプトにインポートして使用を開始します。

まず、コンパイルされたライブラリをインポートし、以前のシーケンシャルモデルを変更して、カスタムSoftshrinkカーネルを使用します。

同じモデルをもう一度実行して、結果を確認しましょう。

新しく追加されたカスタムオペレーターにより、モデルははるかに効率的に実行されます。

CPUへのフォールバックによって作成されたすべてのコピーと中間テンソルがなくなり、シーケンシャルモデルははるかに高速に実行されます。

それでは、ネットワークをさらに改善する方法をさらに探りましょう。

PyTorch MPSバックエンドは自動混合精度をサポートするようになりました。これにより、より少ないメモリを使用し、品質を損なうことなく、より速くトレーニングできます。

混合精度を理解するために、まずサポートされているデータ型を確認します。

混合精度トレーニングは、単精度浮動小数点と半精度浮動小数点を組み合わせてディープラーニングモデルをトレーニングできるモードです。

macOS Sonoma以降、MPSGraphは新しいデータタイプbfloat16のサポートを追加します。

Bfloat16は、ディープラーニング用の16ビット浮動小数点形式です。

1つの符号ビット、8つの指数ビット、7つのマンティサビットで構成されています。

これは、ディープラーニングアプリケーションを念頭に置いて設計されていない標準のIEEE 16ビット浮動小数点形式とは異なります。

自動混合精度は、float16とbfloat16の両方で有効になります。

自動混合精度は、デフォルトの精度でネットワークのパフォーマンスを測定することにより、レイヤーごとに適切な精度を選択し、精度に影響を与えることなくパフォーマンスを最適化するために、混合精度設定で再び実行されます。

ニューラルネットワークの一部のレイヤーは、畳み込み層や線形層など、より低い精度で実行できます。

削減などの他の層は、多くの場合、より高い精度レベルを必要とします。

ネットワークに自動混合精度サポートを追加するのは非常に簡単なプロセスです。

まず、オートキャストを追加します。

Float16とbfloat16の両方がサポートされています。

オートキャストは、スクリプトの領域を混合精度で実行できるようにするコンテキストマネージャとして機能します。

この地域では、MPS opsは、精度を維持しながらパフォーマンスを向上させるために、オートキャストによって選択されたデータ型で実行されます。

MPSバックエンドも大幅に最適化されています。

PyTorch 2.0とmacOS Sonomaでは、MPSバックエンドは以前のリリースと比較して最大5倍高速です。

PyTorchはそれだけです。では、TensorFlowに移りましょう。

TensorFlow Metalバックエンドは、安定した1.0リリースバージョンに成熟しました。

このリリースでは、グラップラー再マッピングオプティマイザパスがプラグインに追加されました。

Metalプラグインも混合精度サポートを受けており、インストールプロセスは以前よりも簡単になりました。

認識された計算パターンの自動融合を追加することで、TensorFlow Metalバックエンドのパフォーマンスが向上しました。

これらの計算には、融合畳み込みと行列乗算、オプティマイザ操作、およびRNNセルが含まれます。

この最適化は、計算グラフが作成されると、グラップラーパスを介して自動的に行われます。

ここでは、2次元畳み込み演算の一般的な計算の例を示します。

畳み込みには、多くの場合、畳み込みニューラルネットワークの一般的なパターンである加算関数が続きます。

このパターンを識別することで、グラップラーパスは計算を再マッピングすることができます。

これにより、より最適化されたカーネルを使用して同じ出力を達成でき、パフォーマンスが向上します。

PyTorchと同様に、TensorFlowも混合精度サポートを受けています。

TensorFlowを使用すると、混合精度をグローバルに設定できます。

これにより、要求されたデータ型ポリシーを使用してすべてのネットワーク層を自動的に作成できるため、標準ワークフローでこの変更を有効にするには、既存のコードへの最小限の変更が必要です。

グローバルポリシーは、Float16またはBFloat16のいずれかを使用するように設定できます。

パフォーマンスの向上に加えて、メタルアクセラレーションを有効にするユーザーエクスペリエンスが合理化されました。

これからは、パッケージマネージャーを介してTensorFlowホイールとTensorFlow-Metalプラグインをインストールする通常のパスに従うだけで、メタルアクセラレーションが可能になります。

TensorFlow開発の最先端にとどまりたい人のために、メタルアクセラレーションサポートはTensorFlowの夜間リリースでも利用可能になりました。

それでは、JAXの新しいGPUアクセラレーションについて話しましょう。

今年は、PyTorchやTensorFlowと同様に、Metalバックエンドを通じてJAX GPUアクセラレーションがサポートされます。

JAXは、高性能数値計算と機械学習研究のためのPythonライブラリです。

これは、機械学習研究のための3つの重要な拡張機能を備えた、大きな配列で作業するための人気のあるNumPyフレームワークに基づいています。

まず、grad関数を使用した自動差別化をサポートします。

それはPythonの機能の大きなサブセットを通して区別することができ、高次デリバティブを取ることさえできます。

JAXは、高速で効率的なベクトル化もサポートしています。

関数 apply_matrix が与えられた場合、Python ではバッチディメンションをループできますが、最適ではないパフォーマンスで実行される可能性があります。

この場合、vmapを使用してバッチ処理サポートを自動的に追加できます。

さらに、JAXを使用すると、jitと呼ばれるAPIを使用して、関数を最適化されたカーネルにジャストインタイムでコンパイルできます。

同じ場合、jitはvmapの上に関数を変換して、より速く実行するために使用されます。

M2 Maxを搭載したMacBook Proでは、JAX Metalアクセラレーションは驚くべきスピードアップを提供し、これらのネットワーク全体のCPUよりも平均10倍高速です。

JAXの環境設定とインストールの詳細については、Metal Developer ResourcesのWebページを参照してください。

ギアを切り替えてML推論に移りましょう。

まず、ロード時間を最適化するために使用するMPSGraphの新しいシリアル化形式を導入します。

この新しいシリアル化形式は、他のフレームワークから既存のシリアル化されたネットワークから生成できます。

最後に、8ビットの整数量子化を活用して、ネットワークのメモリフットプリントを最適化する方法を紹介します。

始めましょう。

MPSGraphは、完全な柔軟性を備えた高レベルのAPIを使用して、レイヤーごとに作成できます。

詳細については、メタルパフォーマンスシェーダーグラフを使用してカスタマイズされたMLモデルを構築するビデオを参照してください。

カスタムグラフを定義してコンパイルした後、MPSGraphExecutableを介して実行され、結果が得られます。

通常、このプロセスはうまく機能します。

しかし、多くのレイヤーを持つ複雑なグラフでは、この最初のコンパイルはアプリケーションの起動時間を短縮する可能性があります。

MPSGraphには、まさにこの問題に対処するために、MPSGraphPackageと呼ばれる新しいシリアル化フォーマットがあります。

この新しいシリアル化形式を使用すると、MPSGraphExecutableを事前に作成できます。

一度作成すると、最適化されたMPSGraphExecutableはMPSGraphPackageファイルから直接読み込むことができます。

MPSGraphPackageの作成はとても簡単です。

シリアル化記述子を作成し、シリアル化したいMPSGraphExecutableのシリアル化機能に渡すだけです。

また、それを保存するためのパスを通過する必要があります。

パッケージを作成した後、これがグラフをアプリに読み込む方法です。

コンパイル記述子と、保存されたパッケージへのパスが必要です。

次に、それらを使用してMPSGraphExecutableを初期化します。

すでにMPSGraphを使用している場合は、提示したAPIを使用して、新しいシリアル化形式を簡単に採用できます。

しかし、他のフレームワークから来ている場合は、新しいMPSGraphToolを使用してMPSGraphPackageに簡単に移行できるようになりました。

CoreMLのユーザーは、MPSGraphPackageを作成するMPSGraphToolでMLプログラムを渡すことができます。

ONNXでも同じで、ONNXファイルを入力として使用できます。

この新しいツールを使用すると、推論モデルを手動でエンコードすることなく、既存のモデルをMPSGraphアプリケーションにすばやく含めることができます。

コマンドラインツールの使い方は次のとおりです。

MPSGraphToolに、入力モデルタイプ、この場合はCoreMLパッケージを宣言するフラグを付けます。

また、出力先へのパスと出力モデルの名前も提供します。

さらに、ターゲットプラットフォームと最小OSバージョンを定義します。

変換後、生成されたMPSGraphPackagesをアプリにロードして直接実行できます。

次に、8ビットの整数量子化を使用して計算の効率を向上させる方法について説明します。

16ビット浮動小数点形式など、トレーニングや推論を行うために浮動小数点形式を使用するのが一般的です。

しかし、推論では、これらのモデルは結果を予測するのに時間がかかるかもしれません。

代わりに、多くの場合、低い精度または8ビットの整数を使用することをお勧めします。

これは、メモリバンドを節約し、モデルのメモリフットプリントを減らすのに役立ちます。

8ビット整数形式の場合、量子化には対称と非対称の2種類があります。

MPSGraphは、両方のAPIをサポートするようになりました。

対称量子化と比較して、非対称の量子化では、ここでゼロポイントで示される量子化バイアスを指定できます。

それでは、入力としてInt8形式の活性化と重みから始めて、例を通して量子化された計算を使用することを掘り下げてみましょう。

これらの入力は、MPSGraphのdequantizeTensor opを使用して浮動小数点形式にデ量子化されます。

これで、浮動小数点入力を畳み込み操作に入力できます。

結果の浮動小数点テンソルは、quantizeTensor opを使用してInt8に量子化することができます。

MPSGraphは、これらすべてのカーネルを自動的に1つの操作に統合するため、メモリ帯域幅を節約し、パフォーマンスを向上させる可能性があります。

そして、これはMPSGraphで量子化サポートを使用する方法です。

以前の新機能に加えて、MPSGraphはさらに多くの機械学習オペレーターをサポートしています。

今年から、ほとんどのグラフ操作で複雑な型がサポートされています。

単精度または半精度浮動小数点形式で複素数を使用できます。

複雑なデータ型に基づいて、MPSGraphは高速フーリエ変換を計算するための演算子を追加します。

複雑から複雑、複雑から現実、現実から複雑な変換を最大4次元まで適用できます。

これらは、オーディオ、ビデオ、および画像処理アプリケーションで非常に一般的です。

さらに、MPSGraphを使用すると、3次元畳み込み、グリッドサンプリング、ソートとArgSort、および合計、製品、最小値、最大値などの累積操作を実行できるようになりました。

これでMPSGraphの新機能に関する議論は終わりです。

このセッションで今日発表されたものを復習しましょう。

PyTorchやTensorFlowのような一般的なMLフレームワークをMetalで加速する際の改善点について調べました。

これで、新しいMetalアクセラレーションJAXフレームワークを利用することもできます。

また、新しいシリアル化ツールを使用して、既存のモデルを他のフレームワークからMPSGraphにシームレスに統合する方法についても説明しました。

これで私たちの話は終わりです。

これらすべての機能を使用して作成する素晴らしいコンテンツを見るのが待ちきれません。

見てくれてありがとう。

♪ ♪