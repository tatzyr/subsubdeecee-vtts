10082

♪まろやかなインストゥルメンタルヒップホップ♪

♪

ライアン・テイラー:こんにちは!私の名前はライアンです。

コナー・ブルックス:そして、私はコナーです。

ライアン：このセッションでは、空間コンピューティング用のARKitを紹介します。

この新しいプラットフォームで果たす重要な役割と、それを活用して次世代のアプリを構築する方法について説明します。

ARKitは、洗練されたコンピュータビジョンアルゴリズムを使用して、あなたの周りの世界とあなたの動きの理解を構築します。

開発者が手のひらで使用できる素晴らしい拡張現実体験を作成する方法として、この技術をiOS 11に初めて導入しました。

このプラットフォームでは、ARKitは本格的なシステムサービスに成熟し、新しいリアルタイム基盤でゼロから再構築されました。

ARKitはオペレーティングシステム全体のファブリックに深く織り込まれており、ウィンドウとのやり取りから没入型ゲームプレイまで、あらゆるものに電力を供給しています。

この旅の一環として、私たちはAPIを完全にオーバーホールしました。

新しいデザインは、iOSで学んだすべてのことと、空間コンピューティングのユニークなニーズの結果であり、あなたはそれを気に入ると思います。

ARKitは、仮想コンテンツをテーブルに置くなど、素晴らしいことをするために組み合わせることができるさまざまな強力な機能を提供します。

まるで本当にそこにあるかのように、手を差し伸べてコンテンツに触れ、コンテンツが現実世界と相互作用するのを見ることができます。

それは本当に魔法のような経験です。

この新しいプラットフォームでARKitを使用して何が達成できるかを垣間見たので、私たちの議題をご案内しましょう。

APIを構成する基本的な概念とビルディングブロックの概要から始めます。

次に、現実世界に関連して仮想コンテンツを配置するために不可欠な世界追跡に飛び込みます。

次に、周囲に関する有用な情報を提供するシーン理解機能を探ります。

その後、最新の機能であるハンドトラッキングを紹介します。これは、手に対して仮想コンテンツを配置したり、他のタイプのオーダーメイドのインタラクションを構築したりするために活用できるエキサイティングな新しい追加です。

そして最後に、私たちは一周し、少し前にお見せしたビデオのコードを調べることによって、これらの機能のいくつかの実用的なアプリケーションを見ていきます。

よし、始めよう！

私たちの新しいAPIは、モダンスウィフトとクラシックCの2つの爽快なフレーバーで細心の注意を払って作られています。

すべてのARKit機能は現在、アラカルトで提供されています。

私たちは、開発者が可能な限り柔軟性を持ち、エクスペリエンスを構築するために必要なものを簡単に選択できるようにしたかったのです。

ARKitデータへのアクセスは、プライバシー第一のアプローチで設計されています。

私たちは、開発者のシンプルさを維持しながら、人々のプライバシーを保護するためのセーフガードを導入しています。

APIは、セッション、データプロバイダー、アンカーの3つの基本的な構成要素で構成されています。

アンカーから始めて、セッションに戻りましょう。

アンカーは、現実世界での位置と向きを表します。

すべてのアンカーには、一意の識別子と変換が含まれています。

一部のタイプのアンカーも追跡可能です。

追跡可能なアンカーが追跡されていない場合は、アンカーした仮想コンテンツを非表示にする必要があります。

データプロバイダーは、個々のARKit機能を表します。

データプロバイダーを使用すると、アンカーの変更などのデータの更新をポーリングまたは観察できます。

さまざまな種類のデータプロバイダーは、さまざまな種類のデータを提供します。

セッションは、特定の体験のために一緒に使用したいARKit機能の組み合わせを表します。

一連のデータプロバイダーを提供することで、セッションを実行します。

セッションが実行されると、データプロバイダーはデータの受信を開始します。

更新は、データの種類に応じて、非同期かつ異なる周波数で到着します。

では、プライバシーとアプリがARKitデータにアクセスする方法について話しましょう。

プライバシーは基本的人権です。

それはまた、私たちのコアバリューの1つです。

ARKitのアーキテクチャとAPIは、人々のプライバシーを保護するために思慮深く設計されています。

ARKitがあなたの周りの世界の理解を構築するために、このデバイスには多くのカメラやその他のタイプのセンサーがあります。

カメラフレームなどのこれらのセンサーからのデータは、クライアントスペースに送信されることはありません。

代わりに、センサーデータは、アルゴリズムによる安全な処理のためにARKitのデーモンに送信されます。

これらのアルゴリズムによって生成される結果のデータは、アプリなどのデータを要求しているクライアントに転送される前に慎重にキュレーションされます。

ARKitデータにアクセスするための前提条件がいくつかあります。

まず、アプリはフルスペースを入力する必要があります。

ARKitは、共有スペースにあるアプリにデータを送信しません。

第二に、一部のタイプのARKitデータにはアクセス許可が必要です。

その人が許可を与えない場合、私たちはその種類のデータをあなたのアプリに送信しません。

これを容易にするために、ARKitは許可を処理するための便利な承認APIを提供します。

セッションを使用して、アクセスしたいデータの種類の承認をリクエストできます。

これを行わないと、必要に応じて、セッションを実行するときにARKitが自動的にその人に許可を求めます。

ここでは、ハンドトラッキングデータへのアクセスを要求しています。

必要なすべての承認タイプを1つのリクエストに一括処理できます。

承認結果が得られたら、それらを反復し、各承認タイプのステータスを確認します。

その人が許可を与えた場合、ステータスは許可されます。

その人がアクセスを拒否したデータを提供するデータプロバイダーとセッションを実行しようとすると、セッションが失敗します。

では、ワールドトラッキングから始めて、ARKitがこのプラットフォームでサポートしている各機能を詳しく見てみましょう。

ワールドトラッキングを使用すると、現実世界で仮想コンテンツを固定できます。

ARKitは、デバイスの動きを6つの自由度で追跡し、各アンカーを更新して、周囲に対して同じ場所にとどまるようにします。

ワールドトラッキングが使用するDataProviderのタイプはWorldTrackingProviderと呼ばれ、いくつかの重要な機能を提供します。

これにより、WorldAnchorsを追加できます。これにより、デバイスが移動するにつれて、ARKitが更新され、人々の周囲に対して固定されたままになります。

WorldAnchorsは、仮想コンテンツの配置に不可欠なツールです。

追加したWorldAnchorsは、アプリの起動と再起動で自動的に保持されます。

この動作があなたが構築している経験にとって望ましくない場合は、あなたがそれらを終えたときに単にアンカーを取り外すことができ、それらはもはや持続しません。

永続性が利用できない場合があることに注意することが重要です。

また、WorldTrackingProviderを使用して、アプリのオリジンに対するデバイスのポーズを取得することもできます。これは、Metalを使用して独自のレンダリングを行う場合に必要です。

まず、WorldAnchorとは何か、なぜ使用したいのかを詳しく見てみましょう。

WorldAnchorは、アプリの原点に対してアンカーを配置したい位置と向きである変換を取る初期化子を備えたTrackableAnchorです。

アンカーされていない仮想コンテンツとアンカーされているコンテンツの違いを視覚化するのに役立つ例を用意しました。

ここには2つのキューブがあります。

左側の青い立方体はWorldAnchorによって更新されていませんが、右側の赤い立方体はWorldAnchorによって更新されています。

両方のキューブは、アプリが起動されたときにアプリの原点を基準にして配置されました。

デバイスが動き回ると、両方のキューブは配置された場所に残ります。

クラウンを長押しして、アプリを更新できます。

最近の更新が発生すると、アプリのオリジンが現在の場所に移動されます。

固定されていない青い立方体は、アプリの原点との相対的な配置を維持するために再配置されることに注意してください。一方、固定されている赤い立方体は、現実世界に対して固定されたままです。

WorldAnchorの永続性がどのように機能するかを見てみましょう。

デバイスが動き回ると、ARKitは周囲の地図を作成します。

WorldAnchorsを追加すると、マップに挿入し、自動的に保持します。

WorldAnchorの識別子と変換のみが保持されます。

仮想コンテンツなどの他のデータは含まれていません。

WorldAnchor識別子を関連付ける仮想コンテンツへのマッピングを維持するのはあなた次第です。

マップは場所に基づいているため、デバイスを新しい場所（たとえば、自宅からオフィスまで）に持ち込むと、自宅の地図がアンロードされ、別のマップがオフィス用にローカライズされます。

この新しい場所で追加したアンカーは、そのマップに入ります。

一日の終わりにオフィスを出て家に帰ると、ARKitがオフィスで構築していた地図と、そこに置いたアンカーがアンロードされます。

しかし、もう一度、私たちはあなたのアンカーと一緒に自動的にマップを永続化しています。

帰国後、ARKitは場所が変更されたことを認識し、この場所の既存の地図をチェックして再ローカライズのプロセスを開始します。

見つけたら、それをローカライズし、以前に自宅で追加したすべてのアンカーが再び追跡されます。

デバイスのポーズに移りましょう。

WorldAnchorsの追加と削除に加えて、WorldTrackingProviderを使用してデバイスのポーズを取得することもできます。

ポーズは、アプリの原点に対するデバイスの位置と向きです。

完全に没入型体験でMetalとCompositorServicesで独自のレンダリングを行う場合は、ポーズのクエリが必要です。

このクエリは比較的高価です。

コンテンツの配置など、他のタイプのアプリロジックに対してデバイスのポーズを照会するときは注意してください。

ARKitからCompositorServicesにデバイスのポーズを提供する方法を示すために、簡略化されたレンダリング例を簡単に見てみましょう。

セッション、ワールドトラッキングプロバイダー、最新のポーズを開催するレンダラー構造体があります。

レンダラーの初期化時には、セッションを作成することから始めます。

次に、世界追跡プロバイダーを作成し、各フレームをレンダリングするときにデバイスのポーズを照会するために使用します。

これで、必要なデータプロバイダーとのセッションを実行できます。

この場合、ワールドトラッキングプロバイダーのみを使用しています。

また、レンダリング関数での割り当てを避けるためにポーズを作成します。

フレームレートで呼び出すレンダリング関数にジャンプしてみましょう。

CompositorServicesのdrawableを使用して、ターゲットレンダリング時間を取得します。

次に、ターゲットレンダリング時間を使用して、デバイスのポーズを照会します。

成功すれば、アプリの原点に対するポーズの変換を抽出できます。

これは、コンテンツのレンダリングに使用する変換です。

最後に、合成用のフレームを提出する前に、ドローアブルにポーズを設定して、コンポジターがフレームのコンテンツをレンダリングするために使用したポーズを知るようにします。

独自のレンダリングの詳細については、Metalを使用して没入型アプリを作成するための専用セッションを参照してください。

さらに、空間コンピューティングのパフォーマンスに関する考慮事項に関する素晴らしいセッションがあり、チェックすることをお勧めします。

次に、シーンの理解を見てみましょう。

シーンの理解は、さまざまな方法で周囲について知らせる機能のカテゴリです。

平面検出から始めましょう。

平面検出は、ARKitが現実世界で検出する水平および垂直表面のアンカーを提供します。

平面検出が使用するDataProviderのタイプは、PlaneDetectionProviderと呼ばれます。

飛行機が周囲で検出されると、PlaneAnchorsの形で提供されます。

PlaneAnchorsは、テーブルの上に仮想オブジェクトを配置するなど、コンテンツの配置を容易にするために使用できます。

さらに、床や壁などの基本的な平坦なジオメトリで十分な物理シミュレーションに平面を使用できます。

各PlaneAnchorには、水平または垂直のアライメント、平面のジオメトリ、および意味的分類が含まれます。

平面は、床やテーブルなど、さまざまな種類の表面に分類できます。

特定の表面を特定できない場合、提供された分類は、状況に応じて、不明、未定、または利用できないとマークされます。

では、シーンジオメトリに移りましょう。

シーンジオメトリは、現実世界の形状を推定する多角形メッシュを含むアンカーを提供します。

シーンジオメトリが使用するDataProviderのタイプは、SceneReconstructionProviderと呼ばれます。

ARKitがあなたの周りの世界をスキャンすると、私たちはあなたの周囲を細分化されたメッシュとして再構築し、MeshAnchorsの形で提供されます。

PlaneAnchorsと同様に、MeshAnchorsはコンテンツの配置を容易にするために使用できます。

また、シンプルで平らな表面だけでなく、オブジェクトと対話するために仮想コンテンツが必要な場合に、より忠実度の高い物理シミュレーションを実現することもできます。

各メッシュアンカーには、メッシュのジオメトリが含まれています。

このジオメトリには、面ごとの頂点、法線、面、およびセマンティック分類が含まれています。

メッシュ面は、さまざまな種類のオブジェクトに分類できます。

特定のオブジェクトを識別できない場合、提供された分類はなしになります。

最後に、画像追跡を見てみましょう。 

画像追跡を使用すると、現実世界で2D画像を検出できます。

画像追跡が使用するDataProviderのタイプは、ImageTrackingProviderと呼ばれます。

検出したいReferenceImagesのセットでImageTrackingProviderを設定します。

これらのReferenceImagesは、いくつかの異なる方法で作成できます。

1つのオプションは、プロジェクトのアセットカタログのARリソースグループからそれらをロードすることです。

または、CVPixelBufferまたはCGImageを提供することで、ReferenceImageを自分で初期化することもできます。

画像が検出されると、ARKitはImageAnchorを提供します。

ImageAnchorsは、既知の静的に配置された画像にコンテンツを配置するために使用できます。

たとえば、映画のポスターの横に映画に関する情報を表示できます。

ImageAnchorsは、検出された画像のサイズが指定した物理サイズとアンカーが対応するReferenceImageとどのように比較されるかを示す推定スケールファクターを含むTrackableAnchorsです。

さて、私たちの新機能、ハンドトラッキングについてお話しし、例を順を追って説明します、これがコナーです。

コナー:こんにちは。ARKitに新しく追加されたハンドトラッキングを見てみましょう。

ハンドトラッキングは、各手の骨格データを含むアンカーを提供します。

ハンドトラッキングが使用するDataProviderのタイプは、HandTrackingProviderと呼ばれます。

あなたの手が検出されると、ハンドアンカーの形で提供されます。 ハンドアンカーの形で提供されます。

ハンドアンカーは追跡可能なアンカーです。

ハンドアンカーには、骨格とキラリティが含まれています。

キラリティは、これが左手か右手かを教えてくれます。

HandAnchorの変換は、アプリのオリジンに対する手首の変換です。

スケルトンは関節で構成されており、名前で照会できます。

関節には、親関節、その名前、親関節に相対するlocalTransform、根関節に相対するrootTransform、そして最後に、各関節には、この関節が追跡されているかどうかを示すブールが含まれています。

ここでは、手の骨格で利用可能なすべての関節を列挙します。

ジョイントの階層のサブセットを見てみましょう。

手首は手の根関節です。

各指について、最初の関節は手首に親が付けられています。例えば、1は0に親が付けられます。

その後の指の関節は、前の関節に育てられます。例えば、2は1に育てられます。

HandAnchorsは、あなたの手に関連してコンテンツを配置したり、カスタムジェスチャーを検出したりするために使用できます。

HandAnchorsを受け取るには2つのオプションがあります。更新をポーリングするか、利用可能なときにアンカーを非同期に受信することができます。

後でSwiftの例で非同期更新を見ていきますので、先ほどのレンダラーにハンドアンカーポーリングを追加しましょう。

これが更新された構造体の定義です。

左右のアンカーとともに、ハンドトラッキングプロバイダーを追加しました。

更新されたinit関数では、新しいハンドトラッキングプロバイダーを作成し、実行するプロバイダーのリストに追加します。次に、ポーリング時に必要な左右のアンカーを作成します。

レンダーループでの割り当てを避けるために、これらを事前に作成することに注意してください。

構造体を更新して初期化すると、レンダリング関数でget_latest_anchorsを呼び出すことができます。

プロバイダーと事前に割り当てられたハンドアンカーを渡します。

私たちのアンカーには、利用可能な最新のデータが入力されます。

最新のアンカーが入力されたので、私たちの経験で彼らのデータを使用できるようになりました。

とてもかっこいい。

では、先ほどお見せした例を再検討する時間です。

ARKitとRealityKitの機能を組み合わせて、この体験を構築しました。

シーンジオメトリは物理学やジェスチャーのコライダーとして使用され、ハンドトラッキングはキューブエンティティと直接対話するために使用されました。

この例をどのように構築したかを見てみましょう。

まず、アプリの構造とビューモデルを確認します。

次に、ARKitセッションを初期化します。

次に、指先用のコライダーを追加し、シーンの再構築からコライダーを追加します。

最後に、ジェスチャーでキューブを追加する方法を見ていきます。

すぐに飛び込みましょう。

これが私たちのアプリ、TimeForCubeです。

比較的標準的なSwiftUIアプリとシーン設定があります。

私たちのシーンの中で、私たちはImmersiveSpaceを宣言します。

ARKitデータにアクセスするには、フルスペースに移動する必要があるため、IimmersiveSpaceが必要です。

ImmersiveSpace内では、ビューモデルのコンテンツを表示するRealityViewを定義します。

ビューモデルは、私たちのアプリのロジックのほとんどが住む場所です。

ざっと見てみましょう。

ビューモデルは、ARKitセッション、使用するデータプロバイダー、作成する他のすべてのエンティティを含むコンテンツエンティティ、およびシーンマップとハンドコライダーマップの両方を保持します。

ビューモデルは、アプリから呼び出すさまざまな機能も提供します。

アプリからこれらのそれぞれをコンテキストで確認します。

最初に呼び出す関数は、contentEntityを設定するためのRealityViewのmakeクロージャ内です。

ビューモデルがビューのコンテンツにエンティティを追加できるように、このエンティティをRealityViewのコンテンツに追加します。

setupContentEntityは、マップ内のすべてのフィンガーエンティティをcontentEntityの子として追加し、それを返します。

いいね！

セッションの初期化に移りましょう。

セッションの初期化は、3つのタスクのいずれかで実行されます。

最初のタスクはrunSession関数を呼び出します。

この機能は、2つのプロバイダーとのセッションを実行するだけです。

セッションを実行すると、アンカーの更新の受信を開始できます。

キューブと対話するために使用する指先コライダーを作成して更新しましょう。

これは、手の更新を処理するためのタスクです。

その機能は、プロバイダーのアンカー更新の非同期シーケンスを反復します。

ハンドアンカーが追跡されていることを確認し、人差し指の関節を取得し、関節自体も追跡されていることを確認します。

次に、アプリのオリジンに対する人差し指の先端の変換を計算します。

最後に、どのフィンガーエンティティを更新するかを調べて、その変換を設定します。

フィンガーエンティティマップを再検討しましょう。

ModelEntityへの拡張を介してハンドごとにエンティティを作成します。この拡張は、衝突形状の5mmの球体を作成します。

運動物理学のボディコンポーネントを追加し、不透明度コンポーネントを追加してこのエンティティを非表示にします。

ユースケースのためにこれらを非表示にしますが、すべてが期待どおりに機能していることを確認するために、指先エンティティを視覚化すると良いでしょう。

一時的に不透明度を1に設定し、エンティティが正しい場所にあることを確認しましょう。

すごい！

指先があるところに球体が見えます!

注意してください、私たちの手は球体を部分的に覆っています。

これはハンドオクルージョンと呼ばれ、人が仮想コンテンツの上に自分の手を見ることができるシステム機能です。

これはデフォルトで有効になっていますが、球体をもう少し明確に見たい場合は、シーンでupperLimbVisibilityセッターを使用してハンドオクルージョンの可視性を設定できます。

手足の視認性を非表示に設定すると、手がどこにあるかに関係なく、球体全体を見ることができます。

この例では、上肢の可視性をデフォルト値として残し、不透明度をゼロに戻します。

きちんとした！では、シーンコライダーを追加しましょう。これらを物理学やジェスチャーターゲットとして使用します。

これが私たちのモデルの関数を呼び出すタスクです。

プロバイダーのアンカー更新の非同期シーケンスを反復し、MeshAnchorからShapeResourceを生成してから、アンカー更新のイベントをオンにします。

アンカーを追加する場合は、新しいエンティティを作成し、その変換を設定し、衝突と物理ボディコンポーネントを追加し、入力ターゲットコンポーネントを追加して、このコライダーがジェスチャーのターゲットになるようにします。

最後に、新しいエンティティをマップに追加し、コンテンツエンティティの子として追加します。

エンティティを更新するには、マップから取得し、その変換と衝突コンポーネントの形状を更新します。

削除するには、対応するエンティティを親とマップから削除します。

ハンドコライダーとシーンコライダーができ、ジェスチャーを使ってキューブを追加できます。

任意のエンティティをターゲットにしたSpatialTapGestureを追加します。これにより、誰かがRealityViewのコンテンツ内のエンティティをタップしたかどうかを知らせます。

そのタップが終了すると、グローバル座標からシーン座標に変換する3Dロケーションを受け取ります。

この場所を視覚化しましょう。

タップの位置に球体を追加した場合、次のことがわかります。

次に、ビューモデルに、この場所に対して相対的に立方体を追加するように指示します。

キューブを追加するには、まずタップ位置から20センチ上の配置位置を計算します。

次に、キューブを作成し、その位置を計算された配置位置に設定します。

InputTargetComponentを追加します。これにより、エンティティが応答するジェスチャーの種類を設定できます。

私たちのユースケースでは、フィンガーチップコライダーが直接的な相互作用を提供するため、これらのキューブの間接的な入力タイプのみを許可します。

物理学の相互作用を少し良くするために、カスタムパラメータを持つPhysicsBodyComponentを追加します。

最後に、キューブをコンテンツエンティティに追加します。つまり、ついにキューブの時間です。

最後に、私たちの例を端から端まで見てみましょう。

シーンコライダーやキューブをタップするたびに、タップ位置の上に新しいキューブが追加されます。

物理システムは、キューブをシーンコライダーに落下させ、ハンドコライダーはキューブと対話することを可能にします。

RealityKitの詳細については、空間コンピューティングにRealityKitを使用することに関する入門セッションをご覧ください。

また、このプラットフォームに持ち込むことに興味があるiOSの既存のARKitエクスペリエンスがすでにある場合は、このトピックに関する専用セッションを必ずご覧ください。

私たちのチーム全体が、あなたがARKitの新しいバージョンを手に入れることに非常に興奮しています。

このエキサイティングな新しいプラットフォームのために作成する画期的なアプリをすべて見るのが待ちきれません。

ライアン：見てくれてありがとう！

♪