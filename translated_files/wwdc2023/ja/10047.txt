10047

♪ ♪

Pulkit：こんにちは、私はPulkitで、Core MLチームのエンジニアです。

Core ML Toolsに行われたいくつかのアップデートを共有できることを嬉しく思います。

これらのアップデートは、機械学習モデルのサイズとパフォーマンスを最適化するのに役立ちます。

モデルの機能が大幅に向上するにつれて、ますます多くの機能が機械学習によって推進されています。

その結果、1つのアプリに展開されるモデルの数が増えています。

それに伴い、アプリ内の各モデルも大きくなり、アプリのサイズに上向きの圧力をかけています。

したがって、モデルのサイズを抑えることが重要です。

モデルサイズを小さくすることにはいくつかの利点があります。

各モデルが小さい場合は、同じメモリ予算でより多くのモデルを出荷できます。

また、より大きく、より有能なモデルを出荷することもできます。

また、モデルをより速く実行するのにも役立ちます。

これは、モデルが小さいと、メモリとプロセッサの間を移動するデータが少なくなるためです。

だから、モデルのサイズを小さくするのは素晴らしいアイデアのようです。

モデルを大きくするものは何ですか?

あなたが理解するのを助けるために例を説明させてください。

ResNet50は人気のある画像分類モデルです。

その最初の層は、約9,000のパラメータを持つ畳み込み層です。

そして、さまざまなサイズの合計53の畳み込み層があります。

最後に、それは約210万のパラメータを持つ線形層を持っています。

これはすべて最大2500万のパラメータを追加します。Float16精度を使用してモデルを保存すると、重量あたり2バイトを使用し、サイズ50メガバイトのモデルが得られます。

50メガバイトのモデルは大きいですが、安定した拡散のような新しいモデルにたどり着くと、さらに大きなモデルになります。

さて、より小さなモデルを得るためのいくつかの道について話しましょう。

1つの方法は、より少ない、より小さな重みで優れたパフォーマンスを達成できる、より効率的なモデルアーキテクチャを設計することです。

もう1つの方法は、既存のモデルの重みを圧縮することです。

このモデル圧縮のパスは、私が焦点を当てるものです。

モデルの圧縮のための3つの有用なテクニックを説明することから始めます。

次に、これらのモデル圧縮技術を統合した2つのワークフローを実演します。

次に、最新のCore MLツールが、これらのテクニックとワークフローをモデルに適用するのにどのように役立つかを説明します。

そして最後に、Srijanはモデル圧縮がランタイムパフォーマンスに与える影響について議論します。

圧縮技術から始めましょう。

モデルの重みを圧縮する方法はいくつかあります。

最初の方法は、スパース行列表現を使用して、それらをより効率的にパックすることです。

これは、剪定と呼ばれる技術を使用することで達成できます。

もう1つの方法は、重みを保管するために使用される精度を下げることです。

これは、量子化またはパレット化のいずれかによって達成することができます。

これらの戦略は両方とも損失があり、圧縮されたモデルは通常、非圧縮モデルと比較して精度がわずかに低い。

それでは、これらのテクニックのそれぞれを詳しく見てみましょう。

重量剪定は、まばらな表現でモデルの重みを効率的に梱包するのに役立ちます。

重み行列をスパース化または剪定することは、重み値の一部を0に設定することを意味します。

私はウェイトマトリックスから始めます。

それを剪定するには、最小のマグニチュードウェイトを0に設定できます。

今、私はゼロ以外の値を格納するだけです。

導入されたゼロごとに約2バイトのストレージを節約することになります。

もちろん、後で濃密な行列を再構築するために、ゼロの位置も保存する必要があります。

モデルサイズは、導入されたスパースのレベルとともに直線的に低下します。

50%スパースモデルは、その重量の50%がゼロであることを意味し、ResNet50モデルの場合、サイズは約28メガバイトで、これはFloat16サイズの約半分です。

2番目の重み圧縮技術は量子化で、8ビットの精度を使用して重みを格納します。

量子化を実行するには、重み値とスケールを取り、シフトし、INT8の範囲になるように丸めます。

この例では、スケールは2.35で、最小値を-127にマッピングし、バイアスは0です。

モデルによっては、非ゼロバイアスも使用でき、量子化誤差を減らすのに役立つ場合があります。

スケールとバイアスは、後で重みを非量子化して元の範囲に戻すことができます。

重量精度を8ビット以下に下げるには、重量クラスタリングまたはパレット化と呼ばれる技術を使用できます。

この手法では、同様の値を持つ重みがグループ化され、それらが属するクラスター重心の値を使用して表されます。

これらの重心はルックアップテーブルに保管されています。

そして、元の重み行列はインデックステーブルに変換され、各要素は対応するクラスター中心を指します。

この例では、私は4つのクラスターを持っているので、2ビットを使用して各重みを表すことができ、Float16で8倍の圧縮を実現します。

重みを表すために使用できる一意のクラスターセンターの数は、nの累乗に対して2に等しく、nはパレット化に使用される精度です。

したがって、4ビットのパレット化は、16のクラスターを持つことができることを意味します。

クオンタイゼーションはモデルサイズを半分に減らしますが、パレット化は最大8倍に小さくするのに役立ちます。

要約すると、重量圧縮には3つの異なるテクニックがあります。

それぞれが異なる方法で重みを表現しています。

それらは、剪定のためのスパースの量やパレット化のためのビット数など、それぞれのパラメータによって制御できるさまざまなレベルの圧縮を提供します。

次に、これらのテクニックをモデル開発ワークフローに統合する方法を説明します。

まず、Core MLモデル変換のワークフローから始めましょう。

お気に入りのPythonトレーニングフレームワークでモデルをトレーニングすることから始めて、Core ML Toolsを使用してそのモデルをCore MLに変換できます。

このワークフローはさらに一歩拡張して、トレーニング後の圧縮ワークフローになることができます。

これを行うには、全体的なサイズを小さくするために、すでに訓練され変換されたモデルウェイトで動作する圧縮ステップを追加します。

このワークフローはいつでも開始できることに注意してください。

たとえば、トレーニングデータやすでに変換されたCore MLモデルを必要としない、事前にトレーニングされたモデルから始めることができます。

このワークフローを適用すると、適用される圧縮の量を選択するオプションがあります。

圧縮を適用すればするほど、結果のモデルは小さくなりますが、予想通り、いくつかのトレードオフがあります。

具体的には、一定の精度を達成する非圧縮モデルから始めます。

圧縮を適用すると、モデルのサイズが小さくなりますが、精度にも影響する可能性があります。

より多くの圧縮を適用すると、この影響がより顕著になり、精度の低下が受け入れられなくなる可能性があります。

この傾向と許容可能なトレードオフは、ユースケースごとに異なり、モデルとデータセットに依存します。

このトレードオフを実際に見るために、画像内のオブジェクトをセグメント化するモデルを見てみましょう。

私の画像では、モデルはソファに属する各ピクセルの確率を返します。

ベースラインFloat16モデルは、オブジェクトを非常にうまくセグメント化します。

10%の剪定されたモデルの場合、出力は基本モデルと非常によく似ています。

アーティファクトは30%のスパーシティで現れ始め、より高いレベルで増加します。

40%の剪定になると、モデルは完全に故障し、確率マップが認識できなくなります。

同様に、8ビットの量子化と6ビットのパレット化は、基本モデルの出力を保持します。

4ビットのパレット化では、いくつかのアーティファクトが見え始め、2ビットのパレット化では、モデルはオブジェクトを完全にセグメント化できません。

より高い圧縮率でモデルパフォーマンスの低下を克服するには、別のワークフローを使用できます。

このワークフローはトレーニング時間圧縮と呼ばれます。

ここでは、重みを圧縮しながら、いくつかのデータでモデルを微調整します。

圧縮は、重みが課せられた新しい制約に再調整できるように、徐々に微分可能な方法で導入されます。

モデルが満足のいく精度を達成したら、それを変換して圧縮されたCore MLモデルを取得できます。

既存のモデルトレーニングワークフローにトレーニング時間の圧縮を組み込むか、事前にトレーニングされたモデルから始めることができることに注意してください。

トレーニング時間の圧縮により、モデルの精度と圧縮量の間のトレードオフが改善され、より高い圧縮率で同じモデルのパフォーマンスを維持できます。

同じ画像セグメンテーションモデルをもう一度見てみましょう。

トレーニング時間の剪定のために、モデルの出力は40%のスパースまで変更されていません。

これは、トレーニング後の精度が崩壊したところです。

実際、今では50%と75%のスパースでも、このモデルは基本モデルと同様の確率マップを達成しています。

モデルの精度の大幅な低下を観察し始めるのは90%のスパースです。

同様に、トレーニング時間の量子化とパレット化は、この場合最大2ビットの圧縮であっても、ベースラインモデルの出力も保持します。

要約すると、モデル変換中またはモデルトレーニング中に重量圧縮を適用できます。

後者は、より長いトレーニング時間を犠牲にして、より良い精度のトレードオフを提供します。

2番目のワークフローはトレーニング中に圧縮を適用するため、圧縮アルゴリズムを実装するために微分可能な操作を使用する必要があります。

それでは、これらの圧縮ワークフローをCore ML Toolsでどのように実行できるかを探りましょう。

トレーニング後のモデル圧縮APIは、圧縮utilsサブモジュールの下での剪定、パレット化、および量子化のためにCore ML Tools 6で利用可能です。

しかし、トレーニング時間圧縮用のAPIはありませんでした。

Core ML Tools 7では、トレーニング時間の圧縮機能を提供するために、新しいAPIが追加されました。

そして、古いAPIと新しいAPIをcoremltools.optimizeと呼ばれる単一のモジュールに統合しました。

トレーニング後の圧縮APIはcoremltools.optimize.coremlで移行され、新しいトレーニング時間APIはcoremltools.optimize.torchで利用できます。

後者はPyTorchモデルで動作します。

まず、トレーニング後のAPIを詳しく見てみましょう。

トレーニング後の圧縮ワークフローでは、入力はCore MLモデルです。

これは、私が説明した3つの圧縮技術のそれぞれを適用するOptimize.coremlモジュールで利用可能な3つの方法によって更新することができます。

これらのメソッドを使用するには、まずOptimizationConfigオブジェクトを作成し、モデルを圧縮する方法を説明します。

ここでは、75%のターゲットスパースでマグニチュードプルーニングをしています。

設定が定義されたら、prune_weightsメソッドを使用してモデルをプルーニングできます。

圧縮されたモデルを取得するためのシンプルでワンステップのプロセスです。

これらのテクニックに固有の設定を使用して、重みをパレット化および量子化するために同様のAPIを使用できます。

トレーニング時間の圧縮ワークフローを考えてみましょう。

この場合、先に説明したように、トレーニング可能なモデルとデータが必要です。

具体的には、Core ML Toolsでモデルを圧縮するには、おそらく事前にトレーニングされた重みを持つPyTorchモデルから始めます。

次に、Optimize.torchモジュールで利用可能なAPIの1つを使用して更新し、圧縮レイヤーが挿入された新しいPyTorchモデルを取得します。

そして、データと元のPyTorchトレーニングコードを使用して、それを微調整します。

これは、圧縮を可能にするために重みが調整されるステップです。

そして、MPS PyTorchバックエンドを使用して、ローカルでMacBookでこのステップを実行できます。

モデルが精度を取り戻すように訓練されたら、それを変換してCore MLモデルを取得します。

コード例を通して、これをさらに詳しく見てみましょう。

圧縮したいモデルを微調整するために必要なPyTorchコードから始めます。

わずか数行のコードを追加することで、Core ML Toolsを簡単に活用して、トレーニング時間の剪定を追加できます。

まず、モデルをプルーニングする方法を説明するMagnitudePrunerConfigオブジェクトを作成します。

ここでは、目標のスパーシティを75%に設定しています。

設定をyamlファイルに書き込み、from_yamlメソッドを使用してロードすることもできます。

次に、圧縮するモデルと作成した設定でプルーナーオブジェクトを作成します。

次に、準備を呼び出すと、モデルに剪定レイヤーを挿入します。

モデルを微調整しながら、ステップAPIを呼び出すと、プルーナーの内部状態が更新されます。

トレーニングの最後に、剪定マスクを重みに折りたたむためにファイナライズを呼び出します。

このモデルは、変換APIを使用してCore MLに変換できます。

同じワークフローを量子化とパレット化にも使用できます。

さて、Srijanは、Core ML Tools APIを使用してオブジェクト検出モデルをパレット化する方法を示すデモを紹介します。

スリジャン:ありがとう、プルキット。

私の名前はSrijanです。Core ML Tools optimize APIのデモを案内します。

画像内の人を検出するために、ResNet18バックボーンを備えたSSDモデルを使用します。

まず、いくつかの基本的なモデルとトレーニングユーティリティをインポートしましょう。

先ほど話したSSD ResNet18モデルのインスタンスを入手することから始めます。

物事を簡素化するために、事前に書かれたget_ssd_modelユーティリティを呼び出すだけです。

モデルがロードされたので、いくつかのエポックのためにそれを訓練しましょう。

物体検出モデルであるため、トレーニングの目標は、検出タスクのSSDの損失を減らすことです。

簡潔さのために、train_epochユーティリティは、異なるバッチを介してフォワードの呼び出し、損失の計算、勾配降下の実行など、エポックのモデルを訓練するために必要なコードをカプセル化します。

トレーニング中、SSDの損失は減少しているようです。

今からモデルをCore MLモデルに変換します。

これを行うには、まずモデルをトレースしてから、coremltools.convert APIを呼び出します。

インポートされたユーティリティを呼び出して、モデルのサイズを確認しましょう。

モデルのサイズは23.6メガバイトです。

次に、Core MLモデルで予測を実行します。

私はロンドン旅行から自分の画像と、検出をテストするために別の画像を選択しました。

モデルがオブジェクトを検出するための信頼しきい値は30%に設定されているため、オブジェクトが存在することを少なくとも30%確信しているボックスのみをプロットします。

その検出は的確なようです。

私は今、このモデルのサイズを小さくできるかどうか興味があります。

私は最初にトレーニング後のパレット化を試してみるつもりです。

そのために、coremltools.optimize.coremlからいくつかの設定クラスとメソッドをインポートします。

私は今、モデルの重みを6ビットでパレット化するつもりです。

そのために、OpPalettizerConfigオブジェクトを作成し、モードをkmeans、nbitsを6として指定します。

これにより、opレベルでパラメータが指定され、各opを異なる方法でパラメータ化できます。

しかし、今、私はすべての操作に同じ6ビットモードを適用するつもりです。

OptimizationConfigを定義して、このop_configをグローバルパラメータとして渡します。

その後、最適化設定は、変換されたモデルとともに palettize_weights メソッドに渡され、パレット化されたモデルを取得します。

サイズが今までに減ったものを見てみましょう。

モデルのサイズは約9メガバイトに減少しましたが、テスト画像のパフォーマンスに影響しましたか？

調べてみましょう。

うわー、検出はまだうまく機能します。

私は今、2ビットのトレーニング後のパレット化を試すことに私の運をプッシュすることに本当に興奮しています。

これを行うには、OpPalettizerConfigでnbitsを6から2に変更し、palettize_weights APIを再度実行するだけです。

ユーティリティを使用して、このCore MLモデルのサイズとパフォーマンスを見てみましょう。

予想通り、モデルのサイズは縮小し、約3メガバイトに減少しました。

しかし、モデルは両方の画像で人を検出できないため、パフォーマンスは最適ではありません。

モデルによって予測されたボックスのいずれも、30%のしきい値を超える信頼確率がないため、予測にはボックスは表示されません。

2ビットのトレーニング時間のパレット化を試して、それがより良いパフォーマンスを発揮するかどうかを見てみましょう。

これを行うには、coremltools.optimize.torchからDKMPalettizerConfigとDKMPalettizerをインポートすることから始めます。

DKMは、アテンションベースの微分可能なkmeans操作を実行することによって、重みクラスターを学習するアルゴリズムです。

さあ、パレット化設定を定義する時が来ました。

Global_configでn_bitsを2として指定するだけで、サポートされているすべてのモジュールが2ビット化されます。

そしてここで、モデルと設定からパレットイザーオブジェクトを作成します。

今すぐ準備APIを呼び出すと、パレット化に優しいモジュールがモデルに挿入されます。

いくつかのエポックのためにモデルを微調整する時間です。

モデルが微調整されたので、パレット化された重みをモデルの重みとして復元し、プロセスを完了するfinalize APIを呼び出します。

次のステップは、モデルのサイズを確認することです。

そのために、トーチモデルをCore MLモデルに変換します。

Torch.jit.traceを使ってモデルをトレースすることから始めましょう。

変換APIを呼び出し、今回はPassPipelineという追加のフラグを使用し、その値をDEFAULT_PALETTIZATIONに設定します。

これは、変換された重みにパレット化された表現を使用するようにコンバーターに示します。

テスト画像でモデルのサイズとその性能を見てみましょう。

トレーニングタイムパレット化されたモデルも約3メガバイトであり、8倍の圧縮になることがわかりますが、トレーニング後のパレット化されたモデルとは異なり、このモデルはテスト画像の検出を正しく実行しています。

これはデモだったので、2つのサンプル画像でモデルのパフォーマンスをテストしました。

現実世界のシナリオでは、平均平均精度などのメトリックを使用し、検証データセットで評価します。

要約しましょう。

私は訓練されたモデルから始めて、Float16の重みを持つ23.6メガバイトのモデルを得るためにそれを変換しました。

次に、palettize_weights APIを使用して、6ビットの重みを持つ小さなモデルをすばやく取得し、データでうまく機能しました。

しかし、さらに2ビットにプッシュすると、パフォーマンスが明らかに低下しました。

これを投稿し、私は optimize.torch APIsでトーチモデルを更新し、微分可能なkmeansアルゴリズムを使用していくつかのエポックを微調整しました。

私はそれにより、2ビット圧縮オプションで良好な精度を得ることができました。

デモでは特定のモデルと最適化アルゴリズムの組み合わせを採用していますが、このワークフローはユースケースに一般化され、必要な圧縮の量とモデルの再トレーニングに必要な時間とデータとの間のトレードオフを把握するのに役立ちます。

これは私たちの最後のトピック、パフォーマンスに私たちをもたらします。

アプリで展開されたときに、このようなモデルをより効率的に実行するために、Core MLランタイムに加えられた改善について簡単に触れたいと思います。

iOS 16とiOS 17のランタイムのいくつかの重要な違いを見てみましょう。

iOS 16では、重量のみの圧縮モデルがサポートされていましたが、iOS 17では、8ビットアクティベーション量子化モデルも実行できます。

iOS 16では、ウェイト圧縮モデルはフロートウェイトを持つ対応するモデルと同じ速度で実行されますが、iOS 17ではCore MLランタイムが更新され、圧縮モデルは特定のシナリオでより速く実行されるようになりました。

同様のランタイムの改善は、macOS、tvOS、watchOSの新しいバージョンでも利用可能です。

しかし、これらの改善はどのように達成されますか?

重みのみが圧縮されるモデルでは、活性化は浮動小数点精度であるため、畳み込みや行列乗算などの操作が発生する前に、他の入力の精度と一致するように重み値を解凍する必要があります。

この解凍のステップは、iOS 16のランタイムで事前に行われます。

したがって、この場合、モデルは実行前にメモリ内の完全浮動小数点式精度モデルに変換されます。

したがって、推論遅延に変化は観察されません。

ただし、iOS 17では、特定のシナリオでは、操作が実行される直前に、重みが解凍されます。

これには、すべての推論呼び出しで解凍を行うコストで、メモリからより小さなビットウェイトをロードするという利点があります。

ニューラルエンジンなどの特定のコンピューティングユニット、およびメモリにバインドされている特定のタイプのモデルでは、これは推論の利益につながる可能性があります。

これらのランタイムの利点を説明するために、私はいくつかのモデルを選択してプロファイリングし、Float16バリアントと比較して推論が加速される相対的な量をプロットしました。

予想通り、スピードアップの量はモデルとハードウェアに依存します。

これらは、iPhone 14 Pro Maxの4ビットパレット化モデルのスピードアップの範囲です。

改善点は大きく5%から30%です。

スパースモデルについても、モデルタイプに基づいてさまざまな改善があり、一部のモデルはFloat16バリアントよりも75%速く実行されます。

今、疑問が生じます:最高のレイテンシパフォーマンスを得るための戦略は何ですか?

これは、フロートモデルから始めて、Optimize.coreml APIを使用して、モデルのさまざまな表現を探索することです。

モデルの再トレーニングを必要としないため、これは迅速です。

次に、興味のあるデバイスでプロファイルします。

このため、XcodeのCore MLパフォーマンスレポートは、操作が実行される場所など、推論に多くの可視性を提供します。

次に、どの構成があなたに最高の利益をもたらすかに基づいてショートリストします。

この後、精度の評価と改善に集中できます。モデルを完成させる前に、トーチとコアMLツールでトレーニング時間の圧縮を適用する必要があるかもしれません。

要約すると、モデルのサイズを小さくすることが重要であり、今では新しいCore ML Tools APIでこれまで以上に簡単にそれを行うことができ、より低いメモリフットプリントと推論のスピードアップを実現できます。

より多くのオプションとベンチマークデータを確認するには、ドキュメントをご覧ください。

また、今日のスライドで取り上げなかったCore MLフレームワークの改善について語る「非同期予測でCore ML統合を改善する」ビデオにチューニングすることを強くお勧めします。

ありがとう、そして幸せな圧縮。