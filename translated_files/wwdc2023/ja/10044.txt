10044

♪ ♪

デビッド：こんにちは。私の名前はデビッド・フィンドレイです。

私はCreate MLチームの機械学習エンジニアです。

私たちは、Create MLアプリとフレームワークのいくつかの大きな改善に取り組んできました。

あなたを新機能のツアーに連れて行くのが楽しみです。

大規模なモデルをゼロから訓練するには、数千時間、何百万もの注釈付きファイル、および専門的なドメイン知識が必要です。

私たちの目標は、すべてのオーバーヘッドなしで機械学習を使用する優れたアプリを構築するためのツールを提供することです。

私たちは、写真アプリの検索体験やアクセシビリティのカスタムサウンド認識など、多くの機能を強化する最先端のモデルを作成するプロセスを経てきました。

Create MLを使用すると、最新のテクノロジーにアクセスできるので、手間をかけずに独自のカスタム機械学習体験を構築できます。

MLを作成するために行った改善から始めます。 

次に、複数のラベルを持つシーンを理解するための機械学習モデルを構築するまったく新しい方法を紹介します。

最後に、トレーニングデータが制限されている場合に、モデルの品質を向上させるために設計された新しいオーグメンテーションAPIについて説明します。

テキスト分類の改善から始めましょう。

テキスト分類器は、自然言語テキストのパターンを認識するように設計された機械学習タスクです。

このようなモデルを訓練するには、テキストとラベルのペアの表を提供するだけです。

この例では、スポーツ、エンターテイメント、自然のラベルがあります。

事前に訓練された埋め込みモデルを特徴抽出器として使用する転送学習アルゴリズムを選択できます。

今年は、新しい埋め込みモデルを設計し、何十億ものラベル付きテキストの例でトレーニングしました。

これは、変圧器モデル、略してBERTからの双方向エンコーダ表現です。

新しいオプションは、Create MLアプリの[設定]タブのモデルパラメータセクションにあります。

BERT埋め込みモデルは多言語であるため、トレーニングデータに複数の言語を含めることができます。

多言語テキスト分類器をサポートすることに加えて、BERTはモノリンガルテキスト分類器の精度を高めることもできます。

iOS 17、iPadOS 17、macOS SonomaでBERTを活用できます。

私たちはすべての詳細をカバーするビデオ全体を持っています。

詳細については、「Explore Natural Language多言語モデル」を必ずご覧ください。

次に、画像分類タスクで転送学習をどのように使用するかについて話したいと思います。

Create MLの画像分類器は、質問に答えるためのモデルを構築するのに役立つように設計されています。画像の内容を説明するのに最適なラベルは何ですか?

テキスト分類器と同様に、画像分類器は事前に訓練されたモデルを活用して、画像から関連情報を抽出します。

Apple Neural Scene Analyzerの最新バージョンは、トレーニングデータがほとんどない最先端のモデルを構築するために利用可能になりました。

OSの画像理解モデルは、可能な限り最高の体験を提供するために進化し続けています。

詳細については、機械学習研究ウェブサイトの記事をご覧ください。

Create MLアプリでは、[設定]タブのモデルパラメータセクションに新しい機能抽出オプションが表示されます。

新機能抽出器は、以前のバージョンと比較して出力埋め込みサイズが小さくなります。

一般的な改善に加えて、これは分類器の精度を高め、トレーニング時間を短縮し、抽出された機能を保存するために必要なメモリを減らすことができます。

Create MLの改善点を取り上げたので、新しいマルチラベル画像分類器について話したいと思います。

私がそこに着く前に、シングルラベル画像分類は、画像の内容を記述する最適なラベルを予測するように設計されていることを思い出してください。

たとえば、この画像を犬または屋外と表現するかもしれません。

しかし、あなたは1つを選ぶ必要があります。

オブジェクトに興味がある場合は、オブジェクト検出器を使用してシーン内のオブジェクトを見つけることができます。

例えば、私は犬の周りにバウンディングボックスを描き、ボールの周りに別のバウンディングボックスを描きました。

さて、これは素晴らしいですが、オブジェクトが入っているシーンにも興味があります。

犬が公園や屋外にいることを表すためにバウンディングボックスを描くことはできません。

そこで、新しいマルチラベル画像分類器が登場します。

これにより、画像の一連のオブジェクト、属性、またはラベルを予測できます。

例えば、この画像には犬、おもちゃ、草、公園が含まれています。

Create MLを使って1つ作りに行きましょう。

いつものように、私が最初にする必要があるのは、いくつかのトレーニングデータを収集することです。

私は少し楽しんで、さまざまなシーンで複数の多肉植物を検出する分類器を構築することにしました。

例えば、ここには、窓枠の上の鉢の中のハワーシア、ジェイド、アロエの画像があります。

次の画像では、鍋にサボテンを持っている人がいます。

トレーニング画像を収集しながら、アロエの写真のように、ラベルが1つしかない画像を含めることもできます。

注釈をJSONファイルに整理する必要があります。

あなたがする必要があるのは、注釈のセットで各ファイルに注釈を付けることだけです。

では、Create MLアプリでモデルを構築するデモをしましょう。

Create MLアプリで、新しいマルチラベル画像分類テンプレートを選択します。

そして、Succulent Classifierという名前のプロジェクトを作成します。

これにより、設定タブに移動します。

まず、トレーニングデータをドラッグします。これにより、クラス数とトレーニング画像の概要が表示されます。

検証データをドラッグするオプションもありますが、今のところトレーニングデータをランダムに分割することを選択します。

デフォルトの反復回数を使用し、拡張も省きます。

モデルの設定が終わったので、先に進んで「電車」をクリックします。

このモデルは、私のMacでトレーニングするのに数分しかかかりません。

すぐに、モデルは先ほど導入した新しい機能抽出器を使用して機能を抽出し始めます。

それが完了すると、アプリは分類器のトレーニングを開始します。

トレーニングプロセス中、アプリは平均平均精度スコア、略してMAPを計算することで、私のモデルの品質を測定します。

一般的に、私のモデルは、私のデータセット内のすべてのラベルの平均でより高い精度とより高いリコールの両方を持っていることを意味するので、私はMAPスコアを最大化したいです。

私のモデルはトレーニングを終了し、74回の反復で早期に収束し、MAPスコアはトレーニングセットで97%、検証セットで93%でした。

次のステップは、テストデータで私のモデルを評価することです。

デスクトップからフォルダをドラッグして、テストボタンをクリックします。

テストデータには、モデルのトレーニングに使用したのと同じクラスラベルのセットが含まれている必要があります。

このアプリは、MAPスコアや、どのクラスラベルが最高と最低の精度とリコールを持つかなど、いくつかの高レベルの統計を計算しました。

メトリクスタブに焦点を当てましょう。

このアプリは、偽陽性、偽陰性、精度、リコール、信頼しきい値など、各クラスラベルのメトリクスを計算します。

私のモデルで予測を行う場合、信頼度が特定のクラスラベルのしきい値を上回っている場合、予測は正しいです。

モデルがアロエの信頼しきい値を上回って予測した画像を探りましょう。

次に、例をクリックします。

モデルは、この画像には90%の信頼度を持つアロエが含まれていると予測し、これは40%のアロエ信頼しきい値を上回っている。

他のラベルについては、モデルはそれぞれのしきい値を下回る信頼度を予測しました。

言い換えれば、モデルはそれらを予測しなかった。

次に、私のモデルがアロエを予測しなかったが、アロエとラベル付けされている画像を調べたいと思います。

偽陰性の結果タイプを選択することで、それを行うことができます。

この画像は面白いです。さらに探検しましょう。

ここでは、アロエはバレルサボテンとムーンサボテンの背後にあるので、モデルはアロエを予測するのに苦労しています。

しかし、良いニュースは、モデルが他の2つのラベルを正しく予測していることです。

次に、プレビュータブに進みます。

これは、まだラベル付けしていない画像のモデル予測をプレビューできる場所です。

私は自分で植えた多肉植物のアレンジメントを持っていて、試してみるのが楽しいと思います。

それを釘付けにした。

私のモデルは、私の台所で私の月のサボテン、ウサギの耳のサボテン、バレルサボテンを正しく予測しました。

私は私のモデルの品質にかなり満足していますが、私は間違いなく私のモデルの限界を理解するだけでなく、私のデータセットにより多くの多肉植物やシーンを追加するために実験を続けます。

とりあえず、先に進みましょう。

[出力]タブから、トレーニングしたモデルをディスクに保存できます。

予測を作成するために書く必要があるコードを見てみましょう。 予測を作成するために書く必要があります。

最初のステップは、コンパイルされたCore MLモデルからVisionモデルを作成することです。

次に、ビジョンフレームワークを使用して、ソース画像で画像要求ハンドラを作成し、要求を実行します。

最後に、分類観測値を取得し、興味のある精度とリコール値を使用してフィルタリングすることができます。

ユースケースで機能する精度とリコール値を選択する方法の詳細については、WWDC 2019「Understanding Images in Vision Framework」のビデオをご覧ください。

先に進む前に、少し時間を取って、画像分類器やマルチラベル画像分類器と同様に、オブジェクト検出の探索オプションで評価タブを強化したことに言及したいと思います。

必ず確認してください。

あなたの焦点を私の最後のトピックに移し、拡張を使用して限られたデータで機械学習モデルを訓練したいと思います。

うまく一般化するモデルを取得するには、トレーニングセット内の画像には、さまざまな照明条件、向き、背景など、さまざまな特性が必要です。

しかし、さまざまな状況でトレーニング画像をキャプチャしてラベル付けするには時間がかかる場合があります。

データ拡張は、変換を適用することにより、既存のトレーニング例から新しいトレーニング例を生成する手法です。

画像の場合、変換は、いくつかの例を挙げると、水平または垂直の反転、トリミング、またはコントラストにすることができます。

この例では、多肉植物の画像から始めて、4つのバリエーションを生成します。

そして、反転やコントラストの増加など、変換を組み合わせることができます。

拡張は、特に小さなトレーニングデータセットがある場合、モデルの品質を向上させることができます。

モデルがシーン内のオブジェクトの正確な位置などの属性を学習するのを防ぐため、モデルの一般化を改善するためにそれらを使用することができます。

ただし、各トレーニング反復で特徴抽出が行われるため、トレーニングは通常遅くなることを考慮することが重要です。

まだ見ていない場合は、Create ML Componentsを導入したWWDC 2022のビデオを必ずご覧ください。

トランスフォーマーや見積もりなどのコンポーネントを使用して、カスタム機械学習モデルを構築するのに役立つフレームワークを設計しました。

今年は、独自のカスタム拡張パイプラインを設計するために使用できる新しい拡張APIを追加しました。

SwiftUIの経験があるなら、これはおなじみかもしれません。

最初のステップは、オーグメンターを作成することです。

SwiftUIと同様に、オーグザは結果ビルダーを使用します。

オーグメンターの本体では、データを強化するための変換を追加できます。

オーグメンターは汎用的であるため、データは画像、ラベル付きサウンド、または何か他のものにラベルを付けることができます。

重要な部分は、各変換の入力タイプと出力タイプが一致する必要があることです。

例えば、画像を撮って画像を生成します。

50%の確率で水平に反転して画像を増強したい。

オーグメンターにApplyRandomlyを追加することから始めます。

これは、与えられた確率の変換を適用します。

次に、本体に水平反転トランスを追加します。

さて、オーグメンターが手になったので、適用されたメソッドを呼び出すことで、それを使ってオーグメンテーションを作成できます。

拡張を設計するときは、データの性質を慎重に検討することが重要です。

逆さまの多肉植物に遭遇する可能性は低い。

したがって、この場合、垂直反転増強を適用することは意味がありません。

または、代わりに交通標識を分類したいと想像してみてください。

フリップ増強を適用すると、ラベルが画像を正しく表現できなくなる可能性があります。

したがって、カスタム拡張を設計する前に、データの性質を考慮することを忘れないでください。

さて、次のステップは、オーグメンターにより多くの変換を追加することです。

今回は、UniformRandomFloatingPointParameterを使用して画像をランダムに回転させます。

これにより、増強を適用するたびにランダムな角度が生成されます。

そして最後に、画像をランダムにトリミングします。

オーグマーの各変換は順番に適用されることに注意してください。

まず、私の画像はランダムに反転します。

結果はランダムに回転し、ランダムにトリミングされます。

オーグメンターでできることの表面を引っ掻いただけです。

ここでは、あなたが始めるために私たちが提供するコンポーネントのいくつかを紹介します。

しかし、さらに進みたい場合はどうなりますか?

カスタム変換を構築し、それらを使用して画像を拡張する方法の例を見てみましょう。

堅牢な分類器を構築するには、さまざまなシーンや環境でトレーニング画像をキャプチャすることが重要です。

この例では、ランダムなシーンのランダムな場所に多肉植物を配置するカスタム増強を作成します。

RandomImageBackgroundを定義することから始めます。

これは、トランスフォーマーに似ていますが、乱数発生器を必要とする新しいプロトコル、RandomTransformerに準拠しています。

拡張は、異なる背景シーンで多肉植物をランダムに配置したいので、背景画像を撮影する初期化子を作成します。

RandomTransformerプロトコルに準拠するには、適用されたメソッドを実装する必要があります。

拡張を適用するとき、最初のステップは背景をランダムに選択することです。

次に、背景画像で入力画像を配置したいランダムな場所を選択し、トリミングしないように注意します。

次に、入力画像をランダムな場所に翻訳します。

そして最後に、ランダムに選択された背景の上に入力画像を配置します。

異なる背景に多肉植物を配置する前に、反転、回転、トリミングが行われるように、最後に新しいカスタム拡張を追加します。

そして、これは私の最後の増強です。

私のオーグメンターを使用して、私は分類器を訓練するためのいくつかの本当に興味深い画像を生成することができます。

それが次のステップです。

拡張を使用するときは、更新方法を使用して徐々にトレーニングする方が理にかなっています。

それを念頭に置いて、増強を使用したトレーニングの例を見てみましょう。

そして、私は遠慮しません。また、バッチ処理、ランダム化、早期停止も組み込みます。

空の画像分類器モデルを作成することから始めます。

次に、トレーニングループを作成します。

トレーニングループでは、最初のステップは、トレーニング画像をシャッフルして拡張することです。

拡張する前にシャッフルして、各反復でバッチに異なる画像が含まれるようにします。

オーグザの結果は非同期シーケンスであり、変換が怠惰に行われることを意味します。

バッチメソッドを使用すると、拡張の非同期シーケンスをグループにグループ化できます。

この場合、私は16のバッチサイズを使用しました。

最後に、拡張画像のバッチごとにモデルを更新します。

これが、データ拡張を行う際に更新方法を使用することが良い選択である理由です。

反復ごとに、新しい画像セットを取得します。

オーグザによっては、更新方法がまったく同じ画像に複数回遭遇する可能性は低いです。

これは、暗記するのではなく、モデルを一般化することを奨励します。

私は100回の反復を選択しましたが、理想的には、検証精度の向上が停止したときにトレーニングを中止する必要があります。

この例では、トレーニングの精度は向上し続けていますが、数回の反復後に検証精度が低下し始めることに注意してください。

これは、モデルがトレーニングデータを記憶し、新しいデータへの一般化が少なくなり、検証とテストデータのパフォーマンスが悪化していることを意味します。

この後もトレーニングを続けることは有害です。

最後の例として、トレーニングループに早期停止を追加します。

更新ステップの後、検証メトリクスを計算できます。

検証精度を使用してトレーニングを早期に停止しています。これは分類器モデルで機能します。

しかし、それは本当にあなた次第です。

検証損失を使用したり、独自のメトリックを設計してモデルの品質を評価したりすることもできます。

その後、過去5回の反復で精度が向上していない場合は、トレーニングループから抜け出します。

そして、私の拡張画像分類器はこれで終わりです。

このビデオでは、アプリで優れた機械学習体験を構築するために使用できる、OSに出荷された新しい最先端のモデルを紹介しました。

次に、新しいマルチラベル画像分類器を使用してシーンを理解するためのモデルを構築する方法の例を挙げました。

最後に、モデルの品質を向上させるために独自のカスタム拡張を構築する方法について詳しく説明しました。

そして、それはラップです。

Create MLでアプリを強化する時が来ました。