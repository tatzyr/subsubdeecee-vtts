10093

♪まろやかなインストゥルメンタルヒップホップ♪

♪

クリストファー・フィゲロア：こんにちは、アップルのARKitチームのクリストファー・フィゲロアです。

ピーター・クーン:そして、私はユニティのピーター・クーンです。

クリストファー：UnityはエンジンとXRエコシステムをこの新しいプラットフォームに持ち込み、あなたのようなUnity開発者が簡単にプロジェクトを持ち込むことができます。

ピーターと私は、すでに慣れ親しんでいるUnityワークフローを使用して、ここのRec Roomのような完全に没入感のある体験を構築する方法を紹介します。

完全な没入型スタイルで没入型スペースを作成することから始めます。

これにより、アプリはパススルーを隠し、誰かを別の世界に転送することができます。

完全に没入型の体験で、Unityはコンポジターサービスを利用し、アプリにメタルレンダリング機能のパワーを提供します。

Unityはまた、ARKitを利用して、骨格のハンドトラッキングなど、あなたの体の位置と周囲を認識します。

Unityはこれらの技術に基づいて構築され、Unity Engineで同じサービスを提供します。

Unityを使用して、このプラットフォームで没入型体験を作成するには、主に2つのアプローチがあります。

このプラットフォームに完全に没入型のUnity体験をもたらし、プレイヤーの周囲を独自の環境に置き換えることができます。

または、コンテンツとパススルーを組み合わせて、周囲に溶け込む没入型体験を作成することもできます。

2番目のアプローチに興味がある場合は、「没入型Unityアプリの作成」をチェックすることをお勧めします。

さて、ピーターは、これらの新しいAppleテクノロジーが、Unity開発者が完全に没入型VRゲームをこのプラットフォームに持ち込むのにどのように役立つかを説明します。

ピーター:ありがとう、クリストファー。

まず、Against GravityのRec Roomを見せることから始めましょう。

これは、ユーザーが世界中の他の人とゲームや体験を作成してプレイできる人気のあるVRソーシャルプラットフォームです。

これは、ゲーム開発のための強力で柔軟なプラットフォームを提供するUnityゲームエンジン上に構築されています。

Rec Roomのように、VRコンテンツをこの新しいプラットフォームに簡単に持ち込むためのツールや技術のいくつかを紹介します。

Unityのコンテンツをこの新しいプラットフォームに持ち込むことを計画する際に留意すべき点をいくつかお伝えします。

まず、Unityからデバイスにコンテンツをデプロイするために使用するワークフローについて説明します。

このプラットフォームのグラフィックに関連して、心に留めておくべきことがいくつかあります。

そして最後に、コントローラ入力をハンド入力に適応させる方法と、この移行を支援するためにUnityが提供するツールのいくつかについて話します。

まず、すでに慣れているはずのビルドと実行のワークフローがあります。

このプラットフォームの完全なサポートをUnityに組み込んだので、ほんの数ステップでこのデバイスで実行されているプロジェクトを確認できます。

1つ目は、このプラットフォームのビルドターゲットを選択することです。

次に、他のVRプラットフォームと同様に、XRプラグインを有効にします。

アプリがネイティブプラグインに依存している場合は、このプラットフォーム用に再コンパイルする必要があります。

一方、生のソースコードや.mmファイルを使用している場合は、すでに大丈夫です。

Unityから構築すると、iOS、Mac、またはApple TVターゲットの場合と同様に、Xcodeプロジェクトが生成されるようになりました。

その後、Xcode内から、より高速な反復のために、デバイスまたはデバイスシミュレータのいずれかにビルドして実行することができます。

誰かの周囲を完全に没入型体験に変えるために使用するグラフィックパイプラインは、あなたにも馴染みがあるでしょう。

しかし、理解することが重要な新しい概念がいくつかあります。

すべてのプロジェクトが最初に行う1つの選択肢は、どのレンダリングパイプラインを使用するかです。

ユニバーサルレンダリングパイプラインは理想的な選択です。

これは、Foveated Renderingと呼ばれるこのプラットフォームに固有の特別な機能を可能にします。

Foveated Renderingは、目が集中する可能性が高い各レンズの中央により多くのピクセル密度を集中させ、目が細部に敏感ではない画面の周辺機器の細部を少なくする技術です。

これにより、デバイスを使用している人にとってはるかに高品質の体験が得られます。

ユニバーサルレンダリングパイプラインを使用すると、静的フォベレーションレンダリングがパイプライン全体に適用されます。

また、後処理、カメラスタッキング、HDRなど、すべてのURP機能で動作します。

Foveated Renderingの恩恵を受けるカスタムレンダリングパスがある場合、Unity 2022には、この技術を活用できる新しいAPIがあります。

レンダリングは非線形空間で行われるようになったので、その再マッピングを処理するシェーダーマクロもあります。

静的フォベレーションレンダリングを活用することは、重要なピクセルにリソースを費やし、より高品質のビジュアル体験を生み出すことを意味します。

このプラットフォームでグラフィックスを最適化するもう1つの方法は、シングルパスインスタンスレンダリングを使用することです。

Unityでは、シングルパスインスタンスレンダリングがMetalグラフィックスAPIをサポートするようになり、デフォルトで有効になります。

シングルパスインスタンスレンダリングでは、エンジンは両方の目に対して1つのドローコールのみを送信し、カリングやシャドウなどのレンダリングパイプラインの特定の部分のオーバーヘッドを削減します。

これにより、シーンをステレオでレンダリングするCPUオーバーヘッドが削減されます。

良いニュースは、アプリがすでにシングルパスインスタンスレンダリングを使用して他のVRプラットフォームで正しくレンダリングされている場合、シェーダーマクロはここでも機能することを保証することです。

最後に考慮すべきことが1つあります。

アプリがピクセルごとに深度バッファに正しく書き写っていることを確認してください。

システムコンポジターは、再投影に深度バッファーを使用します。

深度情報が欠落している場合、システムは表示としてエラー色をレンダリングします。

1つの例は、通常、ユーザーから無限に離れているスカイボックスなので、逆Zでゼロの深さを書き込みます。

これには、デバイスに表示されるように修正する必要があります。

深度バッファに正しい値を書き込むためにUnityのすべてのシェーダーを修正しましたが、カスタムスカイボックス、または水効果や透明効果などのカスタム効果がある場合は、各ピクセルの深度に値が書き込まれていることを確認してください。

グラフィックをデバイスにレンダリングしたので、インタラクティブにする時が来ました。

このデバイスでのインタラクションはユニークです。

人々は手と目を使ってコンテンツと対話します。

このプラットフォームでUnityアプリにインタラクションを追加する方法はいくつかあります。

XRインタラクションツールキットは、既存のプロジェクトを簡単に適応できるようにハンドトラッキングを追加します。

Unity Input Systemを使用して、組み込みのシステムジェスチャーに反応することもできます。

また、Unity Handsパッケージとのカスタムインタラクションのために、生のハンドジョイントデータにアクセスできます。

XRインタラクションツールキットは、XRIとも呼ばれ、高レベルのインタラクションシステムを提供します。

このツールキットは、入力をインタラクションに簡単に変換できるように設計されています。

3DオブジェクトとUIオブジェクトの両方で動作します。

XRIは、ハンドトラッキングなどの入力の種類を抽象化し、その入力をアプリが応答できるアクションに変換します。

これは、入力コードがさまざまなタイプの入力を受け入れるプラットフォーム間で機能できることを意味します。

XRIを使用すると、3D空間と3D空間世界のUIの両方で、ホバー、グラブ、選択などの一般的なインタラクションに簡単に対応できます。

ツールキットには移動システムも含まれているので、人々は完全に没入型の空間をより快適に旅行することができます。

人々があなたの世界と交流するとき、視覚的なフィードバックは没頭するために重要です。

XRIを使用すると、各入力制約の視覚的反応を定義できます。

XRIの中核は、基本的なInteractableコンポーネントとInteractorコンポーネントのセットです。

インタラクティブは、入力を受信できるシーン内のオブジェクトです。

インターアクタを定義し、ユーザーがインターアクタブルとどのようにやり取りできるかを指定します。

インタラクションマネージャーは、これらのコンポーネントを結び付けます。

最初のステップは、シーン内のどのオブジェクトと対話できるか、およびそれらの相互作用が発生したときにどのように反応するかを決定することです。

これを行うには、オブジェクトにInteractableコンポーネントを追加します。

3つの組み込みタイプがあります。

シンプルは、オブジェクトを受信インタラクションとしてマークします。

このコンポーネントを使用して、SelectEnteredやSelectExitedなどのイベントを購読できます。

Grabでは、オブジェクトが選択またはつかまれると、Interactorをたどり、リリース時にその速度を継承します。

TeleportAreaやTeleportAnchorなどのテレポートインタラクション機能を使用すると、プレイヤーがテレポートするエリアやポイントを定義できます。

また、独自のカスタムInteractablesを作成できます。

インタラクターは、インタラクティブとしてタグ付けしたオブジェクトを選択または操作する責任があります。

彼らは、各フレームにカーソルを合わせたり選択したりできる可能性のあるインタラクタブルのリストを定義します。

インターアクターにはいくつかの種類があります。

ダイレクトインタラクターは、それに触れているインタラクタブルを選択します。

人の手がいつ相互作用可能なオブジェクトに触れるか、または相互作用可能なオブジェクトに近いかを知りたい場合は、これらのいずれかを使用します。

レイ・インターアクターは、遠くから対話するために使用されます。

このInteractorは、曲線と直線、およびプロジェクトのビジュアルスタイルに適応させるのに役立つカスタマイズ可能な視覚化で高度に構成可能です。

ユーザーがインタラクションを開始すると、そのインタラクションがどのように機能するかに関するオプションがあります。

たとえば、グラブインタラクションの場合、オブジェクトをユーザーの手に移動することができます。

そして、レイ・インターアクターは、あなたのゲームプレイのニーズに合うように、グラブの自由度を制限することを可能にします。

完全に没入型体験における一般的な相互作用は、オブジェクトをつかみ、そのオブジェクトのコンテキストのどこかに配置することです。

例えば、バッテリーをソケットに入れる。

ソケットインターアクタは、特定の領域がオブジェクトを受け入れることができることをプレーヤーに示します。

これらのインターアクターは手に取り付けられていません。

代わりに、彼らは世界のどこかに住んでいます。

ハンドトラッキングやコントローラーでさえ、ユーザーが自然に実行したい一般的なタイプのインタラクションは、ポークインタラクションです。

これは、インタラクションをトリガーするために正しい動きを実行する必要があるように方向フィルタリングが含まれていることを除いて、直接のインターアクターに似ています。

人々が見て交流したい場合、Gaze InteractorはRay Interactorにいくつかの拡張機能を提供し、視線を少し簡単に処理できるようにします。

たとえば、Gaze Interactorsは、Interactablesのコライダーを自動的に大きくして、選択しやすくすることができます。

すべてをまとめるために、インタラクションマネージャーはInteractorsとInteractablesの間の仲介者として機能し、インタラクションの交換を促進します。

その主な役割は、登録されたInteractorsとInteractablesの指定されたグループ内で相互作用状態の変更を開始することです。

通常、すべてのインタラクタがすべてのインタラクタブルに影響を与える可能性を可能にするために、単一のインタラクションマネージャーが確立されます。

あるいは、複数の補完的なインタラクションマネージャーを利用することができ、それぞれが独自のInteractorsとInteractablesの品揃えを持っています。

これらのマネージャーは、特定のインタラクションセットを有効または無効にするために有効または無効にすることができます。

たとえば、シーンごとに異なるインタラクタブルのセット、またはメニューにある場合があります。

最後に、XRコントローラーコンポーネントは、受信する入力データを理解するのに役立ちます。

手または追跡されたデバイスから入力アクションを受け取り、インターアクタに渡して、その入力に基づいて何かを選択またはアクティブにすることを決定できます。

選択など、XRインタラクション状態ごとに入力アクション参照をバインドする必要があります。

ハンドまたはコントローラーごとに1つのXRコントローラーコンポーネントに限定されないため、両手とコントローラーを独立してサポートする柔軟性が得られます。

XRIにバンドルされているサンプルコードは、これを行う方法を示しています。

XRIの高度な機能に加えて、Unity入力システムから直接システムジェスチャー入力を使用するオプションもあります。

その後、タップジェスチャーなどのプラットフォームの組み込みインタラクションを独自のインタラクションシステムにマッピングできます。

Unity Input Systemのバインディングパスを使用して、これらのシステムジェスチャーにアクセスして応答できます。

たとえば、ピンチジェスチャーは、アクティブなときに、位置と回転で値として表示されます。

これらは入力アクションにバインドできます。

その人が焦点を向けているところは、ピンチジェスチャーと同じフレームで、位置と回転で通り抜けます。

さらに柔軟性を高めるために、Unity Handsサブシステムを使用して、Unity Handsパッケージを介してシステムからすべての生のハンドジョイントデータにアクセスできます。

Unity Handsパッケージは、プラットフォーム間で一貫性のある低レベルのハンドジョイントデータへのアクセスを提供します。

たとえば、各関節を見て、ポーズが親指や人差し指などの特定のジェスチャーにどれだけ近いかを判断し、それらをゲームプレイアクションに変換するコードを書くことができます。

これは強力ですが、すべての人の手はサイズが異なり、人々はさまざまな動きを持っているので、正しく取得するのが難しい場合があります。

このコードは、人差し指が拡張されているかどうかを示す方法を定義します。

OnHandUpdateイベントからこのメソッドを呼び出して、片方の手に渡すことができます。

まず、人差し指が伸びているかどうかを確認するために、いくつかの特定の関節を取得します。

それらのいずれかが無効な場合、falseを返します。

すべての関節が有効な場合は、簡単なチェックを行い、人差し指がカールしていないことを確認してください。

このロジックを他の指に拡張して、基本的なジェスチャー検出を実装し始めることができます。

生のハンドジョイントデータのもう1つの用途は、カスタムハンドメッシュビジュアルにマッピングすることです。

これは、手があなたのゲームのアートスタイルにもっとフィットさせるのに役立ちます。

たとえば、Rec Roomは生のハンドジョイントデータを使用して、ビジュアルスタイルに合った様式化されたハンドモデルを表示しました。

彼らはまた、より多くの没入感のために他のプレイヤーハンドモデルを示しています。

Unity Handパッケージには、生のハンドジョイントアクセスについてもっと知りたい場合は、始めるためのサンプルコードがいくつかあります。

あなたのVR体験がこの新しいプラットフォームにやってくるのを楽しみにしています。

このプラットフォームに対するUnityのサポートに関する詳細情報を入手し、早期ベータアクセスにサインアップするには、unity.com/spatialにアクセスしてください。

クリストファー：これらは、すでに慣れ親しんでいるUnityワークフローを使用して、この新しいプラットフォームに完全に没入型のVR体験をもたらすために使用できるツールです。

ピーター：要約すると、このセッションでは、Rec Roomのように、VRコンテンツをこの新しいプラットフォームに簡単に持ち込むことができるツールと技術のいくつかを紹介しました。

新しいプロジェクトを開始する場合は、Unity 2022以降を使用してください。

既存のプロジェクトがある場合は、2022年にアップグレードを開始してください。

ユニバーサルレンダリングパイプラインの採用を検討してください。

組み込みのグラフィックパイプラインはサポートされていますが、将来のすべての改善はユニバーサルパイプラインで行われます。

コントローラーベースのインタラクションを手に適応させ始めます。

XRインタラクションツールキットとUnity Handsパッケージから今日から始めることができます。

クリストファー：最後に、Unityを使用してパススルーで没入型体験を作成する方法の詳細については、「没入型Unityアプリを作成する」ことをお勧めします。

そして、「空間コンピューティングのための素晴らしいゲームを構築する」をチェックして、このプラットフォームでゲーム開発者に何が可能かの概要を確認してください。

ピーター：あなたがプラットフォームに何をもたらすのか、私たちは楽しみにしています。

クリストファー:見てくれてありがとう。

♪