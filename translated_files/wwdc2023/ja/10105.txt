10105

ロブ：こんにちは。

私はカメラソフトウェアチームのロブ・シムティスと、写真チームのセバスチャン・メディナです。「より反応の良いカメラ体験を創造する」というセッションへようこそ。

AVFoundationキャプチャクラスとPhotoKitフレームワークで、多くの新しい強力なAPIを紹介します。

まず、延期された写真処理について話します。

次に、シャッターラグゼロで本当に「瞬間を捉える」方法を紹介します。

第三に、新しいレスポンシブキャプチャAPIについて説明します。

そして最後に、更新されたビデオエフェクトで応答するための別の方法を見ていきます。

iOS 13以降では、AVCapturePhotoSettingの写真品質優先順位付け列挙値を使用して、応答性を変更したり、処理された写真をキャプチャして取り戻したり、次の写真を撮影したりできます。

アプリは、適切な応答性を達成するために、さまざまな列挙値から選択できますが、画質に影響を与えました。

または、常に品質価値を使用したい場合は、ショットツーショットの時間に影響を与える可能性があります。

iOS 17では、このAPIを引き続き使用できますが、新しい補完的なAPIを導入しているため、高品質の写真を取得しながら、必要なショットを取得する可能性を高めることができます。

私のチームがセッションのために構築したこのアプリをご案内します。

各トグルスイッチを「オン」に切り替えることで、今年は新機能を有効にすることができ、より応答性の高い写真体験を作るためにコンセプトを1つずつ構築します。

では、延期された写真処理から始めましょう。

今日、AVCapturePhotoOutputから最高品質の写真を取得するために、写真をキャプチャするときに設定で「品質」の写真品質の優先順位付け列挙値を使用します。

私たちの「バランスのとれた」値と「品質」の値のために、これは通常、ある種のマルチフレーム融合とノイズリダクションを含みます。

iPhone 11 Pro以降のモデルでは、私たちの最も先進的な技術の1つは「ディープフュージョン」と呼ばれています。

これは、高解像度の写真で驚くほど鮮明なディテールを提供します。

このディープフュージョンショットでは、オウムの羽は超シャープで、本当に際立っています。

しかし、それにはコストがかかります。

この処理は、次のキャプチャ要求が開始される前に完了する必要があり、完了するまでに時間がかかる場合があります。

実際の例を見てみましょう。 

カメラソフトウェアチームが仕事を終わらせるためにオフィスに行く方法についてのプレゼンテーションをまとめています。

同僚のデビンが最新の自転車技術を使ってアップルパークを歩き回っています。

タップすると、次のショットを撮る前に、シャッターボタンがワンショットから回転し終わるのを待っています。

最終結果は素晴らしいディープフュージョンの写真です。

そのひげの詳細をチェックしてください!

しかし、私がタップしている間、処理は同期的に実行され、ショットからショットへの時間は少し遅い感じがします。

だから、鮮明なディテールで良い写真を手に入れたかもしれませんが、私が探していたショットではないかもしれません。

イベントの図を見てみましょう。 

設定でAVCapturePhotoOutputのcapturePhotoメソッドを呼び出すと、デリゲートはプロセスのさまざまなポイントでコールバックを受け取ります...

resolvedSettingsのwillBeginCaptureなど。

カメラソフトウェアスタックは、センサーからフレームをつかみ、当社の処理技術を使用してそれらをディープフュージョン画像に融合させます...

その後、didFinishProcessingPhotoデリゲートコールバックを介して写真が返送されます。

この処理は、次のキャプチャが発生する前に完了する必要があり、それには時間がかかる場合があります。

didFinishProcessingPhotoコールバックが発火する前にcapturePhotoを呼び出すこともできますが、前の写真の処理が完了するまで開始されません。

延期された写真処理では、このタイムラインは縮小します。

写真を要求し、適切な場合、カメラパイプラインは、新しいdidFinishCapturingDeferredPhotoProxyデリゲートコールバックを介して、軽く処理された「プロキシ」写真を配信します。

このプロキシ写真をプレースホルダとしてライブラリに保存します。

そして、次の写真はすぐに撮影できます。

カメラセッションが却下されると、システムは後で処理を実行して最終的な写真を取得します。

したがって、アプリの設定で遅延写真処理をオンにすると、キャプチャセッションは、必要に応じてキャプチャ時にプロキシ写真を配信するように再構成されます。

そして、私は以前と同じシャープで詳細な写真を得ることができますが、最終的な処理を後で延期することで、その瞬間にいる間にそれらをもっと撮ることができます。

ナイスウイリー。それらの自転車はしっかりしています。

行くよ。それは私のプレゼンテーション、ひげ、そしてすべてのための素晴らしい写真です。

だから、あなたに延期処理された写真を与えるために相互作用するすべての部分を見てみましょう。

以前のWWプレゼンテーションの簡単な復習コースとして、写真をキャプチャするためにAVCaptureSessionを設定するときは、カメラであるAVCaptureDeviceでAVCaptureDeviceInputをセッションに追加します。

次に、セッションにAVCapturePhotoOutputを追加し、特定のフォーマットまたはセッションプリセット、通常はアプリがphotoOutputでcapturePhotoを呼び出すときに「写真」セッションプリセットを選択します。

遅延写真処理に適したタイプの写真の場合は、didFinishCapturing Deferred Photo Proxyで折り返しお電話します。

そこから、プロキシデータをフォトライブラリに送信します。

だから、あなたが今あなたのライブラリに持っているのはプロキシ写真ですが、最終的には最終的な画像を使用または共有したいと思うでしょう。

最終的な写真処理は、ライブラリから画像データを要求するときにオンデマンドで、またはデバイスがアイドル状態であるなど、システムがそうするのが良いと判断したときにバックグラウンドで行われます。

そして今、同僚のセバスチャンにこれをコード化する方法を教えてもらいます。

あなたに、セバスチャン。

セバスチャン:ありがとう、ロブ。

こんにちは、私の名前はセバスチャン・メディナで、写真チームのエンジニアです。

今日は、PhotoKitを介して最近キャプチャされた画像を撮影して、遅延処理をトリガーします。

次に、PHImageManagerリクエストから画像を受信する際の新機能を示すために、同じ画像をリクエストします。

ただし、PhotoKitを通じて資産を処理する前に、遅延処理用の新しいカメラAPIが設定されていることを確認する必要があります。

これにより、私のアプリは、PhotoKitを介して送信できる遅延写真プロキシ画像を受け入れることができます。

さて、私は先に進んで、これを利用するためのコードを書きます。

ここでは、AVCapturePhotoOutputとAVCaptureSessionオブジェクトを設定しました。

これで、セッションの設定を開始できます。

この場合、遅延処理を利用できるように、セッションにタイプ写真のプリセットが必要です。

次に、キャプチャデバイスをつかんで、デバイス入力を設定します。

その後、可能であれば、デバイス入力を追加します。

次に、photoOutputを追加できるかどうかを確認します。

もしそうなら、それを追加します。

さて、新しいもののために。

新しいautoDeferredPhotoDeliverySupported値が真であるかどうかを確認し、遅延処理でキャプチャした写真を送信できることを確認します。

これが合格した場合、私は先に進み、プロパティautoDeferredPhotoDeliveryEnabledで新しい延期写真の配信にオプトインすることができます。

この延期された写真の配信チェックと有効化は、延期された写真を有効にするためにカメラコードに追加する必要があるすべてです。

最後に、セッション設定をコミットします。

したがって、capturePhotoメソッドに呼び出しが行われると、受信したデリゲートコールバックは遅延プロキシオブジェクトを保持します。

これらのコールバックの例を見てみましょう。1つ。

このフォトキャプチャコールバックでは、最近キャプチャした画像に関連するAVCapturePhotoOutputとAVCaptureDeferredPhotoProxyオブジェクトをカメラから受信しています。

まず、適切な写真出力値を受信していることを確認することをお勧めしますので、エラーパラメータの値を確認します。

では、PhotoKitを使用して画像をフォトライブラリに保存し始めます。

共有されたPHPhotoLibraryで変更を実行します。

ただし、フォトライブラリへの書き込みアクセスのみが必要です。 注意してください。

次に、AVCaptureDeferredPhotoProxyオブジェクトから写真データをキャプチャします。

フォトライブラリの変更を実行するため、関連するperformChangesインスタンスメソッドを設定する必要があります。

資産を保存する場合と同様に、PHAssetCreationRequestを使用します。

次に、リクエストに応じて「addResource」メソッドを呼び出します。

パラメータについては、新しいPHAssetResourceType「.photoProxy」を使用します。

これは、画像で遅延処理をトリガーするようにPhotoKitに指示するものです。

その後、以前にキャプチャしたプロキシ画像データを追加できます。

そして、この場合、私はオプションを使用しません。

ここでは、遅延処理を必要としない画像データにこの新しいリソースタイプを使用すると、エラーが発生することを知っておくことが重要です。

そして、エラーといえば、私は先に進み、完了ハンドラー内でそれらをチェックします。

そして、それはそれと同じくらい簡単です。

先に進み、アプリケーションが適切と判断した場合、完了ハンドラ内の成功とエラーを処理してください。

さて、私たちの資産を取り戻したいと言ってください。

PHImageManagerリクエストでそれを達成できるので、それを行うためにコードを調べます。

パラメータについては、PhotoKit経由で送信した画像のPHAssetオブジェクト、返される画像のターゲットサイズ、およびコンテンツモードがあります。

デフォルトのPHImageManagerオブジェクトを取得し、imageManagerオブジェクトを使用して、requestImageForAssetメソッドのリクエストイメージアセットを呼び出すことができます。

パラメータについては、以前にフェッチしたアセット、ターゲットサイズ、コンテンツモードを使用します。この場合、オプションは使用しません。

これで、resultImageがUIImageであり、infoが画像に関連する辞書であるresultHandlerを介してコールバックを処理できます。

今日、最初のコールバックは、情報ディクショナリーキーPHImageResultIsDegradedKeyで低解像度の画像を保持しますが、最終的な画像コールバックは保持しません。

だから、私はここでそれらのためにチェックすることができます。

PhotoKitを介して処理された画像を作成する追加は、開発者がrequestImageForAssetメソッドからセカンダリ画像を受け取ることを可能にする新しいAPIを立ち上げる良い機会を提供します。

遅延処理を通過する画像が完成するのに時間がかかる可能性があるため、その間にこの新しいセカンダリ画像を表示できます。

この新しい画像を受け取るには、PHImageRequestOptionsの新しいallowSecondaryDegradedImageを使用します。

この新しいイメージは、requestImageForAssetメソッドからの現在の2つのコールバックの間に入れられます。

また、画像に関連する情報辞書には、今日最初の画像コールバックで使用されているPHImageResultIsDegradedKeyのエントリがあります。

何が起こっているかをよりよく説明するために、今日、requestImageForAssetメソッドは2つの画像を提供します。

1つ目は、最終的な高品質の画像を準備している間、一時的に表示するのに適した低品質の画像です。

この新しいオプションでは、現在の2つのオプションの間に、最終的な処理中に表示する新しい高解像度の画像が提供されます。

この新しい画像を表示すると、最終的な画像が処理されるのを待っている間、ユーザーはより快適な視覚体験が得られます。

では、これを利用するためのコードを書いてみましょう。

ただし、今回はPHImageRequestOptionsオブジェクトを作成します。

次に、新しいallowSecondaryDegradedImageオプションをtrueに設定します。

このようにして、リクエストは新しいセカンダリイメージコールバックを返送することを知っています。

ここでは、以前に書いたrequestImageForAssetメソッドを再利用することができますが、作成したばかりの画像リクエストオプションオブジェクトを追加します。

新しいセカンダリ画像情報辞書は、最初のコールバックと同様に、PHImageResultIsDegradedKeyの真の値を保持するので、ここで確認します。

そして、それは新しい二次画像表現を受け取るためのものです。

アプリを最もよくサポートするために、結果ハンドラー内で画像を処理することを忘れないでください。

これで、遅延処理でフォトライブラリに画像を追加する方法と、最終的な画像が処理を完了するのを待っている間にアプリに表示する画像リクエストから二次的な高品質の画像を受け取る方法がわかりました。

これらの変更は、新しい延期処理PhotoKitの変更とともに、iOS 17、tvOS 17、macOS Sonomaから利用可能になります。

では、より反応の良いカメラを作成するための新しいツールの詳細については、ロブに返します。

ロブ：すごい。ありがとう、セバスチャン!

延期された写真処理で素晴らしい経験を確実にするために、細かい詳細に入りましょう。

フォトライブラリから始めます。

遅延写真処理を使用するには、プロキシ写真を保存するためのフォトライブラリへの書き込み許可と、アプリが最終的な写真を表示する必要がある場合、または何らかの方法で変更したい場合は読み取り許可が必要です。

しかし、顧客に代わって最大限のプライバシーと信頼を維持するために、顧客から必要に応じて図書館へのアクセスの最小量のみを要求する必要があることを覚えておいてください。

また、プロキシを受け取ったら、できるだけ早くファイルデータ表現をライブラリに取り入れることを強くお勧めします。

アプリがバックグラウンド化されている場合、システムが一時停止する前に実行する時間は限られています。

メモリの圧力が大きすぎると、バックグラウンドのウィンドウ中にアプリがシステムによって自動的に強制的に終了される可能性があります。

プロキシをできるだけ早くライブラリに取り込むことで、顧客のデータ損失の可能性を最小限に抑えることができます。

次に、通常、フィルターの適用などの写真のピクセルバッファに変更を加える場合、またはAVCapturePhoto File Data Representation Customizerを使用してAVCapturePhotoのメタデータやその他のプロパティに変更を加える場合、処理が完了すると、これらはライブラリ内の完成した写真には有効になりません。

PhotoKit APIを使用して写真を調整するため、後でこれを行う必要があります。

また、コードは、同じセッションで遅延プロキシと非遅延写真の両方を処理できる必要があります。

これは、すべての写真が必要な余分な手順で処理するのが理にかなっているわけではないからです。

たとえば、「品質」の写真品質の優先順位付け列挙値の下で撮影されたフラッシュキャプチャは、Deep Fusion写真のようにショットツーショットの節約の恩恵を受ける方法で処理されません。

また、AVCapturePhotoSettingsにオプトインまたはオプトアウトのプロパティがないことに気付くかもしれません。

それは、延期された写真処理が自動だからです。

オプトインすると、カメラパイプラインがより長い処理時間を必要とする写真を撮ると、プロキシが返送されます。

適切でない場合は、最終的な写真が送信されますので、ショットごとにオプトインまたはオプトアウトする必要はありません。

キャプチャセッションを開始する前に、isAutoDeferredPhotoProcessingEnabledをtrueとしてAVCapturePhotoOutputに伝えるだけです。

最後に、ユーザーエクスペリエンスについて話しましょう。

遅延写真処理は、迅速なショットツーショットタイムで最高の画質を提供しますが、最終的な処理が後になるまで遅れるだけです。

あなたのアプリが、ユーザーが共有や編集のためにすぐに画像を欲しがる可能性があり、私たちが提供する最高品質の写真にそれほど興味がない場合は、延期された写真処理の使用を避けることは理にかなっているかもしれません。

この機能は、iPhone 11 Proと11 Pro Max、およびそれ以降のiPhoneから利用できます。

そして、AVCapturePhotoOutputでの作業とライブラリ権限の処理に関する素晴らしい関連ビデオをいくつか紹介します。

そして今、ゼロシャッターラグに目を向けて、スケートボードについて話しましょう。

カメラソフトウェアチームの輸送モードについての私の今後のプレゼンテーションのために、私たちはいくつかの映像をつかむためにスケートパークに行きました。

私はiPhone 14 Proでアクションモードで同僚を撮影していますが、スライド用に高品質のヒーローアクションショットも撮りたいです。

しかし、スポイラーアラート、私はスケートボードをしません。

シャッターボタンをタップして、空気を吸っている同僚のトモの写真を撮ります。

写真を調べるためにカメラロールに行くと、これが私が得たものでした。

彼がジャンプの高さにいたとき、私はシャッターボタンをタップしましたが、写真は彼の着陸です。

それはまさに私が望んでいたものではありません。

それで、何が起こったの?シャッターラグ。

シャッターラグが発生しました。

「シャッターラグ」は、キャプチャをリクエストしてから、センサーから1つ以上のフレームを読み出して写真に融合して配信するまでの遅延と考えることができます。

ここでは、時間は左から右へ、左は古いフレーム、右は新しいフレームです。

フレーム5はカメラのファインダーにあるものだとしましょう。

今日、AVCapturePhotoOutputの設定でcapturePhoto:を呼び出すと、カメラパイプラインはセンサーからフレームをつかみ始め、処理技術を適用します。

しかし、キャプチャされたフレームのブラケットは、タッチダウン後、フレーム5の後に始まります。

あなたが得るのは、フレーム6から9、またはそれ以降のフレームに基づく写真です。

毎秒30フレームで、各フレームは33ミリ秒間ファインダーにあります。

あまり聞こえませんが、行動が終わるのに本当に時間はかかりません。

それはトモが着陸するのに十分な長さであり、私はそのヒーローショットを得るのを逃しました。

ゼロシャッターラグを有効にすると、カメラパイプラインは過去のフレームのローリングリングバッファを保持します。

さて、フレーム5はファインダーで見るもので、タップしてキャプチャし、カメラパイプラインは少しタイムトラベルを行い、リングバッファからフレームをつかみ、それらを融合させ、あなたが望む写真を取得します。

だから今、アプリの設定ペインで2番目のトグルを使用してゼロシャッターラグを有効にすると、トモが空気をキャッチするので、シャッターボタンをタップすると、プレゼンテーションに欲しかった「ヒーロー」ショットの1つを手に入れました。

アプリでゼロシャッターラグを取得するために何をする必要があるかについて話しましょう。

絶対に何もない！

AVCaptureSessionPresetsとAVCaptureDeviceFormatsのiOS 17以降にリンクするアプリで、ゼロシャッターラグを有効にしました。

しかし、テスト中に望む結果が得られないことがわかった場合は、AVCapturePhotoOutput.isZeroShutter LagEnabledをfalseに設定してオプトアウトすることができます。

また、出力がセッションに接続されると、isZeroShutterLagSupportedがtrueかどうかを確認することで、photoOutputが設定されたプリセットまたはフォーマットのゼロシャッターラグをサポートしているかどうかを確認できます。

フラッシュキャプチャ、手動露出用のAVCaptureDeviceの設定、ブラケットキャプチャ、複数のカメラからフレームを同期した構成写真配信などの特定の種類の静止画キャプチャは、ゼロシャッターラグを取得しません。

カメラパイプラインはリングバッファからフレームをつかむために時間をさかのぼっているため、キャプチャを開始するジェスチャーと写真出力写真設定を送信するときに長い遅延がある場合、ユーザーは写真にカメラの揺れを誘発する可能性があります。

したがって、タップイベントと写真出力のcapturePhoto API呼び出しの間に行う作業を最小限に抑えたいと思うでしょう。

よりレスポンシブな写真体験を作るための機能を締めくくり、レスポンシブキャプチャAPIについて説明します。

これは、顧客が重複するキャプチャを撮影し、写真の品質を調整することでショットツーショットの時間に優先順位を付け、次の写真を撮ることができるときに優れたUIフィードバックを提供するAPIのグループです。

まず、メインAPI、レスポンシブキャプチャ。

スケートパークに戻って、以前に2つの機能を有効にすると、毎秒約2枚の写真を撮ることができます。

明確にするために、映像を遅くしました。

毎秒2フレームでは、空中でのトモの行動はあまり見えず、これは私が終わった最高の写真でした。

かなり良いですが、もっとうまくできるかどうか見てみましょう。

次に、3番目と4番目のスイッチをオンにして、レスポンシブキャプチャ機能を有効にします。

ファストキャプチャの優先順位付けを少し確認します。

しかし、まず、公園に戻ってください!

そして、もう一度やってみましょう。

レスポンシブキャプチャを使用すると、同じ時間でより多くの写真を撮ることができ、適切な写真を撮る可能性が高くなります。

そして、私のプレゼンテーションの開始のための「ヒーロー」ショットがあります。

チームはきっと気に入るよ！

設定方法を使用したAVCapturePhotoOutput.capturePhotoへの呼び出しは、センサーからフレームをキャプチャし、それらのフレームを最終的な非圧縮画像に処理し、写真をHEICまたはJPEGにエンコードする3つの異なるフェーズを経ると考えることができます。

エンコーディングが完了した後、写真の出力はデリゲートのdidFinishProcessingPhotoコールバックを呼び出すか、遅延写真処理APIにオプトインした場合は、適切なショットであれば、おそらくdidFinishCapturing Deferred Photo Proxyを呼び出します。

しかし、「キャプチャ」フェーズが完了し、「処理」が開始されると、写真の出力は理論的には別のキャプチャを開始する可能性があります。

そして今、その理論は現実であり、あなたのアプリで利用可能です。

メインのレスポンシブキャプチャAPIを選択すると、写真出力がこれらのフェーズと重なり、別のリクエストが処理段階にある間に新しい写真キャプチャリクエストを開始でき、顧客により速く、より一貫性のあるバックツーバックショットを提供します。

これにより、写真出力で使用されるピークメモリが増加するため、アプリも多くのメモリを使用している場合は、システムに圧力をかけます。その場合、オプトアウトを好むか、オプトアウトする必要があるかもしれません。

タイムライン図に戻ると、ここで2枚の写真を連続して撮ります。

あなたの代理人は、willBeginCaptureFor resolvedSettingsのために呼び戻され、写真AのdidFinishCaptureFor resolvedSettingsのために呼び戻されます。

しかし、写真がエンコードされて配信される写真AのdidFinish Processing Photoコールバックを取得する代わりに、写真Bの最初のwillBeginCapture For resolvedSettingsを取得できます。

現在、機内写真リクエストが2つあるため、コードがインターリーブ写真のコールバックを適切に処理していることを確認する必要があります。

これらの重複したレスポンシブキャプチャを取得するには、サポートされているときにまずゼロシャッターラグを有効にします。

レスポンシブキャプチャサポートを受けるには、オンになっていなければなりません。

次に、AVCapturePhotoOutput isResponsiveCaptureSupported APIを使用して、写真出力がプリセットまたはフォーマットでサポートしていることを確認し、AVCapturePhotoOutputを設定してオンにします。

.isResponsiveCaptureEnabled to true。

先ほど、「高速キャプチャの優先順位付け」を有効にしたので、簡単に説明します。

写真出力のためにオンにすると、短期間で複数のキャプチャが撮影されているときに検出され、それに応じて、ショットツーショット時間を維持するために、最高品質の設定からより多くの「バランスのとれた」品質設定に写真の品質を適応させます。

しかし、これは写真の品質に影響を与える可能性があるため、デフォルトではオフになっています。

Camera.appの設定ペインでは、これは「より速い撮影を優先する」と呼ばれています。

デフォルトでは、一貫したショットツーショット時間がより重要であると考えているため、Camera.appでデフォルトでオンにすることを選択しましたが、アプリと顧客によって異なる選択をするかもしれません。

予想通り、サポートされているときに写真出力の「高速キャプチャの優先順位付けがサポートされています」プロパティを確認できます。また、サポートされている場合は、あなたや顧客がこの機能を使用したい場合は、「高速キャプチャの優先順位付けが有効になっています」をtrueに設定できます。

では、ボタンの状態と外観の管理についておしゃべりしましょう。

写真の出力は、次のキャプチャを開始する準備ができているとき、または処理されているときの指標を与えることができ、写真のキャプチャボタンを適切に更新することができます。

これは、AVCapturePhotoOutput CaptureReadinessと呼ばれる値の列挙型を介して行われます。

写真の出力は、「実行中」、「準備完了」、および「一時的に」、「キャプチャを待っている」、または「処理を待っている」の3つの「準備ができていない」状態にすることができます。

「準備ができていない」列挙型は、設定でcapturePhotoを呼び出すと、キャプチャと写真の配信の間に待ち時間が長くなり、以前に話したシャッターラグが増加することを示しています。

アプリは、新しいクラスAVCapturePhotoOutputReadinessCoordinatorを使用して、この状態の変化を聞くことができます。

これにより、写真出力の準備状況が変更されたときに、指定したデリゲートオブジェクトにコールバックされます。

レスポンシブキャプチャまたはファストキャプチャ優先順位付けAPIを使用していなくても、このクラスを使用できます。

Readiness CoordinatorとReadiness列挙型を使用して、シャッターの可用性を伝え、ボタンの外観を変更する方法は次のとおりです。

私たちのセッションのアプリは、「準備ができていない」列挙値を処理するときにキャプチャボタンのユーザーインタラクションイベントをオフにし、追加のリクエストが複数のタップで誤ってキューに入れられるのを防ぎ、シャッターラグが長くなります。

タップし、設定要求付きの capturePhoto がキューにされた後、captureReadiness 状態は .ready と .notReadyMomentarily 列挙値の間になります。

フラッシュキャプチャは.notReadyWaitingForCapture状態にヒットします。

フラッシュが発火するまで、写真の出力はセンサーからフレームを取得していないので、ボタンは暗くなります。

最後に、今年はゼロシャッターラグのみを使用し、他の機能を使用しない場合は、各写真のキャプチャと処理が完了しているため、.notReadyWaitingForProcessingの列挙値が現在の準備である間にスピナーを表示する場合があります。

コードで準備コーディネーターを利用する方法は次のとおりです。

まず、写真出力の準備コーディネーターを作成し、準備状態に関するコールバックを受信する適切なデリゲートオブジェクトを設定します。

次に、各キャプチャ時に、通常どおり写真の設定を設定します。

次に、準備コーディネーターに、これらの設定のキャプチャ要求の準備状態の追跡を開始するよう伝えます。

そして、写真の出力でcapturePhotoを呼び出します。

その後、準備コーディネーターはcaptureReadinessDidChangeデリゲートコールバックを呼び出します。

受信した準備列挙値に基づいてキャプチャボタンの状態と外観を更新し、顧客が次にキャプチャできる時期について最善のフィードバックを提供します。

レスポンシブキャプチャとファストキャプチャ優先順位付けAPIは、A12 Bionicチップ以降を搭載したiPhoneで利用でき、準備コーディネーターはAVCapturePhotoOutputがサポートされている場所ならどこでも利用できます。

そして今、私はアプリですべての新機能を有効にし、可能な限り最も反応の良いカメラ体験を提供し、超シャープで高品質の写真を提供します。

しかし、あなたは改善された経験を得るためにそれらすべてを使用する必要はありません。

アプリに適したものだけを使用できます。

本日、更新されたビデオエフェクトでセッションを終了します。

以前は、macOSのコントロールセンターは、センターステージ、ポートレート、スタジオライトなどのカメラストリーミング機能のオプションを提供していました。

macOS Sonomaでは、ビデオエフェクトをコントロールセンターから独自のメニューに移動しました。

カメラや画面共有のプレビューが表示され、ポートレートモードやスタジオライトなどのビデオエフェクトを有効にすることができます。

ポートレートとスタジオライトのエフェクトは強度が調整可能になり、スタジオライトはより多くのデバイスで利用できます。

そして、「リアクション」と呼ばれる新しいエフェクトタイプがあります。

ビデオ通話をしているときは、スピーカーを中断することなく継続させながら、アイデアを愛していることを表現したり、良いニュースについて親指を立てたりしたいと思うかもしれません。

反応は、あなたのビデオと風船、紙吹雪などをシームレスにブレンドします。

反応は、ポートレートとスタジオの照明効果のテンプレートに従います。これはシステムレベルのカメラ機能であり、アプリにコードの変更を必要とせずに、箱から出してすぐに利用できます。

ポートレートとスタジオライトの効果の詳細については、2021年のセッション「カメラキャプチャの新機能」をご覧ください。

ビデオストリームで反応を表示する3つの方法があります。まず、macOSの新しいビデオエフェクトメニューの下部ペインで反応効果をクリックすることができます。

第二に、アプリはAVCaptureDevice.performEffectを呼び出すことができます: 反応タイプ。たとえば、アプリのビューの1つに一連のリアクションボタンがあり、参加者はクリックしてリアクションを実行できます。

そして第三に、反応が有効になると、ジェスチャーをすることで送信できます。

これをチェックしてみましょう。

あなたは親指アップ、親指ダウン、2つの親指アップで花火、ハート、1つの勝利サインで風船、2つの親指ダウンで雨、2つの勝利サインで紙吹雪、そして私の個人的なお気に入り、レーザー、角の2つのサインを使用して行うことができます。

ねえ、それは素敵な効果の束です。

キャプチャセッションで使用するAVCaptureDeviceFormatのreactionEffectsSupportedプロパティを見ることで、反応効果のサポートを確認できます。

AVCaptureDeviceには、ジェスチャー認識がオンになっているときと反応効果が有効になっているときを知るために、読み取ることができるプロパティまたはキー値観察があります。

これらはユーザーの管理下にあるため、アプリはオンまたはオフを切り替えることができないことを覚えておいてください。

iOSでは、それは同じ考えです。

参加者はコントロールセンターに行き、ジェスチャー認識のオン/オフを切り替え、これが起こったときに値を観察することができます。

ただし、iOSのアプリでエフェクトをトリガーするには、プログラムで行う必要があります。

では、今、それを行う方法を見てみましょう。

「canPerformReactionEffects」プロパティがtrueの場合、reactionTypeメソッドのperformEffectを呼び出すと、リアクションがビデオフィードにレンダリングされます。

アプリは、効果をトリガーするボタンを提供する必要があります。

ジェスチャーを介して入ってくる反応は、検出に使用されるキューに応じて、performEffect\を呼び出すときとは異なる場所でビデオでレンダリングされる場合があります。

AVCaptureDeviceがキャプチャセッションで認識し、ビデオコンテンツにレンダリングできるサムズアップやバルーンなど、さまざまな反応効果のすべてのAVCaptureReactionTypeと呼ばれる新しい列挙型があります。

また、「AVCaptureDevice.availableReactionTypes」プロパティは、設定された形式またはセッションプリセットに基づいてAVCaptureReactionTypesのセットを返します。

これらのエフェクトには、独自のビューに配置できるシステムUIImagesも組み込まれています。

AVCaptureReactionTypeを取り込み、UIImage systemNameコンストラクタで使用する適切な文字列を返す新しい関数AVCaptureReactionType.systemImageNameからリアクションのsystemNameを取得できます。

そして、反応効果が進行中であるAVCaptureDevice.reactionEffectsInProgressという適切な名前のAPIがあります。

ユーザーが複数の反応効果を順番に実行すると、一時的に重なる可能性があるため、ステータスオブジェクトの配列が返されます。

キー値観察を使用して、これらがいつ始まり、いつ終わるかを知ることができます。

Voice-over-IP会議アプリの場合は、この情報を使用して、特に発信者が帯域幅の理由でビデオをオフにしている場合に、エフェクトに関するメタデータをリモートビューに送信することもできます。

たとえば、別の発信者に代わってUIにエフェクトアイコンを表示することができます。

ビデオストリームにエフェクトアニメーションをレンダリングすることは、ビデオエンコーダにとって難しい場合があります。

それらはコンテンツの複雑さを増大させ、それをエンコードするためにより大きなビットレートの予算が必要になる場合があります。

キー値がreactionEffectsInProgressを観察することで、レンダリング中にエンコーダの調整を行うことができます。

アプリで実現可能であれば、エフェクトがレンダリングされている間にエンコーダのビットレートを上げることができます。

または、VideoToolboxを介して低遅延ビデオエンコーダを使用し、MaxAllowedFrameQP VTCompressionPropertyKeyを設定している場合は、サポートされている解像度、フレームレート、ビットレート層などのさまざまなビデオ構成を使用してアプリでテストを実行し、効果の進行中に最大許容FrameQPを調整することをお勧めします。

MaxAllowedFrameQP値が低いと、エフェクトのフレームレートが損なわれる可能性があり、ビデオフレームレートが低くなることに注意してください。

2021年のセッション「VideoToolboxで低遅延ビデオエンコーディングを探索する」には、この機能の操作に関するより優れた情報があります。

また、エフェクトが進行中の場合、ビデオフレームレートが変化する可能性があることも知っておく必要があります。

たとえば、AVCaptureSessionを毎秒60フレームで実行するように設定した場合、エフェクトが実行されていない間は毎秒60フレームになります。

しかし、エフェクトが進行中ですが、毎秒30フレームなど、異なるフレームレートが得られる場合があります。

これは、エンドフレームレートが指定したよりも低くなる可能性があるポートレートとスタジオライトエフェクトのモデルに従います。

そのフレームレートを確認するには、デバイスで設定しているフォーマットのAVCaptureDeviceFormat.videoFrameRateRange ForReactionEffectsInProgressをチェックしてください。

他のAVCaptureDeviceFormatプロパティと同様に、これはあなたが制御できるものではなく、あなたのアプリに情報を提供するものです。

macOSとContinuity Cameraを使用するtvOSアプリでは、反応効果は常に有効になっています。

iOSとiPad OSでは、アプリケーションはInfo.plistを変更してオプトインできます。

UIBackgroundModes配列のVoIPアプリケーションカテゴリであることを宣伝するか、NSCameraReactionEffectsEnabledをYESの値で追加することで、オプトインします。

反応効果とジェスチャー認識は、iPhone 12、Apple Silicon Mac、Intel Mac、Continuity Cameraデバイスを使用したApple TV、USB-C iPadまたはApple Silicon Macに接続されたApple Studioディスプレイ、USB-C iPadまたはApple Silicon Macに接続されたサードパーティ製カメラなど、A14チップ以降を搭載したiPhoneおよびiPadで利用できます。

そして、今年の新しいAPIを使用したレスポンシブカメラ体験に関するセッションを締めくくります。

遅延写真処理、ゼロシャッターラグ、レスポンシブキャプチャAPIについて話し、画質が向上した最も応答性の高い写真アプリを作成するための新しい可能性を提供し、新しい「リアクション」を含む更新されたビデオエフェクトでユーザーが本当に自分自身を表現する方法についても取り上げました。

私はあなたがすべての素晴らしい新機能にどのように反応するかを見るのが待ちきれません。

見てくれてありがとう。

。