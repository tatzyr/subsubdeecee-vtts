10039

♪ ♪

Xin chào, và chào mừng đến với "Phân loại các tư thế và hành động tay với Create ML."

Tôi là Nathan Wertman.

Và hôm nay, tôi sẽ được tham gia bởi các đồng nghiệp của tôi, Brittany Weinert và Geppy Parziale.

Hôm nay, chúng ta sẽ nói về việc phân loại các vị trí tay.

Nhưng trước khi chúng ta đào sâu vào điều đó, hãy nói về chính bàn tay.

Với hơn hai chục xương, khớp và cơ bắp, bàn tay là một kỳ quan kỹ thuật.

Bất chấp sự phức tạp này, bàn tay là một trong những công cụ đầu tiên trẻ sơ sinh sử dụng để tương tác với thế giới xung quanh.

Trẻ sơ sinh học những điều cơ bản về giao tiếp bằng cách sử dụng các động tác tay đơn giản trước khi chúng có thể nói.

Một khi chúng ta học nói, bàn tay của chúng ta tiếp tục đóng một vai trò trong giao tiếp.

Họ chuyển sang thêm sự nhấn mạnh và biểu hiện.

Trong năm qua, bàn tay của chúng ta đã trở nên quan trọng hơn bao giờ hết để đưa mọi người đến gần nhau hơn.

Vào năm 2020, Khung Tầm nhìn đã giới thiệu Phát hiện Tư thế Tay, cho phép các nhà phát triển xác định các bàn tay trong khung, cũng như mỗi trong số 21 khớp có thể nhận dạng có trong tay.

Đây là một công cụ tuyệt vời nếu bạn đang cố gắng xác định xem một bàn tay có tồn tại hay một bàn tay ở đâu trong khung, nhưng nó có thể là một thách thức nếu bạn đang cố gắng phân loại những gì bàn tay đang làm.

Mặc dù khả năng biểu cảm của bàn tay là vô hạn, nhưng trong phần còn lại của phiên này, tôi muốn tập trung vào các tư thế ngắn, một tay, như dừng lại, và yên tĩnh, và hòa bình, và các hành động ngắn, một tay, như lùi lại, biến mất và đến đây.

Tôi vừa đề cập đến các tư thế và hành động bằng tay.

Làm thế nào về một số định nghĩa cụ thể hơn?

Chà, một mặt, chúng ta có những tư thế, có ý nghĩa như một hình ảnh tĩnh.

Mặc dù hai video này bị tạm dừng, ý định của chủ đề được thể hiện rõ ràng.

Hãy nghĩ về một tư thế như một hình ảnh.

Và mặt khác, chúng ta có một hành động, đòi hỏi chuyển động để thể hiện đầy đủ ý nghĩa.

Ý nghĩa của hai hành động này không rõ ràng.

Nhìn vào một khung hình đơn giản là không đủ.

Nhưng với một loạt các khung hình theo thời gian, như video hoặc ảnh trực tiếp, ý nghĩa của hành động là rõ ràng.

Một lời "xin chào" thân thiện và "đến đây."

Với điều đó đã được làm sáng tỏ, tôi rất vui mừng được giới thiệu hai mẫu Tạo ML mới trong năm nay, Phân loại tư thế bằng tay và Phân loại hành động bằng tay.

Những mẫu mới này cho phép bạn đào tạo các mô hình Hand Pose và Action bằng ứng dụng Create ML hoặc khung Create ML.

Các mô hình này tương thích với macOS Big Sur trở lên, cũng như iOS và iPadOS 14 trở lên.

Và mới cho năm nay, chúng tôi đã thêm khả năng đào tạo các mô hình trên các thiết bị iOS bằng cách sử dụng khung Tạo ML.

Bạn có thể tìm hiểu thêm về điều này trong phiên "Xây dựng ứng dụng iOS động với Tạo khung ML".

Đầu tiên, tôi muốn nói về Phân loại tư thế tay, cho phép bạn dễ dàng đào tạo các mô hình học máy để phân loại các vị trí tay được phát hiện bởi Khung tầm nhìn.

Vì bạn chịu trách nhiệm đào tạo mô hình, bạn xác định tư thế nào mà ứng dụng của bạn nên phân loại để phù hợp nhất với nhu cầu của nó.

Hãy để tôi cung cấp cho bạn một bản demo ngắn gọn về một mô hình được đào tạo đang hoạt động.

Bắt đầu từ một ứng dụng nguyên mẫu đơn giản, tôi có thể dễ dàng tích hợp mô hình Hand Pose Classifier.

Ứng dụng của tôi bây giờ có thể phân loại các tư thế tay và hiển thị biểu tượng cảm xúc tương ứng và độ tin cậy của tư thế được phân loại.

Nó phân loại tư thế bàn tay Một cũng như Hai.

Nhưng bạn sẽ nhận thấy rằng tất cả các vị trí mà nó không nhận ra đều được phân loại là một phần của nền.

Điều này bao gồm tư thế Open Palm, mà tôi muốn thêm hỗ trợ.

Tôi sẽ trao mô hình này cho đồng nghiệp Brittany của tôi trong giây lát, để chỉ cho bạn cách tích hợp mô hình Hand Pose Classifier vào ứng dụng của bạn.

Trước khi tôi làm vậy, tôi muốn thêm hỗ trợ cho tư thế Open Palm.

Nó sẽ thực sự dễ dàng, nhưng chúng ta nên nói về cách một người mẫu được đào tạo trước.

Cũng giống như tất cả các dự án Tạo ML khác, việc tích hợp Hand Pose Classifier vào ứng dụng của bạn rất đơn giản.

Quá trình này có ba bước.

Thu thập và phân loại dữ liệu đào tạo, đào tạo mô hình, tích hợp mô hình vào ứng dụng của bạn.

Hãy bắt đầu bằng cách nói về việc thu thập dữ liệu đào tạo.

Đối với Hand Pose Classifier, bạn sẽ cần hình ảnh.

Hãy nhớ rằng, các tư thế hoàn toàn biểu cảm như hình ảnh.

Những hình ảnh này nên được phân loại thành các thư mục có tên khớp với tư thế có trong hình ảnh.

Ở đây chúng tôi có hai tư thế mà chúng tôi muốn xác định: Một, Hai, cũng như một lớp Nền.

Lớp Nền là một danh mục bắt tất cả cho các tư thế mà ứng dụng của bạn không quan tâm đến việc xác định chính xác.

Trong bản demo của tôi, điều này bao gồm nhiều vị trí tay không phải là Một hoặc Hai.

Lớp Nền được xác định rõ ràng giúp ứng dụng của bạn biết khi nào người dùng của bạn không tạo ra một tư thế quan trọng.

Có hai loại hình ảnh tạo nên một lớp Nền.

Đầu tiên, chúng tôi có một loại tư thế tay ngẫu nhiên không phải là tư thế quan trọng mà bạn muốn ứng dụng của mình phân loại.

Những tư thế này nên bao gồm một tập hợp đa dạng các tông màu da, độ tuổi, giới tính và điều kiện ánh sáng.

Thứ hai, chúng tôi có một tập hợp các vị trí rất giống với các biểu thức mà bạn muốn ứng dụng của mình phân loại.

Những tư thế chuyển tiếp này thường xảy ra khi người dùng đang di chuyển bàn tay của họ về phía một trong những biểu hiện mà ứng dụng của bạn quan tâm.

Khi tôi giơ tay để tạo tư thế Open Palm, hãy chú ý rằng tôi chuyển qua một số vị trí tương tự nhưng không hoàn toàn giống như những gì tôi muốn ứng dụng của mình xem xét Open Palm.

Những tư thế này cũng xảy ra khi tôi hạ cánh tay của mình sau đó.

Điều này không phải là duy nhất đối với Open Palm.

Cùng một loại tư thế chuyển tiếp xảy ra khi tôi giơ cánh tay lên để tạo tư thế Hai, cũng như khi tôi hạ thấp nó xuống.

Tất cả các tư thế chuyển tiếp này nên được thêm vào lớp Nền, cùng với các tư thế ngẫu nhiên.

Sự kết hợp này cho phép người mẫu phân biệt chính xác các tư thế mà ứng dụng của bạn quan tâm và tất cả các tư thế nền khác.

Với dữ liệu đào tạo được thu thập và phân loại, bây giờ là lúc để đào tạo mô hình của chúng tôi bằng ứng dụng Tạo ML.

Vậy chúng ta hãy làm bẩn tay chúng ta.

Tôi sẽ bắt đầu với dự án Create ML hiện tại mà tôi đã sử dụng để đào tạo mô hình trong bản demo trước đây của mình.

Kết quả đào tạo có vẻ tốt, vì vậy tôi hy vọng mô hình này sẽ hoạt động khá tốt.

May mắn thay, ứng dụng Create ML cho phép bạn xem trước mô hình của mình trước khi tích hợp nó vào ứng dụng của mình.

Trên tab Xem trước, bạn sẽ thấy rằng, đối với Hand Pose Classifiers, chúng tôi đã thêm khả năng Xem trước Trực tiếp vào bản phát hành này.

Live Preview tận dụng lợi thế của camera FaceTime để hiển thị cho bạn các dự đoán trong thời gian thực.

Sử dụng Live Preview, chúng tôi có thể xác minh rằng mô hình này phân loại chính xác các tư thế Một và Hai.

Và tôi cũng muốn nó phân loại chính xác Open Palm, nhưng nó hiện đang phân loại tư thế đó như một phần của lớp Nền.

Trong Nguồn Dữ liệu mà tôi đã sử dụng để đào tạo mô hình này, lưu ý rằng nó không bao gồm lớp Open Palm, chỉ bao gồm các lớp cho Một, Hai và Nền.

Hãy đào tạo một mô hình mới hỗ trợ Open Palm ngay bây giờ.

Đầu tiên, tôi sẽ tạo một nguồn mô hình mới cho việc này.

Tôi có một bộ dữ liệu bao gồm một lớp Open Palm mà tôi muốn sử dụng cho khóa đào tạo này.

Tôi sẽ chọn tập dữ liệu này.

Nhảy vào nguồn dữ liệu mới này, chúng tôi thấy rằng bây giờ nó bao gồm một mục nhập cho Open Palm cũng như các lớp từ tập dữ liệu trước đó.

Quay lại nguồn mô hình, tôi muốn thêm một vài bổ sung để mở rộng dữ liệu đào tạo và làm cho mô hình của tôi mạnh mẽ hơn.

Thế là xong. Đã đến lúc lên tàu.

Trước khi khóa đào tạo có thể bắt đầu, Create ML cần thực hiện một số xử lý hình ảnh sơ bộ, cũng như trích xuất tính năng.

Chúng tôi đã nói với Create ML để đào tạo cho 80 lần lặp lại.

Đây là một điểm khởi đầu tốt, nhưng bạn có thể cần điều chỉnh con số đó dựa trên tập dữ liệu của mình.

Quá trình này sẽ mất một thời gian.

May mắn thay, tôi đã đào tạo một người mẫu.

Hãy để tôi lấy nó ngay bây giờ.

Xem trước trực tiếp cho thấy mô hình mới được đào tạo của chúng tôi hiện xác định chính xác tư thế Open Palm.

Và chỉ để chắc chắn, tôi sẽ xác minh rằng nó tiếp tục xác định tư thế Một và Hai.

Điều đó không dễ sao?

Tôi sẽ gửi mô hình này cho đồng nghiệp của tôi, Brittany, và cô ấy sẽ nói về việc tích hợp nó vào ứng dụng của mình.

Cảm ơn, Nathan, vì người mẫu.

Xin chào. Tôi là Brittany Weinert.

Và tôi là thành viên của nhóm Vision Framework.

Khi tôi lần đầu tiên biết về Phân loại Tư thế Tay, tôi ngay lập tức nghĩ rằng, tôi có thể sử dụng điều này để tạo ra các hiệu ứng đặc biệt bằng tay của mình.

Tôi biết rằng sử dụng CoreML để phân loại tư thế tay và Vision để phát hiện và theo dõi bàn tay sẽ là công nghệ hoàn hảo để sử dụng cùng nhau.

Hãy xem liệu chúng ta có thể tự cho mình siêu năng lực hay không.

Tôi đã tạo bản nháp đầu tiên của đường ống cho một bản demo có thể làm được điều đó.

Hãy xem lại nó.

Đầu tiên, chúng ta sẽ có một máy ảnh cung cấp một luồng khung hình và chúng ta sẽ sử dụng từng khung hình cho một yêu cầu Tầm nhìn để phát hiện vị trí và các điểm chính của bàn tay trong khung hình.

DetectHumanHandPoseRequest sẽ là yêu cầu mà chúng tôi đang sử dụng.

Nó sẽ trả về một HumanHandPoseObservation cho mỗi bàn tay mà nó tìm thấy trong khung.

Dữ liệu chúng tôi sẽ gửi đến mô hình Phân loại Tư thế Tay CoreML là MLMultiArray và một thuộc tính trên HumanHandPoseObservation được gọi là keypointsMultiArray.

Bộ phân loại tư thế tay của chúng tôi sau đó sẽ trả lại cho chúng tôi nhãn hành động tay ước tính hàng đầu với điểm tin cậy của nó, sau đó chúng tôi có thể sử dụng để xác định hành động trong ứng dụng.

Bây giờ chúng ta đã xem qua các chi tiết cấp cao của ứng dụng, hãy xem mã.

Hãy bắt đầu bằng cách xem cách sử dụng Vision để phát hiện bàn tay trong khung hình.

Đối với những gì chúng tôi muốn làm, chúng tôi chỉ cần một phiên bản của VNDetectHumanHandPoseRequest, và chúng tôi chỉ cần phát hiện một tay, vì vậy chúng tôi đặt maximumHandCount thành một.

Nếu bạn đặt Số tay tối đa và có nhiều tay hơn được chỉ định trong khung, thay vào đó, thuật toán sẽ phát hiện các kim trung tâm và nổi bật nhất trong khung.

Giá trị mặc định cho maximumHandCount là hai.

Chúng tôi khuyên bạn nên đặt bản sửa đổi ở đây, vì vậy bạn không ngạc nhiên khi cập nhật yêu cầu sau này.

Nhưng nếu bạn luôn muốn chọn tham gia thuật toán mới nhất được hỗ trợ bởi SDK mà bạn được liên kết, bạn không cần phải đặt nó.

Ngoài ra, như một lưu ý, chúng tôi sẽ thực hiện phát hiện trên mọi khung hình được ARSession truy xuất thông qua ARKit, nhưng đây chỉ là một cách để lấy khung hình từ nguồn cấp dữ liệu máy ảnh.

Bạn có thể sử dụng bất kỳ phương pháp nào bạn thích.

AVCaptureOutput cũng sẽ là một giải pháp thay thế hữu ích.

Đối với mỗi khung hình nhận được, chúng ta cần tạo một VNImageRequestHandler, xử lý tất cả các yêu cầu trên một hình ảnh nhất định.

Thuộc tính kết quả trên yêu cầu tư thế tay sẽ được điền với VNHumanHandPoseObservations, tối đa số tay là một, như chúng tôi đã chỉ định trên yêu cầu trước đó.

Nếu yêu cầu phát hiện không có tư thế tay, chúng tôi có thể muốn xóa bất kỳ hiệu ứng nào hiện đang được hiển thị.

Nếu không, chúng ta sẽ có một quan sát bằng một tay.

Tiếp theo, chúng tôi muốn dự đoán tư thế tay của chúng tôi đang sử dụng mô hình CoreML của chúng tôi.

Chúng tôi không muốn đưa ra dự đoán từng khung hình, vì chúng tôi không muốn việc hiển thị hiệu ứng bị rung chuyển.

Thực hiện dự đoán theo các khoảng thời gian sẽ tạo ra trải nghiệm người dùng mượt mà hơn.

Khi chúng tôi muốn đưa ra dự đoán, chúng tôi bắt đầu bằng cách chuyển MLMultiArray sang mô hình Hand Pose CoreML và chúng tôi lấy lại nhãn hàng đầu và sự tự tin từ dự đoán duy nhất được trả về.

Tôi muốn kích hoạt các thay đổi đối với các hiệu ứng chỉ được hiển thị khi nhãn được dự đoán với mức độ tin cậy cao.

Đây cũng là chìa khóa để bảo vệ chống lại hành vi trong đó hiệu ứng có thể bật và tắt quá nhanh và trở nên bồn chồn.

Ở đây, phân loại nền tảng đang giúp chúng tôi bằng cách cho phép chúng tôi giữ ngưỡng tin cậy rất cao.

Nếu One được dự đoán với sự tự tin cao, chúng ta có thể đặt effectNode để hiển thị.

Nếu One không được dự đoán với sự tự tin cao, tôi muốn dừng hiệu ứng trên màn hình để phù hợp với những gì tay tôi đang làm.

Hãy kiểm tra xem chúng ta có gì.

Nếu tôi thực hiện tư thế Một, nó sẽ kích hoạt một hiệu ứng chùm năng lượng duy nhất.

Rất tuyệt!

Người mẫu có thể nói rằng tôi đã tạo ra tư thế Một và kích hoạt hiệu ứng.

Mặc dù nó sẽ còn tuyệt hơn nếu nó đi theo ngón tay của tôi.

Thậm chí tốt hơn nếu nó hiển thị tại một điểm cụ thể trên ngón tay của tôi.

Hãy quay lại mã và thay đổi nó.

Những gì chúng ta cần làm là đưa vị trí điểm chính của bàn tay vào nội dung đồ họa, có nghĩa là sử dụng chế độ xem để dịch các điểm chính chuẩn hóa vào không gian chế độ xem của máy ảnh.

Bạn cũng có thể cân nhắc việc cắt tỉa những điểm chính mà bạn tiết kiệm được bằng cách nhìn vào điểm tin cậy.

Ở đây, tôi chỉ quan tâm đến đầu ngón trỏ.

Chúng ta cần dịch điểm chính sang không gian tọa độ, vì Vision sử dụng tọa độ chuẩn hóa.

Ngoài ra, điểm gốc của Vision nằm ở góc dưới bên trái của hình ảnh, vì vậy hãy ghi nhớ điều đó khi bạn thực hiện chuyển đổi.

Cuối cùng, hãy lưu vị trí chỉ mục và nếu không tìm thấy điểm chính nào, chúng tôi mặc định là số không.

Hãy xem mã chịu trách nhiệm hiển thị hiệu ứng và cách tôi có thể điều chỉnh nó theo ngón tay của mình.

Chúng tôi muốn tìm vị trí mà vị trí của đối tượng đồ họa đang được thiết lập.

setLocationForEffects đang được gọi không đồng bộ là mọi khung hình.

Theo mặc định, chúng tôi đặt hiệu ứng xuất hiện ở trung tâm của chế độ xem.

Chuyển nó sang indexFingerTipLocation CGPoint từ trước đó, chúng ta có thể có được hiệu quả như mong muốn.

Tuyệt vời!

Điều này đang bắt đầu trông thật ngầu.

Hãy thực hiện thêm một bước nữa.

Để tạo ra một câu chuyện đồ họa thú vị hơn xung quanh các siêu năng lực, sẽ rất tốt nếu sử dụng thêm một vài Phân loại Tư thế Tay trong ứng dụng của chúng tôi.

Trong trường hợp này, chúng tôi sẽ chọn phân loại Two và Open Palm.

Tôi đã mở rộng ứng dụng của mình để thực hiện hành động khi cả hai tư thế này được phát hiện.

Ở đây, tôi đang căn giữa chùm năng lượng xuất hiện ở đầu ngón trỏ của mình, như được hiển thị trước đây, cho tư thế Một.

Hai chùm năng lượng ở đầu ngón giữa và ngón trỏ của tôi cho tư thế Hai.

Và chùm năng lượng cuối cùng được kích hoạt bởi Hand Pose Open Palm và được neo giữa một điểm chính ở dưới cùng của ngón giữa của tôi và điểm chính cổ tay.

Được rồi.

Mọi thứ Nathan và tôi đã giới thiệu bao gồm các bước tích hợp đầy đủ mô hình Phân loại Tư thế Tay của riêng bạn.

Có một tính năng mới nữa trong Vision mà bạn có thể thấy hữu ích, vì vậy hãy để tôi giới thiệu cho bạn một API có thể giúp kích hoạt và kiểm soát chức năng của ứng dụng này.

Vision đang giới thiệu một thuộc tính mới cho phép người dùng phân biệt giữa tay trái và tay phải trên HumanHand- PoseObservation: chirality.

Đây là một Enum chỉ ra bàn tay nào mà HumanHandPoseObservation rất có thể là và có thể là một trong ba giá trị: tay trái, tay phải và không xác định.

Bạn có thể đoán được ý nghĩa đằng sau hai giá trị đầu tiên, nhưng giá trị chưa biết sẽ chỉ xuất hiện nếu phiên bản cũ hơn của HumanHandPoseObservation bị hủy nối tiếp và thuộc tính chưa bao giờ được đặt.

Như Nathan đã đề cập trước đó, bạn có thể nhận thêm thông tin về phát hiện tư thế tay Vision bằng cách tham khảo lại phiên WWDC 2020, "Phát hiện tư thế cơ thể và tay với Vision."

Như một lưu ý phụ, đối với mỗi bàn tay được phát hiện trong một khung, thuật toán cơ bản sẽ cố gắng dự đoán chirality của từng bàn tay riêng biệt.

Điều này có nghĩa là dự đoán của một tay không ảnh hưởng đến dự đoán của các tay khác trong khung hình.

Hãy để tôi chỉ cho bạn một ví dụ về mã sử dụng chirality có thể trông như thế nào.

Chúng tôi đã đề cập đến việc thiết lập để tạo và chạy một VNDetectHumanHandPoseRequest.

Sau khi thực hiện yêu cầu, các quan sát sẽ có tính chirality thuộc tính Enum và bạn có thể sử dụng nó để thực hiện hành động hoặc sắp xếp thông qua các quan sát tư thế tay Vision như vậy.

Mọi thứ cho đến nay đều là về cách sử dụng Phân loại Tư thế Tay.

Nhưng như Nathan đã đề cập trước đó, Phân loại Hành động Tay là một công nghệ mới khác trong năm nay.

Đây là Geppy để nói chuyện với bạn về nó.

Cảm ơn, Brittany.

Xin chào, tên tôi là Geppy Parziale, và tôi là một kỹ sư học máy từ nhóm Create ML.

Ngoài Phân loại Tư thế Tay, năm nay, Create ML giới thiệu một mẫu mới để thực hiện Phân loại Hành động Tay và tôi sẽ chỉ cho bạn cách sử dụng nó trong các ứng dụng của bạn.

Vì lý do này, tôi sẽ mở rộng bản demo siêu năng lực của Brittany với một số hành động tay và làm nổi bật một số khác biệt quan trọng giữa tư thế tay và hành động tay.

Vui lòng tham khảo phiên "Xây dựng Phân loại Hành động với Tạo ML" từ WWDC 2020 để biết thêm thông tin và so sánh vì Hành động Tay và Hành động Cơ thể là hai nhiệm vụ rất giống nhau.

Nhưng bây giờ, hãy để tôi giải thích Hand Action là gì.

Hành động tay bao gồm một chuỗi các tư thế tay mà mô hình ML cần phân tích trong quá trình chuyển động của bàn tay.

Số lượng tư thế trong một chuỗi phải đủ lớn để ghi lại toàn bộ hành động tay từ đầu đến cuối.

Bạn sử dụng video để ghi lại các hành động bằng tay.

Đào tạo Phân loại Hành động Tay giống hệt với đào tạo Phân loại Tư thế Tay, như Nathan đã chỉ cho chúng ta trước đó, với một số khác biệt nhỏ.

Trong khi một hình ảnh tĩnh đại diện cho tư thế tay, các video được sử dụng để chụp và đại diện cho các hành động tay.

Vì vậy, để đào tạo Trình phân loại hành động bằng tay, bạn sử dụng các video ngắn, trong đó mỗi video đại diện cho hành động bằng tay.

Những video này có thể được sắp xếp thành các thư mục, trong đó mỗi tên thư mục đại diện cho một lớp hành động.

Và hãy nhớ bao gồm một lớp Nền chứa các video có hành động không giống với hành động mà bạn muốn trình phân loại nhận ra.

Để đại diện thay thế, bạn có thể thêm tất cả các tệp video ví dụ của mình vào một thư mục duy nhất.

Sau đó, bạn thêm một tệp chú thích, sử dụng định dạng CSV hoặc JSON.

Mỗi mục nhập trong tệp chú thích đại diện cho tên của tệp video, lớp liên quan, thời gian bắt đầu và kết thúc của hành động tay.

Ngoài ra, trong trường hợp này, hãy nhớ bao gồm một lớp Nền.

Hãy nhớ rằng, bạn đào tạo người mẫu với các video có cùng độ dài, ít nhiều.

Thật vậy, bạn cung cấp thời lượng hành động như một tham số đào tạo và sau đó Tạo ML lấy mẫu ngẫu nhiên một số khung hình liên tiếp theo giá trị bạn cung cấp.

Bạn cũng có thể cung cấp tốc độ khung hình video và các lần lặp lại đào tạo.

Thêm vào đó, ứng dụng cung cấp các loại tăng cường dữ liệu khác nhau sẽ giúp mô hình khái quát hóa tốt hơn và tăng độ chính xác của nó.

Đặc biệt, Nội suy thời gian và Giảm khung hình là hai phần tăng cường được thêm vào Phân loại hành động bằng tay để cung cấp biến thể video gần hơn với các trường hợp sử dụng thực tế.

Vì vậy, tôi đã đào tạo một Hand Action Classifier cho bản demo của mình - hãy xem nó hoạt động.

Chà, vì tôi là một siêu anh hùng, tôi cần một nguồn năng lượng.

Đây là của tôi.

Ở đây, tôi sử dụng tư thế tay để hình dung nguồn năng lượng của mình.

Nhưng bây giờ, hãy để tôi sử dụng siêu năng lực của mình để kích hoạt nó.

Trong trường hợp này, tôi đang sử dụng hành động tay.

Điều này thật tuyệt.

Và bây giờ, Hand Pose và Hand Action Classifier đang thực hiện đồng thời.

Tôi đang tận dụng tính năng chirality mới từ Vision và sử dụng tay trái để tạo dáng tay và tay phải để thực hiện hành động tay.

Điều này thật tuyệt.

Vì vậy, điều này có thể thực hiện được nhờ sự tối ưu hóa mà Create ML áp dụng, thời gian đào tạo cho mọi mô hình để giải phóng tất cả sức mạnh của Apple Neural Engine.

Và bây giờ, hãy để tôi quay trở lại thế giới thực của mình và giải thích cho bạn cách tích hợp Create ML Hand Action Classifier vào bản demo của tôi.

Trước tiên hãy xem xét đầu vào của mô hình.

Khi bạn tích hợp Hand Action Classifier vào ứng dụng của mình, bạn cần đảm bảo cung cấp cho mô hình số lượng tư thế hand dự kiến chính xác.

Mô hình của tôi đang mong đợi một MultiArray có kích thước 45 x 3 x 21, vì tôi có thể kiểm tra trong Bản xem trước XCode.

Ở đây, 45 là số lượng tư thế mà người phân loại cần phân tích để nhận ra hành động.

21 là số lượng khớp được cung cấp bởi Vision Framework cho mỗi bàn tay.

Cuối cùng, 3 là tọa độ x và y và giá trị tin cậy cho mỗi khớp.

45 đến từ đâu?

Đó là kích thước cửa sổ dự đoán và phụ thuộc vào độ dài video và tốc độ khung hình của các video được sử dụng tại thời điểm đào tạo.

Trong trường hợp của tôi, tôi quyết định đào tạo mô hình của mình với các video được quay ở tốc độ 30 khung hình / giây và dài 1,5 giây.

Điều này có nghĩa là người mẫu đã được đào tạo với 45 khung hình video cho mỗi hành động tay, vì vậy trong quá trình suy luận, người mẫu đang mong đợi cùng một số lượng tư thế tay.

Một sự cân nhắc bổ sung cần được tính đến liên quan đến tần suất các tư thế tay đến tại thời điểm suy luận.

Điều rất quan trọng là tỷ lệ của các tư thế tay được trình bày cho người mẫu trong quá trình suy luận phải khớp với tỷ lệ của các tư thế được sử dụng để đào tạo người mẫu.

Trong bản demo của tôi, tôi đã sử dụng ARKit.

Vì vậy, tôi đã phải giảm một nửa số lượng tư thế đến mỗi giây, vì ARKit cung cấp khung hình ở tốc độ 60 khung hình / giây và trình phân loại của tôi được đào tạo với video ở tốc độ 30 khung hình / giây.

Nếu làm khác đi, trình phân loại có thể đưa ra những dự đoán sai.

Bây giờ chúng ta hãy nhảy vào mã nguồn để chỉ cho bạn cách triển khai điều này.

Đầu tiên, tôi sử dụng bộ đếm để giảm tốc độ của các tư thế đến từ Vision từ 60 khung hình / giây xuống 30 khung hình / giây, phù hợp để tốc độ khung hình mà mô hình của tôi mong đợi hoạt động bình thường.

Sau đó, tôi nhận được mảng chứa các khớp và chirality cho mỗi bàn tay trong cảnh.

Tiếp theo, tôi loại bỏ các điểm chính khỏi tay trái của mình vì, trong bản demo của mình, tôi sử dụng tay phải để kích hoạt một số hiệu ứng bằng hành động tay.

Được rồi, bây giờ tôi cần tích lũy các tư thế tay cho bộ phân loại.

Để làm như vậy, tôi sử dụng hàng đợi FIFO và tích lũy 45 tư thế tay và đảm bảo hàng đợi luôn chứa 45 tư thế cuối cùng.

Hàng đợi ban đầu trống rỗng.

Khi một tư thế tay mới đến, tôi thêm nó vào hàng đợi và tôi lặp lại bước này cho đến khi hàng đợi đầy.

Khi hàng đợi đầy, tôi có thể bắt đầu đọc toàn bộ nội dung của nó.

Tôi có thể đọc hàng đợi mỗi khi tôi nhận được tư thế tay mới từ Vision.

Nhưng hãy nhớ rằng, bây giờ tôi đang xử lý 30 khung hình mỗi giây.

Và tùy thuộc vào trường hợp sử dụng, điều này có thể là một sự lãng phí tài nguyên.

Vì vậy, tôi sử dụng một bộ đếm khác để đọc hàng đợi sau khi tôi xác định số lượng khung hình.

Bạn nên chọn tỷ lệ lấy mẫu hàng đợi như một sự đánh đổi giữa khả năng phản hồi của ứng dụng và số lượng dự đoán mỗi giây bạn muốn có được.

Tại thời điểm này, tôi đã đọc toàn bộ chuỗi 45 tư thế tay, được sắp xếp trong MLMultiArray và nhập nó vào một bộ phân loại để dự đoán hành động của tay.

Sau đó, tôi trích xuất nhãn dự đoán và giá trị tin cậy.

Cuối cùng, nếu giá trị tin cậy lớn hơn ngưỡng xác định của chúng tôi, tôi sẽ thêm các hiệu ứng hạt vào cảnh.

Vì vậy, hãy nhớ rằng, khi bạn tích hợp Create ML Hand Action Classifier trong ứng dụng của mình, hãy đảm bảo nhập chuỗi các tư thế tay ở tốc độ khung hình mà người mẫu đang mong đợi.

Khớp với cùng tốc độ khung hình của video được sử dụng để đào tạo trình phân loại.

Sử dụng hàng đợi vào trước ra trước để thu thập các tư thế tay cho dự đoán mô hình.

Đọc hàng đợi ở tốc độ khung hình phù hợp.

Tôi rất mong được xem tất cả các ứng dụng thú vị mà bạn sẽ xây dựng với các mô hình hành động tay được đào tạo với Create ML.

Và bây giờ, quay lại với Nathan để xem xét và tóm tắt lần cuối.

Cảm ơn, Geppy.

Bạn và Brittany đã làm rất tốt trên ứng dụng của bạn.

Tôi rất hào hứng để thử nó.

Nhưng trước khi mọi thứ vượt khỏi tầm kiểm soát, đây là một vài điều bạn nên ghi nhớ để đảm bảo trải nghiệm chất lượng cao cho người dùng của mình.

Hãy chú ý bàn tay cách máy ảnh bao xa.

Khoảng cách nên được giữ dưới 11 feet, hoặc 3 1/2 mét, để có kết quả tốt nhất.

Tốt nhất là tránh các điều kiện ánh sáng khắc nghiệt, quá tối hoặc quá sáng.

Găng tay cồng kềnh, lỏng lẻo hoặc nhiều màu sắc có thể gây khó khăn cho việc phát hiện chính xác tư thế tay, điều này có thể ảnh hưởng đến chất lượng phân loại.

Giống như tất cả các nhiệm vụ học máy, chất lượng và số lượng dữ liệu đào tạo của bạn là chìa khóa.

Đối với Trình phân loại tư thế bằng tay được hiển thị trong phiên này, chúng tôi đã sử dụng 500 hình ảnh cho mỗi lớp.

Hand Action Classifier, chúng tôi đã sử dụng 100 video cho mỗi lớp, nhưng các yêu cầu dữ liệu cho trường hợp sử dụng của bạn có thể khác nhau.

Điều quan trọng nhất là bạn thu thập đủ dữ liệu đào tạo để nắm bắt biến thể dự kiến mà mô hình của bạn sẽ thấy trong ứng dụng của bạn.

Bây giờ cảm thấy như là thời điểm tốt để tóm tắt lại.

Vậy chúng ta đã học được gì?

Chà, bắt đầu từ năm 2021, bạn có thể xây dựng các ứng dụng diễn giải biểu cảm của bàn tay con người.

Chúng tôi đã thảo luận về sự khác biệt giữa hai loại biểu cảm tay, tư thế và hành động.

Chúng tôi đã nói về cách chuẩn bị dữ liệu đào tạo, bao gồm cả lớp Nền, để sử dụng trong ứng dụng Tạo ML để đào tạo một mô hình.

Chúng tôi đã nói về cách tích hợp một mô hình được đào tạo vào một ứng dụng.

Và cuối cùng, chúng tôi đã nói về việc kết hợp nhiều mô hình vào một ứng dụng duy nhất và sử dụng chirality để phân biệt bàn tay.

Rõ ràng, bản demo hôm nay chỉ làm trầy xước bề mặt.

Khung Tầm nhìn là một công nghệ mạnh mẽ để phát hiện sự hiện diện của bàn tay, tư thế, vị trí và tính chirality.

Tạo ML là một cách thú vị và dễ dàng để rèn luyện và phân loại các tư thế tay và các hành động tay.

Khi được sử dụng cùng nhau, chúng cung cấp những hiểu biết sâu sắc về một trong những công cụ mạnh mẽ và biểu cảm nhất của loài người, và chúng tôi nóng lòng muốn xem bạn làm gì với chúng.

Tạm biệt.

[Nhạc lạc quan].