10247

♪ ♪

こんにちは、私の名前はロイです。

私はカメラソフトウェアチームのエンジニアです。

今日は、最も人気のあるビデオフォーマットで行ったエキサイティングな写真品質の改善と、アプリケーションがそれらを利用してさらに優れた体験を提供する方法について説明します。

iPhoneは世界で最も人気のあるカメラであり、長年にわたり、開発者は強力なカメラシステムを活用して、プロの写真アプリからビデオストリーミングツールまで、多様な世界クラスの体験を提供してきました。

異なるシナリオでは、異なるレベルの写真品質が必要です。

たとえば、静止画の撮影専用のアプリは、カメラが提供できる絶対的な最高の品質を要求します。

一方、ソーシャルアプリは、ストリーミングされるビデオフレームの上にフェイスエフェクトオーバーレイを適用する必要があるかもしれません。

そして、このカスタムレンダリングは計算コストがかかるかもしれません。

フレームのドロップを避けるために、開発者は低解像度のフレームを好むかもしれないので、フレームごとに処理するピクセルが少なくなります。

ユースケースのこの多様性は、品質とパフォーマンスの規模で着地したい場所を指定する簡単な方法を必要とします。

しかし、写真の品質に飛び込む前に、一般的にiOSで写真がどのように撮影されるかについて簡単に復習しましょう。

AVCaptureSessionオブジェクトから始めて、その周りにオブジェクトグラフを作成できます。

写真を撮っているので、AVCaptureDeviceとしてカメラを使用します。

次に、AVCaptureDeviceInputがそのデバイスに基づいてインスタンス化され、セッションに入力データを提供します。

その後、AVCapturePhotoOutputが写真の受信者としてグラフに追加されます。

そして、これらの要素はすべて、AVCaptureConnectionを使用して一緒に接続されます。

セッションの実行が開始されたら、AVCapturePhotoOutputインスタンスでcapturePhotoメソッドを呼び出すことで写真をキャプチャできます。

さらなるカスタマイズは、capturePhotoメソッドに渡されたAVCapturePhotoSettingsオブジェクトを使用して行うことができます。

キャプチャされた写真は、デリゲートメソッドで受け取るAVCapturePhotoオブジェクトとして表されます。

2016年のセッション「iOS写真の進歩」で、これらのAPIについて非常に詳細な議論を行いました。

まだチェックしていない場合は、チェックしてください。

iOSで一般的に写真を撮る方法がわかったので、高品質の写真がどのように撮れるか見てみましょう。

歴史的に、可能な限り最高の品質の写真をキャプチャしたい場合は、AVCapturePhotoSettingsのisAutoStillImageStabilization Enabledプロパティをtrueに設定します。これは、静止画像の安定化が高品質の写真を得るための主な方法だったためです。

しかし、長年にわたり、私たちは写真の品質向上アルゴリズムを継続的に進化させてきました。

静止画安定化に加えて、スマートHDRやディープフュージョンなど、さまざまなマルチ画像融合技術など、はるかに豊富な技術が手ぶれています。

その結果、AutoStillImageStabilization Enabledという名前は、高画質のプロキシとしてかなり時代遅れになっています。

この問題を解決するために、iOS 13ではAVCapturePhotoOutput.QualityPrio ritizationを導入しました。

写真撮影の品質に優先順位を付ける方法をAVCapturePhotoOutputに伝えるのはとても簡単な方法です。

以前のWWDCでは、この重要なAPIについて話す機会がなかったので、少し時間を取って、それがどのように機能するかを見てみましょう。

速度、バランス、品質の3つの品質優先順位付けレベルから選択できます。

スピードでは、写真の品質を犠牲にしても、キャプチャの速度が最も気にかけていることをフレームワークに伝えています。

写真の品質と配信速度のバランスを取る必要がある場合は、バランスを取る必要があります。

品質はスピードとは正反対です。

キャプチャプロセスの潜在的な遅さは許容されるが、画質は何よりも優先されるべきであると述べている。

指定された品質の優先順位付けは、AVCapturePhotoOutputのヒントとしてのみ機能し、使用するアルゴリズムを指示しないことに注意してください。

最終的に、AVCapturePhotoOutputはさまざまな制約を考慮し、現在のシーンに最も適したアルゴリズムを選択します。

たとえば、薄暗い状況では、明るい空間とは異なる方法を選択するかもしれません。

そうは言っても、異なるキャプチャ期間に基づいて、ユーザーエクスペリエンスを異なる方法で計画したいと思うかもしれないことを理解しています。

したがって、AVCapturePhotoOutputDelegateメソッドの一部に渡されたAVCaptureResolvedPhotoSettingsオブジェクトでは、写真をデリゲートに配信するのにかかる時間を示すphotoProcessingTimeRangeというプロパティを提供します。

これは、たとえば、キャプチャに時間がかかる場合、スピナーを出すかどうかを判断するのに役立ちます。

コードでどのように機能するか見てみましょう。

AVCapturePhotoOutputを設定するときは、特定のキャプチャセッションで必要とされる最大品質の優先順位付けを指定できます。

そうしないことを選択した場合、デフォルト値はバランスが取れています。

これは一度だけ設定する必要があります。

そうすることの重要性は、異なる設定に応じて、キャプチャパイプラインを異なる方法で設定することです。

たとえば、速度の優先順位付けを超えないことがわかっている場合は、バランスの取れた優先順位付けよりもはるかに少ないメモリと電力を消費するキャプチャパイプラインを構築できます。

そのため、責任を持って選択し、必要なものだけを取ることをお勧めします。

capturePhotoメソッドを呼び出す前に、AVCapturePhotoSettingsオブジェクトにphotoQualityPrioritizationプロパティを設定することで、この特定のキャプチャの品質優先順位付けをカスタマイズできます。

デフォルト値はバランスが取れています。

ここで示されているように、私たちは2つの異なる状況で2つの異なるレベルを使用しています。

キャプチャごとの品質の優先順位付けは、AVCapturePhotoOutputの最大品質の優先順位付けを超えることはできません。そうしないと、例外が発生します。

3つのレベルのパフォーマンス特性は、使用する基礎となるアルゴリズムによって決定されます。

マッピングは使用するフォーマットの種類によって異なり、写真とビデオフォーマットの違いについて少し説明します。

写真フォーマットでは、スピードはWYSIWYG写真を取得します--それはあなたが見るものはあなたが得るものです写真です--これは、いくつかのノイズリダクションが適用された場合にのみ軽く処理されます。

Balancedが指定されている場合は、やや遅いキャプチャレートでWYSIWYG写真よりもはるかに優れた写真品質を生成する高速融合アルゴリズムのコレクションから選択します。

品質については、現在のデバイスとルクスレベルに応じて、フレームワークは可能な限り最高の写真品質を提供するために、ディープフュージョンなどのいくつかの重い機械を使用します。

写真は素晴らしく見えますが、無料のランチはありません。

あなたはより多くの時間を使ってそれを支払います。

一方、ビデオフォーマットの場合、すべてのレベルは、可能な限り迅速に写真を配信するために最も軽い処理を使用します。

私たちはしばらくの間、写真やビデオのフォーマットについて話してきました。

それらの違いを詳しく見てみましょう。

写真フォーマットを使用することで、静止画を撮ることが最も気にかけていることがフレームワークに合図されます。

たとえば、AVCaptureVideoDataOutputを写真形式で使用している場合、デフォルトで取得するサンプルバッファはプレビュー解像度のみになります。これは、写真を撮ることが最優先事項であることを知っているため、これらのフレームはビデオ録画ではなくプレビューに使用されると仮定できます。

写真フォーマットを選択する正当な理由は、いくつかの写真中心の機能が、Live PhotoやProRAWなどの写真フォーマット専用であることです。

それがあなたがやりたいことなら、写真フォーマットが進むべき道です。

写真フォーマットには利用可能な最高の解像度が付属していますが、フレームレートは毎秒30フレームに制限されています。

写真のフォーマットを選択するには、セッションのプリセットを写真に設定できます。

または、isHighestPhotoQualitySupportedがtrueであるフォーマットを選択することもできます。

一方、ビデオフォーマットの使用は、経験がビデオを中心にすることを示しています。

録画やストリーミングに適した解像度を取得し、60fpsなどの高いフレームレートを使用できるようになります。

フォーマットが写真フォーマットでない場合、それはビデオフォーマットと見なされます。

したがって、写真以外のセッションプリセットを使用するか、isHighestPhotoQualitySupportedがfalseであるフォーマットを選択することで、1つを選択できます。

なぜ強力なアルゴリズムのいくつかをビデオフォーマットに適用していないのか疑問に思うかもしれません。

それは私たちが怠け者だからではありません。

それには正当な理由があります。

多くのアプリは、重いカスタム処理を行う必要があるため、ビデオフォーマットを使用することを選択し、ビデオフォーマットはオーバーヘッドが低いため、この目的に適しています。

前述の写真強化技術のいくつかを活用すれば、これらのアプリの体験に劣化をもたらす可能性があります。

たとえば、ARアプリを使用すると、ユーザーが操作している3Dシーンの写真を撮ることができます。

この時点で既存のフュージョンアルゴリズムを実行すると、アプリのカメラフィードにフレームドロップが導入され、コア機能が中断される可能性があります。

そのため、品質とスピードの微妙なバランスを非常に意識しており、最も厳しい条件下でも反応して動作するようにビデオフォーマットを設計しました。

しかし、これらの妥協はiOS 15で今日停止します。

私たちは、最も人気のあるビデオフォーマットで写真品質に大きな飛躍を遂げています。

いくつかの改良されたアルゴリズムにより、アプリのエクスペリエンスの他の側面に影響を与えることなく、写真の品質を根本的に改善できるようになりました。

この新機能により、アプリは洗練されたカスタム計算を実行するための同じ柔軟性を維持しながら、素晴らしい写真を撮ることができます。

では、ここでどれだけ大きな品質の飛躍について話しているのでしょうか？

前後の比較を見てみましょう。 比較後の比較を見てみましょう。

改善はかなり大きいです。

右側の小さな男の子の顔は騒音がはるかに少ないので、はるかに自然に見えます。

そして、私たちは彼の髪から出てくる光をよりよく知覚することができます。

被写体の目のキャッチライトは、単により鮮やかで活発です。

この屋外の低照度の状況では、彼女の顔と服に優れたノイズ除去があります。

最後に、環境も良く見えます。

椅子の革の質感ははるかによく保存されています。

魅了されたので、サポートされているビデオフォーマットのアルゴリズムマッピングを見てみましょう。

スピードは、まだあなたに軽く処理されたWYSIWYG写真を取得します。

彼らはまだ写真を取得する最速の方法であり、スピードが今あなたの最優先事項であるため、これは法案に完全に適合するので、私たちはそれを変更しませんでした。

ビデオ録画のフレームドロップやプレビューフィードの中断は得られないでしょう。

しかし、Balancedを使用すると、品質が大幅に向上しますが、写真の処理時間がわずかに増加するだけです。

そして、スピードと同じように、ビデオ録画にはフレームドロップはありません。

それらの見栄えの良い写真が撮影され、処理されても、プレビューフィードは中断されません。

最後に、品質のために、私たちはさらに優れた品質を得るために、より高価なアルゴリズムを実行しています。

これにより、デバイスの最新さに応じて、フレームがドロップされたり、プレビューフィードが中断されたりする可能性があります。

この機能は、iPhone XSまでサポートされているすべてのiPhoneで利用できます。

このアップグレードを受けているビデオフォーマットは、最も人気のあるものです:毎秒30フレームと60フレームの両方をサポートする1280x720。

1920x1080、また30fpsと60fpsの両方。

1920x1440、30fps。

また、30fpsの4Kのサポートも追加しました。

では、コードで正しいフォーマットを使用していることを確認するにはどうすればよいですか?

それはとても簡単です。

iOS 15では、AVCaptureDevice.FormatタイプにisHighPhotoQualitySupportedという新しいプロパティを導入しています。

この機能をサポートするフォーマットでは、このプロパティはtrueになります。

このプロパティが真である任意のフォーマットは、ビデオフォーマットであることが保証されているので、誤って写真のフォーマットを選ぶことを心配する必要はありません。

そのようなフォーマットを手に入れたいとしましょう。

AVCaptureDeviceインスタンスで利用可能なフォーマットを入手するだけです。

次に、isHighPhotoQualitySupportedがtrueであるものを選択してください。

この機能を使用するようにサンプルコードAVCamを更新しました。

実用的な例を見たい場合は、それをチェックしてください。

新しいプロパティisHighPhotoQualitySupportedと既存のisHighestPhotoQualitySupportedを混同する可能性があります。

先に述べたように、後者はフォーマットが写真フォーマットであるかどうかを教えてくれ、ビデオフォーマットが高写真品質をサポートしているかどうかは教えてくれません。

さて、この新機能を取得するには、何か作業を行う必要がありますか?

答えは多分です。

すでにAVCapturePhotoOutputと.balanced優先順位付けを使用している場合は、おめでとうございます。iOS 15で自動的に見栄えの良い写真を取得します。

アプリが速度の優先順位付けを使用している場合は、バランスに更新するだけで、フレームのドロップを心配することなく、より良い写真を受け取ることができます。

非推奨のAVCaptureStillImageOutputをまだ使用している場合は、これが切り替える大きなインセンティブを与えることを願っています。

現在、品質の優先順位付けを使用すると、ビデオにフレームドロップが発生する可能性があるため、オプトインせずにアプリに新しい動作を課したくありません。

そのため、リンク時間チェックを入れて、アプリがビデオ形式で品質の優先順位付けを使用していて、iOS 15より前にコンパイルされている場合は、自動的にバランスの取れたものに変更します。

本当に最高の品質を手に入れたい場合は、iOS 15 SDKでアプリを再コンパイルするだけです。

注意すべき点がいくつかあります。注意点があります。

この機能は現在、AVCaptureSessionでのみ動作し、AVCaptureMultiCamSessionでは動作しません。

非推奨のAVCaptureStillImageOutputは、この機能をサポートしません。

.Balancedまたは.quality優先順位付けを使用している場合、使用するアルゴリズムのいくつかは、ダイナミックレンジを改善するために、いくつかの異なる露出画像を融合させる可能性があります。

写真は素晴らしい品質ですが、同時に録画されるビデオとは違って見えるかもしれません。

ビデオと写真がまったく同じに見えるようにする必要がある場合は、代わりに.speedを使用してください。

最後に、今取り上げたことをまとめましょう。

アプリのエクスペリエンスを設計するときは、品質とスピードのどちらかを選択する決定に注意してください。

ユースケースで写真の品質が果たす役割を理解したら、それを達成するために適切な優先順位付けレベルを使用してください。

そして、最小限の作業で、時にはまったく作業なしで、あなたは今、ビデオフォーマットで素晴らしい写真を得るでしょう。

どうもありがとうございます。

[パーカッシブミュージック]。