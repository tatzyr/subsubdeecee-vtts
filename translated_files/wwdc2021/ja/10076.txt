10076

♪ベース音楽の演奏♪

♪

マイケル・パトリック・ジョンソン:こんにちは!

私の名前はマイケル・パトリック・ジョンソンで、オブジェクトキャプチャチームのエンジニアです。

今日、同僚のデイブ・マッキノンと私は、macOSの新しいフォトグラメトリAPIを使用して、現実世界のオブジェクトを3Dモデルに変える方法を紹介します。

ARKitとRealityKitフレームワークを使用して拡張現実アプリを作成することにすでに精通しているかもしれません。

また、Reality ComposerとReality Converterを使用して、AR用の3Dモデルを作成した可能性があります。

そして今、Object Capture APIを使用すると、現実世界のオブジェクトの画像を詳細な3Dモデルに簡単に切り替えることができます。

キッチンテーブルの前に焼きたてのピザがあるとしましょう。

美味しそうですよね？

前景のピザを3Dモデルとしてキャプチャしたいとします。

通常、形状と質感をモデル化するには、プロのアーティストを何時間も雇う必要があります。

でも、待って、自分のオーブンで焼くのに数分しかかからなかった！

オブジェクトキャプチャでは、あらゆる角度からオブジェクトの写真を撮ることから始めます。

次に、新しいオブジェクトキャプチャAPIをサポートするMacに画像をコピーします。

「フォトグラメトリ」と呼ばれるコンピュータビジョン技術を使用して、2D画像のスタックはわずか数分で3Dモデルに変換されます。

出力モデルには、幾何学的なメッシュとさまざまなマテリアルマップの両方が含まれており、アプリに直接ドロップするか、ARクイックルックで表示する準備ができています。

では、これらの各ステップをもう少し詳しく見てみましょう。

まず、あらゆる側面から物体の写真を撮ります。

画像は、iPhoneやiPad、デジタル一眼レフ、さらにはドローンでも撮影できます。

オブジェクトの周りのあらゆる角度から鮮明な写真を取得することを確認する必要があります。

セッションの後半でキャプチャのベストプラクティスを提供します。

iPhoneまたはiPadでキャプチャする場合、サポートされているデバイスからのステレオ深度データを使用して、実際のオブジェクトサイズと重力ベクトルを回復できるため、モデルが自動的に右向きに作成されます。

画像のフォルダをキャプチャしたら、それらをMacにコピーし、Object Capture APIを使用してわずか数分で3Dモデルに変換する必要があります。

このAPIは最近のIntelベースのMacでサポートされていますが、Apple Neural Engineを利用してコンピュータビジョンアルゴリズムを高速化できるため、すべての最新のAppleシリコンMacで最速で動作します。

また、始めるのに役立つサンプルコマンドラインアプリであるHelloPhotogrammetryも提供しています。

また、画像のフォルダで直接使用して、コードを書く前に自分でモデルを構築してみることもできます。

最後に、USDZ出力モデルをMacでプレビューできます。

さまざまなユースケースに最適化された4つの詳細レベルでモデルを提供できます。これについては、後で詳しく説明します。

縮小、中、および完全な詳細は、ここに示されているピザのように、箱から出してすぐに使用できます。

Rawはカスタムワークフローを対象としています。

ミディアムディテールレベルでUSDZ出力を選択すると、iPhoneまたはiPadのARクイックルックで新しいモデルを表示できます。

そして、AR用に最適化されたリアルなオブジェクトを手に入れるのはそれだけです!

待って、前のピザを覚えてる？

私たちは白状しなければならない。

この画像は実際には写真ではありませんでしたが、実際にはいくつかのピザでオブジェクトキャプチャを使用して作成されました。

その後、これらのモデルはポストプロダクションツールでこのシーンに結合され、高度なマテリアルマップでレイトレーサーを使用してレンダリングされました。

したがって、Object Captureは、iPhoneやiPadのARアプリから映画対応の制作資産まで、さまざまなターゲットユースケースをサポートできます。

このセッションの残りの部分では、Object Capture APIの使用を開始する方法を紹介し、最高品質の結果を達成するためのベストプラクティスを提供します。

はじめにセクションでは、Object Capture APIについて詳しく説明し、アプリを作成するために不可欠なコードの概念を紹介します。

次に、画像キャプチャ、オブジェクト選択、詳細レベル選択のベストプラクティスについて説明します。

macOSでAPIを使用する上で重要なステップに取り組むことから始めましょう。

このセクションでは、Object Capture APIの基本的なコンポーネントと、それらをまとめる方法を学びます。

ARで見るために3Dモデルに変えたいこのクールな新しいスニーカーを持っているとしましょう。

ここでは、このセクションで検討する基本的なワークフローのグラフィカルな図を紹介します。

このプロセスには2つの主要なステップがあります。オブジェクトの画像のセットを指すセットアップ。次に、構築したいモデルの生成を要求するプロセスです。

まず、セッションを作成し、関連する出力ストリームを接続する2つのサブステップで構成されるセットアップブロックに焦点を当てます。

有効なセッションを取得したら、それを使用してモデルを生成できます。

最初にする必要があるのは、PhotogrammetrySessionを作成することです。

セッションを作成するには、オブジェクトの画像のフォルダがすでにあることを前提としています。

すぐに始めるために、APIドキュメントにいくつかのサンプル画像キャプチャフォルダを提供しました。

PhotogrammetrySessionは、APIの主要なトップレベルクラスであり、主な制御ポイントです。

セッションは、結果の3Dモデルを生成するために写真測量アルゴリズムが適用される固定画像セットのコンテナと考えることができます。

ここには、iPhone 12 Pro Maxを使用して撮影されたスニーカーの123枚のHEIC画像があります。

現在、使用する画像のセットを指定する方法はいくつかあります。

最も簡単なのは、画像のディレクトリへのファイルURLです。

セッションでは、これらを1つずつ取り込み、発生した問題を報告します。

HEIC画像に深度データが埋め込まれている場合は、オブジェクトの実際のスケールを回復するために自動的に使用されます。

ほとんどの人がフォルダ入力を好むことを期待していますが、一連のカスタムサンプルを提供するための高度なワークフローのインターフェースも提供しています。

PhotogrammetrySampleには、画像に加えて、深度マップ、重力ベクトル、カスタムセグメンテーションマスクなどの他のオプションデータが含まれています。

入力ソースからセッションを作成したら、モデル再構築のリクエストを行います。

セッションは、結果のモデルとステータスメッセージを出力メッセージストリームに出力します。

セッションとは何かわかったので、APIを使用してセッションを作成する方法を見てみましょう。

ここでは、画像のフォルダからセッションの初期設定を実行するためのコードを参照してください。

PhotogrammetrySessionはRealityKitフレームワーク内にあります。

まず、入力フォルダをファイルURLとして指定します。

ここでは、すでにローカルディスクにスニーカーの画像を含むフォルダがあると仮定します。

最後に、入力ソースとしてURLを渡してセッションを作成します。

パスが存在しない場合、または読み取ることができない場合、初期化子はエラーをスローします。

オプションで高度な設定パラメータを提供できますが、ここではデフォルトのみを使用します。

セッションを作成するのに必要なのはそれだけです!

セッションオブジェクトを正常に作成したので、メッセージが到着したときにメッセージを処理できるように、セッションの出力ストリームを接続する必要があります。

メッセージストリームが接続されたら、そのストリームに到着するモデルをリクエストする方法を確認します。

今年のSwiftの新しい機能であるAsyncSequenceを使用して、出力のストリームを提供します。

出力メッセージには、リクエストの結果や、進捗状況の更新などのステータスメッセージが含まれます。

最初のプロセスコールを行うと、メッセージは出力メッセージストリームで流れ始めます。

セッションが生きている間、出力メッセージシーケンスは終了しません。

セッションが初期化解除されるか、致命的なエラーが発生するまで、メッセージが生成され続けます。

では、受信するメッセージの種類を詳しく見てみましょう。

リクエストが行われた後、各リクエストの見積もりが完了した定期的なリクエストプログレスメッセージを受け取る予定です。

Object Capture APIを呼び出すアプリを構築している場合は、これらを使用して、各リクエストのステータスを示すプログレスバーを駆動できます。

リクエストの処理が完了すると、モデルやバウンディングボックスなど、結果のペイロードを含むrequestCompleteメッセージを受け取ります。

処理中に何か問題が発生した場合、代わりにそのリクエストに対してrequestErrorが出力されます。

便宜上、キューに入れられたすべてのリクエストが処理を完了すると、processingCompleteメッセージが出力されます。

セッション出力ストリームの概念が紹介され、主要な出力メッセージを見たので、メッセージストリームを処理するコードの例を見てみましょう。

これを手に入れたら、モデルをリクエストする方法を見ていきます。

以下は、メッセージが到着したときに処理する非同期タスクを作成するコードです。

それは多くのコードのように思えるかもしれませんが、私たちが見るように、そのほとんどは単にメッセージの送信です。

「For try await」ループを使用して、session.outputsのメッセージが到着したときに非同期に反復します。

コードの大部分は、出力メッセージを切り替えるメッセージディスパッチャです。

出力は、異なるメッセージタイプとペイロードを持つ列挙型です。

各ケースステートメントは異なるメッセージを処理します。

それらを通り抜けましょう。

まず、進捗メッセージが表示されたら、値を印刷するだけです。

リクエストごとに進捗メッセージが届くことに注意してください。

たとえば、リクエストが完了すると、結果のペイロードは、モデルが保存された場所へのURLを持つmodelFileであることを期待します。

そのような要求をすぐに行う方法を見ていきます。

フォトグラメトリエラーのためにリクエストが失敗した場合、代わりにエラーメッセージが表示されます。

プロセスコールからのリクエストのセット全体が終了すると、processingCompleteメッセージが生成されます。

コマンドラインアプリの場合、ここでアプリを終了することができます。

最後に、読み込めなかったフォルダ内の画像に関する警告など、ドキュメントで読むことができる他のステータスメッセージがあります。

そして、それはメッセージ処理のためです!

このメッセージ処理タスクは、セッションが生きている限り、メッセージを非同期に反復および処理し続けます。

さて、私たちがワークフローのどこにいるか見てみましょう。

セットアップフェーズを完全に完了し、セッションの準備が整いました。

これで、モデルを処理するためのリクエストを行う準備が整いました。

コードに飛び込む前に、私たちができるさまざまな種類のリクエストを詳しく見てみましょう。

セッションから受信できるデータには、ModelFile、ModelEntity、BoundingBoxの3つの異なるデータタイプがあります。

これらのタイプには、リクエスト列挙型に関連付けられたケースがあります: modelFile、modelEntity、および境界。それぞれ異なるパラメータを持ちます。

modelFileリクエストは最も一般的であり、基本的なワークフローで使用するものです。

USDZ拡張子と詳細レベルを持つファイルURLを指定するmodelFileリクエストを作成するだけです。

インタラクティブなワークフローで使用するオプションのジオメトリパラメータがありますが、ここでは使用しません。

USDAまたはOBJの出力形式が必要になる可能性のある、より複雑な後処理パイプラインについては、代わりに詳細レベルとともに出力ディレクトリURLを提供できます。

その後、セッションは、テクスチャや素材などのすべての参照アセットとともに、USDAとOBJファイルをそのフォルダに書き込みます。

GUIアプリは、インタラクティブなプレビューと洗練のためにRealityKit ModelEntityとBoundingBoxをリクエストすることもできます。

modelEntityリクエストは、詳細レベルとオプションのジオメトリも取ります。

境界要求は、オブジェクトの推定キャプチャボリュームBoundingBoxを返します。

このボックスはUIで調整し、再構成ボリュームを調整するための後続の要求のジオメトリ引数で渡すことができます。

これがどのように機能するかは、セッションの後半で見てみましょう。

ほとんどのリクエストには詳細レベルも必要です。

プレビューレベルは、インタラクティブなワークフローのみを対象としています。

視覚品質は非常に低いですが、最速で作成されます。

品質とサイズを向上させる順の主な詳細レベルは、削減、中、およびフルです。

これらのレベルはすべて箱から出してすぐに使用できます。

さらに、Rawレベルはプロの使用のために提供されており、適切に使用するためにポストプロダクションワークフローが必要になります。

これらについては、ベストプラクティスのセクションで詳しく説明します。

さて、どのようなリクエストができるかを見たので、コードでこれを行う方法を見てみましょう。

1回の呼び出しで2つのモデルを同時に生成する方法を見ていきます。それぞれに異なる出力ファイル名と詳細レベルがあります。

ここでは、セッションで処理する最初の呼び出しが表示されます。

一連のリクエストが必要であることに注意してください。

これは、一度に2つのモデルをリクエストする方法です。

削減された詳細レベルで1つのモデルとMediumで1つのモデルを要求し、それぞれが異なるUSDZファイルに保存します。

1回の呼び出しでオブジェクトキャプチャのすべての必要な詳細レベルを同時に要求すると、エンジンは計算を共有でき、すべてのモデルを順番に要求するよりも速く生成されます。

すべての詳細レベルを一度に尋ねることもできます。

出力場所を書き込むことができない場合など、要求が無効な場合、プロセスはすぐにエラーをスローする可能性があります。

この呼び出しはすぐに返され、すぐにメッセージが出力ストリームに表示され始めます。

そして、それは基本的なワークフローの終わりです!

画像でセッションを作成し、出力ストリームを接続してから、モデルを要求します。

各モデルの処理時間は、画像の数と品質レベルによって異なります。

処理が完了すると、モデルが利用可能であるという出力メッセージが届きます。

作成したスニーカーの結果のUSDZファイルをMacで開き、底面を含むあらゆる角度から3Dで結果を調べることができます。

このセッションの後半では、複数のキャプチャを組み合わせる必要性を避けて、1回のキャプチャセッションでオブジェクトのすべての側面のカバレッジを実現する方法を紹介します。

すごく良さそうだね！

基本的なワークフローを見たので、Object Capture APIもサポートする、より高度なインタラクティブワークフローの概要を説明します。

インタラクティブなワークフローは、最終的な再構築の前にプレビューモデルでいくつかの調整を行うことができるように設計されています。これにより、ポストプロダクションモデルの編集の必要性を排除し、メモリの使用を最適化することができます。

まず、このワークフローの両端にあるセットアップステップとプロセスステップは以前と同じであることに注意してください。

引き続きセッションを作成し、出力ストリームを接続します。

また、以前と同様に最終モデルをリクエストします。

ただし、プレビューモデルのインタラクティブな編集のために3D UIが提示される中央にブロックを追加したことに注意してください。

このプロセスは、プレビューに満足するまで反復されます。

その後、以前と同じように最終的なモデル要求を続行できます。

まず、プレビューの詳細レベルのモデルリクエストを指定して、プレビューモデルをリクエストします。

プレビューモデルは視覚品質が低く、できるだけ早く生成されます。

モデルファイルをリクエストして自分でロードするか、RealityKit ModelEntityの表示を直接リクエストできます。

通常、キャプチャボリュームをプレビューおよび編集するために、境界要求も同時に行われます。

キャプチャボリュームを調整して、キャプチャ中にオブジェクトを直立させるために必要な台座など、キャプチャ内の不要なジオメトリを削除できます。

ルート変換を調整して、モデルをスケーリング、変換、回転させることもできます。

先ほど見たリクエストのジオメトリプロパティでは、モデルが生成される前にキャプチャボリュームと相対ルート変換を提供できます。

これにより、すぐに使用できる3Dモデルが出力されます。

このプロセスの実行を見てみましょう。 

ここでは、このインタラクティブなワークフローを実証するためにAPIを使用して作成したインタラクティブなオブジェクトキャプチャアプリの例を示します。

まず、装飾的な岩の画像を含む画像フォルダと、最終的なUSDZが書き込まれる出力フォルダを選択します。

次に、プレビューを押して、プレビューモデルと推定キャプチャボリュームを要求します。

しばらくすると、私たちの岩のプレビューモデルとそのキャプチャボリュームが現れます。

しかし、底が地下にあるかのように、出力に岩の上部だけが欲しいとしましょう。

モデルの底を再構築しないように、バウンディングボックスを調整できます。

満足したら、リファインモデルを押して、この変更されたキャプチャボリュームに制限された新しいプレビューを生成します。

これはまた、この部分だけに対して出力モデルを最適化します。

洗練されたモデルの準備が整うと、新しいプレビューが表示されます。

新しいモデルのジオメトリが箱の中にとどまるようにクリップされているのを見ることができます。

これは、オブジェクトを保持する台座などのキャプチャ内の不要なアイテムを削除するのに便利です。

トリミングされたプレビューに満足したら、作成プロセスを開始する完全な詳細の最終レンダリングを選択できます。

しばらくすると、フルディテールモデルが完成し、プレビューモデルを置き換えます。

今、私たちは実際のモデルの完全な詳細を見ることができ、それは素晴らしく見えます。

モデルは出力ディレクトリに保存され、追加の後処理を必要とせずにすぐに使用できます。

そして、新しいオブジェクトキャプチャAPIを使い始めるのはそれだけです。

画像のフォルダなどの入力ソースからセッションを作成する方法を見ました。

非同期出力ストリームをディスパッチメッセージに接続する方法を見ました。

次に、2つの異なるレベルの詳細モデルを同時にリクエストする方法を見ました。

最後に、ObjectCapture用のRealityKit GUIアプリの例を使用して、インタラクティブなワークフローについて説明しました。

次に、同僚のDave McKinnonに渡します。Dave McKinnonは、Object Captureでベストプラクティスについて話し合います。

デイブ・マッキノン:ありがとう、マイケル。

こんにちは、私はデイブ・マッキノンで、オブジェクトキャプチャチームで働くエンジニアです。

次のセクションでは、最高品質の結果を達成するためのベストプラクティスについて説明します。 

まず、適切な特性を持つオブジェクトを選択するためのヒントとコツを見ていきます。

最良の結果を得るために環境条件とカメラを制御する方法についての議論が続きます。

次に、CaptureSampleアプリの使い方を説明します。

このアプリでは、深度データと重力情報に加えて画像をキャプチャして、オブジェクトの真のスケールと向きを回復できます。

手持ちとターンテーブルのキャプチャの両方でこのアプリの使用を説明します。

最後に、ユースケースに適した出力詳細レベルを選択する方法と、さらに読むためのリンクについて説明します。

スキャンを行う際に最初に考慮すべきことは、適切な特性を持つオブジェクトを選ぶことです。

最良の結果を得るには、適切なテクスチャディテールを持つオブジェクトを選択してください。

オブジェクトにテクスチャレスまたは透明な領域が含まれている場合、結果のスキャンには詳細が欠けている可能性があります。

さらに、反射率の高い領域を含むオブジェクトを避けるようにしてください。

物体が反射している場合は、スキャン時に照明を拡散することで最良の結果が得られます。

キャプチャ全体を通してオブジェクトを反転させる予定がある場合は、形状が変わらないように硬質であることを確認してください。

最後に、細かい表面の詳細を含むオブジェクトをスキャンする場合は、詳細を回復するために表面の多くのクローズアップ写真に加えて、高解像度カメラを使用する必要があります。

では、典型的なスキャンプロセスを実演します。

まず、最良の結果を得るには、オブジェクトがはっきりと目立つように、オブジェクトを整頓された背景に配置します。

基本的なプロセスには、オブジェクトの周りをゆっくりと移動し、すべての側面から均一にキャプチャすることが含まれます。

オブジェクトの下部を再構築したい場合は、それを反転して画像をキャプチャし続けます。

画像を撮影するときは、オブジェクトをキャプチャする視野の部分を最大化するようにしてください。

これは、APIができるだけ多くの詳細を回復するのに役立ちます。

これを行う1つの方法は、オブジェクトの寸法と向きに応じて縦向きまたは横向きモードを使用することです。

また、画像間の高度な重複を維持するようにしてください。

オブジェクトによっては、20〜200枚のクローズアップ画像が良い結果を得るのに十分であるはずです。

iOSで深度と重力で高品質の写真を撮影し始めるのを助けるために、CaptureSampleアプリを提供しています。

これは、あなた自身のアプリの出発点として使用できます。

SwiftUIで書かれており、開発者ドキュメントの一部です。

このアプリは、オブジェクトキャプチャのために高品質の写真を撮る方法を示しています。

手動シャッターモードと時時モードがあります。

ターンテーブルと同期するようにアプリを変更することもできます。

iPhoneとiPadをデュアルカメラで使用して深度データをキャプチャし、出力HEICファイルに直接埋め込む方法を示しています。

このアプリは、重力データを保存する方法も示します。

ギャラリーを表示して、深さと重力を備えたすべての良質の写真を持っていることをすばやく確認し、悪いショットを削除することができます。

キャプチャフォルダはアプリのドキュメントフォルダに保存され、iCloudまたはAirDropを使用してMacに簡単にコピーできます。

また、このセクションで説明する良いキャプチャを得るためのベストプラクティスガイドラインのいくつかをまとめたヘルプ画面もあります。

この情報は、開発者ドキュメントでも確認できます。

可能な限り最良の結果を得るために、ターンテーブルキャプチャをお勧めします。

始めるには、ここにあるようなセットアップが必要です。

これにはキャプチャ用のiOSデバイスが含まれていますが、デジタル一眼レフ、オブジェクトを回転させるための機械式ターンテーブル、ライトテントに加えていくつかの照明パネルを使用することもできます。

目標は、均一な照明を持ち、硬い影を避けることです。

軽いテントはこれを達成するための良い方法です。

この場合、CaptureSampleアプリは、ターンテーブルの動きと同期した時差シャッターモードを使用して画像をキャプチャします。

また、オブジェクトを反転させ、複数のターンテーブルパスを行い、あらゆる側面からオブジェクトをキャプチャすることもできます。

これは、macOSのプレビューに表示されるターンテーブルキャプチャから得られたUSDZファイルです。

画像をキャプチャするためのヒントとコツを説明したので、正しい出力を選択する方法に関する最後のセクションに移りましょう。

スキャンにはさまざまな出力詳細設定があります。

見てみましょう。 

これは、詳細レベルを示す表です。

サポートされているレベルは左側に沿って表示されます。

ReducedとMediumは、AR Quick Lookで3Dコンテンツを表示するなど、Webベースおよびモバイルエクスペリエンスでの使用に最適化されています。

三角形やマテリアルチャンネルが少なく、その結果、メモリの消費が少なくなります。

FullとRawは、コンピュータゲームやポストプロダクションワークフローなどのハイエンドのインタラクティブな使用を目的としています。

彼らは最高の幾何学的ディテールを含み、焼きたての材料と未焼成の材料の間で選択する柔軟性を与えます。

縮小および中程度の詳細レベルは、インターネットやモバイルデバイスに表示したいコンテンツに最適です。

この場合、Object Captureは、Raw結果からの幾何学的および材料情報を、ARアプリまたはARクイックルックでの表示に適したレベルに圧縮します。

削減と中の両方の詳細レベルには、拡散、正常、および周囲の閉塞PBR材料チャネルが含まれています。

1回のスキャンを詳細に表示したい場合、Mediumはファイルサイズに対して品質を最大化し、幾何学的および材料の詳細の両方を提供します。

ただし、同じシーンで複数のスキャンを表示したい場合は、縮小された詳細設定を使用する必要があります。

オブジェクトキャプチャを使用してモバイルまたはWeb ARエクスペリエンスを作成する方法の詳細については、「ARクイックルック、オブジェクトキャプチャを満たす」セッションを参照してください。

フル出力レベルでのエクスポートは、プロのワークフローに最適です。

この場合、スキャンに利用可能な最大限の詳細を取得しています。

フルは、スキャンのジオメトリを最適化し、拡散、ノーマル、アンビエントオクルージョン、粗さ、および変位情報を含むPBR材料に詳細を焼きます。

この出力レベルは、最も困難なレンダリングに必要なものすべてを提供すると思います。

最後に、材料ベーキングを必要としない場合、またはこれのための独自のパイプラインがある場合、Rawレベルは、さらなる処理のために最大拡散テクスチャの詳細とともに最大ポリカウントを返します。

macOSのプロワークフローでオブジェクトキャプチャを使用する方法の詳細については、「USDで3Dワークフローを作成する」セッションを参照してください。

最後に、そして最も重要なことは、iOSとmacOSの両方でスキャンを使用する予定がある場合は、複数の詳細レベルを選択して、現在および将来のユースケースに適した出力を確認できます。

そして、それはラップです。

私たちが学んだことをまとめましょう。

まず、例を通して、オブジェクトキャプチャAPIの背後にある主な概念を取り上げました。

オブジェクトキャプチャセッションを作成し、このセッションを使用して画像のコレクションを処理して3Dモデルを作成する方法を紹介しました。

キャプチャボリュームとモデル変換を調整できるように、APIがインタラクティブなプレビューアプリケーションをサポートする方法の例を紹介しました。

次に、スキャンのベストプラクティスを取り上げました。

最適な結果をもたらす環境、照明、カメラの設定だけでなく、使用するオブジェクトの種類についても議論しました。

最後に、アプリケーションに適した出力詳細設定を選択する方法について説明しました。

オブジェクトキャプチャを自分のアプリに持ち込む方法を学びたい場合は、iOSキャプチャとmacOS CLI処理アプリの両方をチェックして始めましょう。

これらのアプリに加えて、ベストプラクティスを体現し、独自のスキャンをキャプチャする方法を計画する際に役立つさまざまなサンプルデータが付属しています。

さらに、developer.apple.comでオンラインでのベストプラクティスに関する詳細なドキュメントと、これらの関連するWWDCセッションをチェックしてください。

残っているのは、外に出て自分のスキャンにオブジェクトキャプチャを使用することだけです。

私たちは、あなたがスキャンして共有するオブジェクトを見ることに興奮しています。

♪