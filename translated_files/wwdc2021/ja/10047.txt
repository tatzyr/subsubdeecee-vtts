10047

♪ベース音楽の演奏♪

♪

ブラッド・フォード:こんにちは、「カメラキャプチャの新機能」へようこそ。

私はカメラソフトウェアチームのブラッド・フォードです。

最小焦点距離レポートから始まる新しいカメラ機能のホストを紹介します。 10ビットHDRビデオをキャプチャする方法。 メインコース、コントロールセンターのビデオエフェクト。 そして、新機能からの短い休息 - カメラアプリから最高のパフォーマンスを得るための戦略を確認します。

そして最後に、あなたのバッグにまったく新しいパフォーマンストリック、IOSurface圧縮を紹介します。

今日説明するすべての機能は、AVFoundationフレームワークにあります。具体的には、AVCaptureの接頭辞が付いたクラスです。

簡単に確認するために、主なオブジェクトは、カメラまたはマイクを表すAVCaptureDevicesです。 AVCaptureDeviceInputsは、デバイスをラップし、AVCaptureグラフの中央制御オブジェクトであるAVCaptureSessionに接続できるようにします。

AVCaptureOutputsは、さまざまな方法で入力からデータをレンダリングします。

MovieFileOutputはQuickTimeムービーを記録します。

PhotoOutputは、高品質の静止画とLive Photosをキャプチャします。

VideoDataOutputやAudioDataOutputなどのデータ出力は、カメラやマイクからアプリにビデオまたはオーディオバッファを配信します。

メタデータや深度など、他にもいくつかの種類のデータ出力があります。

ライブカメラのプレビューには、CALayerのサブクラスであるAVCaptureVideoPreviewLayerという特別なタイプの出力があります。

データは、これらの矢印で表されるAVCaptureConnectionsを介して、キャプチャ入力から互換性のあるキャプチャ出力に流れます。

AVFoundationカメラキャプチャを初めてご利用になる場合は、developer.apple.comのカメラとメディアキャプチャのスタートページで詳細をご覧ください。

さて、最初の新機能に飛び込みましょう。

最小焦点距離は、レンズからシャープな焦点を達成できる最も近いポイントまでの距離です。

これは、デジタル一眼レフカメラやスマートフォンなど、すべてのレンズの属性です。

iPhoneのカメラにも最小の焦点距離がありますが、これまでに公開したことがありません...

...今まで、それは。

iOS 15以降、最小焦点距離はiPhoneのオートフォーカスカメラの公開プロパティです。

これは最近のiPhoneのサンプルです。

このチャートは、広角カメラと望遠カメラの最小焦点距離がモデルによってどのように異なるかを示しています。

iPhone 12 Proと12 Pro Max Wideカメラには顕著な違いがあり、Pro MaxはProの12と比較して15センチメートルの最小距離に焦点を合わせています。

これは、iPhone 12 Pro Maxのセンサーシフト安定化技術によるものです。

同様に、テレの最小焦点距離は12 Pro Maxで12 Proよりも遠いです。

これは、望遠レンズのリーチが長くなったためです。

それは2.5倍対2倍のズームを持っています。

最小焦点距離の報告が重要な理由の簡単なデモをお見せしましょう。

これはAVCamBarcodeというサンプルアプリです。

AVFoundationバーコード検出APIを紹介しています。

UIは、スキャンのために長方形の中にオブジェクトを配置するようにユーザーを導きます。

この例では、紙にかなり小さなQRコードを選択しました。

バーコードの幅はわずか20ミリメートルです。

メタデータボタンをタップすると、AVCaptureMetadataOutputでサポートされているさまざまなオブジェクトタイプのリストが表示されます。

たくさんありますよ

QRコードを選択し、iPhone 12 Pro Maxカメラを配置して、長方形をQRコードで埋めます。

残念ながら、それはとても小さいので、プレビューを埋めるためにページに非常に近づかなければなりません。

それはカメラの最小焦点距離よりも近いです。

コードがぼやけているので、スキャンされません。

ユーザーを後退させるには、カメラのプレビューにズームファクターを適用する必要があります...

...そうであるように。

画面上でズームされた画像を見ると、カメラを紙から遠くに物理的に動かすように促されます。

スライダーボタンでできますが、アプリが自動的にズームを処理した方がはるかに良いでしょう。

そこで、AVCaptureDeviceの新しいminimumFocusDistanceプロパティが登場します。

iOS 15では新機能です。

カメラの水平視野、スキャンしたい最小バーコードサイズ（ここでは20ミリメートルに設定しました）、カメラプレビューウィンドウの幅をパーセンテージとして、そのプレビュー幅を埋めるために必要な最小被写体距離を計算するために少し計算することができます。

次に、カメラの新しいminimumFocusDistanceプロパティを使用して、カメラが近くにフォーカスできないときを検出し、ユーザーを後退させるのに十分な大きさのズーム係数を計算することができます。

そして最後に、設定のためにロックし、ズーム係数を設定してからロックを解除することで、カメラに適用します。

デモアプリを再コンパイルした後、UIは自動的に正しいズーム量を適用します。

アプリが起動すると、すでに正しいスペースにズームされています。

ぼやけたバーコードはもうありません!

そして、私がそれをタップすると、それが私をどこに連れて行くかを見ることができます。

ああ！「iPhone写真で奥行きを捉える」 - オールディーだがグッディだ。

私はそのセッションに深く感謝しています。

最小焦点距離を組み込む方法のベストプラクティスと、バーコードをスキャンするための他の多くのベストプラクティスについては、新しいAVCamBarcodeサンプルをチェックしてください。

次は10ビットHDRビデオです。

HDRはハイダイナミックレンジの略で、iOS 4.1にさかのぼって以来、静止画技術として存在しています。

ハイダイナミックレンジの維持は、通常、シーンの複数の露出を取り、ハイライトとシャドウの両方を維持するためにそれらをブレンドすることによって達成されます。

しかし、ビデオHDRはどうですか?

1秒間に30または60フレームを配信しなければならないので、それは挑戦です。

正確にはビデオHDRではありませんが、2018年、AppleはiPhone XSのカメララインにEDRまたは拡張ダイナミックレンジを導入しました。

EDRは、ビデオ用のHDRのようなソリューションです。

基本的にキャプチャフレームレートを2倍にし、標準露出と短い露出を交互にしますが、キャプチャ間に垂直ブランキングが事実上ないため、タイミングが設定されています。

名目上毎秒30フレームをキャプチャする場合、EDRビデオは実際には毎秒60フレームでカメラを実行しています。

シーンで必要な場合、EV-からの色相マップはEV0画像に動的に適用され、クリップされたハイライトを回復しますが、影の細部を犠牲にすることなく。

暗い場所では効果が低下するため、完全なHDRソリューションではありませんが、中程度から良好な光で驚くべき結果が得られます。

さて、ここに紛らわしい部分があります。

EDRは、videoHDRのあだ名でAVCaptureDeviceプロパティのスイートとして提示されました。

AVCapture APIでvideoHDRSupportedまたはvideoHDREnabledが表示される場合は、精神的にEDRを置き換える必要があります。

それがそれです。

AVCaptureDeviceには「automaticallyAdjusts VideoHDREnabled」というプロパティもあり、デフォルトはtrueです。

したがって、EDRは利用可能なときはいつでも自動的に有効になります。

何らかの理由で無効にしたい場合は、automaticAdjustsVideo HDREnabledをfalseに設定し、videoHDREnabledもfalseに設定する必要があります。

今、物語はさらに良くなっています。

10ビットHDRビデオについてあなたに話すことができるように、EDRについてあなたに話す必要がありました。

10ビットHDRビデオは、より多くのビットを持っているので、本当にハイダイナミックレンジです!

それは編集性の向上を意味します。

それはハイライト回復のためのEDRを持っており、それは常にオンです。

ハイブリッド対数ガンマ曲線とBT.2020色空間を使用し、Rec 709よりも明るい明るい色のコントラストをさらに高めます。

また、AVCaptureMovieFileOutputまたはAVCaptureVideoDataOutputとAVAssetWriterを使用する場合でも、フレームごとのドルビービジョンメタデータをムービーに自動的に挿入し、ドルビービジョンディスプレイと互換性があります。

10ビットHDRビデオは、iPhone 12で初めて導入されました。

10ビットHDRビデオフォーマットは、独自のピクセルフォーマットタイプで識別できます。

古いiPhoneモデルでは、カメラには常にペアで来るAVCaptureDeviceFormatsがあります。

解像度とフレームレートの範囲ごとに、420vと420fのフォーマットがあります。

これらは8ビット、バイプラナー、YUVフォーマットです。

420vのVはビデオ範囲（または16〜235）を表し、420fのFはフルレンジ（または0〜255）を表します。

iPhone 12モデルでは、いくつかのフォーマットは3つのクラスターで提供されます。

420vフォーマットと420fフォーマットの後、同じ解像度とフレームレート範囲のx420フォーマットが登場します。

420vと同様に、x420はビデオ範囲のバイプレーナー420フォーマットですが、x420のxは8ビットではなく10ビットを表します。

コード内の10ビットHDRビデオフォーマットを見つけて選択するには、ピクセルフォーマットがx420、または-深呼吸-420YpCbCr10BiPlanarVideoRangeに一致するものが見つかるまで、AVCaptureDeviceフォーマットを反復するだけです。

もちろん、幅、高さ、最大フレームレートなど、他の検索条件を含めることができます。

AVCamのサンプルコードを更新して、利用可能な場合は10ビットHDRビデオをサポートしました。

「tenBitVariantOfFormat」と呼ばれる便利なユーティリティ機能があり、現在選択されているデバイスのアクティブフォーマットが何であれ、10ビットHDRバリアントを見つけることができます。

見てください。

10ビットHDRビデオは、720p、1080p、4Kなど、最も人気のあるすべてのビデオフォーマットでサポートされています。

また、12メガピクセルの高解像度写真をサポートする4×3フォーマット（1920×1440）も含めました。

10ビットHDRビデオのキャプチャは簡単ですが、適切に編集して再生するのは難しいです。

「AVFoundationでHDRビデオを編集して再生する」と題した2020年のコンパニオンセッションをご覧ください。

よし、HDRビデオはそれだけだ。

では、メインイベント：コントロールセンターのビデオエフェクト。

簡単に言えば、これらはコードの変更なしでアプリで利用できるシステムレベルのカメラ機能です。

そして、ユーザーがコントロールしています。

これは私たちにとってちょっとした出発です。

伝統的に、iOSやmacOSに新しいカメラ機能を導入すると、Appleのアプリは箱から出してそれを採用しています。

私たちは新しいAVCapture APIを公開し、あなたは今やっているのと同じようにそれらについて学び、自分のペースで機能を採用します。

これは安全で保守的なアプローチですが、多くの場合、ユーザーがお気に入りのカメラアプリの素晴らしい機能を逃す長いリードタイムになります。

コントロールセンターのビデオエフェクトでは、コードの変更なしで誰でもすぐに利用できるシステムレベルのパッケージ化されたカメラ機能を導入し、ユーザーがコントロールできます。

これらの機能の新しいAPIを公開し続けているので、リリーススケジュールが許せばすぐにアプリのエクスペリエンスを調整できます。

これらの効果を見てみましょう。 では、これらの効果を見てみましょう

1つ目は、5月のSpring Loaded Appleイベントで発表され、「センターステージ」と呼ばれています。

最近リリースされたM1 iPad Proモデルで利用可能で、信じられないほどの12メガピクセルの超広角フロントカメラを利用しています。

センターステージは、FaceTimeビデオ通話の制作価値を本当に高めます。

また、他のすべてのビデオ会議アプリで箱から出してすぐに機能します。

ほら、お見せしましょう。

App StoreからSkypeをダウンロードしたところです。これはアプリのストックバージョンです。

Skype通話を開始すると、すぐにセンターステージが行動を起こすのが見えます。

それはあなた自身の個人的なカメラオペレーターを持っているようなものです。

あなたがタイトに来るか、戻ってペースが好きかにかかわらず、あなたを完璧にフレームに保つために、あなたがシーンを動き回っているときにあなたをフレーミングします。

カメラから顔をそらすと、あなたを追跡することさえできます。

それは、顔だけでなく、体を追跡するからです。

ユーザーとして、コントロールセンターを下にスワイプし、新しいビデオエフェクトモジュールをタップして選択するだけで、センターステージを制御できます。

センターステージをオフにしてアプリに戻ると、センターステージ効果がなくなります。

アプリに変更はありません。

すべてのビデオ会議アプリにも得られるコンパニオン新機能があり、それは「ポートレート」と呼ばれています。

ポートレートモードは、美しくレンダリングされた浅い被写界深度効果を提供します。

単純なプライバシーのぼかしだけではありません。Appleのニューラルエンジンと訓練された単眼深度ネットワークを使用して、ワイドオープンレンズで実際のカメラを近似します。

最後に、下にスワイプしてマイクモードモジュールを選択して、マイクモードを見てみましょう。標準、音声分離、またはワイドスペクトラムのどちらかを選択できます。

マイクモードは、ビデオチャットのオーディオ品質を向上させます。

これらについては1分で詳しく。

センターステージ、ポートレート、マイクモードはコントロールセンターで画面の不動産を共有しますが、API処理は多少異なります。

最初にセンターステージAPIを紹介し、次にポートレートモードとマイクモードを紹介します。

センターステージは、M1 iPad Proのすべてのフロントカメラで利用できます。

新しい前面の超広角カメラ、トリミングされた従来の視野を示すバーチャルワイドカメラ、または仮想TrueDepthカメラを使用しているかどうかにかかわらず、センターステージが利用可能です。

TrueDepthカメラにはいくつかの条件が付属しており、すぐに説明します。

コントロールセンターのビデオエフェクトモジュールは、アプリごとにオン/オフの切り替えを表示します。

これにより、会議アプリでセンターステージをデフォルトでオンにしながら、手動でショットをフレーミングしたいプロ写真アプリでデフォルトでオフにすることができます。

アプリごとに1つの状態があり、カメラごとに1つの状態ではありません。

センターステージのオン/オフトグルはカメラごとではなくアプリごとであるため、AVCaptureDeviceのクラスプロパティのセットとしてAPIに表示されます。

これらは読み取り可能で、書き込み可能で、キー値が観察可能です。

centerStageEnabledは、コントロールセンターのセンターステージUIのオン/オフ状態と一致します。

センターステージコントロールモードは、誰が有効な状態を切り替えることができるかを決定します。

それについては1分で詳しく。

すべてのカメラやフォーマットがセンターステージをサポートしているわけではありません。

任意のカメラのフォーマット配列を反復して、その機能をサポートするフォーマットを見つけ、アクティブフォーマットとして設定できます。

さらに、センターステージのアクティブなプロパティを照会または観察することで、特定のカメラでセンターステージが現在アクティブであるかどうかを知ることができます。

センターステージの限界に注意する必要があります。

センターステージは、30fpsフォーマットであるウルトラワイドカメラのフル12メガピクセルフォーマットを使用しているため、最大フレームレートは30に制限されています。

センターステージは、画質を維持するためにアップスケーリングを回避するため、1440年までに1920の最大出力解像度に制限されています。

パンとズームはセンターステージコントロールの下にとどまらなければならないので、ビデオズームファクターは1つにロックされます。

幾何学的歪み補正はセンターステージの人々のフレーミングに不可欠であり、深度生成にはRGBおよび赤外線カメラからの完全な視野画像を一致させる必要があるため、深度配信はオフにする必要があります。

では、コントロールモードの概念に入りましょう。

センターステージには、ユーザー、アプリ、協力の3つのサポートモードがあります。

ユーザーモードは、すべてのアプリのデフォルトのセンターステージコントロールモードです。

このモードでは、ユーザーのみが機能のオンとオフを切り替えることができます。

アプリがセンターステージの有効状態をプログラムで変更しようとすると、例外がスローされます。

次はアプリモードです。ここでは、アプリのみが機能を制御できます。

トグルがグレー表示されているため、ユーザーはコントロールセンターを使用できません。

このモードの使用はお勧めできません。

センターステージがアプリと互換性がない場合にのみ使用してください。

オプトアウトする必要がある場合は、コントロールモードをappに設定し、isCenterStageEnabledをfalseに設定できます。

センターステージの可能な限り最高のユーザーエクスペリエンスは、ユーザーがコントロールセンターの機能を制御でき、アプリが独自のUIで制御できる協力モードです。

しかし、あなたはいくつかの余分な仕事をする必要があります。

AVCaptureDevice .isCenterStageEnabledプロパティを観察し、UIを更新して、ユーザーがオンにしたいときにセンターステージがオンになっていることを確認する必要があります。

コントロールモードを協力的に設定した後、たとえば、アプリのボタンに基づいて、センターステージを有効にしてtrueまたはfalseに設定できます。

協力モードのポスターチャイルドはFaceTimeです。

FaceTime通話中に、アプリ内のボタンを使用してセンターステージをオンにして追跡したり、コントロールセンターで下にスワイプしてセンターステージのオン/オフを切り替えたりする従来の方法を使用できます。

FaceTimeとコントロールセンターは、センターステージの状態に協力しているため、常にユーザーの意図と一致します。

FaceTimeはまた、機能が相互に互換性がないときを知るのに十分スマートです。

だから、例えば、深さを必要とするアニ文字をオンにしようとしたら...

...これら2つの機能は相互に互換性がないため、センターステージをオフにすることを知っています。

タップしてセンターステージをオンに戻すと、FaceTimeはアニ文字を無効にすることを知っています。

それはセンターステージAPIを締めくくります。

コントロールセンター、ポートレートのセンターステージのルームメイトに移行しましょう。

簡単に言えば、それは美しくレンダリングされた浅い被写界深度効果で、広絞りレンズのように見えるように設計されています。

iOSでは、ポートレートはApple Neural Engineを搭載したすべてのデバイスでサポートされています。これは2018年以降の携帯電話とパッドです。

フロントカメラのみがサポートされています。

また、同様にAppleのNeural Engineを含むすべてのM1 Macでもサポートされています。

ポートレートは計算的に複雑なアルゴリズムです。

したがって、ビデオレンダリングのパフォーマンスをレスポンシブに保つために、最大解像度は1920×1440、最大解像度は毎秒30フレームに制限されています。

センターステージと同様に、ポートレートエフェクトはアプリごとにスティッキーオン/オフ状態になります。

そのAPIはセンターステージよりも簡単で、ユーザーはコントロールセンターを通じて常に制御でき、特定のクラスのアプリでのみデフォルトで利用できます。

iOSでは、VoIP UIBackgroundModeを使用するアプリは自動的にオプトインされます。ユーザーはコントロールセンターでエフェクトのオン/オフを切り替えることができます。

他のすべてのiOSアプリは、アプリのInfo.plist: NSCameraPortraitEffectEnabledに新しいキーを追加して、ポートレート効果の対象であることを宣言するためにオプトインする必要があります。

macOSでは、すべてのアプリが自動的にオプトインされ、すぐに使用できる効果を使用できます。

ポートレート効果は、常にコントロールセンターを通じてのみユーザー制御下にあります。

センターステージと同様に、すべてのカメラやフォーマットがポートレートをサポートしているわけではありません。

任意のカメラのフォーマット配列を反復して、その機能をサポートするフォーマットを見つけ、アクティブなフォーマットとして設定できます。

また、isPortraitEffectActiveプロパティを照会または観察することで、ポートレートが特定のカメラで現在アクティブであるかどうかを調べることもできます。

マイクモードAPIはポートレートに似ています。

ユーザー選択はアプリごとにスティッキーです。

ユーザーはいつでもコントロールできます。アプリはマイクモードを直接設定することはできません。

一部のアプリは、この機能を使用するためにオプトインする必要があります。

マイクモードはAVFoundationのAVCaptureDeviceインターフェイスで提示され、3つのフレーバーがあります。標準のオーディオDSPを使用する標準。デバイス周辺のすべての音をキャプチャするための処理を最小限に抑える広いスペクトルですが、エコーキャンセルも含まれています。音声を強化し、キーボードでの入力、マウスクリック、または近所のどこかで実行されているリーフブロワーなどの不要なバックグラウンドノイズを除去する音声分離。

これらのフレーバーは、コントロールセンターでユーザーのみ設定できますが、ユーザーが選択したモードであるAVCaptureDeviceの優先マイクモードと、現在のオーディオルートを考慮して、現在使用されているモードであるactiveMicrophoneModeを使用して、その状態を読み取って観察することができます。これは、ユーザーの好みのマイクモードをサポートしていない可能性があります。

マイクモードを使用するには、アプリがCore Audio AUVoiceIOオーディオユニットを採用する必要があります。

これは、エコーキャンセルを実行するため、ビデオ会議アプリで人気のあるインターフェースです。

また、マイクモード処理は、2018年以降のiOSおよびmacOSデバイスでのみ利用可能です。

ポートレートモードとマイクモードでは、ユーザーが常に制御できますが、新しいAVCaptureDevice .showSystemUserInterfaceメソッドを呼び出すことで、機能をオンまたはオンにするように促すことができます。

そして、videoEffectsまたはmicrophoneModesのいずれかを渡すことができます。

このAPIを呼び出すと、コントロールセンターが開き、適切なサブモジュールにディープリンクされます。

ここでは、ユーザーがポートレートをオフにすることを選択できるビデオエフェクトモジュールにドリルダウンしています。

それはポートレートをラップし、コントロールセンターのビデオエフェクトをラップします。

コードの行を変更せずにアプリに注入されたシステムレベルのカメラ機能の例をお見せしました - かなり強力なコンセプトです!

「ビデオフォーマットを使用して高品質の写真をキャプチャする」と呼ばれるコンパニオンセッションでは、コードを変更せずにアプリの画質を静止させるための改善について学びます。

それをチェックしてください。

私たちは多くの新機能を取り上げました。

セッションのこの時点で、一息ついてパフォーマンスについて話したいと思います。

センターステージとポートレートは素晴らしい新しいユーザー機能ですが、パフォーマンスコストがかかります。

それでは、カメラアプリがポートレートやセンターステージなどの新機能の準備ができていることを確認するために、パフォーマンスのベストプラクティスを確認しましょう。

カメラアプリは、AVCaptureクラスを使用して幅広い機能を提供します。

最も一般的なインターフェイスはAVCaptureVideoDataOutputです。これにより、ビデオフレームを操作、表示、エンコード、録画のプロセスに直接取得できます。

何でも。

VideoDataOutputを使用する場合は、フレームドロップがないように、アプリがリアルタイムの締め切りに追いついていることを確認することが重要です。

デフォルトでは、VideoDataOutputは、常にDiscardsLateVideoFramesプロパティをtrueに設定することで、遅れるのを防ぎます。

これにより、ビデオデータ出力の処理パイプラインの最後に1のバッファキューサイズが強制され、常に最も新鮮なフレームを提供し、処理する準備ができていなかったフレームをドロップすることで、定期的または慢性的な遅い処理からあなたを救います。

AVAssetWriterなど、受信しているフレームを記録する必要がある場合は役に立ちません。

処理された結果を記録する場合は、常にDiscardsLateVideoFramesをオフにし、処理時間に細心の注意を払う必要があります。

VideoDataOutputは、提供されたcaptureOutput didDrop sampleBufferデリゲートコールバックを呼び出すことで、フレームドロップが発生したときに通知します。

didDropコールバックを受け取ったら、含まれているsampleBufferの添付ファイルでdropFrameReasonを検査できます。

これは、さらなるフレームドロップを軽減するために何をすべきかを知らせることができます。

3つの理由があります。FrameWasLateは、処理に時間がかかりすぎていることを意味します。OutOfBuffersは、あまりにも多くのバッファを保持している可能性があることを意味します。そして、不連続性は、システムが遅くなるか、ハードウェアの障害があることを意味します。

では、フレームドロップにどのように反応するかについて話しましょう。

最良の方法の1つは、デバイスのフレームレートを動的に下げることです。

そうすることで、プレビューや出力に不具合が発生することはありません。

実行時にAVCaptureDeviceに新しいactiveMinVideoFrameDurationを設定するだけです。

2番目の方法は、作業負荷を簡素化して、それほど時間がかからないようにすることです。

それでは、カメラアプリの優れたユーザーエクスペリエンスにとって非常に重要な別のパフォーマンス指標であるシステムプレッシャーについて話しましょう。

システム圧力とは、システムがひずみや圧力にさらされることを意味します。

AVCaptureDeviceには、要因と全体的なレベルで構成されるsystemPressureStateと呼ばれるプロパティがあります。

SystemPressureStateの寄与要因は、3つの可能性のある貢献者のビットマスクです。

システム温度とは、デバイスがどれだけ熱くなっているかを指します。

peakPowerは、バッテリーの経年劣化と、バッテリーがピーク電力需要を満たすのに十分な速さで電圧を上げることができるかどうかに関するものです。

そして、depthModuleTemperatureは、TrueDepthカメラの赤外線センサーの熱さを指します。

SystemPressureStateのレベルは、ユーザーエクスペリエンスが損なわれる前に行動を起こすのに役立つ指標です。

名目上の場合、すべてが順調です。

フェアは、システム圧力がわずかに上昇していることを示しています。

これは、処理をほとんど行わなくても発生する可能性がありますが、周囲温度は高いです。

深刻な場合、システム圧力は非常に高くなります。キャプチャ性能が影響を受ける可能性があります。

フレームレートのスロットリングをお勧めします。

クリティカルに達すると、システムの圧力は大幅に高まります。キャプチャの品質とパフォーマンスは大きな影響を受けます。

フレームレートのスロットリングを強くお勧めします。

そして、システムの圧力が重大を超えているシャットダウンに物事をエスカレートさせたくありません。

このレベルでは、AVCaptureSessionは自動的に停止し、デバイスをサーマルトラップから解放します。

さまざまな方法で高気圧に反応できます。

キャプチャフレームレートを下げます。これは常にシステム圧力に役立ちます。

フレームレートを下げることがオプションでない場合は、特定の機能をオフにするなど、CPUまたはGPUのワークロードを軽減することを検討してください。

また、機能をオンにしておくかもしれませんが、おそらくより小さな解像度またはより少ない頻度で処理することによって、品質を低下させるかもしれません。

AVCaptureSessionは、それがあなたのアプリの許容可能な品質劣化戦略であるかどうかわからないので、あなたに代わってフレームレートスロットルを決してしません。

それはパフォーマンスのベストプラクティスを締めくくります。

さて、私たちのデザートコース、IOSurface圧縮。

ISPを経由し、最終的には写真、映画、プレビュー、またはバッファに流れるビデオの全体的なメモリ帯域幅要件についてできることはあまりないので、パフォーマンスセクションでメモリ帯域幅について話すことを慎重に避けました。

しかし、それでも、メモリ帯域幅は、どのカメラ機能が同時に実行できるかを決定する上で重要なリミッターになる可能性があります。

iOSとmacOSで非圧縮ビデオを操作する場合、多くのレイヤーが関係しています。

それはロシアの巣作り人形に少し似ています。

トップレベルには、あらゆる種類のメディアデータ、タイミング、メタデータをラップできるCMSampleBufferがあります。

1つのレベル下に、メタデータの添付ファイルとともにピクセルバッファデータを具体的にラップするCVPixelBufferがあります。

最後に、その最低レベルであるIOSurfaceに到達し、メモリをカーネルに配線し、プロセス間で大きなビデオバッファを共有するためのインターフェイスを提供します。

IOSurfacesは巨大です。

それらは、非圧縮ビデオの大きなメモリ帯域幅の要件を考慮しています。

ありがたいことに、IOSurface圧縮はメモリ帯域幅の問題に対する解決策を提供します。

iOS 15の新機能では、ロスレスインメモリビデオ圧縮フォーマットのサポートを導入しています。

これは、ライブビデオの総メモリ帯域幅を下げるための最適化です。

これは、iOSデバイスとMacの主要なハードウェアブロックによって理解される交換形式です。

すべてのiPhone 12のバリエーション、2020年秋のiPad Air、2021年春のM1 iPad Proで利用できます。

圧縮されたIOSurfacesを扱う主要なハードウェアブロックはどれですか?

まあ、たくさんあります。

ここにリストされているすべてのサービスは、圧縮されたIOSurfacesの読み書き方法を理解しています。

この時点で、あなたは「素晴らしい、どうやってサインアップすればよいですか?」と言っているかもしれません。

さて、良いニュースです。

サポートされているハードウェアでビデオをキャプチャしていて、AVCaptureSessionがプロセスにバッファを配信する必要がない場合、おめでとうございます。セッションは、メモリ帯域幅を削減できるときはいつでもIOSurface圧縮を利用しています。

圧縮されたサーフェスをビデオデータ出力に配信したい場合は、いくつかのルールについて知っておく必要があります。

物理メモリのレイアウトは不透明で変更される可能性があるため、ディスクに書き込むことはせず、すべてのプラットフォームで同じレイアウトを想定せず、CPUを使用して読み書きしないでください。

AVCaptureVideoDataOutputは、いくつかのフレーバーのIOSurface圧縮をサポートしています。

講演の先ほど、iOSカメラが420vと420fをネイティブにサポートしていることを学びました - 8ビットYUVフォーマット。1つのビデオと1つのフルレンジ。

そしてその後、10ビットHDRビデオフォーマットであるx420について学びました。

ビデオデータ出力は、要求された場合、内部的に16ビット/ピクセルBGRAに拡張することもできます。

これらのそれぞれには、iOS 15以降、AVCaptureVideoDataOutputを通じてリクエストできるIOSurface圧縮同等物があります。

あなたが4文字のコードのアンパサンドのファンなら、これはあなたの幸運な日です。

これは再びアイチャート形式です。

これらは、コードで使用するべき実際の定数です。

2年前、私たちは「AVMultiCamPiP」というサンプルコードをリリースしました。

このサンプルでは、フロントカメラとバックカメラはマルチカムセッションを使用してVideoDataOutputsに同時にストリーミングされ、メタルシェーダーを使用してピクチャインピクチャとして合成され、複合をレンダリングしてプレビューし、AVAssetWriterを使用してムービーに書き込みます。

これは、これらの操作はすべてハードウェアで実行されるため、IOSurface圧縮の完璧な候補です。

AVMultiCamPiPの既存のVideoDataOutputセットアップコードは次のとおりです。

BGRAで作業するのが好きなので、そのピクセルフォーマットタイプを生成するようにVideoDataOutputのvideoSettingsを構成します。

新しいコードには、いくつかのチェックが組み込まれているだけです。

まず、IOSurface圧縮バージョンのBGRAが利用可能かどうかがわかります。

もしそうなら、それを選択します。else句はフォールバックとしてそこにあります。

そして、ちょうどそのように、私たちは終わりにたどり着きました。

最小フォーカス距離のレポート、10ビットHDRビデオのキャプチャ方法、コントロールセンターのビデオエフェクトとマイクモード、パフォーマンスのベストプラクティス、IOSurface圧縮について学びました。

楽しんでいただけたでしょうか!

見てくれてありがとう。

♪