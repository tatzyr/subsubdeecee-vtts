10039

♪ ♪

こんにちは、「Create MLで手のポーズとアクションを分類する」へようこそ。

私はネイサン・ワートマンです。

そして今日、同僚のBrittany WeinertとGeppy Parzialeが参加します。

今日は、ハンドポジションの分類について話します。

しかし、それを掘り下げる前に、手そのものについて話しましょう。

2ダース以上の骨、関節、筋肉で、手はエンジニアリングの驚異です。

この複雑さにもかかわらず、手は幼児が周囲の世界と交流するために使用する最初のツールの1つです。

赤ちゃんは、話すことができるようになる前に、簡単な手の動きを使ってコミュニケーションの基礎を学びます。

話すことを学ぶと、私たちの手はコミュニケーションの役割を果たし続けます。

彼らは強調と表現を加えることに移行します。

昨年、私たちの手は人々を近づけるためにこれまで以上に重要になりました。

2020年、ビジョンフレームワークは、開発者がフレーム内の手と手に存在する21の識別可能な関節のそれぞれを識別できるハンドポーズ検出を導入しました。

手が存在するかどうか、または手がフレーム内のどこにあるかを特定しようとしている場合、これは素晴らしいツールですが、手が何をしているかを分類しようとすると、課題になる可能性があります。

手の表現力は無限ですが、このセッションの残りの部分では、ストップ、静か、平和のような短い片手ポーズ、そしてバックアップ、離れて、ここに来るような短い片手の行動に焦点を当てたいと思います。

私はちょうど手のポーズと行動について言及しました。

もう少し具体的な定義はいかがですか?

さて、一方では、静止画として意味のあるポーズがあります。

これら2つのビデオは一時停止されていますが、主題の意図は明確に表現されています。

ポーズをイメージのように考えてください。

一方、私たちには、意味を完全に表現するために動きを必要とする行動があります。

これら2つの行動の意味は不明です。

単一のフレームを見るだけでは不十分です。

しかし、ビデオやライブ写真など、時間の経過とともに一連のフレームでは、アクションの意味は明らかです。

フレンドリーな「こんにちは」と「ここに来て」。

それが明確になったので、今年は2つの新しいCreate MLテンプレート、Hand Pose ClassificationとHand Action Classificationを紹介することに興奮しています。

これらの新しいテンプレートを使用すると、Create MLアプリまたはCreate MLフレームワークを使用して、ハンドポーズとアクションモデルをトレーニングできます。

これらのモデルは、macOS Big Sur以降、iOSおよびiPadOS 14以降と互換性があります。

そして、今年の新機能は、Create MLフレームワークを使用してiOSデバイスでモデルをトレーニングする機能を追加しました。

詳細については、「Create ML Frameworkを使用して動的iOSアプリを構築する」セッションで詳しく知ることができます。

まず、ビジョンフレームワークによって検出された手の位置を分類するために機械学習モデルを簡単にトレーニングできるハンドポーズ分類について話したいと思います。

あなたはモデルのトレーニングを担当しているので、アプリがそのニーズに最も合うように分類すべきポーズを定義します。

訓練されたモデルの簡単なデモをしましょう。

シンプルなプロトタイプアプリから始めて、Hand Pose Classifierモデルを簡単に統合することができました。

私のアプリは、手のポーズを分類し、対応する絵文字と分類されたポーズの自信を表示できるようになりました。

ハンドポーズを1と2に分類します。

しかし、認識していないすべてのポジションが背景の一部として分類されていることに気付くでしょう。

これには、サポートを追加したいオープンパームポーズが含まれます。

すぐにこのモデルを同僚のブリタニーに手渡し、ハンドポーズ分類モデルをアプリに統合する方法を紹介します。

その前に、オープンパームポーズのサポートを追加したい。

それは本当に簡単ですが、最初にモデルがどのように訓練されるかについて話すべきです。

他のすべてのCreate MLプロジェクトと同様に、Hand Pose Classifierをアプリに統合するのは非常に簡単です。

このプロセスには3つのステップがあります。

トレーニングデータを収集して分類し、モデルをトレーニングし、モデルをアプリケーションに統合します。

トレーニングデータの収集について話しましょう。

ハンドポーズ分類器の場合、画像が必要になります。

ポーズは画像として完全に表現力豊かであることを覚えておいてください。

これらの画像は、画像に存在するポーズと一致する名前のフォルダに分類する必要があります。

ここでは、特定したい2つのポーズがあります。1つ、2つ、そしてバックグラウンドクラスです。

バックグラウンドクラスは、アプリが正しく識別することを気にしないポーズのためのキャッチオールカテゴリです。

私のデモでは、これには1つまたは2つではない多くのハンドポジションが含まれています。

明確に定義されたバックグラウンドクラスは、ユーザーが重要なポーズをとっていないときにアプリが知るのに役立ちます。

背景クラスを構成する画像には2つのタイプがあります。

まず、アプリに分類したい重要なポーズではないハンドポーズのランダムな品揃えがあります。

これらのポーズには、肌のトーン、年齢、性別、照明条件の多様なセットを含める必要があります。

第二に、アプリに分類したい表現と非常によく似た一連のポジションがあります。

これらの過渡的なポーズは、ユーザーがアプリが気にかけている表現の1つに向かって手を動かしているときに頻繁に発生します。

オープンパームのポーズをとるために手を挙げると、アプリがオープンパームを検討してほしいものと似ているが、そうではないいくつかの位置を移行していることに気付きます。

これらの位置は、その後も腕を下げるときに発生します。

これはオープンパームに特有のものではありません。

腕を上げて2つのポーズをとるときも、それを下げるときも同じタイプの過渡的なポーズが起こります。

これらの過渡的なポーズはすべて、ランダムなポーズとともにバックグラウンドクラスに追加する必要があります。

この組み合わせにより、モデルはアプリが気にしているポーズと他のすべての背景のポーズを適切に区別できます。

トレーニングデータが収集され、分類されたので、Create MLアプリを使用してモデルをトレーニングする時が来ました。

だから、手を汚しましょう。

前回のデモでモデルをトレーニングするために使用した既存のCreate MLプロジェクトから始めます。

トレーニングの結果は良さそうだったので、このモデルはかなり良いパフォーマンスを発揮することを期待しています。

幸いなことに、Create MLアプリでは、アプリに統合する前にモデルをプレビューできます。

プレビュータブでは、ハンドポーズ分類器のために、このリリースでライブプレビュー機能が追加されていることがわかります。

ライブプレビューは、FaceTimeカメラを利用して、リアルタイムで予測を表示します。

ライブプレビューを使用して、このモデルがポーズ1と2を正しく分類していることを確認できます。

また、Open Palmも正しく分類したいのですが、現在、そのポーズをバックグラウンドクラスの一部として分類しています。

このモデルをトレーニングするために使用したデータソースでは、Open Palmクラスは含まれておらず、1、Two、およびBackgroundのクラスのみが含まれていることに注意してください。

今すぐOpen Palmをサポートする新しいモデルをトレーニングしましょう。

まず、これのための新しいモデルソースを作成します。

このトレーニングに使用したいOpen Palmクラスを含むデータセットがあります。

このデータセットを選択します。

この新しいデータソースに飛び込むと、Open Palmのエントリと以前のデータセットのクラスが含まれていることがわかりました。

モデルソースに戻って、トレーニングデータを拡張し、モデルをより堅牢にするために、いくつかの拡張を追加したいと思います。

それでおそれ。電車に乗る時間です。

トレーニングが始まる前に、Create MLは予備的な画像処理と特徴抽出を行う必要があります。

私たちはCreate MLに80回の反復のために訓練するように言いました。

これは良い出発点ですが、データセットに基づいてその数を微調整する必要があるかもしれません。

このプロセスにはしばらく時間がかかります。

幸いなことに、私はすでにモデルを訓練しました。

今それをつかませてください。

ライブプレビューは、新しく訓練されたモデルがオープンパームのポーズを正しく識別するようになったことを示しています。

そして、念のため、私はそれが1つと2つのポーズを識別し続けることを確認するつもりです。

簡単じゃなかった？

私はこのモデルを同僚のブリタニーに送るつもりです、そして彼女はそれを彼女のアプリに統合することについて話します。

ネイサン、モデルをありがとう。

こんにちは。私はブリタニー・ワイナートです。

そして、私はビジョンフレームワークチームのメンバーです。

最初にハンドポーズの分類について学んだとき、私はすぐにこれを使って手で特殊効果を作成できると思いました。

CoreMLを使用して手のポーズを分類し、ビジョンを使用して手を検出して追跡することは、一緒に使用するのに最適な技術であることを知っています。

自分自身に超大国を与えることができるかどうか見てみましょう。

私はすでにそれを行うことができるデモのためのパイプラインの最初のドラフトを作成しました。

それを見直しましょう。

まず、フレームのストリームを提供するカメラを用意し、各フレームをビジョンリクエストに使用して、フレーム内の手の位置とキーポイントを検出します。

DetectHumanHandPoseRequestは、私たちが使用しているリクエストになります。

フレーム内で見つかった各ハンドのHumanHandPoseObservationを返します。

CoreMLハンドポーズ分類モデルに送信するデータは、MLMultiArrayとkeypointsMultiArrayと呼ばれるHumanHandPoseObservationのプロパティです。

ハンドポーズ分類器は、信頼スコアでトップ推定ハンドアクションラベルを返送し、アプリ内のアクションを決定するために使用できます。

アプリの高レベルの詳細を詳しく確認したので、コードを見てみましょう。

ビジョンを使ってフレーム内の手を検出する方法から始めましょう。

私たちがやりたいことは、VNDetectHumanHandPoseRequestのインスタンスが1つだけ必要で、片手を検出するだけでよいので、maximumHandCountを1つに設定します。

maximumHandCountを設定し、フレームで指定されたよりも多くのハンドがある場合、アルゴリズムは代わりにフレーム内の最も目立つ中央の手を検出します。

maximumHandCountのデフォルト値は2です。

後でリクエストの更新に驚かないように、ここでリビジョンを設定することをお勧めします。

しかし、リンクされているSDKでサポートされている最新のアルゴリズムを常にオプトインしたい場合は、設定する必要はありません。

また、メモとして、ARKitを介してARSessionによって取得されたすべてのフレームの検出を行いますが、これはカメラフィードからフレームを取得する方法の1つにすぎません。

どちらの方法でもかからかおうか。

AVCaptureOutputも有用な代替手段になります。

受信したフレームごとに、特定の画像のすべての要求を処理するVNImageRequestHandlerを作成する必要があります。

ハンドポーズリクエストの結果プロパティには、以前のリクエストで指定したように、最大ハンド番号1までのVNHumanHandPoseObservationsが入力されます。

リクエストで手のポーズが検出されない場合は、現在表示されているエフェクトをクリアすることをお勧めします。

そうでなければ、片手で観察します。

次に、CoreMLモデルを使用して、ハンドポーズが何であるかを予測します。

効果のレンダリングを不安にしたくないので、すべてのフレームで予測したくありません。

間隔をあけて予測を行うと、よりスムーズなユーザーエクスペリエンスが得られます。

予測をしたいときは、MLMultiArrayをHand Pose CoreMLモデルに渡すことから始め、返された単一の予測からトップラベルと信頼を取得します。

ラベルが高レベルの自信を持って予測された場合にのみ、表示される効果の変更をトリガーしたい。

これはまた、効果があまりにも早くオンとオフになり、不安になる可能性のある行動から保護するための鍵です。

ここでは、背景分類は、信頼のしきい値を非常に高く保つことで、私たちを助けています。

非常に自信を持って予測されている場合は、レンダリングするeffectNodeを設定できます。

自信を持って予測されない場合は、私の手がしていることに合わせて画面上の効果を止めたいです。

私たちが持っているものを試してみましょう。

私がワンポーズに手を入れると、それは単一のエネルギービーム効果をトリガーするはずです。

とてもかっこいい！

モデルは、私がポーズワンを作り、その効果を引き起こしたことがわかります。

それが私の指に従えば、さらに涼しいでしょうが。

それが私の指の特定のポイントでレンダリングされたら、さらに良いです。

コードに戻って変更しましょう。 では変更しましょう。

私たちがする必要があるのは、手の重要なポイントの位置をグラフィックアセットに供給することです。これは、ビューを使用して正規化されたキーポイントをカメラビュースペースに変換することを意味します。

また、自信スコアを見て、保存するキーポイントの剪定を検討することもできます。

ここでは、人差し指の指先しか気にしません。

ビジョンは正規化された座標を使用するため、キーポイントを座標空間に変換する必要があります。

また、ビジョンの原点は画像の左下隅にあるので、変換を行うときは覚えておいてください。

最後に、インデックスの場所を保存し、キーポイントが見つからない場合、デフォルトはnilです。

効果のレンダリングを担当するコードと、指をたどるように調整する方法を見てみましょう。

グラフィカルオブジェクトの場所が設定されている場所を見つけたいです。

setLocationForEffectsは、すべてのフレームを非同期に呼び出されています。

デフォルトでは、ビューの中央に表示される効果を設定します。

以前からindexFingerTipLocation CGPointに切り替えると、意図した効果が得られます。

すごい！

これはかっこよく見え始めています。

もう一歩踏み出しましょう。

超大国を取り巻くより興味深いグラフィカルなストーリーを作成するには、私たちのアプリケーションでハンドポーズ分類のいくつかを利用するのが良いでしょう。

この場合、分類2とオープンパームを選択します。

これらのポーズの両方が検出されたときに行動を起こすために、私はすでにアプリケーションを拡張しました。

ここでは、前に示したように、ポーズワンのために、人差し指の先端に表示されるようにエネルギービームを集中させています。

ポーズ2のために私の中指と人差し指の先端に2つのエネルギービーム。

そして、最後のエネルギービームはハンドポーズオープンパームによってトリガーされ、中指の下部にあるキーポイントと手首のキーポイントの間に固定されています。

わかった。

ネイサンと私が紹介したものはすべて、あなた自身のハンドポーズ分類モデルを完全に統合する手順をカバーしています。

ビジョンには、役に立つかもしれないもう1つの新機能がありますので、このアプリの機能のトリガーと制御に役立つAPIを紹介します。

ビジョンは、ユーザーがHumanHand- PoseObservation: chiralityで左手と右手を区別できる新しいプロパティを導入しています。

これは、HumanHandPoseObservationがどの手であるかを示す列挙型であり、左手、右手、不明の3つの値の1つである可能性があります。

おそらく最初の2つの値の背後にある意味を推測できますが、未知の値は、古いバージョンのHumanHandPoseObservationがデシリアライズされ、プロパティが設定されていない場合にのみ表示されます。

ネイサンが先に述べたように、WWDC 2020セッション「視覚で身体と手のポーズを検出する」を参照することで、ビジョンハンドポーズ検出に関する詳細情報を得ることができます。

余談ですが、フレーム内で検出された各ハンドについて、基礎となるアルゴリズムは各ハンドのキラリティを別々に予測しようとします。

これは、片手の予測がフレーム内の他の手の予測に影響を与えないことを意味します。

キラリティを使用するコードがどのように見えるかの例をお見せしましょう。

VNDetectHumanHandPoseRequestを作成して実行するためのセットアップについては、すでに説明しました。

リクエストを実行した後、オブザベーションはEumプロパティのキラリティを持ち、それを使用してアクションを実行したり、ビジョンハンドポーズオブザベーションをソートしたりできます。

これまでのすべては、ハンドポーズ分類の使い方についてでした。

しかし、ネイサンが先に述べたように、ハンドアクション分類は今年のもう一つの新技術です。

それについてあなたに話すためのゲッピーです。

ありがとう、ブリタニー。

こんにちは、私の名前はGeppy Parzialeで、Create MLチームの機械学習エンジニアです。

ハンドポーズ分類に加えて、今年、Create MLはハンドアクション分類を実行するための新しいテンプレートを導入し、アプリでの使用方法を紹介します。

このため、私はいくつかのハンドアクションでブルターニュの超大国のデモを拡張し、ハンドポーズとハンドアクションのいくつかの重要な違いを強調します。

ハンドアクションとボディアクションは2つの非常によく似たタスクであるため、追加情報と比較については、WWDC 2020のセッション「Create MLでアクション分類器を構築する」を参照してください。

しかし、今、ハンドアクションとは何かを説明しましょう。

ハンドアクションは、MLモデルが手の動き中に分析する必要がある一連のハンドポーズで構成されています。

シーケンス内のポーズの数は、最初から最後までハンドアクション全体をキャプチャするのに十分な大きさでなければなりません。

ビデオを使用してハンドアクションをキャプチャします。

ネイサンが先に示したように、ハンドアクション分類器のトレーニングは、ハンドポーズ分類器のトレーニングと同じですが、いくつかの小さな違いがあります。

静止画像は手のポーズを表しますが、ビデオは手の動きをキャプチャして表現するために使用されます。

したがって、ハンドアクション分類器を訓練するには、各ビデオがハンドアクションを表す短いビデオを使用します。

これらのビデオは、各フォルダ名がアクションクラスを表すフォルダに整理できます。

また、分類器に認識させたいアクションとは異なるアクションを持つビデオを含む背景クラスを含めることを忘れないでください。

代替表現として、すべてのサンプルビデオファイルを1つのフォルダに追加できます。

次に、CSVまたはJSON形式を使用して注釈ファイルを追加します。

注釈ファイルの各エントリは、ビデオファイルの名前、関連するクラス、ハンドアクションの開始時刻と終了時刻を表します。

また、この場合、バックグラウンドクラスを含めることを忘れないでください。

多かれ少なかれ、同じ長さのビデオでモデルを訓練することを忘れないでください。

実際、アクション期間をトレーニングパラメータとして指定し、Create MLは、提供した値に従って連続した数のフレームをランダムにサンプリングします。

また、ビデオフレームレートとトレーニングの反復を提供することもできます。

それに加えて、このアプリは、モデルがよりよく一般化し、精度を高めるのに役立つさまざまな種類のデータ拡張を提供します。

特に、タイム補間とフレームドロップは、実際のユースケースに近いビデオバリエーションを提供するために、ハンドアクション分類に追加された2つの拡張です。

だから、私はすでに私のデモのためにハンドアクション分類器を訓練しました--それを実際に見てみましょう。

さて、私はスーパーヒーローなので、エネルギー源が必要です。

これが私のです。

ここでは、手のポーズを使ってエネルギー源を視覚化します。

しかし、今、私の超能力を使ってそれを活性化させてください。

この場合、私はハンドアクションを使っています。

これはかっこいい。

そして今、ハンドポーズとハンドアクション分類器が同時に実行されています。

私はビジョンの新しいキラリティ機能を活用し、左手を手のポーズに、右手を手のアクションに使っています。

これは超かっこいい。

したがって、これはCreate MLが適用される最適化、Apple Neural Engineのすべてのパワーを解き放つためのすべてのモデルへのトレーニング時間のために可能です。

そして今、私の現実の世界に戻って、Create ML Hand Action Classifierを私のデモに統合する方法を説明します。

まず、モデルの入力を見てみましょう。 

ハンドアクション分類器をアプリに統合するときは、モデルに正しい数の予想されるハンドポーズを提供する必要があります。

私のモデルは、XCodeプレビューで検査できるので、サイズ45 x 3 x 21のマルチアレイを期待しています。

ここでは、45は分類器がアクションを認識するために分析する必要があるポーズの数です。

21は、各ハンドのビジョンフレームワークによって提供されるジョイントの数です。

最後に、3はx座標とy座標と各ジョイントの信頼値です。

45はどこから来たのですか?

これは予測ウィンドウのサイズであり、トレーニング時に使用されたビデオの長さとフレームレートに依存します。

私の場合、私は30fpsと1.5秒の長さで録画されたビデオでモデルを訓練することにしました。

これは、モデルがハンドアクションごとに45のビデオフレームで訓練されたことを意味するので、推論中、モデルは同じ数のハンドポーズを期待しています。

推論時に到着する手のポーズの頻度に関して、追加の考慮を考慮する必要があります。

推論中にモデルに提示されたハンドポーズのレートが、モデルをトレーニングするために使用されるポーズのレートと一致することが非常に重要です。

私のデモでは、ARKitを使いました。

ARKitは60fpsでフレームを提供し、私の分類器は30fpsのビデオで訓練されたので、私は毎秒ごとに到着するポーズの数を半分にしなければなりませんでした。

そうでなければ、分類器は間違った予測を提供する可能性があります。

ソースコードに飛び込んで、これを実装する方法を紹介しましょう。

まず、カウンターを使用して、ビジョンから到着するポーズのレートを60fpsから30fpsに減らし、モデルが正常に動作することを期待しているフレームレートに一致させます。

次に、シーン内の各手の関節とキラリティを含む配列を取得します。

次に、デモでは右手を使ってハンドアクションで効果の一部をアクティブにするので、左手からキーポイントを破棄します。

さて、今、私は分類器のために手のポーズを蓄積する必要があります。

これを行うには、FIFOキューを使用して45のハンドポーズを蓄積し、キューに常に最後の45のポーズが含まれていることを確認します。

キューは最初は空です。

新しいハンドポーズが到着したら、それをキューに追加し、キューがいっぱいになるまでこのステップを繰り返します。

キューがいっぱいになったら、そのコンテンツ全体を読み始めることができます。

ビジョンから新しい手のポーズを受け取るたびにキューを読むことができました。

しかし、覚えておいてください、今、私は毎秒30フレームを処理しています。

そして、ユースケースによっては、これはリソースの無駄になる可能性があります。

そのため、フレーム数を定義した後、別のカウンターを使用してキューを読み取ります。

アプリケーションの応答性と取得したい1秒あたりの予測数の間のトレードオフとして、キューサンプリングレートを選択する必要があります。

この時点で、MLMultiArrayで整理された45のハンドポーズのシーケンス全体を読み、それを分類器に入力してハンドアクションを予測します。

次に、予測ラベルと信頼値を抽出します。

最後に、信頼値が定義されたしきい値よりも大きい場合は、パーティクル効果をシーンに追加します。

したがって、Create ML Hand Action Classifierをアプリに統合するときは、モデルが期待するフレームレートでハンドポーズのシーケンスを必ず入力してください。

分類器のトレーニングに使用されたビデオと同じフレームレートを一致させます。

ファーストインファーストアウトキューを使用して、モデル予測へのハンドポーズを収集します。

適切なフレームレートでキューを読み取ります。

Create MLで訓練されたハンドアクションモデルで構築するすべてのクールなアプリケーションを見るのを楽しみにしています。

そして今、最終的な検討と要約のためにネイサンに戻ります。

ありがとう、ジェッピー。

あなたとブリタニーはあなたのアプリで素晴らしい仕事をしました。

試してみるのが楽しみです。

しかし、物事が手に負えなくなる前に、ユーザーに高品質の体験を確実にするために心に留めておくべきことがいくつかあります。

手がカメラからどれだけ離れているかに注意してください。

最良の結果を得るには、距離を11フィート、または3 1/2メートル以下に保つ必要があります。

また、暗すぎるか明るすぎるか、極端な照明条件を避けるのが最善です。

かさばる、緩い、またはカラフルな手袋は、手のポーズを正確に検出することが困難になり、分類品質に影響を与える可能性があります。

すべての機械学習タスクと同様に、トレーニングデータの質と量が重要です。

このセッションで示されたハンドポーズ分類器では、クラスごとに500枚の画像を使用しました。

ハンドアクション分類器では、クラスごとに100本のビデオを使用しましたが、ユースケースのデータ要件は異なる場合があります。

最も重要なことは、モデルがアプリに表示される予想されるバリエーションをキャプチャするのに十分なトレーニングデータを収集することです。

今、要約するのに良い時期のように感じます。

それで、私たちは何を学びましたか?

さて、2021年から、人間の手の表情を解釈するアプリを構築できます。

私たちは、手の表情、ポーズ、行動の2つのカテゴリーの違いについて議論しました。

モデルをトレーニングするためにCreate MLアプリで使用するために、バックグラウンドクラスを含むトレーニングデータを準備する方法について話しました。

訓練を受けたモデルをアプリに統合する方法について話しました。

そして最後に、複数のモデルを1つのアプリに組み込み、キラリティを使用して手を区別することについて話しました。

明らかに、今日のデモは表面を傷つけるだけです。

ビジョンフレームワークは、手の存在、ポーズ、位置、キラリティを検出するための強力な技術です。

Create MLは、手のポーズと手のアクションを訓練し、分類する楽しくて簡単な方法です。

一緒に使用すると、人類の最も強力で表現力豊かなツールの1つに深い洞察を提供し、あなたが彼らと何をするかを見るのが待ちきれません。

さようなら。

[明るい音楽]。