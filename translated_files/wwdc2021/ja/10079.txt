10079

こんにちは、私の名前はBharathで、AppleのCore Audioチームに所属しています。

そして今日は、PHASEのジオメトリ対応オーディオについてお話ししたいと思います。

新しいPHASEフレームワークを使用したい理由について説明します。

フレームワークが提供する機能のいくつかを紹介します。

その後、同僚のDavid Thallに渡して、このAPIを使用した概念とサンプルのユースケースを深く説明します。

始めましょう。

オーディオは、あらゆるゲーム体験の重要な側面です。

ヘッドフォンの空間オーディオは、全体的なゲームプレイを次のレベルに引き上げ、より関与していると感じさせます。

今日のゲームでは、物理学、アニメーション、視覚効果などのエンジンのさまざまなサブシステムがすべて互いに通信し、プレイヤーの行動に基づいてゲームフローやストーリーを前進させます。

ただし、オーディオサブシステムは通常、他のサブシステムとは別に管理および駆動されます。

彼らはまた、シミュレーションを常に認識していないミドルウェアを介して作成されることもあります。

オーディオアセットは、ゲームのビジュアルに合ったオーディオストーリーを伝えるために、ポストプロダクションされ、事前に焼かれ、手作業で調整されています。

ビジュアルが進化するにつれて、オーディオシステム、サウンドデザイン、および関連する資産は、幅広いプラットフォームでオーディオ体験の結束力を維持するために再生する必要があります。

この反復開発プロセスは、ゲーム開発中に考慮する必要があります。

これは通常、ゲームプレイの視覚的な側面に遅れをとっているオーディオ体験につながります。

より良いゲームオーディオ体験を提供するために、オーディオシステムを他のサブシステムに近づけたいと考えています。

また、これらすべてのサポートされているデバイスで一貫した空間オーディオ体験を提供できるアプリケーションを簡単に作成できるようにしたいと考えています。

それでは、新しいオーディオフレームワークPHASEとその機能について紹介しましょう。

PHASEは、オーディオエンジンにジオメトリ情報を提供し、サウンドデザインに優しいイベント駆動型オーディオ再生システムを構築し、サポートされているすべてのデバイスで一貫した空間オーディオ体験を自動的に提供し、既存のオーサリングソリューションやパイプラインと統合できるアプリケーションを書くことを可能にする新しいフレームワークです。

PHASEについてもっと学ぶ前に、一般的に使用されるゲームオーディオワークフローを確認しましょう。

これは、リスナー、音源、流れる小川、オクルーダー、納屋の屋外シーンの例です。

オクルーダーは、ソースとリスナーの間の音を減衰させる可能性のあるシーン内のオブジェクトです。

通常、エリアに沿って複数のポイントソースを配置します。

リスナーが移動するにつれて、レイトレーシングなどのさまざまな技術を使用して、ポイントソース間の適切なフィルタリングとミックス比を決定し、優れたオーディオ体験を提供するためにそれらの間で手動でブレンドする必要があります。

ゲーム開発の自然な過程で、例のシーンの納屋など、ビジュアルシーンが変更された場合は、ビジュアルシーンの変更に合わせてオーディオエクスペリエンスを更新して手で調整する必要があります。

オーディオソースがシーンに基づいて管理およびミックスする必要があるポイントではなく、オーディオシステムが自動的に管理できる領域や音量から発せられる音としてアプリケーションを構築できると想像してみてください。

フェーズはまさにそれを行います。

体積源を紹介します。

新しいフレームワークは、音源を幾何学的形状として基礎となるオーディオエンジンに渡すことができるAPIを提供します。

体積音源に加えて、シーン内のオクルーダーを幾何学的形状として渡すこともできます。

また、プリセットのセットから音響材料の特性を選択し、オクルーダーに取り付けることもできます。

PHASEフレームワークでは、アプリケーションの要求に応じて、ポイントソースの媒体伝播とソース指向性を設定することもできます。

私たちはここで例として屋外のシーンを見ています。

ただし、アプリケーションに屋内シーンがある場合は、プリセットのライブラリから早期反射と後期残響プロパティを選択できます。

さまざまな音源、オクルーダー、リスナーがどこにあるかをフレームワークに伝えると、PHASEは重い持ち上げを支援し、シーン内のさまざまな音源の閉塞と拡散効果をモデル化します。

アプリケーションのオーディオシステムはジオメトリを認識しているため、ゲーム開発が進化するにつれて、視覚的なシーンの変化にはるかに迅速に適応できます。

ジオメトリの認識に加えて、PHASEはイベントベースのインタラクティブ再生システムも提供します。

サウンドイベントは、PHASEのオーディオ再生イベントを記述するための基本的な単位です。

オーディオアセットの選択、ブレンド、再生をカプセル化します。

サウンドイベントは、ワンショット再生、ループなどの単純なイベントから、再生イベントのサブツリーをブレンドまたは切り替えることができるツリーとして整理された複雑なシーケンスまでさまざまです。

足音を弾く簡単な例を見てみましょう。

ここでは、砂利の上のフットステップの3つの異なる音から自動的に選択するランダムなノードがあります。

私たちは、布の錆のための別のサウンドイベントを持つことができます。

両方のイベントツリーは、この例では「近く」の別の木に接ぎ木して、布のざわめと足音のミックスを再生することができます。

この例では、キャラクターが遠くにいるときに異なる音のセットを再生するために、別の木「遠く」を持つことができます。

近い木と遠い木の組み合わせは、ゲームプレイに基づいて距離で制御できるようになりました。

たとえば、雪や草の足跡のようなイベントツリーを追加し、ユーザーインタラクションや物理学やアニメーションなどのサブシステムによってトリガーできる再生イベントの複雑なシーケンスを構築できます。

PHASEを使用すると、サウンドは単純なチャンネル構成、または向きと位置を持つ3D空間、または音が向きを持っているが位置がないアンビエントベッドとして再生できます。

基礎となるエンジンは、サポートされているiOS、macOSデバイス、およびAir Podsファミリーのヘッドフォンですでに利用可能な空間オーディオレンダリング機能に基づいて構築されています。

これにより、サポートされているすべてのデバイスで一貫した空間オーディオ体験を提供するアプリケーションを自動的に構築できます。

次に、David ThallにPHASEを深く掘り下げてもらい、いくつかの例のユースケースの概念とAPIについてもっと話したいと思います。

みなさん、こんにちは。私の名前はDavid Thallで、PHASEのシステムアーキテクト兼開発リードです。

今日はPHASE APIを順を追って説明します。

このセクションでは、一般的な概念を紹介します。

これに続いて、私はあなたが始めるためにいくつかのサンプルユースケースを実行します。

PHASE APIは3つの主要な概念に分けることができます。

エンジンは資産を管理します。

ノードは再生を制御します。

そして、ミキサーは空間化を制御します。

PHASEエンジンは3つの主要なセクションに分けることができます。

アセットレジストリ、シーングラフ、レンダリング状態。

エンジンのライフサイクルを通じて、エンジンに資産を登録および登録解除します。

今日、PHASEは健全な資産と健全なイベント資産の登録をサポートしています。

サウンドアセットは、オーディオファイルから直接ロードするか、独自のアセットに生のオーディオデータとしてパックして埋め込み、エンジンに直接ロードすることができます。

サウンドイベントアセットは、サウンド再生を制御する1つ以上の階層ノードと、空間化を制御するダウンストリームミキサーのコレクションです。

シーングラフは、シミュレーションに参加するオブジェクトの階層です。

これには、リスナー、ソース、およびオクルーダーが含まれます。

リスナーは、シミュレーションを聞く空間内の場所を表すオブジェクトです。

ソースは、音がどこから来たのかを表すオブジェクトです。

Bharathが先に述べたように、PHASEはポイントソースと体積ソースの両方をサポートしています。

オクルーダーは、環境内を移動する音の伝達に影響を与えるシミュレーションのジオメトリを表すオブジェクトです。

オクルーダーには、音の吸収と伝達方法に影響を与える材料も割り当てられます。

PHASEには、段ボール箱からガラス窓、レンガの壁まですべてをシミュレートするために、オクルーダーに割り当てることができる材料プリセットのライブラリが付属しています。

シーンにオブジェクトを追加すると、それらを階層に整理し、直接的または間接的にエンジンのルートオブジェクトにアタッチします。

これにより、フレームからフレームまでシミュレーションに参加できます。

レンダリング状態は、サウンドイベントとオーディオIOの再生を管理します。

最初にエンジンを作成すると、オーディオIOが無効になります。

これにより、オーディオIOを実行することなく、アセットを登録したり、シーングラフを作成したり、サウンドイベントを構築したり、他のエンジン操作を実行したりできます。

サウンドイベントを再生する準備ができたら、オーディオIOを内部的に起動するエンジンを起動できます。

同様に、サウンドイベントの再生が終わったら、エンジンを停止することができます。

これにより、オーディオIOが停止し、サウンドイベントの再生が停止します。

PHASEのノードは、オーディオコンテンツの再生を制御します。

ノードは、オーディオ再生を生成または制御するオブジェクトの階層的なコレクションです。

ジェネレータノードはオーディオを生成します。

それらは常にノード階層のリーフノードです。

制御ノードは、空間化の前に発電機がどのように選択され、混合され、パラメータ化されるかのロジックを設定します。

コントロールノードは常に親ノードであり、複雑なサウンド設計シナリオのために階層に整理することができます。

サンプラーノードはジェネレータノードの一種です。

サンプラーは、登録されたサウンドアセットを再生します。

構築が完了したら、サンプラーノードにいくつかの基本的なプロパティを設定して、正しく再生させることができます。

再生モードは、オーディオファイルの再生方法を決定します。

再生モードをOneShotに設定すると、オーディオファイルは一度再生され、自動的に停止します。

これは、効果音をトリガーするなど、「火事で忘れる」シナリオで使用できます。

再生モードをループに設定すると、サンプラーを明示的に停止するまで、オーディオファイルは無期限に再生されます。

淘汰オプションは、音が聞こえなくなったときに何をすべきかをPHASEに伝えます。

淘汰オプションを終了するように設定すると、音が聞こえなくなると自動的に停止します。

淘汰オプションをスリープに設定すると、音が聞こえなくなるとレンダリングが停止し、聞こえるようになると再びレンダリングが開始されます。

これにより、エンジンによって淘汰されたときに手動で起動したり停止したりする必要がなくなります。

キャリブレーションレベルは、デシベルSPLで音の実際のレベルを設定します。

PHASEは4種類の制御ノードもサポートしています。

これらには、ランダム、スイッチ、ブレンド、コンテナノードが含まれます。

ランダムノードは、重み付けされたランダムな選択に従って、その子の1つを選択します。

たとえば、この場合、左のサンプラーは、次にサウンドイベントがトリガーされたときに選択されるという右のサンプラーに対して4:1のオッズを持っています。

スイッチノードは、パラメータ名に基づいて子を切り替えます。

たとえば、地形の切り替えを「クリーキーな木材」から「柔らかい砂利」に変更できます。

次にサウンドイベントがトリガーされると、パラメータ名に一致するサンプラーが選択されます。

ブレンドノードは、パラメータ値に基づいて子間でブレンドされます。

たとえば、ブレンドノードにウェットパラメータを割り当てることができます。これにより、ドライエンドの大きなフットステップと静かなスプラッシュと、ウェットエンドの静かなフットステップと大きなスプラッシュをブレンドできます。

コンテナノードは、すべての子を一度に再生します。

たとえば、足音を再生する1つのサンプラーと、ラフリングゴーテックスジャケットの音のように、衣服の音を再生する別のサンプラーを持つことができます。

コンテナがトリガーされるたびに、両方のサンプラーが同時に再生されます。

オーディオコンテンツの位相制御空間化のミキサー。

PHASEは現在、チャンネル、アンビエント、および空間ミキサーをサポートしています。

チャンネルミキサーは、空間化や環境への影響なしにオーディオをレンダリングします。

ステレオ音楽やセンターチャンネルの物語の対話など、出力デバイスに直接レンダリングする必要がある通常のステムベースのコンテンツには、チャンネルミキサーを使用します。

アンビエントミキサーは、外部化でオーディオをレンダリングしますが、距離モデリングや環境への影響はありません。

リスナーが頭を回転させると、音は宇宙の同じ相対的な場所から引き続き来ます。

環境ではシミュレートされていないマルチチャンネルコンテンツにはアンビエントミキサーを使用しますが、大きな森で鳴くコオロギの背景など、宇宙のどこかから来ているように聞こえるはずです。

空間ミキサーは完全な空間化を実行します。

音源がリスナーに対して相対的に動くと、パン、距離モデリング、指向性モデリングアルゴリズムに基づいて、知覚された場所、レベル、周波数応答の変化が聞こえます。

これに加えて、ジオメトリを意識した環境影響は、ソースとリスナーの間のパスに適用されます。

ヘッドフォンを着用している場合は、バイノーラルフィルターの適用によって外部化も得ます。

完全な環境シミュレーションに参加すべき音には、空間ミキサーを使用してください。

空間ミキサーは、2つのユニークな距離モデリングアルゴリズムをサポートしています。

距離にわたる自然な減衰のために、標準的な幾何学的拡散損失を設定できます。

また、好みに合わせて効果を増減することもできます。

たとえば、値を下げることは、遠くで会話をブームにしたい場合に役立ちます。

スペクトルの反対側では、距離にわたって減衰の完全な断片的な湾曲したセグメントを追加することができます。

たとえば、範囲の開始時と終了時に自然な距離減衰を持つセグメントのセットを構築できますが、中央の減衰を減らして、増加した距離で重要な対話を聞こえるようにすることができます。

ポイントソースの場合、空間ミキサーは2つの異なる指向性モデリングアルゴリズムをサポートしています。

カーディオイド指向性モデリングを空間ミックスに追加できます。

いくつかの簡単な修正を使用して、カーディオイド指向性パターンまたはハイパーカーディオイドパターンの音響弦楽器の音で人間のスピーカーをモデル化することができます。

コーン指向性モデリングを追加することもできます。

このクラシックモードでは、指向性フィルタリングを特定の回転範囲内に制限することができます。

空間ミキサーは、空間パイプラインに基づくジオメトリを意識した環境効果もサポートします。

空間パイプラインは、有効または無効にする環境影響と、それぞれへの送信レベルを選択します。

PHASEは現在、ダイレクトパス伝送、早期反射、後期リバーブをサポートしています。

直接パス伝送は、ソースとリスナーの間の直接パスと閉塞パスをレンダリングします。

閉塞音では、一部のエネルギーは材料に吸収され、他のエネルギーは物体の反対側に伝達されることに注意してください。

初期の反射は、直接経路に強度変更と着色の両方を提供します。

これらは通常、壁や床からの鏡面反射から構築されています。

より大きな空間では、彼らはまた、経験に顕著なエコーを追加します。

後期リバーブは環境の音を提供します。

これは、空間の最終的な可聴表現に合体する拡散散乱エネルギーの密な蓄積です。

部屋のサイズと形状の手がかりを提供することに加えて、それはまた、あなたに包み込みの感覚を与える責任があります。

PHASEエンジン、ノード、ミキサーの背後にある概念を見直したので、これらの概念をいくつかのサンプルユースケースとまとめる時が来ました。

このセクションでは、オーディオファイルの再生、空間オーディオ体験の構築、行動サウンドイベントの構築について説明します。

これらの3つの重要な分野は、PHASEの機能の幅広い概要を提供し、最初に穏やかに紹介し、中間と終わりに向かってより興味深い機能を深く掘り下げます。

物事を始めるために、オーディオファイルを再生する方法を紹介します。

まず、PHASEエンジンインスタンスを作成しましょう。

次に、オーディオファイルのURLを取得し、サウンドアセットをPHASEに登録します。

後で参照できるように、「ドラム」という名前を付けます。

ここでは、エンジンを作成し、サウンドアセットをコードに登録します。

まず、自動更新モードでPHASEエンジンインスタンスを作成します。

これは物事を稼働させるための好ましいモードなので、簡単な再生をデモするためにここで使用しています。

ゲームがフレーム更新とのより正確な同期を必要とする場合、手動モードが進むべき道であることに注意してください。

詳細については、ドキュメントを確認してください。

次に、アプリケーションバンドルに保存されているオーディオファイルへのURLを取得します。

これは、準備されたドラムループサンプルを備えたモノラル24ビット、48kHzのWAVファイルです。

サウンドアセットをエンジンに登録するときは、いくつかの追加の引数を提供します。

後で参照できるように、サウンドアセットに一意の名前を付けます。

サウンドアセット内のオーディオデータは、リアルタイムでメモリにストリーミングするのではなく、常駐メモリにプリロードする必要があることを指定します。

ドラムループはかなり短いので、これは問題ないはずですし、短い連続で数回再生したいかもしれません。

私はまた、出力デバイスの較正されたラウドネスのためにサウンドアセットを正規化することを選択しています。

一般的に、入力を正規化することをお勧めします。

これにより、サンプラーに割り当ててターゲット出力レベルを設定すると、コンテンツをミックスしやすくなります。

エンジンにサウンドアセットを登録したので、サウンドイベントアセットを構築します。

まず、チャンネルレイアウトからチャンネルミキサーを作成します。

次に、サンプラーノードを作成します。

サンプラーノードは、登録されたサウンドアセットの名前とダウンストリームチャンネルミキサーへの参照を取ります。

次に、サンプラーノードにいくつかの基本的なプロパティを設定して、正しく再生します。

再生モードは、サンプラーがオーディオファイルをループするかどうかを設定し、キャリブレーションレベルはミックス内のサンプラーの知覚ラウドネスを設定します。

サンプラーノードの出力をチャンネルミキサーの入力に接続し、いくつかの基本的なパラメータを設定したので、サウンドイベントアセットをエンジンに登録する時が来ました。

この場合、サウンドイベントアセットをdrumEventという名前で登録し、後で参照するために使用します。

ここでは、健全なイベントアセットをコードに登録します。

モノラルChannelLayoutTagからchannelLayoutを作成します。

次に、モノラルchannelLayoutでチャンネルミキサーを初期化します。

次に、サンプラーノードを作成し、以前にエンジンに登録したモノラルドラムアセットを指す名前のドラムを渡します。

サンプラーノードは、下流のチャネルミキサーにルーティングされます。

再生モードをループに設定します。

これにより、コードから明示的に停止するまで、サウンドが再生され続けます。

CalibrationModeをrelativeSplに設定し、レベルを0デシベルに設定します。

これにより、経験のための快適なリスニングレベルが保証されます。最後に、soundEventAssetをエンジンに登録し、drumEventという名前を渡すので、再生用のサウンドイベントの作成を開始するときに後で参照できます。

サウンドイベントアセットが登録されたら、インスタンスを作成して再生を開始できます。

最初に行うことは、drumEventという名前の登録サウンドイベントアセットからサウンドイベントを作成することです。

サウンドイベントがあるので、先に進んでエンジンを始動します。

これにより、オーディオIOが開始され、出力デバイスに供給されるオーディオを聴くことができます。

最後に、サウンドイベントを開始します。

この時点で、ロードされたサウンドアセットはサンプラーを介して再生され、チャンネルミキサーにルーティングされ、現在の出力フォーマットに再マップされ、出力デバイスを介して再生されます。

ここでは、コードでサウンドイベントを開始します。

サウンドイベントアセットは、登録されたサウンドアセットの名前から構築されます。

ここで、私はdrumEventを通過しています。

先に進んでエンジンを始動し、オーディオIOを起動し、サウンドイベントを開始します。

サウンドイベントの再生が終わったら、エンジンをクリーンアップできます。

まず、サウンドイベントを中止します。

では、エンジンを止めます。

これにより、オーディオIOが停止し、サウンドイベントの再生が停止します。

次に、drumEventという名前のサウンドイベントアセットの登録を解除し、ドラムという名前のサウンドアセットの登録を解除します。

最後にエンジンを破壊します。

ここでは、コードでクリーンアップを行います。

まず、ドラムループを聴き終わったら、サウンドイベントを中止します。

その後、エンジンを停止し、内部的にオーディオIOを停止します。

次に、drumEventという名前のサウンドイベントアセットの登録を解除し、ドラムという名前のサウンドアセットの登録を解除します。

最後にエンジンを破壊します。

基本を説明したので、PHASEでシンプルな空間オーディオ体験を構築する方法を紹介します。

空間ミキサー、体積音源、オクルーダーなどのトピックを取り上げます。

私が最初に行うことは、エンジンにサウンドイベントアセットを登録することです。

この例では、ドラムサウンドイベントがすでに登録されているエンジンから始めます。

ここから、シンプルなチャンネルベースの再生から完全な空間化にミックスをアップグレードします。

私が最初に行うことは、私の音源に異なる環境効果を選択的に適用するための空間パイプラインを構築することです。

次に、空間パイプラインから空間ミキサーを作成します。

構築されたら、空間ミキサーにいくつかの基本的なプロパティを設定して、正しく再生させます。

この例では、距離に対するレベルの減衰を制御するように距離モデルを設定します。

また、リスナーに対するソース間の角度に基づいて、レベル減衰を制御するように指向性モデルを設定します。

次に、サンプラーノードを作成します。

サンプラーノードは、登録されたサウンドアセットの名前と下流の空間ミキサーへの参照を取ります。

次に、サンプラーノードにいくつかの基本的なプロパティを設定して、正しく再生します。

再生モードとキャリブレーションレベルに加えて、ここで淘汰オプションも設定します。

これは、サンプラーが聞こえなくなったときに何をすべきかをPHASEに伝えます。

サンプラーノードの出力を空間ミキサーの入力に接続し、いくつかの基本的なパラメータを設定したので、サウンドイベントアセットをエンジンに登録する時が来ました。

以前と同じ名前を使います。 

ここでは、コードで健全なイベントアセットを作成します。

まず、.directPathTransmissionと.lateReverbをレンダリングするためのspatialPipelineを作成します。また、.lateReverb .sendLevelを設定して、直接対残響比を制御し、後期リバーブシミュレーション用の.mediumRoomプリセットを選択します。

次に、spatialPipelineで空間ミキサーを作成します。

次に、自然な響きのGeometricSpreadingDistanceModelを作成し、空間ミキサーに割り当てます。

淘汰距離を10メートルに設定します。

ソースがこの距離を超えた場合、私は自動的にミックスからそれを淘汰したいと思います。

そして、距離減衰効果を強調しないために、rolloffFactorを少し調整します。

次に、サンプラーノードを作成し、以前にエンジンに登録したモノドラムサウンドアセットを指す「ドラム」という名前を渡します。

playbackModeを.loopingに設定します。

キャリブレーションモードを.relativeSplに設定し、レベルを+12デシベルに設定して、サンプラーの出力レベルを上げます。

そして、cullOptionをスリープするように設定します。

最後に、soundEventAssetをエンジンに登録し、drumEventという名前を渡すので、後でサウンドイベントの作成を開始するときに参照できます。

エンジンにサウンドイベントアセットが登録されたので、シミュレーション用のシーンを作成する必要があります。

これには、リスナー、ソース、およびオクルーダーの作成が含まれます。

この例では、オクルーダーをソースとリスナーの間に配置します。

まず、リスナーを作成します。

その後、その変換を設定します。

シーングラフ内でリスナーをアクティブにする準備ができたら、エンジンのルートオブジェクトまたはその子の1つに添付します。

ここでは、コードでリスナーを設定します。

まず、リスナーを作成します。その後、その変換を設定します。

この例では、ローテーションなしでリスナーをオリジンに設定します。

最後に、リスナーをエンジンのルートオブジェクトに取り付けます。

では、体積源を設定しましょう。

まず、メッシュからソースシェイプを作成します。

次に、形状からソースを作成します。

これは本質的に体積源を構築します。

次に、その変換を設定します。

そして、シーングラフ内でソースをアクティブにする準備ができたら、エンジンのルートオブジェクトまたはその子の1つに添付します。

ここでは、コードに体積ソースを設定します。

まず、Icosahedronメッシュを作成し、HomePod Miniくらいのサイズに拡大します。

次に、メッシュから図形を作成します。

この形状は、体積源の複数のインスタンスを構築するために再利用できます。

たとえば、同じメッシュを共有する複数のHomePod Miniをシミュレーションに配置できます。

次に、形状から体積源を作成します。

入力として形状を取らない初期化子のバージョンを使用して、単純なポイントソースを作成することもできます。

その後、その変換を設定します。

リスナーの前に2メートルのソースを翻訳し、リスナーに向かって回転させて、彼らが互いに向き合うようにします。

最後に、ソースをエンジンのルートオブジェクトにアタッチします。

では、オクルーダーを設定しましょう。

まず、メッシュから形を作ります。

次に、段ボールの材料を作成し、それを形状に割り当てます。

今、形状はジオメトリと関連する材料を持っています。

次に、シェイプからオクルーダーを作成します。

その後、その変換を設定します。

そして、シーングラフ内でオクルーダーをアクティブにする準備ができたら、エンジンのルートオブジェクトまたはその子の1つに添付します。

ここでは、コードでオクルーダーを設定します。

まず、boxMeshを作成し、それに応じて寸法をスケーリングします。

次に、メッシュから図形を作成します。

この形状は、オクルーダーの複数のインスタンスを構築するために再利用できます。

たとえば、同じメッシュを共有する複数のボックスをシミュレーションに配置できます。

次に、段ボール箱のプリセットから材料を作成し、その材料を形状に割り当てます。

次に、シェイプからオクルーダーを作成します。

その後、その変換を設定します。

リスナーの前にオクルーダーを1メートル翻訳し、リスナーに向かって回転させて、互いに向き合うようにします。

これにより、オクルーダーはソースとリスナーの中間になります。

最後に、エンジンのルートオブジェクトにオクルーダーを取り付けます。

この時点で、ソースとリスナーの中間にあるオクルーダーのシーンがあります。

次に、登録されたサウンドイベントアセットからサウンドイベントを作成し、シーングラフのソースとリスナーに関連付けます。

サウンドイベントを開始すると、段ボール箱の反対側にある小さな体積源から遮られたドラムループが鳴っているのが聞こえます。

ここでは、コードでサウンドイベントを開始します。

まず、サウンドイベントでソースとリスナーを空間ミキサーに関連付けます。

次に、drumEventという名前の登録済みサウンドイベントアセットからサウンドイベントを作成します。

残りは以前と同じです。

エンジンが動いていることを確認してから、サウンドイベントを開始します。

空間オーディオをカバーしたので、複雑なサウンドイベントを構築する方法を紹介します。

サウンドイベントは、インタラクティブなサウンドデザインのための行動階層に編成できます。

このセクションでは、各タイプのサウンドイベントノードに基づいて最終的なサウンドイベントを作成する連続した例を紹介します。

ここでは、表面の濡れが変化するさまざまな種類の地形を歩いている騒々しいゴアテックスジャケットを着た俳優をモデル化します。

まず、きしむ木に足音を再生するサンプラーノードを作成します。

コードでは、「footstep_wood_clip_1」という名前の登録サウンドアセットを持つサンプラーノードを作成します。この例では、このノードと他のすべてのノードは、単一の事前に構築されたチャンネルミキサーで再生されます。

今、私はいくつかのランダム性を追加します。

きしむ木材サンプルでわずかに異なる足跡を再生する2つの子サンプラーノードを持つランダムなノードを作成します。

コードでは、2つのサンプラーノードを作成します。

1つ目は「footstep_wood_clip_1」という名前の登録サウンドアセットを使用し、2つ目は「footstep_wood_clip_2」という名前の登録サウンドアセットを使用します。

次に、ランダムなノードを作成し、サンプラーノードを子として追加します。

重み付け係数は、その子が連続した反復にわたって選択される可能性を制御するために、各子ノードに適用されることに注意してください。

この場合、最初の子供は2番目の子供よりも2倍の確率で選ばれます。

次に、地形スイッチを追加します。

子としてスイッチノードと2つのランダムノードを作成します。

この場合、2番目のランダムなノードは、木材上のランダムな足音とは対照的に、砂利の上でランダムな足音を再生します。

地形パラメータを使用してスイッチを制御します。

コードでは、2つのサンプラーノードを作成します。

1つ目は「footstep_gravel_clip_1」という名前の登録サウンドアセットを使用し、2つ目は「footstep_gravel_clip_2」という名前の登録サウンドアセットを使用します。

次に、ランダムなノードを作成し、サンプラーノードを子として追加します。

次に、地形パラメータを作成します。

デフォルト値は「creaky_wood」になります。

次に、地形パラメータによって制御されるスイッチノードを作成します。

ウッドランダムノードと砂利ランダムノードの2人の子供を追加します。

パラメータを「creaky_wood」に設定すると、ウッドランダムノードが選択されます。

同様に、パラメータを「soft_gravel」に設定すると、砂利ランダムノードが選択されます。

次に、ウェットネスブレンドを追加します。

子供の頃、地形スイッチノードとランダムスプラッシュノードを持つブレンドノードを作成します。

新しいランダムスプラッシュノードは、俳優がステップを踏むときにランダムなスプラッシュノイズを再生し、地形スイッチは俳優の足がきしむ木や柔らかい砂利の上を歩いているかどうかを判断します。

乾いた足音と水しぶきのブレンドは、完全に乾いた-大きな足音と水しぶきなし-から、完全に濡れた-静かな足音と大きな水しぶきまで、濡れたパラメータに依存します。

コードでは、2つのサンプラーノードを作成します。

1つ目は「splash_clip_1」という名前の登録サウンドアセットを使用し、2つ目は「splash_clip_2」という名前の登録サウンドアセットを使用します。

次に、ランダムなノードを作成し、サンプラーノードを子として追加します。

次に、湿潤パラメータを作成します。

範囲は0から1で、デフォルト値は0.5になります。

パラメータをゲームでサポートされている任意の値と範囲に設定できることに注意してください。

次に、湿潤パラメータによって制御されるブレンドノードを作成します。

地形スイッチノードとランダムスプラッシュノードの2つの子を追加します。

パラメータを0に設定すると、地形に応じて、きしむ木や砂利で乾いた足音しか聞こえません。

湿気を0から1に増やすと、各足跡に伴うスプラッシュノイズのラウドネスを増やし、濡れた地形をシミュレートします。

最後に、子供としてウェットネスブレンドノードとランダムな騒々しい衣類ノードを持つコンテナノードを作成します。

新しい騒々しい服のノードは、俳優がさまざまな濡れで地形を変える一歩を踏み出すたびに、ゴアテックスジャケットのラフラリングノイズを再生します。

この最終的なノード階層で、私はシーンを歩いている俳優の完全な表現を持っています。

俳優が一歩を踏み出すたびに、地形のパラメータに応じて、ジャケットのラッフルと、きしむ木や柔らかい砂利の足音が聞こえます。

これに加えて、湿気のパラメータに応じて、各フットステップで水しぶきを多かれ少なかれ聞きます。

コードでは、2つのサンプラーノードを作成します。

1つ目は「gortex_clip_1」という名前の登録サウンドアセットを使用し、2つ目は「gortex_clip_2」という名前の登録サウンドアセットを使用します。

次に、ランダムなノードを作成し、サンプラーノードを子として追加します。

最後に、actor_containerノードを作成します。

Wetness_blendノードとnoy_clothing_randomノードの2つの子を追加します。

一緒に、彼らは俳優の完全な音を表しています。

レビューでは、オーディオファイルを再生する方法を学びました。

これに続いて、シンプルでありながら効果的な空間オーディオ体験の構築に真っ先に飛び込むことで、知識を広げました。

ここでは、リスナー、体積源、オクルーダーについて学びました。

最後に、インタラクティブなサウンドデザインのための行動サウンドイベントの構築について学びました。

ここでは、ランダム、スイッチ、ブレンド、およびコンテナノードを接ぎ木して、階層的でインタラクティブなサウンドイベントを形成することについて学びました。

まとめると、PHASEの内部の仕組みを幅広く理解し、次のジオメトリ対応ゲームオーディオ体験を構築する準備ができたら、基礎となるシステムコンポーネントをより深く掘り下げることができるはずです。

ありがとうございます。素晴らしいWWDC21をお過ごしください。

[明るい音楽]。