10073

こんにちは。

ARKitチームのエンジニア、デビッドです。

今日、クリストファーと私はARKit 5の幅広い改善を共有します。

iOS 15の変更点について議論できることを嬉しく思います。

今年は、全面的に多くのアップグレードを行い、複数の機能について話し合います。

その前に、皆さんがLiDARで築いてきた経験を紹介したいと思います。

生産性、フォトフィルターエフェクト、エンターテイメント、さらにはリビングルームでプレイできるゲームなど、シーンの再構築と深度APIを使用して、さまざまなLiDAR対応アプリを見てきました。

ARKitコミュニティが示した創造性と機知を見て、本当にうれしいです。

これらのアプリを作成している間、私たちはあなたに世界最高のARフレームワークを提供し、可能なことの限界を押し広げることに懸命に取り組んでいます。

ARKit 5での変更点を見てみましょう。

まず、現実世界の屋外でのAR体験を可能にするロケーションアンカーのいくつかのアップデートとベストプラクティスを共有します。

次に、アプリクリップコードについて説明します。これは、アプリクリップを発見し、ARでコンテンツを配置するための素晴らしい方法です。

新しいiPad Proの超ワイドフロントカメラを使用したフェイストラッキングのいくつかの改善を強調します。

そして、ARKitモーションキャプチャのいくつかの機能強化で締めくくります。

私たちは、地域のサポートを拡大し、生活の質の向上を提供するために取り組んできたロケーションアンカーから始めます。

また、アプリケーションを作成するためのベストプラクティスもいくつかお勧めします。

ロケーションアンカーは昨年、特定の緯度と経度にARコンテンツを配置できるように導入されました。

彼らの目的は、地理的な場所に結びついたAR体験の作成を可能にすることです。

例を見てみましょう。 例を見てみましょう。

これは、ロケーションアンカーAPIを使用して構築されたScavengARアプリケーションのNew Nature体験です。

ScavengARは、現実世界の場所でARコンテンツをホストし、仮想パブリックアートのインスタレーションやアクティビティの作成を可能にします。

これは、世界が再開するにつれて、ロケーションアンカーが屋外体験にどのように電力を供給できるかの良い例です。

マップアプリは、iOS 15でAPIを使用する新しいAR機能も導入しています。

見てみましょう。 

今年、マップは、ロケーションアンカーAPIを使用して、ARに表示されるターンバイターンの歩行指示を追加します。

彼らは私たちが推奨するいくつかの慣行を取り入れています。

これらについては、後で説明して、優れたアプリケーションを構築する方法を紹介します。

いくつかのサンプルを見たので、GeoTrackingConfigurationを設定するために必要な手順から始めて、ロケーションアンカーを使用してそれらを作成する方法を要約しましょう。

まず、その機能がデバイスでサポートされていることを確認します。

ロケーションアンカーには、A12チップ以降、セルラーとGPSのサポートが必要です。

次に、起動する前に、その機能がその場所で利用可能であることを確認してください。

カメラと位置情報の許可は、デバイスの所有者によって承認されなければなりません。

ARKitは、必要に応じて許可を求めます。

ARKit 4とサンプルプロジェクト「ARでの地理的位置の追跡」を紹介する昨年のプレゼンテーションでは、これらすべてのトピックとAPIの使用をより深くカバーしています。

これらのソースの両方に精通していることを強くお勧めします。

このコードサンプルは、前のスライドのチェックを実行する方法を示しています。

デバイスサポートを照会し、GeoTrackingConfigurationを実行しようとする前に、現在の場所で機能が利用可能かどうかを検証します。

GeoAnchorsは、他のタイプのアンカーと同様にARSessionに追加できます。

それらは緯度-経度座標と、オプションで高度で指定されています。

GeoTrackingConfigurationのステータスを監視して、機能がローカライズされているかどうか、および解決すべき問題が残っているかどうかを確認することが重要です。

開発者サンプルには、ステータスの更新を受信するメソッドを実装する方法の例が含まれています。

デバイスの場所の近くで可用性を確認することは、ジオトラッキングでアプリケーションを開始するために重要です。

私たちは、より多くの地域をサポートするために常に働いています。

ロケーションアンカーは、最初のリリースのために5つのメトロエリアに制限され、それ以来、サポートは米国全土の25以上の都市に拡大しました。

私たちはまた、世界中の都市にロケーションアンカーをもたらすために懸命に働いています。

初めて、米国外の市場を発表できることを嬉しく思います。

ロケーションアンカーがロンドンにやってくる。

私たちは、時間の経過とともに新しい地域を追加する作業を継続します。

サポートされているメトロエリアに住んでいない場合は、録画とリプレイを使用してロケーションアンカーの実験を開始することもできます。このセッションの後半で説明します。

サポートされている地域のリストについては、いつでもARGeoTrackingConfigurationのオンラインドキュメントを参照してください。

ロケーションアンカーがより多くの地域で利用可能になるにつれて、私たちは人々を導くために共通の視覚言語を持つ必要性を認識しています。

一貫したオンボーディングプロセスを支援するために、ARCoachingOverlayViewで使用する新しい.geoTracking目標を追加しています。

世界追跡のための既存のオーバーレイと同様に、人々が良い経験を達成するのを助けるためにアニメーションを表示します。

コーチングオーバーレイは、マップを含む多くの異なるARアプリで使用されているため、人々はすでにそれらに精通しており、対応方法を知っているでしょう。

この機能の学習曲線を容易にするために、コーチングオーバーレイを含めることをお勧めします。

コーチングオーバーレイを使用している間も、追跡状態に関するより詳細な情報を含む.geoTrackingステータスの更新を監視することをお勧めします。

.geoTrackingコーチングオーバーレイは次のようになります。

UIは、デバイスを地面から遠ざけて、建物のファサードに向ける指示を示しています。

数秒後、追跡が成功し、アプリはジオトラッキングされたコンテンツを配置できます。

このアニメーションを表示するためのコードは、他のコーチングオーバーレイに使用されるコードと非常によく似ています。

ユニークなのは、オーバーレイの.geoTracking目標の導入です。

正しいガイドを表示するには、必ずこの目標を設定してください。

コーチングオーバーレイが統一されたオンボーディングプロセスを作成する方法を見てきました。

次に、ジオトラッキングされたARエクスペリエンスを作成するのに役立つ他のベストプラクティスについて説明します。

私たちの最初の推奨事項は、より迅速な開発のために録音とリプレイを使用することです。

ARKitセッションは、App Storeで入手可能なReality Composerを使用してデバイスで録画できます。

これはロケーションアンカーに特に便利なので、テストのために頻繁に外に出る必要はありません。

また、遠隔地のクリエイターとのコラボレーションも可能です。

録画は、Xcodeを使用してデバイスで再生できます。

非互換性の問題を回避するには、同じデバイスとiOSバージョンを使用することをお勧めします。

これは、他のタイプのARKitアプリケーションでも機能します。

リプレイはロケーションアンカーに固有のものではありません。

録音をキャプチャするプロセスを見てみましょう。

録画するには、Reality Composerを開き、右上のその他のオプションをタップします。

次に、開発者ペインを開き、ARセッションを記録するを選択します。

位置情報サービスが有効になっていることを確認してください。

赤いボタンをタップして、録音を開始および停止します。

録画を再生するには、Xcodeを実行しているコンピュータにデバイスを接続します。

[スキームの編集]をクリックし、実行設定のARKitリプレイデータオプションを設定します。

次に、アプリケーションを実行します。

録画と再生は開発のスピードアップに役立ちますが、コンテンツの配置には他のプラクティスをお勧めします。

これはこれらを実演するビデオです。

ARコンテンツが大きくてはっきりと見え、環境の構造と重なすことなく情報が伝えられることに注目してください。

開発時間と配置精度のトレードオフとして、現実世界のオブジェクトと密接に重なろうとするのではなく、空中に浮かぶコンテンツを作成することを検討してください。

コンテンツを配置するための推奨事項は他にもいくつかあります。

オブジェクトを配置する緯度と経度の座標を取得するには、Appleマップアプリを使用し、少なくとも6桁の精度で座標をコピーします。

これの手順は、ARKit 4を紹介するビデオで示されているので、詳細については、そこを参照してください。

アプリケーションを作成するときは、良い経験を生み出すために、必要に応じてロケーションアンカーに対するコンテンツの高度を調整することも重要です。

アプリがより正確なコンテンツの配置が必要な場合は、デバイスがその場所から50メートル以内にあるときにジオアンカーを追加します。

ARKitが正確な高度でアンカーを配置すると、アンカーの高度ソースフィールドを更新してこれを示します。

CLLocationクラスには、2点間の距離をメートル単位で計算するために使用できるメソッドがあります。

これは、アンカーを追加する前に、誰かが場所の近くにいることを確認するために使用できます。

これで、ロケーションアンカーに関するセッションは終了です。

ARKit 5を使用して、アプリにARコンテンツを配置する方法は他にもあります。

だから、クリストファーに渡しましょう。クリストファーはもっと教えてくれます。

ありがとう、デビッド。

こんにちは、私の名前はクリストファーで、ARKitチームのエンジニアです。

ARKit 5の他の素晴らしい新機能についてもっとお話しできることを嬉しく思います。

ARKitのアプリクリップコードから始めましょう。

おそらく、昨年WWDCでApp Clipsを導入したことを覚えているでしょう。

アプリクリップは、アプリ全体をインストールすることなく、アプリの1つのコンテキストワークフローを通して人々を連れて行くアプリの小さなスライスです。

ファイルサイズが小さいため、アプリクリップはダウンロード時間を節約し、現在のコンテキストに非常に関連性の高いアプリの特定の部分に人々を直接連れて行きます。

また、アプリクリップコードを視覚的に発見して起動するための素晴らしい方法であるアプリクリップコードも導入しました。

App Storeへの旅行は必要ありません。

これがアプリクリップコードです。

彼らは様々な形や色で来ることができます。

開発者として、シナリオに最適な外観を作成できます。

また、アプリクリップコードでエンコードするデータと、どのアプリクリップがどのコードに関連付けられているかを決定します。

すべてのアプリクリップコードには、視覚的にスキャン可能なパターンが含まれており、ここに示されている赤、青、オレンジのコードのように、ユーザーの便宜のためにNFCタグも含まれています。

人々はカメラでコードをスキャンしたり、携帯電話を埋め込まれたNFCタグにかざして、関連するアプリクリップを起動することができます。

そして今、AR体験でアプリクリップコードを認識して追跡することもできます。

このセッションの後半で、それがどのように行われるかを見ていきます。

しかし、まず、アプリクリップコードを使用してAR体験を起動するプライマーによって開発されたこのアプリクリップを見てみましょう。

PrimerはCle Tileと提携し、App Clip Codesの助けを借りて、サンプルがARでどのように見えるかを人々に示しました。

iPhoneとiPadをApp Clipコードの上に置くだけで、AR体験を呼び出すことができます。

今、人々はアプリをダウンロードすることなく、自分の壁にタイル見本をプレビューすることができます。

それはかなりかっこいいですよね?

したがって、iOSとiPad 14.3から、AR体験でアプリクリップコードを検出して追跡することができます。

App Clip Codeの追跡には、iPhone XSのようなA12 Bionicプロセッサ以降を搭載したデバイスが必要です。

ARKitでアプリクリップコードを使用する方法を詳しく見てみましょう。

iOS 14.3では、新しいタイプのARAnchor、ARAppClipCodeAnchorを導入しました。

このアンカーには、アプリクリップコードに埋め込まれたURL、URLデコード状態、アプリクリップコードの半径（メートル単位）の3つの新しいプロパティがあります。

説明させてください。

各アプリクリップコードには、正しいコンテンツを表示するためにデコードされたURLが含まれています。

URLのデコードは即時ではありません。

ARKitは、アプリクリップコードの存在をすばやく検出できます。

しかし、ユーザーのコードまでの距離や照明などの他の要因に応じて、ARKitがURLをデコードするのに少し時間がかかる場合があります。

このため、App Clip Codeアンカーには.decoding状態プロパティが含まれており、3つの状態のいずれかにすることができます。

初期状態の.decodingは、ARKitがまだURLをデコードしていることを示します。

ARKitがURLのデコードに成功するとすぐに、状態は.decodedに切り替わります。

URLのデコードが不可能な場合、状態は代わりに.failedに切り替わります。

これは、たとえば、誰かがアプリクリップに関連付けられていないアプリクリップコードをスキャンしたときに発生する可能性があります。

アプリクリップコードトラッキングを使用するには、まずデバイスでサポートされているかどうかを確認する必要があります。

アプリクリップコードの追跡は、A12 Bionicプロセッサ以降を搭載したデバイスでのみサポートされていることを忘れないでください。

次に、設定のappClipCodeTrackingEnabledプロパティをtrueに設定し、セッションを実行します。

アプリクリップコードのURLを読み取るには、ARセッションを監視し、アンカーコールバックを更新し、検出されたアプリクリップコードアンカーのデコード状態を確認します。

ARKitがApp Clipコードをデコードしている間、App Clipコードの上にプレースホルダの視覚化を表示して、App Clipコードが検出されたが、まだデコードする必要があるという即時フィードバックをユーザーに提供したい場合があります。

前述のように、アプリクリップコードのデコードも失敗する可能性があります。例えば、誰かがあなたのアプリクリップに属していないアプリクリップコードに電話を向けたとき。

その場合もフィードバックを提供することをお勧めします。

アプリクリップコードがデコードされると、最終的にそのURLにアクセスし、このアプリクリップコードに適したコンテンツの表示を開始できます。

たとえば、先ほど見たプライマーアプリクリップの場合、URLにはどのタイルスウォッチを表示するかに関する情報が含まれています。

アプリクリップコードがデコードされたら、問題は、このコードに関連付けられているコンテンツをどこに表示すべきかということです。

1つのオプションは、App Clip Codeアンカーの上に直接表示することです。

ただし、ユースケースによっては、App Clip Code自体がコンテンツを表示するのに最適な場所ではない場合があります。

したがって、たとえば、App Clip Codeの近くにコンテンツを固定された相対位置で配置できます。

これは、アプリクリップコードがオブジェクト、例えばコーヒーメーカーに印刷され、機械のボタンの上にそれを操作する方法に関する仮想指示を表示したい場合にうまく機能します。

または、App Clip CodeのトラッキングをARKitでサポートされている他のトラッキング技術と組み合わせることもできます。

例えば、画像追跡。

その実装を見てみましょう。 では、その実装を見てみましょう

次を見るビデオとコードは、developer.apple.comでダウンロードできる「ARでアプリクリップコードと対話する」サンプルコードに基づいています。

あなたが今見ているのは、サンプルのAR体験の記録です。

まず、カメラアプリから始めて、ヒマワリの種パッケージをスキャンします。

たぶん、私は園芸店で買い物をしていて、どの植物の種を買うかを決めようとしています。

iOSはパッケージのアプリクリップコードを認識し、関連するシードショップアプリクリップを起動します。

ここでは、アプリクリップコードを2回目にスキャンすると、成長したヒマワリがシードパッケージに表示されます。

アプリクリップは、シードパッケージ全体の画像追跡を使用し、その上にヒマワリを置くことに注意してください。

このアプローチは、右上の小さなアプリクリップコードではなく、シードパッケージ全体に注目される可能性が高いため、このユースケースでは理にかなっています。

しかし、誰かが自分の庭で植物が育つのを見たいと思ったらどうなりますか?

これがどのように見えるかです。

ここでは、コードが初めてスキャンされると、アプリクリップのダウンロードが呼び出されることがわかります。

次に、アプリクリップ内から同じコードを再度スキャンすると、コードをヒマワリの種箱に関連付け、芝生をタップするとヒマワリが表示されます。

代わりに、アプリクリップがバラの種箱のコードを見たら、芝生にバラの植物を産んだだろう。

アプリクリップには1つのワークフローしか含まれていないことに注意してください。

しかし、アプリクリップは、彼らのスペースでプレビューできる他の植物を体験するために、完全なシードショップアプリをダウンロードするためのボタンを提供することができます。

App Clipコードのトラッキングは、App Clipの親アプリでも機能することを忘れないでください。

芝生にヒマワリを置くために必要なコードを見てみましょう。

まず、tapGestureRecognizerをビューに追加して、画面上のタップを検出します。

人が画面をタップすると、世界に光線をキャストし、デバイスの前の水平平面上の結果の位置を取り戻すことができます。

私たちのシナリオでは、これはその人の芝生になります。

次に、デコードされた最後のアプリクリップコードURLをつかみ、芝生に新しいARAnchorを追加します。

最後に、ヒマワリの3Dモデルをダウンロードし、芝生に表示します。

では、ARKitのアプリクリップコードのベストプラクティスについて話しましょう。

アプリクリップは、さまざまな環境やさまざまなユースケースで使用できます。

NFCアプリクリップコードを作成するオプションであるかどうかを検討してください。

人々が物理的にコードにアクセスできる環境には、NFCアプリクリップコードをお勧めします。

NFCアプリクリップコードを使用する場合は、タグをタップするように人々を導く適切な行動を促すテキストを使用するか、代わりに、コードをスキャンするための明示的なアフォーダンスを提供します。

最後になりましたが、アプリクリップコードがユーザーの環境に適したサイズで印刷されていることを確認する必要があります。

たとえば、レストランのメニューはA4用紙に印刷され、人々は最大50センチメートルの距離からそのメニューの2.5センチメートルのアプリクリップコードを快適にスキャンすることができます。

しかし、映画のポスターは通常はるかに大きく、最大2.5メートル離れた場所から携帯電話でスキャンできる12センチメートルのアプリクリップコードのための十分なスペースがあるかもしれません。

推奨コードサイズの詳細については、アプリクリップコードに関するヒューマンインターフェースガイドラインをご覧ください。

それがARKitでアプリクリップコードを使用する方法です。

アプリクリップとアプリクリップコードを深く掘り下げたい場合は、「アプリクリップの新機能」と「軽くて高速なアプリクリップを構築する」セッションを必ずチェックしてください。

では、フェイストラッキングにジャンプしましょう。

フェイストラッキングを使用すると、フロントカメラで顔を検出し、仮想コンテンツをオーバーレイし、顔の表情をリアルタイムでアニメーション化できます。

iPhone Xの発売以来、ARKitはフェイストラッキングを利用する多くの素晴らしいアプリを見てきました。

複数の顔の追跡から、フロントカメラとバックカメラのユースケースでのフェイストラッキングの実行まで、このAPIは長年にわたって多くの進歩を遂げてきました。

昨年は、A12 Bionicプロセッサ以降を搭載している限り、TrueDepthセンサーのないデバイスにフェイストラッキングを導入しました。

そして今年初め、AR顔追跡体験のための超広視野フロントカメラを提供する新しいiPad Proを発売しました。

見てみましょう。 

ここでは、通常のフロントカメラの視野が見えます。

そして、これは新しいiPad Proの新しい超広視野です。

それは本当に違いを生みますね?

既存のアプリは、フェイストラッキングに通常のカメラを使用し続けることに注意してください。

新しいiPad Proでユーザーエクスペリエンスを超広視野にアップグレードしたい場合は、利用可能なビデオフォーマットを確認し、新しい超広幅フォーマットをオプトインする必要があります。

サポートされているすべてのビデオフォーマットを反復し、builtInUltraWideCameraオプションをチェックすることで、これを行うことができます。

次に、AR設定でこの形式を設定し、セッションを実行します。

注意すべき点の1つは、新しいiPad Proの超広角カメラは、TrueDepthセンサーよりもはるかに広い視野を持っているということです。

したがって、超ワイドビデオフォーマットを使用する場合、ARFrameにキャプチャされたDepthDataバッファは取得されません。

最後になりましたが、モーションキャプチャーについて話しましょう。

2019年の発売以来、モーションキャプチャは、2Dおよび3Dシミュレーションで使用されるとともに、仮想キャラクターをアニメーション化するなど、ARシーンでの実際の人々の堅牢な統合を可能にしました。

iOS 15では、モーションキャプチャがさらに良くなっています。

iPhone 12のようなApple A14 Bionicプロセッサを搭載したデバイスでは、モーションキャプチャがより広い範囲のボディポーズをサポートするようになりました。

そして、これはコードの変更をまったく必要としません。

iOS 15のすべてのモーションキャプチャアプリは、この恩恵を受けるでしょう。

最も注目すべきは、ローテーションがこれまで以上に正確で、スポーツアクションをはるかに正確に追跡するのに役立ちます。

もう1つの大きな改善点は、デバイスカメラがはるかに遠くから体の関節を追跡できるようになったことです。

また、手足の動きの範囲を追跡することも大幅に増加しています。

例を見てみましょう。 例を見てみましょう。

これは私の同僚の1人であるEjlerが、Driven2winアプリで彼のトレーニングを追跡しています。

iOS 15の結果はこれまで以上に正確です。

要約すると、ARKit 5は多くの新機能と改善をもたらします。

ロケーションアンカーは新しい都市で利用可能で、新しいコーチングオーバーレイを備えています。

アプリクリップコードの追跡は、アプリクリップでのARの簡単な発見と使用、および仮想コンテンツの正確な配置を支援します。

顔追跡は、新しいiPad Proの新しい超広視野で動作し、モーションキャプチャはより良い精度とより広い可動域を追加します。

私はあなたがARKit 5で作成するすべての素晴らしい体験を見てとても興奮しています。

[音楽]。