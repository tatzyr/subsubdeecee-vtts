10038

こんにちは、WWDCへようこそ。

私の名前はジョンで、Appleの機械学習フレームワークであるCore MLに取り組んでいます。

同僚のブライアンと一緒に、機械学習の魔法をアプリにもたらしながら、モデルを調整する方法をお見せできることを嬉しく思います。

まず始めに、機械学習APIの機能強化をいくつか紹介します。

その後、さまざまな新しい可能性を開くファイル形式の改善に飛び込みます。

後で、ブライアンは私たちにMLプログラムを見せ、私たちをボンネットの下に連れて行き、タイプされた実行と、それを使用してモデルの精度とパフォーマンスを微調整する方法を説明します。

これらの改善を使用して、ワークフローを合理化し、MLを搭載したエクスペリエンスをさらに押し上げることができます。

APIの改善から始めましょう。

Core MLは、ユーザーのデバイス上のモデルを操作するためのシンプルなAPIを提供します。

これらのモデルは、文字列やプリミティブ値、または画像やマルチアレイなどのより複雑な入力など、さまざまな入力と出力で動作するように設計できます。

この最後のタイプ、MultiArrayについてもっと話しましょう。

Core MLを使用すると、MLMultiArrayを使用して多次元データを簡単に操作できます。

シンプルなAPIですが、データを操作するために書かなければならないコードは、Swiftでは必ずしも自然に感じるとは限りません。

たとえば、多数の整数でMultiArrayを初期化するには、実行時に型を渡す必要があります。

さらに、通常の整数の代わりにNSNumberを使用する必要がありますが、それはタイプセーフではなく、エレガントなSwiftのようには見えません。

Core MLは、多次元データでの作業を容易にするために、MLShapedArrayを導入しています。

MLShapedArrayは、通常の配列に似ていますが、複数の次元をサポートする純粋なSwiftタイプです。

配列と同様に、コピーオンライトセマンティクスと、既存のMLMultiArrayコードで簡単に機能する豊富なスライス構文を備えた値タイプです。

2次元MLMultiArrayを初期化するには、通常、2つのネストされた「for」ループを使用します。

MLShapedArrayを使用すると、同じ2D配列を1行で初期化できます。

MLShapedArrayはSwiftに自然にフィットし、コードの作成とレビューがはるかに簡単になります。

これは別の例です。

2行目をスライスとしてアクセスするには、このようにインデックスを作成するだけです。

複数の行と列をスライスとしてアクセスするには、各次元の範囲を使用できます。

MLShapedArrayとMLMultiArrayは互いに完全に互換性があります。

他のタイプのインスタンスを取る初期化子を使用して、あるタイプを別のタイプに簡単に変換できます。

変換初期化子を使用してデータ型を変換することもできます。

たとえば、このコードは、doublesのMultiArrayをFloatsのShaedArrayに変換します。

形状配列は、多次元データを操作する必要があるときはいつでも便利です。

たとえば、YOLOオブジェクト検出モデルは画像内のオブジェクトを見つけ、2次元配列を出力します。

この表は、1つの予測からのデータを示しています。

各行は境界ボックスを表し、各列の値は0から1の範囲です。

各値は、バウンディングボックスに人、自転車、車などが含まれているというモデルの自信を示しています。

バウンディングボックスごとに最も可能性の高いラベルを選択するためのコードを書きたいです。

これを行う方法の例を次に示します。

コードは、2次元マルチアレイである出力の信頼プロパティから始まります。

この関数は各行をループして、その行で最も高い信頼スコアを見つけます。

整数をNSNumberに頻繁にキャストしなければならないことに注意してください。

このコードは代わりにMLShapedArrayを使用し、読みやすい少ない行で同じ作業を行います。

モデルの予測結果が、信頼値を含むShaedArrayプロパティを与えることに注意してください。

MLShapedArrayとそのスカラーは標準のSwiftコレクションプロトコルに準拠しているため、このコードはより簡単です。

これは、より読みやすく、Swiftで作業する喜びである素敵な強くタイプされた経験を提供します。

次に、Core MLモデルと、それらがファイルシステムでどのように表現されているかについて話しましょう。

Core MLを使用すると、ユーザー向けに豊富な機械学習によるエクスペリエンスを簡単に構築できます。

MLモデルは、これらの経験に命を吹き込むエンジンです。

.Mlmodelファイル形式は、モデルの機能をエンコードおよび抽象化するので、心配する必要はありません。

このフォーマットは、モデルのすべての実装の詳細と複雑さを格納します。

開発者として、それがツリーアンサンブルであろうと、何百万ものパラメータを持つニューラルネットワークであろうと気にする必要はありません。

MLモデルは、他のAPIと同様に、Xcodeプロジェクトに追加し、それで動作するコードを書くだけの単一のファイルです。

各Core MLモデルファイルは、いくつかのコンポーネントで構成されています。

メタデータは、著者、ライセンス、バージョン、簡単な説明などの情報を保存します。

インターフェイスは、モデルの入力と出力を定義します。

アーキテクチャは、モデルの内部構造を定義します。

たとえば、ニューラルネットワークでは、アーキテクチャセクションでは、モデルのレイヤーとそれらの間のすべての接続について説明します。

最後に、最後のセクションでは、モデルがトレーニング段階で学習した膨大な値の配列を保存します。

MLモデルファイルは、これらすべてのセクションをプロトブフバイナリ形式にエンコードし、ファイルシステムとソース管理ソフトウェアは単一のバイナリファイルと見なします。

ソース制御ソフトウェアは、バイナリモデルファイルが実際にはいくつかの異なるコンポーネントの組み合わせであることを伝えることができません。

それを解決するために、Core MLは、macOSの組み込みパッケージ機能を使用して、これらのコンポーネントを別々のファイルに分割する新しいモデルフォーマットを追加しています。

これにより、新しいCore MLモデルパッケージに登場します。

これは、モデルの各コンポーネントを独自のファイルに格納し、アーキテクチャ、重み、メタデータを分離するコンテナです。

これらのコンポーネントを分離することで、モデルパッケージを使用すると、メタデータを簡単に編集し、ソース制御で変更を追跡できます。

また、より効率的にコンパイルし、モデルを読み書きするツールに柔軟性を提供します。

Core MLとXcodeは、依然として元のMLモデルフォーマットを完全にサポートしています。

しかし、モデルパッケージに更新することで、より拡張可能な形式に移行し、より効率的にコンパイルすることができます。

Xcodeでこれを試してみましょう。

これは、物体検出モデルを使用して画像内の動物を識別するシンプルなアプリです。

メタデータフィールドの一部が空であることに注意してください。

メタデータが入力されていないモデルに出くわすのはかなり一般的です。

以前は、Xcodeでこれらのフィールドを編集できませんでした。

しかし、Xcodeがモデルパッケージをサポートした今、あなたはできます。

現在、モデルのファイルタイプはMLモデルですが、編集ボタンをクリックすると、XcodeはMLモデルファイルをMLパッケージに更新するように促します。

Xcodeは、私のワークスペースの参照を元のモデルファイルに更新して、new.mlpackageを指すようにしようとしていることを教えてくれます。

先に進んで、[更新と編集]をクリックします。

XcodeのUIは、モデルがMLパッケージ形式であることを示すようになりました。

これで、不足している値をXcodeで直接入力できます。

先に進んで、説明を「動物」という言葉で更新します。このモデルは私の同僚のジョセフから来たので、著者フィールドに彼の名前を入れます。

MITライセンスとバージョン2.0と言います。

また、追加のメタデータフィールドを追加、変更、削除することもできます。

WWDCでこのモデルを使用した年を示す新しいメタデータ項目を追加します。

だから、私たちは2021年と言います。

現在、UIのサポートに加えて、これらの情報はすべて、実行時にCore MLのMLModelDescription APIを使用してアクセスすることもできます。

「予測」タブでモデルの入力と出力の説明を変更することもできます。

ここでは、この入力の説明を変更します。

「動物の」を追加します。

そしてここで、不足しているハイフンを追加してタイプミスを修正します。

さて、良いメタデータを持つモデルは、良いコメントを持つコードによく似ています。

あなたとあなたのチームがモデルの意図を理解するのに役立つので、モデルの入力と出力について良い説明を書くことが特に重要です。

[完了]をクリックして変更を保存します。

ソースコントロールをクリックしてからコミットすると、Xcodeは差分ビューで変更を表示します。

メタデータは独自の.jsonファイルにあるため、変更を簡単に確認できます。

同様に、機能の説明には独自の別々の.jsonファイルがあります。

62メガバイトのバイナリMLモデルファイルの数バイトを変更していたら、62メガバイトのバイナリ差分があったでしょう。

ただし、モデルパッケージは、特に小さなテキスト変更の場合、はるかに効率的で作業が簡単です。

Xcodeは、モデルパッケージとモデルファイルの両方を均等にサポートしています。

たとえば、プレビュータブを使用してモデルパッケージをテストできます。

2匹のクマの画像を持ち込むと、クマごとに1つずつ、2つのバウンディングボックスが手に入ることがわかります。

同様に、[ユーティリティ]タブに移動して、MLモデルファイルと同じように、モデルパッケージの暗号化キーまたはMLアーカイブを生成できます。

それがXcodeのモデルパッケージです。

パッケージは、モデルメタデータの編集など、モデルファイルができることすべてを行うことができます。

最後に見せたいのは、プロジェクトに追加するモデルごとにXcodeが自動的に生成するコードです。

このアイコンをクリックすると、生成されたコードが表示されます。

先ほど、MLMultiArrayとその新しいSwiftのカウンターパートであるMLShapedArrayを見てみました。

Xcodeは、ラッパークラスのMultiArray出力ごとに新しい形状の配列プロパティを追加するようになりました。

たとえば、生成されたクラスは、モデルの出力に対して confidenceShapedArray プロパティを持つようになりました。

必要に応じて、元の信頼度MLMultiArrayプロパティを引き続き使用できます。

新しい形状の配列プロパティを利用するには、プロジェクトの展開ターゲットがmacOS 12やiOS 15など、これらのOSバージョンの1つである必要があることに注意してください。

これらすべてを実際に見たので、MLモデルとMLパッケージを並べて見てみましょう。

MLパッケージは、ツリー、SVM、ニューラルネットワークなど、MLモデルファイルがサポートするすべての同じタイプをサポートしています。

これらのタイプに加えて、MLパッケージはMLプログラムと呼ばれる強力な新しいモデルタイプもサポートしています。

MLプログラムは、よりコード指向の形式でニューラルネットワークを表すモデルタイプです。

MLプログラムとそれらが有効にする新機能について詳しくお伝えするために、ブライアンに渡します。

ありがとう、ジョン。

私の名前はブライアン・キーンで、MLプログラムと、型付き実行がどのように精度とより良いモデルパフォーマンスをより制御できるかについて話すことに興奮しています。

機械学習モデルがあなたに提示されたかもしれないさまざまな方法があります。

機械学習コースを受講したり、論文を読んだりしている場合は、その数学的または統計的定式化に関して記述されたモデルに遭遇する可能性があります。

しかし、これらの数学的表現はしばしば抽象化され、代わりに計算グラフまたはネットワークの形で提示されます。

中央の2つの図に描かれているこのグラフィカルな表現は、データが一連のレイヤーをどのように流れるかを説明し、それぞれが独自の特定の変換を適用します。

機械学習ソフトウェアライブラリでは、モデルは代わりにコード内の操作として表現されます。

機械学習エンジニアは、ブロック、機能、および制御フローで構成されるこのより一般的なプログラム構造をますます活用しています。

Core MLの新しいMLプログラムモデルタイプは、この最後の表現と一致します。

これは代表的なMLプログラムです。

それは人間が読めるテキスト形式ですが、意図は自分で書く必要がないということです。

MLプログラムは、Core MLのコンバーターによって自動的に生成されます。

MLプログラムはメイン機能で構成されています。

この主な機能は、一連の操作、または操作で構成されています。

各opは変数を生成し、この変数は強く型付けされます。

線形操作や畳み込み操作などの重みを持つ操作の場合、重みは通常、別のバイナリファイルにシリアル化されます。

これは、MLプログラムとニューラルネットワークの比較の簡単な要約です。

ニューラルネットワークにはレイヤーがあり、MLプログラムにはオペレーションがあります。

ニューラルネットワークモデルの重みはレイヤーの説明に埋め込まれていますが、MLプログラムは重みを別々にシリアル化します。

また、ニューラルネットワークは中間テンソル型を指定していません。

代わりに、計算ユニットは実行時にこれらのタイプを決定します。

一方、MLプログラムにはテンソルが強く型付けされています。

今日は、MLプログラムの強く型付けされた構文と、型付けされた中間テンソルがMLプログラムを使用したデバイス上の機械学習に与える影響に焦点を当てます。

しかし、まず、どのようにMLプログラムを取得しますか?

Core MLは以前、統一されたコンバータAPIを導入しました。

この統合コンバータAPIは、単一の関数呼び出しでTensorflowまたはPyTorchからCore MLニューラルネットワークモデルにモデルを取得する便利な手段を提供します。

最小展開ターゲットとしてiOS 15を選択することで、同じAPIを使用してMLプログラムに変換できるようになりました。

ボンネットの下で、Core MLコンバータは、変換時にモデルのディスク上の表現を選択します。

MLプログラムの場合、ディスク上の中間表現は、WWDC 2020で導入された機能であるModel Intermediate Languageによって提供されます。

統合コンバータAPIは、モデルをMLプログラムとして展開することをオプトインできる場所です。

今後、MLプログラムはニューラルネットワークよりも好ましい形式になります。

そして、MLプログラムはiOS15とmacOS Montereyから利用可能です。

Core MLは、ニューラルネットワークモデルのMLモデルとMLパッケージフォーマットの両方をサポートしていますが、MLプログラムは、その重みをアーキテクチャとは別に保存するためのMLパッケージでなければなりません。

コアMLは、将来の基盤としてMLプログラムに投資しています。

ニューラルネットワークは引き続きサポートされますが、MLプログラムは新機能の中心になります。

では、MLプログラムが未来であるならば、今日MLプログラムを採用することの利点は何ですか?

これにより、型付き実行につながります。

MLプログラムで型付き実行の利点を強調するために、まずニューラルネットワークで何が起こるかについて話し合いましょう。

ここに示されているのは、入出力テンソルにFloat32を指定するCore MLニューラルネットワークモデルへの入出力の例です。

入力と出力は、ダブルまたは32ビットの整数型にすることもできます。

したがって、ニューラルネットワークモデルは、これらの入力テンソルと出力テンソルを強くタイプします。

中間テンソルの種類はどうですか?

ニューラルネットワークは、その中間テンソルを強く入力しません。

ディスク上のモデルには、これらのテンソルの種類に関する情報はありません。

代わりに、モデルを実行する計算ユニットは、Core MLがモデルをロードした後にテンソルのタイプを推測します。

Core MLランタイムがニューラルネットワークをロードすると、ネットワークグラフがAppleニューラルエンジンフレンドリー、GPUフレンドリー、CPUのセクションに自動的かつ動的に分割されます。

各コンピューティングユニットは、そのパフォーマンスとモデルの全体的なパフォーマンスを最大化するために、ネイティブタイプを使用してネットワークのセクションを実行します。

GPUとニューラルエンジンはどちらもFloat16を使用し、CPUはFloat32を使用します。

開発者として、モデルの computeUnits プロパティで .all、.cpuAndGPU、または .cpuOnly を選択することで、この実行スキームをある程度制御できます。

このプロパティのデフォルトは.allで、Core MLに実行時にニューラルエンジン、GPU、CPU全体でモデルを分割して、アプリに可能な限り最高のパフォーマンスを提供するように指示します。

また、CPUのみに設定すると、Core MLはニューラルエンジンもGPUも使用せず、モデルがCPU上でFloat32精度のみを実行していることを保証します。

要約すると、ニューラルネットワークには中間テンソルがあり、生成を担当するコンピューティングユニットによって実行時に自動的に入力されます。

許可された計算ユニットのセットを設定することで、それらの精度をある程度制御できますが、そうすることはモデルのグローバル設定であり、テーブルにいくつかのパフォーマンスを残す可能性があります。

MLプログラムはどうですか?

ここに示されているMLプログラムでは、入力テンソルと出力テンソルは強く型付けされており、プログラムのすべての中間テンソルもそうである。

CPUやGPUなど、単一のコンピューティングユニット内で精度サポートを混在させることさえでき、これらのタイプはモデル変換時に明確に定義されています。

これは、Core MLを使用して展開シナリオでモデルをロードして実行するずっと前です。

MLプログラムは、ニューラルエンジン、GPU、およびCPUに作業を分配するのと同じ自動分割スキームを使用します。

ただし、型制約が追加されます。

Core MLはテンソルをより高い精度に昇格させる能力を保持しますが、Core MLランタイムは、MLプログラムで指定された精度よりも低い精度に中間テンソルをキャストすることはありません。

型付き実行のためのこの新しいサポートは、特にGPU上のFloat32 opsとCPU上のFloat16で選択されたopsのために、GPUとCPUの両方で拡張されたopサポートによって可能になりました。

この拡張サポートにより、MLプログラムがFloat32精度を指定しても、GPUのパフォーマンス上の利点を確認できます。

異なる精度でMLプログラムを生成するために、統一されたコンバータAPIを試してみましょう。

さて、私は今、インタラクティブな方法でPythonコードを実行するための便利なツールであるJupyterノートブックにいます。

モデルを新しいMLプログラム形式に変換するプロセスについて確認します。

今日使うモデルはスタイル転送モデルです。

私はすでにオープンソースから事前訓練されたTensorflowモデルをダウンロードしました。

このモデルは画像を取り込み、様式化された画像を生成します。

最初に必要なのは、いくつかのインポートステートメントです。

coremltools、Pythonイメージライブラリ、およびここで使用するコードを簡潔に保つために書いたいくつかのヘルパーライブラリと簡単なヘルパー関数をインポートします。

次に、スタイル転送モデルのパスと、スタイル化する画像へのパスを指定します。

変換の入力タイプも設定します。

この場合、モデルがトレーニングされた画像の寸法を指定する画像入力タイプになります。

最後に、Core MLモデルのポストコンバージョンを実行するために使用できる入力辞書を準備するための追加のセットアップがあります。

したがって、入力がロードされ、ソースモデルが利用可能です。

この時点で、すべての外部リソースはMLプログラムに変換する準備ができています。

変換には、Unified Converter APIを使用します。

最初の引数はソースモデルパスです。

次に、入力タイプの配列を渡します。

ここに1つだけあります。

最後に、最小展開ターゲット引数は、Core ML ToolsがニューラルネットワークまたはMLプログラムを生成するかどうかを決定します。

デフォルトはiOS 13で、ニューラルネットワークを生成します。

今、私はMLプログラムを手に入れたいので、展開目標をiOS 15に設定します。

最終的にはこのモデルをiOSアプリにデプロイしたい。

ターゲットデバイスがMacであれば、macOS 12の展開ターゲットを指定することもできます。

ShiftとEnterキーを押してモデルを変換します。

そして、変換が完了しました。

変換中にMLプログラムに対して自動的に行われるグラフ変換があります。

それはFP16ComputePrecisionパスと呼ばれています。

このグラフパスは、元のTensorflowグラフのすべてのFloat32テンソルをMLプログラムのFloat16テンソルにキャストします。

さて、変換が終わったので、次のステップはMLプログラムの正確性を確認することです。

両方のモデルで同じ画像で予測を呼び出すことで、出力数値を元のTensorflowモデルと比較することができます。

MLプログラムには注目に値します。予測、モデル保存、その他のユーティリティには、前年とまったく同じCore ML Tools APIを使用しています。

比較を行うために、私はすでに_get_coreml_tensorflow_outputというユーティリティメソッドを書きました。

Tensorflowからの出力とCore MLからの出力を評価するために、複数のエラーメトリックを出力します。

したがって、これは画像であるため、最も適切な誤差メトリックは、信号対雑音比、またはSNRである可能性があります。

実際には、20または30を超えるSNRは通常、良い結果を示しています。

ここで私は71のSNRを持っていますが、それはかなり素晴らしいです。

他にもいくつかの指標があります：最大絶対誤差、平均絶対誤差。

しかし、Float16を使用する際の精度コストはいくらですか？

私は何を失いましたか?

調べるには、Float16変換を無効にして再度変換できます。

同じconvertコマンドを使用しますが、今回はcompute_precision引数を指定してFloat32に設定します。

これにより、コンバータにFloat16キャストを注入しないように指示されるため、Core ML ToolsコンバータはFloat32 MLプログラムを生成します。

さて、このFloat32 MLプログラムを元のTensorflowプログラムと比較します。

そして、SNRは100以上に増加し、最大絶対誤差は約1から0.02に減少しました。

Float16モデルで以前に発生したエラーが識別可能な影響を与えたかどうかはまだ答えていません。

これはスタイル転送モデルであるため、出力画像の単純なプロットに基づいて評決を下すことができます。

Float16 MLプログラム、Float32 MLプログラム、Tensorflowモデルの3つのモデルすべてから、ソース画像と様式化されたバージョンをプロットします。

そして、3つのモデル出力の間に違いは見当たりません。

もちろん、いくつかの指標と目視検査で一度、単一の画像のこの評価は、実際には単なる煙のテストです。

物事は大丈夫に見えます。

実際には、大規模なデータセット全体でより多くのエラーメトリックで評価し、機械学習モデルで使用されるパイプライン内の障害ケースを評価し、それらをトリアージします。

小さなデータセットが手元にあり、この例をさらに一歩進めるために、データセット内の各画像の2つのMLプログラムとTensorflowモデルを比較することができます。

Float32 MLプログラムとTensorflowのSNRはXsの赤い線として描かれ、Float16 MLプログラムは円の青い線です。

Float32 MLプログラムは平均SNRが約100のようで、Float16 MLプログラムは約70のままです。

Float16の精度は数値に少し影響しますが、このユースケースでは重要ではないようです。

ただし、131の画像のこの小さなデータセットでも、いくつかの外れ値があります。

全体として、モデルは期待されていることをかなりうまくやっています。

そして、これは大部分の深層学習モデルの場合です。

彼らは通常、Float16の精度でもうまく機能する傾向があります。

そのため、Core MLコンバーターでデフォルトでFloat16変換をオンにしました。

Float16型MLプログラムは、ニューラルエンジンで実行でき、パフォーマンスが大幅に向上し、消費電力を削減することができます。

ランタイムは実行中にテンソルのタイプを最小精度として扱うため、Float32 MLプログラムはGPUとCPUのみの組み合わせで実行されます。

このデモは、MLプログラムが変換時に正しく実行される最小精度を制御することがいかに簡単であるかを示しました。

また、ニューラルネットワークCore MLモデルとは異なり、モデルがより高い精度を必要とする場合、それを達成するためにアプリコードでコンピューティングユニットの設定をcpuOnlyに変更する必要はありません。

そして最後に、このデモノートブックは、Core ML Toolsのドキュメントサイトで例として入手できます。

要約すると、MLプログラムを取得するには、変換関数を使用し、追加の引数を渡して展開ターゲットを指定し、少なくともiOS 15またはmacOS 12に設定します。

デフォルトでは、Core MLコンバータは、ニューラルエンジンで実行できる最適化されたFloat16モデルを生成します。

場合によっては発生する可能性があるため、モデルがFloat16精度に敏感な場合は、代わりに精度をFloat32に設定するのは簡単です。

実際、Core ML Tools APIには、より高度なオプションがあります。これにより、Float32で実行する特定の操作を選択し、残りをFloat16に保持して混合タイプのMLプログラムを作成できます。

これらの例については、ドキュメントをご覧ください。

要約すると、Core MLには、モデルの調整と作業を容易にするいくつかの新しい機能強化があります。

新しいMLShapedArrayタイプにより、多次元データの操作が容易になります。

MLパッケージ形式を使用すると、Xcodeでメタデータを直接編集できます。

新しいMLプログラムモデルタイプのMLパッケージは、GPUでFloat32をサポートするタイプ実行をサポートし、モデルのパフォーマンスと精度を調整する際により多くのオプションを提供します。

モデルをMLパッケージにアップグレードし、MLプログラムを使用することをお勧めします。

私たちのセッションを見てくれてありがとう、そしてWWDCの残りの部分を楽しんでください。

[音楽]。