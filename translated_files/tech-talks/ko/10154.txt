10154

안녕. 제 이름은 스티브이고, 저는 애플의 엔지니어입니다.

안녕. 난 폴이야. 나도 엔지니어야.

이 비디오에서, 우리는 PyTorch 모델을 Core ML로 변환하는 Core ML의 새로운 측면 중 하나에 대해 자세히 알아볼 것입니다.

WWDC 2020에서, 우리는 변환 과정의 많은 측면을 개선한 코어 ML 컨버터에 대한 점검을 발표했다.

우리는 딥 러닝 커뮤니티에서 가장 일반적으로 사용되는 도서관에 대한 지원을 확대했습니다.

우리는 새로운 인메모리 표현을 활용하여 사용자 경험을 개선하기 위해 변환기 아키텍처를 재설계했습니다.

그리고 우리는 API를 통합하여 모든 모델 소스에서 변환을 호출하는 단일 호출이 있습니다.

아직 본 적이 없다면, 이 새로운 컨버터 아키텍처의 세부 사항에 대한 비디오를 확인하는 것이 좋습니다.

하지만 이 비디오에서, 저는 PyTorch 딥 러닝 프레임워크에 구축된 모델부터 시작하여 모델 변환에 집중할 것입니다.

그래서 아마도 당신은 PyTorch를 사용하여 모델을 훈련시키는 데 열심히 일한 ML 엔지니어일 것입니다.

아니면 온라인에서 킬러 PyTorch 모델을 찾은 앱 개발자일 수도 있고 이제 그 모델을 앱에 넣고 싶을 수도 있습니다.

이제 문제는 PyTorch 모델을 Core ML 모델로 어떻게 변환하느냐는 것입니다.

음, 오래된 코어 ML 컨버터는 프로세스의 단계로 모델을 ONNX로 내보내야 했습니다.

그리고 만약 당신이 그 변환기를 사용했다면, 당신은 그것의 한계에 부딪쳤을 수도 있습니다.

ONNX는 개방형 표준이므로, 진화하고 새로운 기능을 도입하는 것은 느릴 수 있다.

게다가, PyTorch와 같은 ML 프레임워크는 최신 모델 기능을 ONNX로 내보내기 위한 지원을 추가할 시간이 필요하다.

따라서 이전 변환기를 사용하면 ONNX로 내보낼 수 없는 PyTorch 모델을 발견하여 Core ML로의 변환을 차단했을 수 있습니다.

음, 이 추가 종속성을 제거하는 것은 새로운 Core ML 변환기에서 변경된 것 중 하나일 뿐입니다.

그래서 이 비디오에서, 우리는 새로운 PyTorch 모델 변환 경로의 세부 사항을 파헤칠 것입니다.

우리는 실제 변환 예제를 포함하여 PyTorch 모델을 Core ML로 변환하는 다양한 방법을 살펴볼 것입니다.

그리고 마지막으로, 도중에 문제가 발생할 경우 따라야 할 몇 가지 유용한 팁을 공유하겠습니다.

이제 새로운 전환 과정에 뛰어들자.

변환하려는 PyTorch 모델부터 시작하여, PyTorch의 JIT 모듈을 사용하여 TorchScript라는 표현으로 변환할 수 있습니다.

궁금하다면, JIT는 Just In Time의 약자입니다.

그런 다음 TorchScript 모델을 손에 들고, 새로운 Core ML 변환기를 호출하여 앱에 드롭할 수 있는 ML 모델을 생성할 수 있습니다.

비디오의 뒷부분에서, 나는 TorchScript 변환 프로세스가 어떻게 생겼는지 파헤칠 것이다.

하지만 이제 새로운 코어 ML 컨버터가 어떻게 작동하는지 살펴봅시다.

변환기는 파이썬으로 작성되었으며, 호출하려면 몇 줄의 코드만 있으면 됩니다.

TorchScript 객체 또는 디스크에 저장된 경로와 모델에 대한 입력에 대한 설명이 될 수 있는 모델을 제공하기만 하면 됩니다.

모델의 출력에 대한 몇 가지 정보를 포함할 수도 있지만, 이는 선택 사항입니다.

변환기는 TorchScript 그래프의 작업을 반복하고 하나씩 Core ML로 변환하여 작동합니다.

때때로 하나의 TorchScript 작업이 여러 Core ML 작업으로 변환될 수 있습니다.

다른 경우, 그래프 최적화 패스는 알려진 패턴을 감지하고 여러 작업을 하나로 융합할 수 있다.

이제, 때때로 모델은 변환기가 이해하지 못하는 사용자 지정 작업을 포함할 수 있다.

하지만 그건 괜찮아. 컨버터는 확장 가능하도록 설계되었으므로, 새로운 작업에 대한 정의를 쉽게 추가할 수 있습니다.

많은 경우에, 당신은 그 작업을 우리가 "복합 작전"이라고 부르는 기존 작업의 조합으로 표현할 수 있습니다.

하지만 그것이 충분하지 않다면, 사용자 지정 스위프트 구현을 작성하고 변환 중에 타겟팅할 수도 있습니다.

나는 이 비디오에서 그것을 하는 방법에 대한 세부 사항에 들어가지 않을 것이지만, 예시와 연습에 대한 온라인 리소스를 확인하세요.

이제 전체 변환 프로세스에 대한 개요를 제공했으므로, 다시 돌아가서 PyTorch 모델에서 TorchScript 모델을 얻는 방법을 파헤칠 때입니다.

PyTorch가 이것을 할 수 있는 두 가지 방법이 있다.

첫 번째는 "추적"이라고 불리고, 두 번째는 "스크립팅"이라고 불린다.

먼저 모델을 추적하는 것이 무엇을 의미하는지 살펴봅시다.

추적은 이 코드 스니펫과 같이 PyTorch의 JIT 모듈의 추적 방법을 호출하여 수행됩니다.

우리는 예제 입력과 함께 PyTorch 모델을 전달하고, 모델과 TorchScript 표현을 반환합니다.

그래서 이 전화는 실제로 무엇을 하나요?

활성 추적은 모델의 순방향 패스를 통해 예제 입력을 실행하고 입력이 모델의 레이어를 통과할 때 호출되는 작업을 캡처합니다.

그 모든 작업의 컬렉션은 모델의 TorchScript 표현이 된다.

이제 추적할 예제 입력을 선택할 때, 사용하기에 가장 좋은 것은 모델이 정상적인 사용 중에 볼 수 있는 것과 유사한 데이터입니다.

예를 들어, 앱이 모델에 제시하는 것과 같은 방식으로 캡처된 검증 데이터 또는 데이터의 한 샘플을 사용할 수 있습니다.

당신은 또한 무작위 데이터를 사용할 수 있습니다.

만약 그렇다면, 입력 값의 범위와 텐서의 모양이 모델이 기대하는 것과 일치하는지 확인하세요.

예시를 통해 이 모든 것을 조금 더 구체적으로 만들어 봅시다.

세분화 모델을 PyTorch에서 Core ML로 변환하는 전체 과정을 안내할 제 동료 Paul을 소개하고 싶습니다.

고마워, 스티브.

내가 세분화 모델을 가지고 있고, 그것이 장치에서 실행되고 싶다고 가정해 봅시다.

세분화 모델이 하는 일에 익숙하지 않다면, 이미지를 가져와 그 이미지의 각 픽셀에 클래스 확률 점수를 할당합니다.

그렇다면 어떻게 하면 내 모델을 장치에서 실행할 수 있을까요?

나는 내 모델을 코어 ML 모델로 변환할 것이다.

이를 위해, 저는 먼저 PyTorch 모델을 추적하여 PyTorch의 JIT 추적 모듈을 사용하여 TorchScript 형식으로 바꿉니다.

그런 다음 새로운 Core ML 변환기를 사용하여 TorchScript 모델을 Core ML 모델로 변환합니다.

마지막으로, 나는 결과 Core ML 모델이 Xcode에 어떻게 원활하게 통합되는지 보여줄 것이다.

이 과정이 코드에서 어떻게 생겼는지 봅시다.

이 Jupyter 노트북에서, 나는 슬라이드에 언급된 PyTorch 세분화 모델을 Core ML 모델로 변환할 것이다.

이 코드를 직접 사용해 보고 싶다면, 이 비디오와 관련된 코드 스니펫에서 사용할 수 있습니다.

먼저, 저는 이 데모에 사용할 몇 가지 종속성을 가져옵니다.

다음으로, 토치비전에서 ResNet-101 세분화 모델과 샘플 입력을 로드합니다: 이 경우, 개와 고양이의 이미지.

PyTorch 모델은 PIL 이미지 객체가 아닌 텐서 객체를 취한다.

그래서 나는 transforms.ToTensor로 이미지를 텐서로 변환한다.

그 모델은 또한 배치 크기를 나타내는 텐서의 추가 차원을 기대하므로, 나는 그것을 추가한다.

슬라이드에서 언급했듯이, 코어 ML 컨버터는 토치스크립트 모델을 받아들인다.

이를 얻기 위해, 저는 PyTorch 모델을 TorchScript 모델로 변환하는 Torch.JIT 모듈의 추적 방법을 사용합니다.

어-오. 추적은 예외를 던졌다.

예외 방법에서 말했듯이, "추적된 함수에서 텐서의 텐서 또는 튜플만 출력할 수 있습니다."

이것은 PyTorch의 JIT 모듈의 한계이다.

여기서 문제는 내 모델이 사전을 반환하고 있다는 것이다.

출력 사전에서 텐서 값만 추출하는 PyTorch 모듈로 모델을 래핑하여 이 문제를 해결합니다.

여기서 저는 PyTorch의 모듈 클래스에서 상속된 클래스 래퍼를 선언합니다.

나는 위에서 사용된 대로 ResNet-101을 포함하는 모델 속성을 정의한다.

이 래핑 클래스의 순방향 방법에서, 나는 "out"이라는 키로 반환된 사전을 인덱싱하고 텐서 출력만 반환합니다.

이제 모델이 사전이 아닌 텐서를 반환하기 때문에, 성공적으로 추적할 것이다.

이제 내가 새로운 코어 ML 변환기를 활용할 시간이다.

먼저, 나는 내 입력과 전처리를 정의해야 한다.

저는 제 입력을 ImageNet 통계로 이미지를 정규화하고 값을 0에서 1 사이로 축소하는 전처리가 있는 ImageType으로 정의합니다.

이 전처리는 ResNet-101이 기대하는 것이다.

다음으로, 저는 단순히 Core ML 도구 변환 메소드를 호출하여 TorchScript 모델과 입력 정의를 전달합니다.

변환 후, Xcode와 같은 다른 프로그램에서 이해할 수 있도록 모델의 메타데이터를 설정할 것입니다.

나는 내 모델의 유형을 세분화로 설정하고 내 모델의 순서대로 클래스를 열거한다.

그래서, 내 변환된 모델이 작동하나요?

Xcode를 통해 모델의 출력을 쉽게 시각화할 수 있습니다.

먼저, 내 모델을 저장할게.

이제 Finder에서 저장된 모델을 클릭하기만 하면 Xcode에 의해 열립니다.

여기서 입력 모양과 유형을 포함한 메타데이터를 볼 수 있습니다.

모델의 출력을 시각화하기 위해, 미리보기 탭으로 이동하여 개와 고양이의 샘플 이미지를 드래그하겠습니다.

내 모델이 이 이미지에서 애완동물을 성공적으로 세분화하고 있는 것 같아.

ResNet-101은 추적할 수 있었지만, 일부 모델은 추적할 수 없다.

이 다른 모델들을 변환하는 방법을 설명하기 위해, 나는 그것을 스티브에게 돌려줄게.

고마워, 폴.

알았어. 나는 우리가 추적을 사용하여 전환이 어떻게 작동하는지 꽤 잘 알고 있다고 생각한다.

하지만 PyTorch는 TorchScript를 얻을 수 있는 두 번째 방법을 제공한다.

그러니 이제 "스크립팅"이라고 불리는 것을 파헤쳐봅시다.

스크립팅은 PyTorch 모델을 가지고 TorchScript 작업으로 직접 컴파일하여 작동합니다.

추적은 데이터가 흐르면서 모델을 캡처했다는 것을 기억하세요.

하지만 추적과 마찬가지로, 모델을 스크립팅하는 것도 정말 쉽다.

PyTorch의 JIT 모듈의 스크립트 방법을 호출하고 모델을 제공하기만 하면 됩니다.

알았어. 나는 당신에게 TorchScript 표현을 얻는 두 가지 다른 방법을 보여주었고, 당신은 언제 하나를 사용해야 할지 궁금할 것입니다.

스크립팅을 사용해야 하는 한 가지 경우는 모델에 제어 흐름이 포함되어 있는 경우입니다.

이유를 이해하기 위해 예를 살펴봅시다.

여기서, 이 모델에는 분기와 루프가 있으며, 스크립팅은 모델을 직접 컴파일하기 때문에 모든 것을 캡처할 것이다.

우리가 모델을 추적한다면, 우리가 얻는 것은 주어진 입력에 대한 모델을 통과하는 경로일 뿐이며, 전체 모델을 캡처하지 않는다는 것을 볼 수 있습니다.

모델을 스크립팅해야 하는 경우, 가능한 한 많은 모델을 추적하고 필요한 모델의 부분만 스크립팅하면 일반적으로 최상의 결과를 얻을 수 있습니다.

이것은 추적이 보통 스크립팅보다 더 간단한 표현을 생성하기 때문이다.

코드를 보고 이 아이디어를 적용하는 방법을 봅시다.

이 예에서, 나는 루프 내에서 고정된 횟수의 코드 덩어리를 실행하는 모델을 가지고 있다.

나는 루프의 본문을 그 자체로 쉽게 추적할 수 있는 것으로 분리한 다음, 모델 전체에 스크립팅을 적용할 수 있다.

우리가 기본적으로 하는 일은 스크립팅을 필요한 제어 흐름의 비트로 제한한 다음 다른 모든 것을 추적하는 것이다.

이 추적과 스크립팅의 혼합은 둘 다 이미 TorchScript로 변환된 코드를 건너뛰기 때문에 작동합니다.

이제 스크립팅을 사용하는 구체적인 예를 살펴볼 때입니다.

내가 그것을 폴에게 돌려줄게, 폴은 너에게 언어 모델을 변환하는 과정을 안내해 줄 거야.

안녕.

장치에서 실행할 수 있도록 코어 ML 모델로 변환하고 싶은 문장 완성 모델이 있다고 가정해 봅시다.

어떤 맥락에서, 문장 완성은 문장 조각을 취하고 그 다음에 올 가능성이 있는 단어를 예측하기 위해 모델을 사용하는 작업이다.

그래서 이것은 계산 단계의 관점에서 어떻게 생겼나요?

나는 문장 조각의 몇 단어로 시작하여 그 단어들을 내 모델이 이해할 수 있는 표현으로 번역하는 인코더라고 불리는 것을 통과할 것이다.

이 경우, 정수 토큰의 시퀀스.

다음으로, 나는 그 토큰 시퀀스를 내 모델에 전달할 것이며, 이는 시퀀스의 다음 토큰을 예측할 것이다.

나는 내 모델에 부분적으로 구성된 문장을 계속 공급할 것이며, 내 모델이 특별한 문장 종료 토큰을 예측할 때까지 마지막에 새로운 토큰을 추가할 것이며, 이는 내 문장이 완료되었음을 의미합니다.

이제 토큰의 완전한 문장을 가졌으니, 토큰을 다시 단어로 변환하는 디코더를 통해 전달할 것이다.

토큰 목록을 완성하는 이 다이어그램의 중간 부분은 내가 코어 ML 모델로 변환할 것이다.

인코더와 디코더는 별도로 처리됩니다.

의사 코드를 보고 무슨 일이 일어나고 있는지 이해하도록 합시다.

내 모델의 핵심은 내 다음 토큰 예측 변수이다.

이를 위해, 나는 Hugging Face의 GPT2 모델을 사용할 것이다.

예측 변수는 토큰 목록을 입력으로 취하고 다음 토큰에 대한 예측을 제공합니다.

다음으로, 나는 문장 끝 토큰을 볼 때까지 계속하기 위해 예측기 주위의 제어 흐름을 마무리할 것이다.

루프 내부에서, 나는 예측된 토큰을 실행 목록에 추가하고 그것을 모든 루프의 예측 변수에 대한 입력으로 사용한다.

내 예측자가 문장 끝 토큰을 반환할 때, 나는 디코딩을 위한 완전한 문장을 반환할 것이다.

이제 이 전체 프로세스가 인코딩되는 것을 보려면, Jupyter Notebook에 뛰어들어 봅시다.

이 공책에서, 나는 문장 조각을 가지고 문장을 완성하는 언어 모델을 구성할 것이다.

수입을 방해하지 말자.

여기 제 모델의 코드가 있습니다.

내 모델은 torch.Module에서 상속되며 문장 끝 토큰, next_token_predictor 모델 및 문장의 시작을 나타내는 기본 토큰에 대한 속성을 포함합니다.

슬라이드에서와 마찬가지로, 나는 토큰 목록을 가지고 다음 것을 예측하는 루프 본문을 작성했다.

루프는 문장 종료 토큰이 생성될 때까지 계속된다.

이런 일이 발생하면, 우리는 문장을 반환할 것이다.

언급했듯이, 나의 다음 토큰 예측 변수는 루프 본문에 상주할 GPT2가 될 것이다.

나는 전체 모델의 스크립팅과 별도로 루프 바디를 추적하는 관행을 따를 것이다.

그래서 나는 다음 토큰 예측기에서만 JIT 트레이서를 실행할 것이다.

토큰 목록을 입력으로 하므로, 추적을 위해 임의의 토큰 목록을 전달할 것입니다.

나는 추적자가 이 추적이 다른 입력으로 일반화되지 않을 수도 있다는 경고를 방출했다는 것을 알 수 있다.

이 경고는 Core ML이 아니라 PyTorch의 JIT 트레이서에서 온 것입니다.

나중에 문제 해결 섹션에서 무슨 일이 일어나고 있는지 설명될 것이지만, 지금은 실제로 문제가 없기 때문에 이 경고를 무시할 것이다.

루프 바디의 대부분을 추적하면, 문장 마무리 모델을 인스턴스화하고 JIT 스크립터를 적용하여 Core ML로 변환할 수 있습니다.

이제 TorchScript 모델로 세분화 데모와 마찬가지로 Core ML 모델로 변환합니다.

이제 내 모델이 문장을 끝낼 수 있는지 볼게.

나는 문장 조각을 만든다: 이 경우, "맨해튼 다리는."

그런 다음 GPT2에 포함된 인코더를 통해 실행하여 조각의 인코딩을 얻고, 토큰 목록을 토치 텐서로 변환합니다.

다음으로, 코어 ML 모델의 입력을 패키징하고, 해당 모델을 실행하고, GPT2에 포함된 디코더로 출력을 디코딩합니다.

좋아. 코어 ML 모델은 문장을 완성할 수 있었다.

맨해튼 다리에 대한 성명을 작성한 것 같다.

모델을 추적하고 스크립팅하여 코어 ML 형식으로 변환할 때 길을 따라 충돌에 부딪힐 수 있습니다.

가는 길에 너를 돕기 위해 스티브에게 돌려줄게.

마무리하기 전에, PyTorch 모델을 Core ML로 변환할 때 닥친 걸림돌을 검토하고 몇 가지 문제 해결 팁과 모범 사례를 검토하고 싶습니다.

세분화 데모를 다시 생각해보면, 우리가 추적하는 동안 오류가 발생했다는 것을 기억하세요.

이것은 우리 모델이 사전을 반환했고 JIT 추적은 텐서나 텐서의 튜플만 처리할 수 있기 때문이다.

우리가 데모에서 보여준 해결책은 모델의 기본 출력을 푸는 모델 주위에 얇은 래퍼를 만드는 것이었다.

기억하세요, 이 예에서, 모델은 사전을 반환했기 때문에, 여기서 우리는 추론 결과를 나타내는 사전 키에 액세스하고 그 텐서를 반환하고 있습니다.

물론, 이 아이디어는 사전에서 여러 항목에 액세스하고 반환하거나 다른 유형의 컨테이너의 압축을 풀어야 하는 경우에도 효과가 있습니다.

이제 언어 모델 데모 중에, 우리는 추적이 다른 입력으로 일반화되지 않을 수도 있다는 추적 경고가 발생했다.

그리고 우리는 트레이서가 성가신 코드 라인을 유용하게 인쇄하는 것을 본다.

그래서 실제로 무슨 일이 일어나고 있어?

경고를 이해하기 위해 모델 소스 코드를 보면, 모델이 다른 텐서의 크기에 따라 하나의 텐서를 자르고 있다는 것을 알 수 있다.

텐서의 크기를 얻는 것은 베어 파이썬 값을 초래합니다. 즉, PyTorch 텐서가 아닙니다. 그리고 트레이서는 이러한 베어 파이썬 값에서 수행되는 수학 연산을 추적할 수 없다고 경고합니다.

그러나, 이 경우 추적자는 이 경고를 방출하는 데 너무 공격적이며, 실제로는 문제가 없다.

베어 파이썬 값에서 작동하는 코드를 추적할 때 좋은 경험 법칙은 내장된 파이썬 작업만 트레이서에 의해 올바르게 캡처된다는 것입니다.

여기 이 아이디어를 설명하는 데 도움이 되는 몇 가지 예가 있습니다.

이것들을 통해 생각하고, 그 경험 법칙에 따라, 그것들이 올바르게 추적될 것인지 아닌지 알아내자.

첫 번째 예는 우리가 데모 중에 본 것과 매우 유사하며, 이 경우 내장 작업이 적용되고 있기 때문에 올바른 추적이 발생할 것입니다.

두 번째 예는 또한 올바르게 추적할 것이며, 이 경우 모듈로 연산자를 사용하며, 이는 다시 내장된 작업이다.

하지만 세 번째 예는 정확하게 추적되지 않을 것이다.

JIT 트레이서는 라이브러리 함수 math.sqrt가 무엇을 하는지 모르며, 추적된 그래프는 텐서 크기와 제곱근을 계산하는 작업 대신 일정한 값을 기록할 것이다.

하지만 math.sqrt를 파이썬의 내장 전력 연산자로 대체하기 위한 모델에 대한 간단한 수정으로, 이것은 올바른 추적을 초래할 것이다.

이제 모델 스크립팅이 실패할 수 있는 경우를 살펴봅시다.

이 모델은 빈 목록으로 시작하여 고정된 정수 세트를 연속적으로 추가합니다.

이것은 매우 유용한 모델이 아니라는 것을 명심하세요.

나는 단지 실패 조건을 설명하기 위해 그것을 사용하고 있다.

이 모델을 스크립팅하면, 유형 불일치를 암시하는 런타임 오류가 발생할 것이다.

JIT 스크립터는 모델을 TorchScript로 바꾸기 위해 유형 정보가 필요하며 컨텍스트에서 객체 유형을 꽤 잘 추론합니다.

그러나, 그것이 불가능할 때가 있으며, 스크립터가 객체의 유형을 알아낼 수 없다면, 객체가 텐서라고 가정합니다.

이 경우, 이 목록은 실제로 정수 목록으로 구축되는 동안 텐서 목록이라고 가정합니다.

그래서 내가 스크립터를 돕기 위해 무엇을 할 수 있을까?

음, 저는 변수의 의미 있는 초기화를 포함하거나 유형 주석을 사용할 수 있습니다.

여기서, 나는 둘 다의 예를 보여주기 위해 모델을 조정했다.

내가 마지막으로 언급하고 싶은 게 하나 있어.

당신은 추적하기 전에 항상 모델이 평가 모드에 있는지 확인하고 싶습니다.

이것은 모든 레이어가 훈련이 아닌 추론을 위해 구성되도록 보장한다.

대부분의 층에서, 이것은 중요하지 않다.

하지만, 예를 들어, 모델에 드롭아웃 레이어가 있다면, 평가 모드를 설정하면 비활성화되어 있는지 확인할 수 있습니다.

그리고 변환기가 비활성화된 작업을 만나면, 그것들을 패스스루 작업으로 취급할 것이다.

우리는 이 비디오에서 많은 자료를 다루었지만, Core ML 변환기 문서, 사용자 지정 op 변환에 대한 정보 및 많은 자세한 TorchScript 예제를 포함하여 비디오와 관련된 링크에서 더 많은 정보를 찾을 수 있습니다.

우리는 PyTorch 모델 변환을 위한 일류 지원을 제공하게 되어 정말 기쁩니다.

새로운 Core ML 변환기가 PyTorch 모델에 대한 광범위한 지원을 가능하게 하고, 장치 내 모델 실행을 최적화할 수 있으며, 모델을 쉽게 변환할 수 있도록 최대한의 지원을 제공할 수 있기를 바랍니다.

봐줘서 고마워.