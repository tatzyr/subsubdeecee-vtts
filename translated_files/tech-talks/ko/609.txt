609

안녕하세요, AR의 고급 장면 이해에 오신 것을 환영합니다.

이 비디오에서는 새로운 iPad Pro의 LiDAR 스캐너로 활성화된 ARKit과 RealityKit의 새로운 기능을 소개하겠습니다.

iOS와 ipadOS는 개발자에게 AR용 앱을 구축하는 데 도움이 되는 두 가지 강력한 프레임워크를 제공합니다.

ARKit은 위치 추적, 장면 이해 및 렌더링 기술과의 통합을 결합하여 후면 또는 전면 카메라를 사용하여 다양한 AR 경험을 가능하게 합니다.

그리고 AR을 위해 특별히 제작된 새로운 고급 프레임워크인 RealityKit은 사실적인 렌더링 및 특수 효과, 확장 가능한 성능 및 Swift 기반 API를 제공하여 훌륭한 AR 경험을 쉽게 프로토타입하고 구축할 수 있습니다.

오늘 우리는 새로운 iPad Pro의 하드웨어 기능에 의해서만 가능한 두 프레임워크의 발전에 대해 이야기하게 되어 기쁩니다.

새로운 iPad Pro에는 LiDAR 스캐너가 장착되어 있습니다.

이것은 빛이 당신 앞에 있는 물체에 도달하고 반사하는 데 걸리는 시간을 나노초 속도로 측정하여 거리를 결정하는 데 사용됩니다.

이것은 최대 5미터 떨어진 곳에서 효과적이며 실내와 실외 모두에서 작동합니다.

그리고 이 기능을 와이드 및 울트라 와이드 카메라로 캡처한 정보와 결합하면 환경에 대한 놀라운 이해를 얻을 수 있습니다.

그래서 이것이 ARKit을 시작으로 각 프레임워크로 구축된 AR 경험을 어떻게 개선하는지 봅시다.

ARKit 버전 3.5의 새로운 업데이트는 새로운 iPad Pro에서 LiDAR 스캐너를 최대한 활용하는 많은 새로운 기능과 개선 사항을 제공합니다.

Scene Geometry는 앱에 주변 환경의 상세한 토폴로지 지도를 제공하는 새로운 API입니다.

LiDAR 스캐너는 또한 표면을 더 빠르고 정확하게 감지하여 AR 온보딩 경험을 단순화합니다.

그리고 모션 캡처, 피플 오클루전 및 레이캐스팅을 포함한 기존 ARKit 기능도 추가 애플리케이션 변경 없이도 도움이 됩니다.

그럼 장면 기하학부터 시작합시다.

장면 기하학은 환경의 위상 매핑을 나타내는 삼각형 메쉬를 제공합니다.

그리고 선택적으로, 그 메쉬는 보이는 것을 분류하는 의미론적 정보를 포함할 수 있다.

여기에는 테이블과 의자, 바닥, 벽, 천장, 창문 등이 포함됩니다.

이 모든 정보는 실제 물체, 환경 의존 물리학, 장면에서 실제 및 가상 물체의 조명에 의한 가상 콘텐츠의 오클루전을 허용하는 데 사용될 수 있습니다.

그래서 이 매핑이 작동하는 것을 봅시다.

여기 실내 장면에서 가져온 예가 있습니다.

우리는 LiDAR 센서를 사용하여 ARKit에 의해 생성된 메쉬로 AR 프레임 이미지를 오버레이하고 있습니다.

우리가 방을 쓸면서 가구의 모양과 환경의 레이아웃을 얼마나 빨리 감지할 수 있는지 알 수 있습니다.

그리고 색상은 메쉬 오버레이의 분류를 기반으로 합니다.

그럼 API를 살펴봅시다.

장면 기하학은 ARWorldTtrackingConfiguration의 새로운 장면 재구성 속성을 통해 활성화됩니다.

이제, 어떤 데이터를 생성하고 싶은지에 따라 두 가지 옵션이 있습니다.

첫 번째 옵션은 메쉬만 생성하는 것인데, 이는 위상 정보만 표면화된다는 것을 의미한다.

이것은 주변 물체의 분류에 의존하지 않는 물체 배치와 같은 일을 하는 앱을 위한 것입니다.

다른 옵션은 .meshWithClassification이다.

그리고 이름에서 알 수 있듯이, 이것은 모든 장면 기하학에 대한 의미론적 분류를 추가한다.

이것은 바닥과 테이블의 다른 조명과 같은 장면에 있는 것에 따라 다른 행동을 원하는 앱에 유용합니다.

그리고 여기 아래에서, 당신은 코드를 볼 수 있습니다; 그것은 꽤 간단합니다.

우리는 세계 추적을 사용하고 있으며, 장면 재구성을 지원하는 장치에서 실행되고 있는지 확인하기 위해 테스트합니다.

그렇다면, 우리는 여기서 메쉬 옵션을 선택하고 세션을 실행하기 시작합니다.

장면 재구성이 활성화된 상태에서 세션이 실행되면, AR 세션은 AR 메쉬 앵커를 반환합니다.

이것들은 다른 앵커와 같으며, 변경 사항은 did add(anchor:), did update(anchor:), did remove(anchor:)와 같은 일반적인 AR 세션 위임 방법을 통해 이루어집니다.

그리고 각 메쉬 앵커는 메쉬 기하학의 지역 영역을 나타낸다.

그것은 앵커와 메쉬 기하학 물체의 변형으로 설명된다.

ARMeshGeometry 객체는 주변 환경을 나타내는 데 필요한 모든 정보를 보유하고 있습니다.

각 객체에는 각 단계에서 활성화된 경우 정점, 법선, 얼굴 및 의미 분류 목록이 포함되어 있습니다.

이것들은 모두 렌더러에 직접 통합될 수 있도록 MTLBuffers로 제공됩니다.

이제 장면 기하학과 평면 감지 사이에 일어나는 흥미로운 상호 작용이 있다.

장면 재구성과 평면 감지가 모두 활성화되면, 구성된 메쉬는 겹치는 평면과 일치하도록 평평해질 것이다.

이것은 매끄러운 물체 움직임을 위해 표면이 일관되게 평평해야 하는 물체 배치에 유용합니다.

반면에, 장면 기하학을 사용하고 평면 감지가 활성화되지 않은 경우, 메쉬는 더 이상 평평하지 않습니다.

하지만 이 조합은 메쉬 표면에서 더 많은 디테일을 보존할 것이다.

그래, 그게 장면 기하학이야.

다음으로, LiDAR 스캐너에 의해 활성화된 몇 가지 개선 사항이 더 있습니다.

첫 번째는 훨씬 더 간단하고 빠른 온보딩이다.

LiDAR 스캐너를 사용하면 평면 표면이 거의 즉각적이고 더 정확하게 감지됩니다.

이것은 흰색 벽과 같은 낮은 특징의 표면에서도 사실이다.

그래서 그 결과는 이전에 몇 초가 걸렸고 어느 정도의 사용자 지침이 필요했던 비행기 탐지 온보딩이 이제 완전히 원활하게 발생할 수 있다는 것이다.

그리고 변화가 필요하지 않습니다.

모든 ARKit 앱은 새로운 iPad Pro에서 실행할 때 이것의 이점을 누릴 수 있습니다.

기존 앱은 또한 개선된 레이캐스팅의 혜택을 받을 것이다.

ARKit의 향상된 장면 이해는 수평 및 수직 평면에 대해 더 빠르고 정확한 레이캐스팅을 가능하게 합니다.

게다가, 새로운 iPad Pro는 그 어느 때보다 더 넓은 범위의 표면에 대해 레이캐스트할 수 있다.

허용 대상을 추정 비행기로 설정하고 LiDAR 스캐너의 데이터는 주변 환경과 일치하는 레이캐스팅 결과를 제공합니다.

예를 들어, 물체는 이제 여기에서 볼 수 있듯이 큰 의자나 소파의 모든 표면에 놓을 수 있습니다.

더 정확한 깊이 정보를 제공하는 LiDAR 스캐너로 인해 모션 캡처와 사람 폐색도 개선되었습니다.

모션 캡처를 사용하는 앱은 더 정확한 스케일 추정의 이점을 누릴 수 있으며, 사람들의 폐색에 대한 깊이 값도 더 정확합니다.

또한, 두 기능이 모두 활성화되면 People occlusion과 Scene Geometry API가 함께 작동할 수 있습니다.

사람들의 매우 역동적인 기하학은 장면 재구성에서 제외될 수 있으며, 이는 차례로 실제 환경의 보다 안정적인 메쉬를 제공한다.

그래서 그것은 새로운 iPad Pro의 ARKit 3.5를 빠르게 살펴보는 것이다.

장면 기하학은 주변 환경의 위상 지도를 제공합니다.

플레이너 표면은 거의 즉각적이고 정확하게 감지되어 온보딩을 단순화합니다.

레이캐스팅은 더 정확하고 장면 기하학을 고려할 수 있으며, 모션 캡처와 사람 폐색도 개선됩니다.

ARKit은 또한 RealityKit이라는 더 높은 수준의 AR 프레임워크와 긴밀하게 통합되어 있습니다.

RealityKit은 사실적인 렌더링, 카메라 효과, 애니메이션, 물리학 등을 제공합니다.

그것은 AR을 위해 특별히 처음부터 지어졌다.

RealityKit은 새로운 ARKit 3.5 기능을 활용하여 신규 또는 기존 RealityKit 앱에 쉽게 통합할 수 있습니다.

이러한 기능은 새로운 장면 이해 API를 통해 접근할 수 있다.

LiDAR 강화 물리학, 폐색 및 조명을 활성화할 수 있는 옵션을 제공하며, ARView의 몇 가지 간단한 설정을 통해 모두 액세스할 수 있으므로 살펴봅시다.

새로운 iPad Pro를 통해 RealityKit은 현실 세계에서 감지하는 표면에서 장면 기하학의 가상 물체 간의 물리적 상호 작용을 결정할 수 있습니다.

그래서 당신은 실제 가구에서 가상 공을 튕겨낼 수 있습니다.

이렇게 하려면, 먼저 가상 콘텐츠인 ModelEntity에 대한 충돌 모양을 생성하고 물리 본문을 초기화합니다.

그런 다음 ARView의 sceneUnderstanding.options 세트에 물리 옵션을 추가하면 RealityKit이 나머지를 처리할 것입니다.

오클루전과 마찬가지로, RealityKit은 출입구, 테이블, 의자와 같은 실제 물체에서 감지된 장면 기하학을 사용하여 장면의 가상 물체를 차단합니다.

이건 완전히 자동이야.

이를 활성화하려면 ARView에 설정된 sceneUnderstanding.options에 오클루전을 추가하기만 하면 됩니다.

RealityKit은 후드 아래의 다른 모든 것을 처리할 것이며, 가상 콘텐츠는 환경의 모든 주요 물체에 의해 차단될 것입니다.

여기 그것의 예가 있습니다.

우리의 가상 로봇은 바닥을 돌아다니고 있다.

하지만 카메라가 기둥 뒤로 움직이면 로봇이 막힌 것을 볼 수 있습니다.

그것의 기하학은 렌더링에서 사라지고 있으며, 그것은 장면에서 적절한 깊이의 환상을 유지하는 데 도움을 준다.

이제 세 번째 작품은 조명을 위한 것이다.

새로운 RealityKit을 사용하면 가상 광원이 실제 표면을 밝힐 수 있습니다.

이것은 우리가 LiDAR 스캐너의 도움으로 그 표면에 매우 정확하게 맞는 장면 기하학을 밝힐 수 있기 때문입니다.

그리고 이전과 마찬가지로, 이것을 가능하게 하는 것은 ARView의 sceneUnderstanding.options 세트에 receivesLighting을 추가하는 것만큼 간단합니다.

그리고 마지막으로, 이러한 기능에 대한 지원은 Reality Composer에 내장된 장면으로 확장됩니다.

씬 지오메트리 메쉬를 사용하여 가상 물체와 현실 세계를 충돌하도록 물리학을 구성할 수 있으며, 인스펙터 패널에서 실제 물체에 의한 가상 콘텐츠의 오클루전을 활성화할 수 있습니다.

자세한 내용은 developer.apple.com을 방문하여 문서, 샘플 코드 또는 이와 같은 개발자 비디오 등에 대한 링크를 찾을 수 있습니다.

봐줘서 고마워!