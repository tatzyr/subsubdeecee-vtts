601

iOS 11은 iPhone 및 iPad용 증강 현실 앱을 만들기 위한 새로운 프레임워크인 ARKit을 도입했습니다.

ARKit은 디지털 물체를 주변 환경에 배치하여 앱을 화면 너머로 가져와 완전히 새로운 방식으로 현실 세계와 상호 작용할 수 있도록 합니다.

WWDC에서 우리는 ARKit의 세 가지 주요 기능을 도입했습니다.

위치 추적은 장치의 포즈를 감지하여 iPhone 또는 iPad를 주변의 디지털 세계로 들어갈 수 있는 창으로 사용할 수 있습니다.

장면 이해는 탁상과 같은 수평 표면을 감지하고, 안정적인 앵커 포인트를 찾고, 주변 조명 조건의 추정치를 제공하며, SpriteKit, SceneKit 및 Metal과 같은 렌더링 기술뿐만 아니라 Unity 및 Unreal과 같은 인기 있는 게임 엔진과의 통합을 제공합니다.

이제 iPhone X와 함께, ARKit은 전면 카메라를 사용하여 얼굴 추적을 제공하여 초점을 맞춥니다.

이 새로운 능력은 6개의 자유도에서 강력한 얼굴 감지와 위치 추적을 가능하게 한다.

얼굴 표정은 또한 실시간으로 추적되며, 앱에는 감지된 얼굴의 50개 이상의 특정 근육 움직임을 나타내는 피팅된 삼각형 메쉬와 가중치가 있는 매개 변수가 제공됩니다.

AR의 경우, 우리는 카메라의 전면 컬러 이미지와 전면 깊이 이미지를 제공합니다.

그리고 ARKit은 당신의 얼굴을 조명 프로브로 사용하여 조명 조건을 추정하고, 렌더링에 적용할 수 있는 구형 고조파 계수를 생성합니다.

그리고 제가 언급했듯이, 이 모든 것은 iPhone X에서만 지원됩니다.

얼굴 추적으로 할 수 있는 정말 재미있는 것들이 있습니다.

첫 번째는 셀카 효과로, 가상 문신이나 페이스 페인트와 같은 효과를 위해 얼굴 메쉬에 반투명 질감을 렌더링하거나, 화장을 하거나, 수염이나 콧수염을 기르거나, 보석, 마스크, 모자, 안경으로 메쉬를 오버레이하는 것입니다.

두 번째는 얼굴 캡처로, 얼굴 표정을 실시간으로 캡처하고 아바타 또는 게임의 캐릭터에 표정을 투사하기 위해 리깅으로 사용합니다.

그러니 세부 사항에 대해 자세히 알아보고 얼굴 추적을 시작하는 방법을 알아봅시다.

가장 먼저 해야 할 일은 ARSession을 만드는 것입니다.

ARSession은 장치 구성부터 다양한 AR 기술 실행에 이르기까지 ARKit에 대한 모든 처리를 처리하는 객체입니다.

세션을 실행하려면, 우리는 먼저 이 앱에 대해 어떤 종류의 추적을 원하는지 설명해야 합니다.

그래서 이렇게 하기 위해, 당신은 얼굴 추적을 위한 특정 ARConfiguration을 만들고 설정할 것입니다.

이제 처리를 시작하려면 세션에서 "실행" 메소드를 호출하고 실행하려는 구성을 제공하기만 하면 됩니다.

내부적으로, ARKit은 카메라 이미지와 센서 데이터를 받기 위해 AVCaptureSession과 CMMotionManager를 구성할 것이다.

그리고 처리 후, 결과는 ARFrames로 출력될 것이다.

각 ARFrame은 카메라 이미지, 추적 데이터 및 앵커 포인트를 제공하는 시간의 스냅샷입니다. 기본적으로 장면을 렌더링하는 데 필요한 모든 것입니다.

이제 얼굴 추적을 위한 ARConfiguration을 자세히 살펴봅시다.

우리는 ARFaceTrackingConfiguration이라는 새로운 서브클래스를 추가했습니다.

이것은 ARSession에 전면 카메라를 통해 얼굴 추적을 활성화하도록 지시하는 간단한 구성 서브클래스입니다.

장치에서 얼굴 추적의 가용성과 조명 추정을 활성화할지 여부를 확인할 수 있는 몇 가지 기본 속성이 있습니다.

그런 다음 "run"을 호출하면 추적을 시작하고 ARFrame을 받기 시작할 것입니다.

얼굴이 감지되면, 세션은 ARFaceAnchor를 생성할 것이다.

이것은 주요 얼굴을 나타낸다 - 카메라의 시야에서 가장 크고 가장 가까운 얼굴이다.

ARFaceAnchor는 슈퍼클래스의 변환 속성을 통해 세계 좌표에서 얼굴 포즈를 제공합니다.

그것은 또한 현재 표정의 3D 토폴로지와 매개 변수를 제공한다.

그리고 보시다시피, 그것은 모두 추적되고, 메쉬와 매개 변수는 초당 60번 실시간으로 업데이트됩니다.

이제 토폴로지에 초점을 맞춘 ARKit은 치수, 모양 및 사용자의 얼굴 표정에 실시간으로 맞는 얼굴의 상세한 3D 메쉬를 제공합니다.

이 데이터는 몇 가지 다른 형태로 사용할 수 있습니다; 첫 번째는 ARFaceGeometry 클래스입니다.

이것은 본질적으로 삼각형 메쉬이므로 렌더러에서 시각화할 수 있는 정점, 삼각형 인덱스 및 텍스처 좌표 배열입니다.

ARKit은 또한 모든 SceneKit 노드에 연결할 수 있는 지오메트리 객체를 정의하는 ARSCNFaceGeometry 클래스를 통해 SceneKit에서 메쉬를 시각화하는 쉬운 방법을 제공합니다.

이제 기하학 메쉬 외에도, 우리는 또한 혼합 모양이라고 부르는 것을 가지고 있다.

블렌드 모양은 현재 표정의 높은 수준의 모델을 제공한다.

그것들은 눈꺼풀, 눈썹, 턱, 코 등 특정 특징의 포즈를 나타내는 명명된 계수의 사전입니다. 모두 중립 위치와 관련이 있습니다.

그것들은 0에서 1까지의 부동 소수점 값으로 표현되며, 모두 실시간으로 업데이트됩니다.

따라서 이러한 혼합 모양 계수를 사용하여 사용자의 얼굴 움직임을 직접 반영하는 방식으로 2D 또는 3D 캐릭터를 애니메이션화하거나 조작할 수 있습니다.

사용 가능한 것에 대한 아이디어를 제공하기 위해, 여기 혼합 모양 계수 목록이 있습니다.

그래서 이들 각각은 독립적으로 추적되고 업데이트됩니다 - 오른쪽과 왼쪽 눈썹, 눈의 위치, 턱, 미소의 모양 등.

얼굴 기하학을 렌더링하거나 3D 캐릭터를 애니메이션화하는 것과 밀접한 관련이 있는 것은 사실적인 조명이다.

그리고 당신의 얼굴을 빛 프로브로 사용함으로써, 얼굴 감지를 실행하는 ARSession은 세계 공간에서의 빛의 강도와 방향을 나타내는 방향 빛 추정치를 제공할 수 있습니다.

대부분의 앱의 경우, 이 조명 벡터와 강도는 충분하다.

하지만 ARKit은 또한 장면에서 감지된 빛의 강도를 나타내는 2도 구형 고조파 계수를 제공한다.

따라서 고급 요구 사항이 있는 앱의 경우, 이것도 활용할 수 있습니다.

그리고 언급해야 할 몇 가지 기능이 더 있다.

색상 데이터가 있는 전면 카메라 이미지 외에도, ARKit은 앱에 전면 깊이 이미지도 제공할 수 있습니다.

그리고 나는 이것을 그레이스케일 이미지로 보여주고 있다.

데이터 자체는 타임스탬프와 함께 AVDepthData 객체로 제공됩니다.

하지만 이것은 ARKit이 60Hz에서 캡처하는 컬러 이미지보다 낮은 주파수인 15Hz에서 캡처되고 있다는 점에 유의하는 것이 중요합니다.

그리고 마지막으로, 모든 ARKit 세션에서 사용할 수 있지만 얼굴 추적에서 특히 흥미로운 기능은 오디오 캡처입니다.

이제 기본적으로 비활성화되어 있지만, 활성화되면 ARSession이 실행되는 동안 마이크에서 오디오 샘플을 캡처하고 일련의 CMSampleBuffers를 앱에 전달합니다.

따라서 이것은 사용자의 얼굴과 목소리를 동시에 포착하고 싶다면 유용합니다.

얼굴 추적에 대한 자세한 정보와 샘플 코드에 대한 링크는 개발자 웹사이트(developer.apple.com/arkit)를 방문하십시오.

봐줘서 고마워!