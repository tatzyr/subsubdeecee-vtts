10154

こんにちは。私の名前はスティーブで、アップルのエンジニアです。

こんにちは。私はポールです。私もエンジニアです。

このビデオでは、PyTorchモデルをCore MLに変換するという、Core MLの新しい側面の1つを深く掘り下げます。

WWDC 2020では、変換プロセスの多くの側面を改善したCore MLコンバータのオーバーホールを発表しました。

私たちは、ディープラーニングコミュニティで最も一般的に使用されるライブラリのサポートを拡大しました。

新しいインメモリ表現を活用して、ユーザーエクスペリエンスを向上させるためにコンバーターアーキテクチャを再設計しました。

そして、APIを統合したので、任意のモデルソースから変換を呼び出すための単一の呼び出しがあります。

まだ見ていない場合は、この新しいコンバータアーキテクチャの詳細に入るビデオをチェックすることをお勧めします。 

しかし、このビデオでは、PyTorchディープラーニングフレームワークに組み込まれたモデルから始めて、モデル変換に焦点を当てます。

だから、あなたはPyTorchを使ってモデルを訓練するのに苦労しているMLエンジニアかもしれません。

あるいは、あなたはオンラインでキラーPyTorchモデルを見つけたアプリ開発者で、そのモデルをアプリにドロップしたいのかもしれません。

問題は、そのPyTorchモデルをCore MLモデルにどのように変換するかです。

さて、古いCore MLコンバーターでは、プロセスのステップとしてモデルをONNXにエクスポートする必要がありました。

そして、そのコンバーターを使用したことがあるなら、その制限のいくつかに遭遇したかもしれません。

ONNXはオープンスタンダードであるため、進化や新機能の導入が遅くなる可能性があります。

それに加えて、PyTorchのようなMLフレームワークは、最新のモデル機能をONNXにエクスポートするためのサポートを追加する時間が必要です。

したがって、古いコンバータを使用すると、ONNXにエクスポートできないPyTorchモデルがあり、Core MLへの変換がブロックされている可能性があります。

さて、この余分な依存関係を削除することは、新しいCore MLコンバータで変更されたことの1つにすぎません。

したがって、このビデオでは、真新しいPyTorchモデル変換パスの詳細を掘り下げます。

実際の変換例など、PyTorchモデルをCore MLに変換するさまざまな方法について説明します。

そして最後に、途中でトラブルに遭遇した場合に従うべきいくつかの役立つヒントを共有します。

それでは、新しい変換プロセスに飛び込みましょう。

変換するPyTorchモデルから始めて、PyTorchのJITモジュールを使用して、TorchScriptと呼ばれる表現に変換します。

興味があれば、JITはJust In Timeの頭字語です。

次に、TorchScriptモデルを手にして、新しいCore MLコンバータを呼び出して、アプリにドロップできるMLモデルを生成します。

ビデオの後半で、そのTorchScript変換プロセスがどのように見えるかを掘り下げます。

しかし、では、新しいCore MLコンバータがどのように機能するかを見てみましょう。

コンバーターはPythonで書かれており、それを呼び出すには数行のコードしかかかりません。

TorchScriptオブジェクトまたはディスクに保存されているオブジェクトへのパス、およびモデルへの入力の説明を提供するだけです。

モデルの出力に関する情報を含めることもできますが、それはオプションです。

コンバーターは、TorchScriptグラフの操作を反復処理し、それらを1つずつCore MLと同等のものに変換することによって機能します。

1つのTorchScript操作が複数のCore ML操作に変換されることがあります。

また、グラフ最適化パスは既知のパターンを検出し、いくつかの操作を1つに融合する可能性があります。

さて、モデルには、コンバーターが理解していないカスタム操作が含まれている場合があります。

でも、それは大丈夫です。コンバーターは拡張可能に設計されているため、新しい操作の定義を簡単に追加できます。

多くの場合、その操作を既存の操作の組み合わせとして表現することができ、私たちはそれを「composite op」と呼びます。

しかし、それが十分でない場合は、カスタムSwift実装を作成し、変換中にそれをターゲットにすることもできます。

このビデオでは、その方法の詳細については説明しませんが、例やウォークスルーについては、オンラインリソースをご覧ください。

変換プロセス全体の概要を説明したので、PyTorchモデルからTorchScriptモデルを取得する方法を掘り下げる時が来ました。

PyTorchには2つの方法があります。

1つ目は「トレース」と呼ばれ、2つ目は「スクリプト」と呼ばれます。

まず、モデルをトレースすることの意味を見てみましょう。

トレースは、このコードスニペットに示すように、PyTorchのJITモジュールのトレースメソッドを呼び出すことによって行われます。

入力例とともにPyTorchモデルを渡し、モデルとTorchScript表現を返します。

では、この呼び出しは実際に何をしますか?

アクティブトレースは、モデルのフォワードパスを介して入力例を実行し、入力がモデルのレイヤーを通過するときに呼び出される操作をキャプチャします。

その後、これらすべての操作のコレクションは、モデルのTorchScript表現になります。

さて、トレースする入力の例を選ぶとき、使用する最善のことは、モデルが通常の使用中で見るものと同様のデータです。

たとえば、検証データの1つのサンプルを使用するか、アプリがモデルに提示するのと同じ方法でキャプチャされたデータを使用できます。

ランダムなデータを使用することもできます。

その場合は、入力値の範囲とテンソルの形状がモデルが期待するものと一致していることを確認してください。

例を通して作業することで、このすべてをもう少し具体的にしましょう。

同僚のポールを紹介したいと思います。彼は、セグメンテーションモデルをPyTorchからCore MLに変換する完全なプロセスを案内します。

ありがとう、スティーブ。

セグメンテーションモデルを持っていて、それをデバイス上で実行したいとします。

セグメンテーションモデルが何をするのかに精通していない場合は、画像を取得し、その画像の各ピクセルにクラス確率スコアを割り当てます。

では、モデルをデバイス上で実行させるにはどうすればよいですか?

私は自分のモデルをCore MLモデルに変換するつもりです。

これを行うには、まずPyTorchモデルをトレースして、PyTorchのJITトレースモジュールを使用してTorchScriptフォームに変換します。

次に、新しいCore MLコンバータを使用して、TorchScriptモデルをCore MLモデルに変換します。

最後に、結果のCore MLモデルがXcodeにシームレスに統合される方法を紹介します。

このプロセスがコードでどのように見えるか見てみましょう。

このJupyter Notebookでは、スライドに記載されているPyTorchセグメンテーションモデルをCore MLモデルに変換します。

このコードを自分で試してみたい場合は、このビデオに関連付けられたコードスニペットで入手できます。

まず、このデモに使用するいくつかの依存関係をインポートします。

次に、トーチビジョンとサンプル入力からResNet-101セグメンテーションモデルをロードします。この場合、犬と猫の画像です。

PyTorchモデルは、PIL画像オブジェクトではなく、テンソルオブジェクトを取ります。

だから私はtransforms.ToTensorで画像をテンソルに変換します。

モデルはまた、バッチサイズを示すテンソルに余分な次元を期待しているので、私もそれを追加します。

スライドで述べたように、Core MLコンバータはTorchScriptモデルを受け入れます。

これを取得するには、PyTorchモデルをTorchScriptモデルに変換するTorch.JITモジュールのトレースメソッドを使用します。

うーん、ああ。トレースは例外を投げました。

例外メソッドで言うように、「テンソルのテンソルまたはタプルのみがトレース関数から出力できます。」

これはPyTorchのJITモジュールの制限です。

ここでの問題は、私のモデルが辞書を返していることです。

出力辞書からテンソル値のみを抽出するPyTorchモジュールにモデルをラップすることでこれを解決します。

ここでは、PyTorchのモジュールクラスから継承するクラスラッパーを宣言します。

上記のように、ResNet-101を含むモデル属性を定義します。

このラッピングクラスのforwardメソッドでは、返された辞書を「out」という名前のキーでインデックスし、テンソル出力のみを返します。

モデルは辞書ではなくテンソルを返すので、正常にトレースします。

今、新しいCore MLコンバーターを利用する時が来ました。

まず、入力とその前処理を定義する必要があります。

私は自分の入力を、ImageNet統計で画像を正規化し、その値を0から1の間に縮小する前処理を備えたImageTypeとして定義します。

この前処理は、ResNet-101が期待するものです。

次に、Core MLツール変換メソッドを呼び出すだけで、TorchScriptモデルと入力定義を渡します。

変換後、Xcodeなどの他のプログラムで理解できるように、モデルのメタデータを設定します。

モデルのタイプをセグメンテーションに設定し、モデルの順序でクラスを列挙します。

それで、私の変換されたモデルは機能しますか?

Xcodeを通じて、モデルの出力を簡単に視覚化できます。

まず、モデルを保存します。

今、私がする必要があるのは、Finderで保存したモデルをクリックするだけで、Xcodeによって開かれます。

ここでは、入力図形や型を含むメタデータを表示できます。

モデルの出力を視覚化するには、[プレビュー]タブに移動し、犬と猫のサンプル画像をドラッグします。

私のモデルは、この画像のペットをうまくセグメント化しているようです。

ResNet-101は追跡できましたが、一部のモデルを追跡することはできません。

これらの他のモデルを変換する方法を説明するために、スティーブにキックバックします。

ありがとう、ポール。

わかりました。トレースを使用して変換がどのように機能するかについて、私たちはかなり良いハンドルを持っていると思います。

しかし、PyTorchはTorchScriptを取得する2番目の方法を提供します。

それでは、「スクリプティング」と呼ばれるものを掘り下げてみましょう。

スクリプトは、PyTorchモデルを取り、TorchScript操作に直接コンパイルすることで機能します。

トレースは、データが流れるにつれてモデルをキャプチャしたことを忘れないでください。

しかし、トレースと同様に、モデルのスクリプト化も本当に簡単です。

PyTorchのJITモジュールのスクリプトメソッドを呼び出して、モデルを提供するだけです。

わかりました。TorchScript表現を取得する2つの異なる方法を紹介しましたが、いつ一方と他方を使用するのか疑問に思うかもしれません。

スクリプトを使用する必要があるケースの1つは、モデルに制御フローが含まれている場合です。

理由を理解するために例を見てみましょう。

ここでは、このモデルには分岐とループがあり、モデルを直接コンパイルしているため、スクリプトはそのすべてをキャプチャします。

モデルをトレースした場合、得られるのは、指定された入力のモデルを通るパスだけで、モデル全体をキャプチャしていないことがわかります。

モデルをスクリプト化する必要がある場合は、通常、できるだけ多くのモデルを追跡し、それを必要とするモデルの部分のみをスクリプト化すると、最良の結果が得られます。

これは、トレースが通常、スクリプトよりも簡単な表現を生成するためです。

いくつかのコードを見て、このアイデアを適用する方法を見てみましょう。

この例では、ループ内で一定回数のコードチャンクを実行するモデルがあります。

ループの本体を自分で簡単に追跡できるものに分離し、モデル全体にスクリプトを適用することができます。

私たちが基本的にやっていることは、スクリプトをそれを必要とする制御フローのビットだけに制限し、他のすべてをトレースすることです。

このトレースとスクリプトの混合は、両方ともすでにTorchScriptに変換されたコードをスキップするため、機能します。

今度は、スクリプトを使用する具体的な例を見てみましょう。

私はそれをポールに返します。ポールは言語モデルの変換を案内します。

やあ。

デバイス上で実行できるように、Core MLモデルに変換したい文補完モデルがあるとします。

いくつかの文脈では、文の完了は、文の断片を取り、モデルを使用してその後に来る可能性が高い単語を予測することを含むタスクです。

では、これは計算ステップの観点からどのように見えますか?

私は文の断片のいくつかの単語から始めて、それらの単語を私のモデルが理解できる表現に翻訳するエンコーダと呼ばれるものに渡します。

この場合、整数トークンのシーケンス。

次に、そのトークンのシーケンスをモデルに渡し、シーケンス内の次のトークンを予測します。

私は私のモデルに部分的に構築された文を与え続け、私のモデルが特別な文末トークンを予測するまで、最後に新しいトークンを追加します。これは私の文が完了したことを意味します。

トークンの完全な文がわかったので、トークンを言葉に戻すデコーダに渡します。

トークンのリストを完成させるこの図の中央部分は、私がCore MLモデルに変換するものです。

エンコーダとデコーダは別々に扱われます。

疑似コードを見て、何が起こっているのかを確実に理解しましょう。 擬似コードを見てみましょう。

私のモデルのコアは、私の次のトークン予測器です。

このために、私はHugging FaceのGPT2モデルを使用します。

予測器はトークンのリストを入力として受け取り、次のトークンの予測を提供します。

次に、予測器の周りにいくつかの制御フローをラップして、文末トークンが表示されるまで続けます。

ループ内では、予測トークンを実行リストに追加し、それをすべてのループの予測因子への入力として使用します。

予測器が文末トークンを返すと、デコードするための完全な文を返します。

さて、このプロセス全体がエンコードされるのを見るために、Jupyter Notebookに飛び込みましょう。

このノートでは、文の断片を取り、文を完成させる言語モデルを構築します。

輸入品を邪魔にならないようにしましょう。

これが私のモデルのコードです。

私のモデルはtarch.Moduleを継承し、文末トークン、next_token_predictorモデル、および文の先頭を示すデフォルトトークンの属性が含まれています。

そのフォワードメソッドでは、スライドと同じように、トークンのリストを取り、次のトークンを予測するループボディを書きました。

ループは、文末トークンが生成されるまで続きます。

これが起こったら、文を返します。

前述のように、私の次のトークン予測は、ループ本体に存在するGPT2になります。

モデル全体のスクリプト化とは別に、ループ本体をトレースする練習に従います。

だから、次のトークン予測器でのみJITトレーサーを実行します。

トークンのリストを入力として取るので、トレースのために、ランダムなトークンのリストを渡すだけです。

トレーサーは、このトレースが他の入力に一般化されない可能性があるという警告を発したことがわかります。

この警告は、Core MLではなく、PyTorchのJITトレーサーからのものであることに注意してください。

トラブルシューティングセクションで何が起こっているのかは後で説明しますが、実際には問題がないので、今のところこの警告を無視します。

ループボディの大部分をトレースすると、センテンスフィニッシュモデルをインスタンス化し、JITスクリプターを適用してCore MLへの変換の準備をすることができます。

今、私のTorchScriptモデルでは、セグメンテーションのデモと同じようにCore MLモデルに変換します。

今、私のモデルが文章を終えることができるかどうかを確認します。

私は文の断片を作成します。この場合、「マンハッタン橋は」です。

次に、GPT2に含まれているエンコーダを介して実行してフラグメントのエンコーディングを取得し、そのトークンのリストをトーチテンソルに変換します。

次に、Core MLモデルからの入力をパッケージ化し、そのモデルを実行し、GPT2に含まれているデコーダで出力をデコードします。

いいね。コアMLモデルは文を完成させることができました。

マンハッタン橋についての声明を生み出したようです。

モデルをトレースしてスクリプト化してCore ML形式にすると、道路に沿ってバンプに遭遇する可能性があります。

途中であなたを助けるためにそれをスティーブに返します。

締めくくる前に、PyTorchモデルをCore MLに変換する際に気付いた障害を確認し、いくつかのトラブルシューティングのヒントとベストプラクティスを確認したいと思います。

セグメンテーションのデモを振り返ってみると、トレース中にエラーが発生したことを覚えておいてください。

これは、私たちのモデルが辞書を返し、JITトレースがテンソルまたはテンソルのタプルしか処理できないためです。

デモで示した解決策は、モデルのネイティブ出力をアンパックするモデルの周りに薄いラッパーを作成することでした。

この例では、モデルが辞書を返したので、ここでは推論結果を表す辞書キーにアクセスし、そのテンソルを返します。

もちろん、このアイデアは、辞書から複数のアイテムにアクセスして返したい場合や、他のタイプのコンテナを解凍する必要がある場合にも機能します。

言語モデルのデモ中に、トレースが他の入力に一般化されない可能性があるというトレーサーの警告に遭遇しました。

そして、トレーサーが面倒なコード行を印刷するのに役立ちます。

それで、実際に何が起こっているの?

警告を理解するためにモデルのソースコードを見ると、モデルが別のテンソルのサイズに基づいて1つのテンソルをスライスしていることがわかります。

テンソルのサイズを取得すると、PyTorchテンソルではなく、ベアPython値になり、トレーサーはこれらのベアPython値で実行されている数学操作をトレースできないと警告しています。

しかし、この場合、トレーサーはこの警告を発するのに少し攻撃的であり、実際には問題はありません。

裸のPython値で動作するトレースコードに関する良い経験則は、組み込みのPython操作のみがトレーサーによって正しくキャプチャされるということです。

このアイデアを説明するのに役立つ例をいくつか紹介します。

これらを通して考え、その経験則に基づいて、それらが正しく追跡されるかどうかを把握しましょう。

最初の例は、デモ中に見たものと非常によく似ており、組み込み操作、この場合は追加が適用されているため、正しいトレースになります。

2番目の例も正しくトレースします。この場合、モジュロ演算子を使用します。これは、これも組み込み操作です。

しかし、3番目の例は正しくトレースできません。

JITトレーサーは、ライブラリ関数math.sqrtが何をするのかを知りません。トレースされたグラフは、テンソルサイズと平方根を計算する操作の代わりに、一定の値が記録されます。

しかし、math.sqrtをPythonの組み込みパワー演算子に置き換えるモデルの簡単な修正により、正しいトレースが得られます。

では、モデルのスクリプティングが失敗する可能性があるケースを見てみましょう。

このモデルは空のリストから始まり、固定された整数セットを連続して追加します。

これはひどく有用なモデルではないことを覚えておいてください。

故障状態を説明するために使っているだけです。

このモデルをスクリプト化すると、型の不一致を示唆するランタイムエラーが表示されます。

JITスクリプターは、モデルをTorchScriptに変えるために型情報を必要とし、コンテキストからオブジェクト型を推測するのにかなり良い仕事をします。

ただし、それが不可能な場合があり、スクリプターがオブジェクトのタイプを把握できない場合は、オブジェクトがテンソルであると仮定します。

この場合、このリストがテンソルのリストであると仮定していますが、実際には整数のリストとして構築されています。

では、スクリプターを助けるために何ができますか?

さて、変数の有意義な初期化を含めるか、型注釈を使用できます。

ここでは、両方の例を示すためにモデルを調整しました。

最後に1つ言及したいことがあります。

トレースする前に、常にモデルが評価モードになっていることを確認する必要があります。

これにより、すべてのレイヤーがトレーニングではなく推論用に構成されることが保証されます。

ほとんどのレイヤーでは、これは問題ではありません。

しかし、たとえば、モデルにドロップアウトレイヤーがある場合、評価モードを設定すると、それが無効になっていることを確認します。

そして、コンバーターが無効になっている操作に遭遇すると、それらをパススルー操作として扱います。

このビデオでは多くの資料を取り上げましたが、Core MLコンバータのドキュメント、カスタムオペ変換に関する情報、多くの詳細なTorchScriptの例など、ビデオに関連するリンクでさらに多くの情報を見つけることができます。

PyTorchモデルを変換するための一流のサポートを提供することに本当に興奮しています。

新しいCore MLコンバータが、PyTorchモデルのより広範なサポートを可能にし、デバイス上のモデル実行を最適化し、モデルを簡単に変換するための最大限のサポートを提供することを願っています。

見てくれてありがとう。