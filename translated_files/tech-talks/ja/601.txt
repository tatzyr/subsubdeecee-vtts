601

iOS 11では、iPhoneとiPad用の拡張現実アプリを作成するための新しいフレームワークであるARKitが導入されました。

ARKitは、デジタルオブジェクトを周囲の環境に配置することで、アプリを画面を超えて、まったく新しい方法で現実世界と対話できるようにします。

WWDCでは、ARKitの3つの主要な機能を導入しました。

位置追跡はデバイスのポーズを検出し、iPhoneやiPadをあなたの周りのデジタル世界への窓として使うことができます。

シーンの理解は、テーブルトップなどの水平面を検出し、安定したアンカーポイントを見つけ、周囲の照明条件の推定値を提供し、SpriteKit、SceneKit、Metalなどのレンダリング技術や、UnityやUnrealなどの一般的なゲームエンジンとの統合を提供します。

iPhone Xでは、ARKitはあなたに焦点を合わせ、フロントカメラを使用した顔追跡を提供します。

この新しい能力は、6つの自由度で堅牢な顔検出と位置追跡を可能にします。

顔の表情もリアルタイムで追跡され、アプリには、検出された顔の50以上の特定の筋肉の動きを表すフィットトライアングルメッシュと加重パラメータが提供されます。

ARの場合、カメラからの前面カラー画像と前面深度画像を提供します。

また、ARKitは顔を光プローブとして使用して照明条件を推定し、レンダリングに適用できる球面高調波係数を生成します。

そして、私が述べたように、これはすべてiPhone Xでのみサポートされています。

フェイストラッキングでできる本当に楽しいことがいくつかあります。

1つ目は自撮り効果で、バーチャルタトゥーやフェイスペイントなどの効果のためにフェイスメッシュに半透明のテクスチャをレンダリングしたり、化粧をしたり、ひげや口ひげを生やしたり、ジュエリー、マスク、帽子、メガネでメッシュを重ねたりします。

2つ目はフェイスキャプチャで、顔の表情をリアルタイムでキャプチャし、それをリギングとして使用して、アバターやゲームのキャラクターに表情を投影します。

では、詳細を掘り下げて、フェイストラッキングを始める方法を見てみましょう。

最初にする必要があるのは、ARSessionを作成することです。

ARSessionは、デバイスの設定からさまざまなARテクニックの実行まで、ARKitのために行われたすべての処理を処理するオブジェクトです。

セッションを実行するには、まずこのアプリにどのようなトラッキングが必要なのかを説明する必要があります。

これを行うには、フェイストラッキング用の特定のARConfigurationを作成して設定します。

処理を開始するには、セッションで「実行」メソッドを呼び出して、実行する設定を提供するだけです。

内部的には、ARKitはAVCaptureSessionとCMMotionManagerを設定して、カメラ画像とセンサーデータの受信を開始します。

そして、処理後、結果はARFrameとして出力されます。

各ARFrameは時間のスナップショットであり、カメラ画像、追跡データ、アンカーポイントを提供します。基本的にシーンをレンダリングするために必要なすべてです。

それでは、顔追跡のためのARConfigurationを詳しく見てみましょう。

ARFaceTrackingConfigurationという新しいサブクラスを追加しました。

これは、正面カメラを介して顔追跡を有効にするようにARSessionに指示する簡単な設定サブクラスです。

デバイスでの顔追跡の可用性と、照明推定を有効にするかどうかをチェックするための基本的なプロパティがいくつかあります。

次に、「run」を呼び出すと、追跡を開始し、ARFrameの受信を開始します。

顔が検出されると、セッションはARFaceAnchorを生成します。

これは主要な顔を表しています - カメラのビューで単一の最大かつ最も近い顔。

ARFaceAnchorは、スーパークラスの変換特性を通じて、世界座標のフェイスポーズを提供します。

また、現在の表情の3Dトポロジとパラメータも提供します。

そして、ご覧のとおり、それはすべて追跡され、メッシュとパラメータはリアルタイムで毎秒60回更新されます。

今、トポロジーに焦点を当てて、ARKitは、寸法、形状、およびユーザーの表情にリアルタイムでフィットする顔の詳細な3Dメッシュを提供します。

このデータは、いくつかの異なる形式で利用できます。1つ目はARFaceGeometryクラスです。

これは本質的に三角形のメッシュなので、頂点、三角形のインデックス、テクスチャ座標の配列で、レンダラーで視覚化することができます。

ARKitは、任意のSceneKitノードにアタッチできるジオメトリオブジェクトを定義するARSCNFaceGeometryクラスを通じて、SceneKitでメッシュを視覚化する簡単な方法も提供します。

ジオメトリメッシュとは別に、ブレンドシェイプと呼ばれるものもあります。

ブレンドシェイプは、現在の表情の高レベルモデルを提供します。

それらは、まぶた、眉毛、顎、鼻など、特定の特徴のポーズを表す名前付き係数の辞書です。

それらはゼロから1までの浮動小数点値として表現され、すべてライブで更新されます。

したがって、これらのブレンド形状係数を使用して、ユーザーの顔の動きを直接反映する方法で、2Dまたは3Dのキャラクターをアニメーション化またはリグすることができます。

利用可能なもののアイデアを与えるために、ここにブレンド形状係数のリストがあります。

したがって、これらのそれぞれは、右と左の眉毛、あなたの目の位置、あなたの顎、あなたの笑顔の形など、独立して追跡され、更新されます。

顔のジオメトリのレンダリングや3Dキャラクターのアニメーション化と密接に関連しているのは、リアルな照明です。

そして、あなたの顔を光プローブとして使用することで、顔検出を実行しているARSessionは、世界空間における光の強度とその方向を表す指向性光の推定値を提供することができます。

ほとんどのアプリでは、この照明ベクトルと強度は十分すぎるほどです。

しかし、ARKitは、シーンで検出された光の強度を表す2度の球面高調波係数も提供します。

したがって、より高度な要件を持つアプリの場合、これも活用できます。

そして、言及すべきさらにいくつかの機能。

カラーデータ付きのフロントカメラ画像に加えて、ARKitはアプリに前面深度画像も提供できます。

そして、私はこれをグレースケール画像としてここに表示しています。

データ自体は、タイムスタンプとともにAVDepthDataオブジェクトとして提供されます。

しかし、これは15Hzでキャプチャされており、ARKitが60Hzでキャプチャするカラー画像よりも低い周波数であることに注意することが重要です。

そして最後に、どのARKitセッションでも使用できますが、フェイストラッキングで特に興味深い機能は、オーディオキャプチャです。

現在はデフォルトで無効になっていますが、有効になっている場合、ARSessionの実行中に、マイクからオーディオサンプルをキャプチャし、一連のCMSampleBuffersをアプリに配信します。

したがって、これはユーザーの顔と声を同時にキャプチャしたい場合に便利です。

フェイストラッキングの詳細、およびサンプルコードへのリンクについては、開発者のウェブサイトdeveloper.apple.com/arkitをご覧ください。

見てくれてありがとう!