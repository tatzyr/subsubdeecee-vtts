10155

こんにちは。私はCreate MLチームのエンジニア、Shreya Jainです。

今日は、オブジェクト検出テンプレートの機能強化を探り、それらを活用してより良いモデルを作成します。

Create MLのオブジェクト検出にまだ慣れていない場合は、WWDC 2019のこのビデオを見ることをお勧めします。

オブジェクト検出は、いくつかの魅力的なアプリ体験を可能にします。

人々がゴミを分別するのに役立つアプリを構築し、ペットの猫に仮想メガネを試すことができます...

そして、検出された成分に基づいてレシピを推薦できるアプリさえも。

このアプリのモデルを構築することは、Create MLとその新機能の動作を見るのに最適な方法です。

物体検出が大幅に改善されました。

より少ないトレーニングデータで正確で小さなモデルをトレーニングし、トレーニングをカスタマイズするためのより多くの構成オプションを公開することができます。

だから、すぐに飛び込みましょう。

はじめに、SpotlightからCreate MLアプリを開きます。

最初に会うのはテンプレートピッカーで、そこでオブジェクト検出を選択します。

これにより、Create MLプロジェクトの詳細を入力するためのダイアログボックスが開きます。

このプロジェクトに「FindMyRecipe」という名前を付け、成分を検出するのに役立つ説明を追加します。

作成する前に、プロジェクトの場所を変更することを選択できます。

次に、設定タブに着陸します。

データと設定のオプションは、トレーニングの前にここで調整できます。

データをロードする前に、このデータの準備方法を説明します。

オブジェクト検出データは、すべてのトレーニング画像とJSONファイル内の注釈を含むフォルダに保存する必要があります。

Annotations.jsonの内容は、この画像を例にとると理解できます。これには、パンのスライスとトマトの2つのオブジェクトがあります。

各オブジェクトの注釈は、オブジェクトのラベルと画像内の位置で構成されています。

場所は画像の左上隅に基づいています。

トレーニングデータの画像内のすべてのオブジェクトは、この方法で注釈を付けることができます。

これらの注釈はすべて、この形式で単一のJSONファイルに追加されます。

この情報を使用してトレーニングデータを準備します。

データの準備が整ったら、Create MLアプリに読み込むことができます。

表示ボタンをクリックすると、データセットのクラス分布が表示されます。

ご覧の通り、私のクラスはトマト、チーズ、パン、バジルです。

[設定]タブに戻ると、モデルが目に見えないデータでうまく機能することを確認するために、検証データをオプションで提供できます。

ここでは、検証データを自動に設定し、Create MLにデータセットのごく一部を使用できるようにします。

また、モデルのトレーニングをより適切に制御できる新しいトレーニングパラメータもあります。

それらはアルゴリズム、反復、バッチサイズ、グリッドサイズです。

トレーニングには2つのアルゴリズムがあります。

まず、完全なネットワークです。

完全なネットワークアルゴリズムを詳しく見てみましょう。

フルネットワークは2019年にCreate MLに導入され、それ以来デフォルトのトレーニングアルゴリズムとなっています。

このアルゴリズムはYOLOv2アーキテクチャに基づいています。

このネットワークのすべてのパラメータは、データを使用してトレーニングされます。

結果として得られるCore MLモデルは、学習したすべてのパラメータをエンコードします。

このCore MLモデルは、16ビットの精度で重みを格納するように量子化されています。

結果として得られるモデルサイズは、先ほどの半分です。

したがって、以前は約65メガバイトだったモデルは、現在33メガバイトになります。

このアルゴリズムは、クラスごとに200以上の境界ボックスなど、大量のトレーニングデータがある場合に推奨されます。

結果のモデルは下位互換性があり、iOS 12にまでさかのぼります。

より少ないトレーニングデータで高精度のモデルを構築できるようにしたかったので、物体検出のための転送学習アルゴリズムを導入しています。

転送学習は、すでにオペレーティングシステムにある機械学習モデルを活用します。

例えば、写真アプリには、検索とメモリーを動かすモデルが含まれています。

写真が使用する事前に訓練されたバックボーンの1つは、オブジェクトプリントと呼ばれています。

これは膨大な量の多様なデータで訓練されています。

転送学習では、これを利用してデータ要件を減らすことができます。

Create MLの転送学習アルゴリズムは、ヘッドネットワークとともにオブジェクト印刷を使用します。

ヘッドネットワークのみがデータでトレーニングされ、学習する必要があるパラメータの数が減少します。

その結果、Core MLモデルにはヘッドネットワークパラメータのみが含まれているため、モデルはフルネットワークよりも5倍小さくなります。

2019年に65メガバイト、量子化後に33メガバイトだったのと同じモデルは、転送学習アルゴリズムを使用してわずか7メガバイトになります。

転送学習は、データが限られており、軽量モデルが必要な場合は素晴らしい選択肢です。

クラスごとにわずか80のトレーニング例でうまくいきます。

結果として得られるモデルでは、OSに同梱されているオブジェクトプリントを活用するためにiOS 14が必要です。

アルゴリズムは新しい構成の1つにすぎません。

反復やバッチサイズなどのパラメータも追加されました。

反復は、モデルのパラメータが更新される回数です。

デフォルト値は、データセットのサイズに基づいて選択されます。

特定のユースケースでは、モデルがまだ収束していない場合は反復を増やしたり、モデルが早期にうまくいっている場合は反復を減らすことができます。

バッチサイズとは、1回の反復で利用されるトレーニング例の数を指します。

デフォルト値は、ハードウェアの制限に基づいて選択されます。

バッチサイズが高い方が良いですが、デフォルト値を使用するか、パフォーマンス制限に基づいて減らすことをお勧めします。

最後に、ネットワーク全体については、グリッドサイズをカスタマイズできます。

グリッドサイズを理解するには、完全なネットワークで予測がどのように機能するかについての知識が必要です。

詳しく見てみましょう。 

この入力画像から始めて...

訓練された完全なネットワークモデルにそれを渡す...

結果、バウンディングボックスを持つ多くの予測オブジェクトが生成されます。

画像内のオブジェクトを見つけるために、モデルはグリッドとアンカーボックスのセットを活用します。

指定されたグリッドは、入力画像のアスペクト比と、モデルが検出されたオブジェクトを探す場所を定義します。

たとえば、モデルが5×5のグリッド次元でどのように動作するかを見てみましょう。

画像のサイズは、グリッド（この場合は正方形の画像）に合わせてサイズが変更され、定義されたセル数に分割されます。

その後、ネットワークはグリッドセルごとに1つずつ予測を生成します。

各予測には、セルにオブジェクトがあるかどうか、オブジェクトのクラス、およびその境界ボックスなどの情報が含まれています。

YOLOは、各オブジェクトが1つのグリッドセルに関連付けられている複数のオブジェクトで正常に動作します。

この画像でわかるように、バナナと犬の中心は同じ細胞に落ちます。

各セルは1つのクラスしか予測できないので、バナナか犬のどちらかを選ぶことになっています。

バナナと犬の両方を予測するために、アンカーボックスが定義されています。

アンカーボックスにはアスペクト比が設定されており、グリッドセル内の複数のオブジェクトを検出します。

Create MLは、13×13のデフォルトのグリッドディメンションを使用し、合計169セルです。

さまざまなアスペクト比の15個のアンカーボックスの固定セットが、各セルで評価されます。

したがって、デフォルトモデルは画像ごとに合計2,535の予測を行っています。

このサイコロの画像と、3×3のグリッド寸法で物体検出がどのように機能するかを考えてみましょう。

同様のアスペクト比の複数のサイコロが1つのセルに存在するため、そのうちの1つだけが検出されます。

大きなグリッドサイズでは、より多くのサイコロが検出されます。

ただし、これにより画像あたりの予測数が増えます。

グリッドサイズを変更する際には、計算コストを考慮することが重要です。

このような非正方形の入力画像では、寸法が1500×800で、この画像に8×8のグリッドを使用すると、情報が失われ、オブジェクトの自然な形状が歪めます。

これにより、トレーニング中にモデルがより細かいパターンをキャプチャするのを防ぎ、予測力を妨げます。

15×8のグリッドサイズを選択すると、入力画像の元のアスペクト比が保持され、より多くの情報を学習し、より良い結果をもたらすことができるモデルになります。

FindMyRecipeプロジェクトのモデルのトレーニングに戻ると、転送学習アルゴリズムを選択し、1000回の反復を設定し、バッチサイズの自動を設定できます。

再生ボタンをクリックすると、モデルはトレーニングを開始します。

トレーニングタブには、バッチが準備中であることがわかります。

このステップは、現実世界のデータに対する堅牢性と一般化を支援するために、一連の標準的な画像拡張を実行します。

すぐにグラフが表示され、各反復の損失値がプロットされます。

トレーニングが進むにつれて、スナップショットボタンをクリックして、その時点でモデルを取得できます。

スナップショットは、トレーニングの進捗状況を確認するのに役立ちます。

このモデルを使用して、いくつかの画像の予測をプレビューできます。

すべての画像について、モデルの予測は[プレビュー]タブに表示されます。

これらのバウンディングボックスをクリックすると、下部にある各クラスの自信を確認できます。

スナップショットは、アプリ内での実験にも使用できます。

トレーニングが完了すると、トレーニングと検証データの評価指標は、[評価] タブで確認できます。

これらの数字はどういう意味ですか?

物体検出モデルの評価は2倍である必要があります。

正しいラベルが欲しいだけでなく、適切な場所にある必要があります。

バウンディングボックスを注釈付きボックスと正確に一致させるのは難しいです。

予測されたボックスが注釈付きボックスにどれだけ近いかをキャプチャする数字が必要になります。

これは、intersection-over-unionと呼ばれるスコアによって測定されます。

それは0%の間の値であり、重複はありません...

100%に、これは完璧なオーバーラップです。

予測が正しいと見なされるには、正しいクラスラベルと、事前定義されたしきい値よりも大きい交点オーバーユニオンスコアが必要です。

ユニオン交差スコアがしきい値を下回っているか、予測されたクラスが正しくない場合、全体的な予測は正しくありません。

この情報は、平均平均精度（mAP）と呼ばれるメトリックを計算するために使用されます。

評価タブに戻って、これらの数字を確認します。

これらの数字は、2つのしきい値で計算されたクラスごとの平均精度を表しています。

1つは50%に固定され、もう1つは複数のしきい値で変化します。

データセットの全体的な平均精度は、右上隅で見ることができます。

より高いmAPは、より正確な予測を反映しています。

私たちのモデルのmAPは全体的に良さそうです。

モデルの予測が正しく見えることを確認するために、いくつかの例でモデルをプレビューします。

すべてが素晴らしく見えます。

これで、このモデルをアプリにドロップできます。

先ほど見た追加機能により、Create MLを使用してオブジェクト検出モデルを作成するのは簡単です。

Create MLは、トレーニングをより詳細に制御することで、モデルをカスタマイズするのに役立ちます。

より少ないデータとより小さな出力サイズで正確なモデルを構築するのに役立ちます。

これらの真新しい機能を使用して、あなたがテーブルにもたらす素晴らしいアイデアを見るのが待ちきれません。