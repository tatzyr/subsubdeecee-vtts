609

こんにちは、ARの高度なシーン理解へようこそ。

このビデオでは、新しいiPad ProのLiDARスキャナで有効になるARKitとRealityKitの新機能を紹介します。

iOSとipadOSは、AR用のアプリを構築するのに役立つ2つの強力なフレームワークを開発者に提供します。

ARKitは、位置追跡、シーン理解、レンダリング技術との統合を組み合わせて、背面カメラまたは前面カメラを使用してさまざまなAR体験を可能にします。

そして、AR用に特別に構築された新しい高レベルフレームワークであるRealityKitは、フォトリアリスティックなレンダリングと特殊効果、スケーラブルなパフォーマンス、SwiftベースのAPIを提供し、優れたAR体験のプロトタイプと構築を容易にします。

今日は、新しいiPad Proのハードウェア機能によってのみ可能になった両方のフレームワークの進歩について話すことに興奮しています。

新しいiPad ProにはLiDARスキャナーが装備されています。

これは、光が目の前の物体に到達して反射するのにかかる時間をナノ秒の速度で測定することによって距離を決定するために使用されます。

これは最大5メートル先で有効で、屋内と屋外の両方で動作します。

そして、この機能をワイドカメラとウルトラワイドカメラでキャプチャされた情報と組み合わせることで、あなたの環境を信じられないほど理解することができます。

それでは、ARKitから始めて、これらのフレームワークのそれぞれで構築されたAR体験をどのように改善するかを見てみましょう。

ARKitバージョン3.5の新しいアップデートは、新しいiPad ProのLiDARスキャナーを最大限に活用する多くの新機能と改善を提供します。

シーンジオメトリは、周囲の環境の詳細なトポロジカルマップをアプリに提供する新しいAPIです。

また、LiDARスキャナーは、サーフェスをより迅速かつ正確に検出することで、ARオンボーディング体験を簡素化します。

また、モーションキャプチャ、ピープルオクルージョン、レイキャスティングなどの既存のARKit機能も、追加のアプリケーションの変更を必要とせずにメリットがあります。

では、シーンジオメトリから始めましょう。

シーンジオメトリは、環境のトポロジカルマッピングを表す三角形のメッシュを提供します。

そして、オプションで、そのメッシュには、見られるものを分類するセマンティック情報を含めることができます。

これには、テーブルや椅子、床、壁、天井、窓などが含まれます。

この情報はすべて、現実世界のオブジェクト、環境に依存する物理学、およびシーン内の実際のオブジェクトと仮想オブジェクトの両方の照明による仮想コンテンツのオクルージョンを可能にするために使用できます。

では、このマッピングの動作を見てみましょう。

これは、屋内のシーンから取られた例です。

LiDARセンサーを使用して、ARKitによって生成されるメッシュでARフレーム画像をオーバーレイしています。

部屋を一掃すると、家具の形状や環境のレイアウトをどれだけ早く検出できるかがわかります。

そして、色はメッシュがオーバーレイするものの分類に基づいています。

では、APIを見てみましょう。

シーンジオメトリは、ARWorldTtrackingConfigurationの新しいシーン再構築プロパティを通じて有効になります。

さて、生成したいデータに応じて、2つのオプションがあります。

最初のオプションは、メッシュのみを生成することです。つまり、トポロジカル情報のみが表面化されます。

これは、周囲のオブジェクトの分類に依存しないオブジェクトの配置などを行うアプリを対象としています。

もう1つのオプションは.meshWithClassificationです。

そして、名前が示すように、これはすべてのシーンジオメトリのセマンティック分類を追加します。

これは、床とテーブルの照明の違いなど、シーンに何があるかに応じて、異なる行動を望むアプリに役立ちます。

そして、この下にコードが見えます。それはかなり簡単です。

私たちはワールドトラッキングを使用しており、シーンの再構築をサポートするデバイスで実行されていることを確認するためにテストしています。

もしそうなら、ここでメッシュオプションを選択し、セッションの実行を開始します。

シーンの再構築を有効にしてセッションを実行すると、ARセッションはARメッシュアンカーを返します。

これらは他のアンカーと変わり、add(anchor:)、update(anchor:)、remove(anchor:)など、通常のARセッションデリゲートメソッドを介して変更が行われます。

そして、各メッシュアンカーは、メッシュジオメトリのローカル領域を表します。

アンカーの変換とメッシュジオメトリオブジェクトで説明されています。

ARMeshGeometryオブジェクトは、周囲の環境を表すために必要なすべての情報を保持します。

各オブジェクトには、頂点、法線、面のリスト、および各フェーズで有効になっている場合の意味的分類が含まれています。

これらはすべてMTLBuffersとして提供され、レンダラーに直接統合できます。

今、シーンジオメトリと平面検出の間に起こるいくつかの興味深い相互作用があります。

シーンの再構築と平面検出の両方が有効になっている場合、構築されたメッシュは重なり合う平面と一致するように平坦化されます。

これは、滑らかなオブジェクトの動きを可能にするために、表面が一貫して平らである必要があるオブジェクトの配置に役立ちます。

一方、シーンジオメトリを使用していて、平面検出が有効になっていない場合、メッシュは平坦化されなくなります。

しかし、この組み合わせは、メッシュされた表面でより多くの詳細を保持します。

さて、それはシーンジオメトリです。

次に、LiDARスキャナによってさらにいくつかの改善が可能になります。

1つ目は、はるかにシンプルで高速なオンボーディングです。

LiDARスキャナーを使用すると、プレーナーの表面もほぼ瞬時に、より正確に検出されます。

これは、白い壁のような低機能の表面でも当てはまります。

その結果、以前は数秒かかり、ある程度のユーザーガイダンスが必要だった飛行機検出のオンボーディングが完全にシームレスに発生できるようになりました。

そして、変更は必要ありません。

新しいiPad Proで実行すると、すべてのARKitアプリが恩恵を受けます。

既存のアプリも、改善されたレイキャスティングの恩恵を受けるでしょう。

ARKitの強化されたシーン理解により、水平面と垂直面に対するより迅速かつ正確なレイキャストが可能になります。

さらに、新しいiPad Proは、これまで以上に幅広い表面に対してレイキャストできます。

許可ターゲットを推定Planeに設定するだけで、LiDARスキャナーからのデータは、周囲の環境に一致するレイキャスティング結果を提供します。

たとえば、ここで見られるように、大きな椅子やソファのすべての表面にオブジェクトを配置できるようになりました。

モーションキャプチャと人々の閉塞も、より正確な深度情報を提供するLiDARスキャナにより改善されています。

モーションキャプチャを使用するアプリは、より正確なスケール推定の恩恵を受け、人々のオクルージョンの深さ値もより正確です。

さらに、両方の機能が有効になっている場合、ピープルオクルージョンとシーンジオメトリAPIは連携できます。

人々の非常にダイナミックなジオメトリは、シーンの再構築から除外することができ、その結果、現実世界の環境のより安定したメッシュを提供します。

だから、新しいiPad ProのARKit 3.5を簡単に見てみましょう。

シーンジオメトリは、あなたの周りの環境のトポロジカルマップを提供します。

プレーナーサーフェスはほぼ瞬時に、より正確に検出され、オンボーディングが簡素化されます。

レイキャスティングはより正確で、シーンジオメトリを考慮に入れることができ、モーションキャプチャと人々の閉塞も改善されます。

ARKitは、RealityKitと呼ばれる高レベルのARフレームワークとも緊密に統合されています。

RealityKitは、フォトリアリスティックなレンダリング、カメラエフェクト、アニメーション、物理学などを提供します。

それはARのために特別にゼロから建てられました。

RealityKitは、新しいARKit 3.5機能を活用し、新規または既存のRealityKitアプリに簡単に統合できるようにします。

これらの機能は、新しいシーン理解APIを通じてアクセスできます。

LiDARで強化された物理学、オクルージョン、照明を有効にするためのオプションを提供し、ARViewのいくつかの簡単な設定を介してすべてアクセスできるので、見てみましょう。

新しいiPad Proでは、RealityKitは、現実世界で検出したサーフェスからシーンジオメトリの仮想オブジェクト間の物理相互作用を決定することができます。

そのため、現実世界の家具から仮想ボールが跳ね返ることができます。

これを行うには、まず仮想コンテンツであるModelEntityの衝突形状を生成し、その物理ボディを初期化します。

次に、ARViewのsceneUnderstanding.optionsセットに物理オプションを追加するだけで、残りはRealityKitが処理します。

オクルージョンと同様に、RealityKitは、出入り口、テーブル、椅子などの現実世界のオブジェクトから検出されたシーンジオメトリを使用して、シーン内の仮想オブジェクトを閉塞します。

これは完全に自動です。

これを有効にするには、ARViewに設定されたsceneUnderstanding.optionsにオクルージョンを追加するだけです。

RealityKitはボンネットの下の他のすべてを処理し、あなたの仮想コンテンツは環境内のすべての主要なオブジェクトによって遮断されます。

これがその例です。

私たちの仮想ロボットは床を歩き回っています。

しかし、カメラが柱の後ろを移動すると、ロボットが閉塞していることがわかります。

そのジオメトリはレンダリングから消えており、それはシーンの適切な深さの錯覚を維持するのに役立ちます。

さて、3番目の作品は照明用です。

新しいRealityKitを使用すると、仮想光源が現実世界の表面を照らすことができます。

これは、LiDARスキャナーの助けを借りて、これらの表面に非常に正確にフィットするシーンジオメトリを照らすことができるためです。

そして、以前と同様に、これを有効にすることは、ARViewのsceneUnderstanding.optionsセットにreceivesLightingを追加するのと同じくらい簡単です。

そして最後に、これらの機能のサポートは、Reality Composerで構築されたシーンにも拡張されます。

物理は、シーンジオメトリメッシュを使用して仮想オブジェクトと現実世界を衝突するように構成でき、現実世界のオブジェクトによる仮想コンテンツのオクルージョンはインスペクタパネルで有効にすることができます。

詳細については、developer.apple.comにアクセスして、ドキュメント、サンプルコード、またはこのような開発者ビデオへのリンクを見つけることができます。

見てくれてありがとう!