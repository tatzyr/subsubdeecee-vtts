10159

- こんにちは、ようこそ。

私の名前はマルコ・ジョルダーノで、アップルのGPUソフトウェアエンジニアリングチームに所属しています。

このセッションでは、Apple M1 GPU全体でワークロードを拡張する方法について説明します。

複雑なコンピューティングワークロードに取り組み、Appleシリコンハードウェアを最大限に活用し、優れたスケーリングを実現する方法を知りたい場合は、この講演が最適です。

まず、コンピューティングスケーラビリティの概念と、アプリケーションがM1 GPUファミリー全体で自然にパフォーマンスを拡張する方法について説明します。

そして、ステップバイステップの「ハウツー」を共有し、ワークロードのコンピューティングスケーリングを最大化するために利用可能なツールについて話します。

スケーラビリティとは何か、なぜそれがワークロードにとって重要なのかを理解することから始めましょう。

Apple M1 GPUは、ゼロからスケールアップし、ワークロードがSOCファミリー全体で優れたパフォーマンスを達成できるように設計されています。

すべてのMetal 3機能をサポートする同じGPUは、8コアのiPadから64コアのMac Studioまで拡張できます。

高レベルのスケーリングを利用するために、M1用に最適化されたアプリを持つことは素晴らしい出発点です。

多くの著名なプロアプリはすでにApple M1用に最適化されており、すべてのデバイスで優れたスケーリングを経験しています。

たとえば、ここには、ポストプロダクション業界の写真とビデオ編集者であるAffinity PhotoとDaVinci Resolveがあります。

これらのアプリは大きなスケーリングを達成しています。

スケーラビリティの本当の意味と、「理想的な」スケーリングを達成する方法を定義しましょう。

GPUワークロードのスケーラビリティは、GPUコアの数を増やしてパフォーマンスを向上させる能力です。

右側のチャートは、GPUコア数の増加によるアプリケーションのスピードアップを示しています。

線形比率の改善は理想的であると考えられています。

しかし、アプリで作業している間、プラトーにヒットし、リターンが減少してスケールするか、GPUタイムラインのギャップのためにまったくスケールしないスケーリングの種類に気付くかもしれません。

または、パフォーマンスが向上するが、ワークロードがいくつかのGPUリミッターに当たっているスタック全体で均一ではない別のタイプのスケーリングが見られるかもしれません。ここのように、24〜32または48〜64コアの間です。

あなたの目標は、可能な限りリニアスケーリングに近づくことであり、ボトルネックを特定し、あなたが望む結果を達成するためのツールとテクニックを紹介します。

次のセクションでは、GPUスケーリングを最大化するためのアプローチについて説明します。

すべてのワークロードについて、まずボトルネックがどこにあるかを特定する必要があります。

ワークロードは、計算または帯域幅のいずれかによって制限できます。

最適化プロセス中に、一方と他方の間でバウンスしてしまう可能性があります。

計算に縛られている場合は、負荷の一部をシフトしてメモリを活用して計算を減らすか、その逆を試みるかもしれません。

ボトルネックはスケールアップすると変化する可能性があります。

良い解決策の1つは、MPSやMPSGraphなどのAppleフレームワークを使用することです。

プリミティブを活用できれば、すべてのコンピューティングカーネルがすべてのハードウェアで最適に動作するようにしました。

ただし、すべてをMPSに置き換えることはできないため、ワークロードをプロファイリングして理解することが重要です。

まず、GPUのギャップを最小限に抑えるのに役立つ3つの項目を取り上げます。作業分布の改善、GPUタイムラインのギャップの解消、およびアトミック操作の考慮事項です。

次に、最初にワークロードの計算グリッド形状とメモリレイアウトの効果を調査し、最後にBlender Cyclesの特定の例を見て、GPUリミッターを最適化する方法を説明します。

GPUのギャップを最小限に抑えることに焦点を当てることから始めます。

この種のスケーリングは、ハードウェアがアイドル状態のGPUタイムラインにギャップがあり、GPUが完全に利用されていない結果である可能性があります。

仕事の分布を調査することで、スケーリングを改善できるかどうか見てみましょう。

小さなワークロードは通常、GPU全体を飽和させず、カーネル同期にはコストがかかるため、どちらも適切なスケーリングを防ぐことができます。

ワークロードがハードウェアにどのようにマッピングされるかを理解することは非常に重要なので、それについて話しましょう。

ワークロードは、スレッドグループの3Dグリッドの形でディスパッチされます。

スレッドグループはGPUコアに均一に分散され、サイズが制限されているが、GPUコアにローカルで非常に高速であるスレッドグループメモリにアクセスできます。

単一のスレッドグループはさらにSIMDグループに分解され、他の計算方言では波やワープとも呼ばれます。

コンピューティングパイプライン状態オブジェクトの「threadExecutionWidth」をチェックすると、SIMD幅が返され、すべてのApple GPUでは32に等しくなります。

スレッドグループは、スレッドグループごとに最大1024スレッドを持つことができ、スレッドは最大32Kのスレッドグループメモリを共有できます。

GPUを忙しく保つために、すべてのGPUコアで十分な作業があるはずです。

以下は、ディスパッチするグリッドの例です。

スレッドグループはGPUクラスターにディスパッチされ、GPUコア間で分散されます。

スレッドグループが少なすぎると、ワークロードはマシンを完全に飽和させません。

これを修正する方法は次のとおりです。

ワークロードが生成するスレッドの数を計算し、ディスパッチがマシン全体を飽和させるかどうかを大まかに確認します。

比較的複雑なカーネルの場合、シェーダーコアあたり1Kから2Kの同時スレッドは非常に良い占有率と考えられているので、経験則としてGPUコアあたり1から2Kのスレッドを取ります。

これで、ハードウェアを完全に飽和させるのに十分な作業があれば計算できます。

ここの表は、異なるSOCを飽和させるための最小推奨スレッド数を示しています。

考慮すべきもう1つのことは、不必要に大きなスレッドグループサイズの使用を避けることです。

スレッドグループを小さくすると、ハードウェアへの負荷がより均一にマッピングされます。

より大きなスレッドグループを使用すると、より均一な分布が妨げられ、GPUコアの不均衡につながる可能性があります。

ワークロードにうまくマッピングされるSIMD幅の最小倍数を使用するのが最善です。

より小さなスレッドグループを使用することで、GPUはワークロードのバランスをとる機会が増えます。

XcodeまたはInstruments GPU Toolsで、常にカーネルランタイムのパフォーマンスを確認してください。

たとえば、このGPUキャプチャでは、いくつかの計算を実行するカーネルがあります。

占有率はかなり低く、これは予想外です。

コンパイラの統計は、Xcode 14で新しい最大理論占有率が100%であることを示しています。

これは、十分なスレッドがない可能性があることを示しており、実際、アルゴリズムがますます少ないスレッドをディスパッチし始め、マシンを飽和させなくなったことがわかります。

占有率が低いと、他にもいくつかの原因があるかもしれません。

すべての詳細を入手するには、MacBook Pro TechトークのMetal Computeを確認してください。

さて、ワークロードが正しく分散されたので、GPUが常にビジーであることを確認する時が来ました。

GPUの活用不足は決して理想的なスケーリングにつながることはなく、利用不足の最悪のケースは、それをアイドル状態に保つことです。

GPUのタイムラインギャップにより、GPUはアイドル状態になる可能性があります。

この例を考えてみましょう。

これは、CPUとGPU間の作業シリアル化により、GPUの50%しか使用しないワークロードです。

この場合、全体的なタスク期間は、重複のないCPUとGPUの作業の合計です。

GPUコアを2倍にすると、GPUトラックの完了が速くなりますが、CPUトラックは影響を受けません。

全体的なパフォーマンスは33%しか増加しませんが、理想的なスケーリングとは程遠いです。

GPUコアが再び2倍にすると、GPUのワークロードはさらに速くなりますが、全体的なレイテンシは元の時間と比較してわずか60%削減されます!

したがって、GPUコアのスケーリングは、そのような場合にリターンの減少をもたらします。

これは理想とは程遠い。直そう！

M1 proからのこのインストゥルメントトレースは、大きなGPUタイムラインギャップを示しており、これは明らかに適切なスケーリングを防ぎます。

M1 Ultraでは、同じワークロードは確かに少し高速ですが、GPUのアイドル時間が高くなり、ワークロードはうまくスケーリングされていません。

大きなギャップは、コマンドバッファのwaitUntilCompletedを使用したCPU同期によって引き起こされます。

待機ロジックを変更し、シリアル化を削除した後、GPUは完全に利用され、これは素晴らしいことです。

前後のワークロードスケーリングを比較すると、スケーリングが理想的なスケーリングにはるかに近づいたと述べることができます。

前の例では、CPU/GPUの同期を完全に削除することができましたが、アプリケーションの性質上、必ずしもそうとは限りません。

アイドル時間を短縮するために取ることができる他のアプローチがあります。

MTLSharedEventsを使用して、CPUに信号を送り、より多くの作業をパイプライン化し、GPU駆動エンコーディングの使用を検討し、同時ディスパッチを使用します。

それでは、GPUのタイムラインギャップを最小限に抑えるためのこれらのアプローチについて話し合いましょう。

そのうちのいくつかはあなたのワークフローに合うかもしれません。

CPUでGPUの完成を待つと、理想的なスケーリングにつながりません。

アプリケーションがWaitUntilCompletedを使用している場合は、代わりにMTLSharedEventsを使用してみてください。

MTLSharedEventsはオーバーヘッドが低く、タイムラインのギャップを減らすのに役立ちます。

次に考慮すべきことは、ワークロードをパイプライン化することです。

アルゴリズムに次のバッチの作業に必要なデータがある場合は、MTLSharedEventsを待つ前に、事前に1つ以上のバッチをエンコードすることができます。

そうすることで、GPUは消耗することはなく、常に処理作業が必要です。

同じキューで作業を事前にエンコードできない場合は、2番目のキューを使用して作業を重ねることを検討してください。

複数のキューを使用すると、独立した作業を送信でき、イベントを待っているときに他の送信スレッドを停止することはありません。

このようにして、GPUは作業を受信して処理し続ける機会があります。

場合によっては、アルゴリズムはGPUから直接作業をエンコードすることができます。

間接コマンドバッファを使用すると、次のバッチのエンコーディングをGPUに直接移動できるため、同期の必要がなくなります。

間接コマンドバッファの詳細については、「Metalによるモダンレンダリング」をチェックしてください。

ワークロードは、CPUとGPU間の高価な同期を可能な限り削除または最小限に抑えるようになりました。

しかし、GPUのタイムラインが忙しくても、スケーリングの課題はまだ存在するかもしれません。

調査しましょう。

このグラフは、画像が一度に1フレームで処理される画像処理ワークロードからのものです。

多くのバックツーバックコンピューティングシリアルディスパッチもスケーリングを制限することができます。

GPUはビジーですが、カーネルの同期にはコストがかかり、さらに、すべてのディスパッチにはスレッドグループが分散され、コアがまだ飽和していない小さなランプアップがあります。

同様に、スレッドグループが終了して引退すると、コアを完全に飽和させるのに十分な作業がない可能性があります。

このような状況では、アドバイスは、可能な限り独立した作業を重ねることです。

視覚的な例を見てみましょう。

ここでは、2つの画像を次々に処理するワークロードがあります。

通常、カーネルは互いに同期する必要があります。

しかし、これは仕事をスケジュールする唯一の方法ではありません。

同時ディスパッチを使用して、2つの画像の独立した作業をインターリーブできます。

ここでは、同時派遣のおかげで、ドライバーは異なる作業をインターリーブすることができます。

以前は背中合わせだった2つのカーネルが、いくつかの独立した作業で分離されていることがわかります。

ただし、MTLDispatchTypeConcurrentを使用する場合は、手動でバリアを入れる必要があります。

同時ディスパッチにより、ドライバーは作業をより緊密にパックし、依存するカーネル間の同期コストのほとんどを隠し、さまざまなカーネルのランプアップとテールエンドを埋めることができます。

この最適化により、M1 MaxからM1 Ultraに移行する際のワークロードのパフォーマンスとスケーリングが大幅に改善されました。

ワークロードは、以前のスケーリングと比較して、2つの画像がインターリーブされた状態で30%速く、3つの画像が並行して70%速くなります。

カーネルが行っている原子操作を慎重に検討することが重要です。

それが最も効率的な方法で作られていることを確認しましょう。

アトミック操作により、複数のスレッドからデータを安全な方法で読み書きできます。

グローバルアトミックは、GPU全体で一貫しています。

多くのスレッドが同じグローバル値を読み書きしようとすると、これは競合につながります。

GPUコアの数を増やすことは役に立たず、実際にはより多くの競合につながります。

アルゴリズムのアトミック動作を改善する方法を例で調べてみましょう。

これは、バッファ内のすべての値が合計される削減アルゴリズムです。

最も簡単な方法は、メインメモリのスレッドごとにアトミック追加操作を実行することです。

しかし、メインメモリの単一の値に大きな圧力をかけ、各メモリ書き込みを効果的にシリアル化するため、これは理想的ではありません。

ハードウェアがアトミックメモリの競合を支援するために提供するものは2つあります。Simbdグループ命令とスレッドグループアトミックです。

Prefix_exlusive sumやsimd_minなどのSIMD命令により、メモリへの往復なしでSIMDグループ内のレジスタ間で操作とメモリ交換を行うことができます。

スレッドグループアトミックは、スレッドグループメモリによって満たされます。

各GPUコアには独自のスレッドグループメモリがあり、GPUコアの数に合わせて拡張できます。

これら2つの機能がワークロードの改善にどのように役立つか見てみましょう。

ここでは同じ削減問題がありますが、今回はSIMDグループ命令、包括的なメモリ合計を使用し始めます。

このような操作は、最後のスレッドのSIMDグループ内のすべての数字の合計を残します。

各SIMDグループの最後のスレッドは、スレッドグループメモリで単一のアトミック追加を実行して、すべてのSIMDグループをスレッドグループメモリ内の単一の値に減らすことができます。

このようにして、SIMDグループ命令とスレッドグループメモリを使用して、メインメモリにまったく触れることなくスレッドグループ全体が削減されました。

各グループは、独立して並行して削減することができます。

各スレッドグループが1つの値に縮小されたので、スレッドグループごとに1つのスレッドがメインメモリで1つのアトミックを実行できます。

これは、スレッドグループごとに1つのアトミックのみを必要とするだけでなく、スレッドグループは異なる時間に完了するため、時間の経過とともにアトミックを散乱させ、メモリの競合をさらに減らします。

要約すると、アトミックの有効性を最大化するために、メモリ局所性を活用し、SIMDグループ操作を使用し、スレッドグループメモリアトミックを活用してみてください。

これらはすべて、スケーリングを防ぐ原子圧の低減に大きく役立つはずです。

GPUのギャップが修正されたので、スケーリングが理想に近いかどうかを確認する時が来ました。

XcodeとMetal System TraceのGPUリミッターは、GPUコア実行パイプラインのボトルネックや非効率性を最適化するのに役立ちます。

たとえば、非効率的なメモリアクセスパターンは、常に高い最終レベルのキャッシュまたはメモリ管理ユニット、またはMMUリミッター、およびかなり低い使用率を引き起こします。

最初に対処するのは、スレッドグループとメモリレイアウトを調整する方法です。

メモリスパンと発散を減らすための鍵は、空間的にも時間的にもワークロードメモリアクセスパターンを明確に理解することです。

それが理解されたら、2つの可能なチューニング方向があります。データアクセスの局所性を改善するためにデータレイアウトを再構成するか、アクセスパターンを調整してデータレイアウトによりよく一致し、メモリとキャッシュの局所性を改善します。

例を見てみましょう。

ここでは、データが次々に水平にレイアウトされるメモリバッファです。

しかし、コンピューティングカーネルがディスパッチされると、正方形のスレッドグループが分散される2Dのようなパターンを持つことが一般的であり、これは非常に空間的にローカライズされています。

このアクセスパターンとデータレイアウトは、データの局所性には最適ではありません。

たとえば、最初のSIMDグループがデータにアクセスすると、リクエストはキャッシュ行にパックされます。

ほとんどのキャッシュ行は使用されませんが、キャッシュ内のスペースを占有します。

たとえば、行全体にまたがるのではなく、ストライプにローカライズされるなど、アクセスパターンに合わせてデータを再配置します。

この新しいメモリレイアウトにより、スレッドグループはキャッシュラインで要求されるデータのほとんどを利用し、発散を減らし、キャッシュ効率を向上させることができます。

もう1つのオプションは、現在のデータレイアウトによりよく合うように3Dグリッドのディスパッチ方法を変更することです。

スレッドグループサイズで遊んで、たとえば、より長方形の形状など、メモリレイアウトによりよくマップされるグループを作成してみてください。

この場合、アクセスパターンはメモリレイアウトと一致し、はるかに高いキャッシュ効率を提供します。

ワークロードに最適なものを見つけるために実験する必要があるかもしれません。

時には、トレードオフをしたり、メモリの局所性のためにスレッドの発散を犠牲にしたり、その逆に、データレイアウト、グリッドディスパッチ、またはそれらすべての組み合わせを変更する必要があります。

すべてのワークロードとアクセスパターンは異なります。

記憶の局所性を改善する方法を認識したので、Blender Cyclesでより具体的な例を見てみましょう。

サイクルは、生産レンダリングのためのBlenderの物理ベースのパストレーサーです。

これは、生産ニーズのための芸術的な制御と柔軟なシェーディングノードで、箱から出して物理的にに基づいた結果を提供するように設計されています。

このインストゥルメントトレースは、低読み取り帯域幅、高トップGPUリミッター、高キャッシュリミッター、低ラストレベルキャッシュ使用率を明確に示しています。

帯域幅とMMUリミッターをコントロールし続けることは、スケーリングにとって重要です。

トップリミッターがラストレベルキャッシュまたはMMUの場合、メモリスパンを短縮し、データの局所性を最大化する必要があります。

例を見てみましょう。

サイクルは、発散を減らすためにデータのソートを使用します。

それは、レイヒットを材料の種類で並べ替えることによってそれを行います。

これはスレッドの発散を減らすのに最適ですが、空間メモリの発散を増加させ、高いMMUリミッターになります。

これを助けるために、データの局所性を高めるためにソートする前にメモリ範囲を分割する実験をしました。

それを視覚化しましょう。

光の移動をシミュレートするために光線がシーンに撮影されると、それらはオブジェクトにヒットし、データはバッファに収集されます。

交差点の点で、私たちは多くのことを知っています - ガラス、金属など、ヒットした材料の種類、交差点の位置、光線など。

簡単にするために、材料の種類だけに焦点を当てましょう。

これはメモリ内のバッファ内の材料です。

レイヒットごとに多くのデータが収集されるため、メモリバッファはかなり大きくなる可能性があります。

多くのメモリを移動しないように、インデックスのリストを入力し、代わりにそれらを並べ替えます。

ソート後、同じ材料タイプのインデックスが一緒に閉じてパックされるようになりました。

SIMDグループは、インデックスの読み込みを開始し、材料を処理できます。

SIMDグループは、インデックスを使用して、対応するデータを元のバッファにロードします。

しかし、SIMDグループはメモリスパン全体にわたって読み取り、MMUに圧力をかけます。

新しいアプローチを調査しましょう。

メモリ範囲は、単に異なるパーティションからのインデックスをミックスさせない理想的なパーティションで分割されます。

ソートするとき、アクセスされたデータの範囲は、以前のように完全なメモリ範囲にまたがるのではなく、パーティション内に含まれていることは明らかです。

これは、スレッド発散とメモリ発散の間のトレードオフとバランスです。

理想的なパーティションの数とサイズは、ワークロードに大きく依存します。

どれが一番うまくいくかを試す必要があるかもしれません。

別のメタルシステムトレースを取り、ワークロードが改善されたかどうかを見てみましょう。

ここでは、最適化されたバージョンのリミッターと使用率を確認します。

トップパフォーマンスリミッターとラストレベルキャッシュリミッターがダウンしました。

その結果、帯域幅とシェーダーのランタイムが大幅に改善されました。

いくらか見てみましょう。

トップリミッターとLLCリミッターは約20%減少しました。

これは、データフローがより効率的であることを意味します。

GPUの読み取り帯域幅が大幅に増加し、より多くのデータをGPUコアにプッシュできるようになりました。

全体として、この実験でメモリの局所性を増やすと、シーンに応じてパフォーマンスが10〜30%向上しました。

これは、メモリアクセスパターンを改善しようとする多くの方法のほんの一例でした。

トップパフォーマンスリミッターの実験と最適化を続けてください。

GPUツールには、調整するためのより便利なカウンターがあります。

Xcodeは、コンパイラ統計ウィンドウに新しい理論的な占有率を持っています。

XcodeとInstrumentsの両方に、いくつかのMMU関連のリミッターとカウンター、特に新しいMMUリミッター、MMU利用カウンター、およびMMU TLBミスレートカウンターがあります。

今日は多くの分野をカバーしました。

GPUのスケーラビリティと、スケールアップ時にボトルネックがどのように変化するか、そしてツールがスケーラビリティの問題を見つけて修正するのにどのように役立つかについて話し合いました。

また、アプリケーションに最適な結果を得るために、どのように実験し、トレードオフを行う必要があるかについても議論しました。

あなたのすべての素晴らしいアプリがAppleシリコンで驚くほどうまくスケールするのを見るのを楽しみにしています。

ご覧いただきありがとうございます。