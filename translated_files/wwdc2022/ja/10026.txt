10026

♪まろやかなインストゥルメンタルヒップホップ音楽♪

♪

こんにちは！私の名前はアダム・ブラッドフォードです。

私はVisionKitチームのエンジニアであり、あなたのアプリにライブテキストを追加しようとしているなら、あなたは正しい場所にいます。

しかし、まず、ライブテキストとは何ですか?

ライブテキストは、画像を分析し、テキストの選択とコピー、ルックアップや翻訳などのアクションの実行、アドレスのマッピング、番号のダイヤル、URLへのジャンプなどのデータ検出ワークフローの提供など、ユーザーがコンテンツと対話するための機能を提供します。

ライブテキストは、QRコードのインタラクションも可能にします。

これをアプリでどのように使用できるか想像してみてください。

もっと知りたいですか？さて、あなたは正しい場所にいます。

このセッションでは、Live Text APIの一般的な概要から始めます。

次に、既存のアプリケーションでこのAPIを実装する方法を探ります。

次に、アプリにライブテキストを追加する際に役立つヒントやコツをいくつか紹介します。

では、Live Text APIの概要を説明します。

高いレベルでは、Live Text APIはSwiftで利用できます。

静止画像で美しく機能し、一時停止したビデオフレームに使用するように適応させることができます。

テキストやQRコードなどのアイテムを検索するためにライブカメラストリームでビデオを分析する必要がある場合、VisionKitにはデータスキャナもあります。

詳細については、同僚のロンからのこのセッションをチェックしてください。

Live Text APIは、Apple Neural Engineを搭載したデバイス、およびmacOS 13をサポートするすべてのデバイスでiOS 16から利用できます。

それは4つの主要なクラスで構成されています。

それを使用するには、まず画像が必要です。

その後、この画像はImageAnalyzerに入力され、非同期分析を実行します。

分析が完了すると、プラットフォームに応じて、結果のImageAnalysisオブジェクトがImageAnalysisInteractionまたはImageAnalysisOverlayViewのいずれかに提供されます。

今のところかなり簡単そうですよね?

次に、既存のアプリケーションに追加する方法を実演します。

そして、これが私たちのアプリケーションです。

これはシンプルな画像ビューアで、スクロールビュー内に画像ビューがあります。

注意してください、私はズームとパンの両方をすることができます。

しかし、私は試してみてください、私はこのテキストのいずれかを選択するか、これらのデータ検出器のいずれかをアクティブにすることはできません。

これは単純にはしません。

これがXcodeのプロジェクトです。

このアプリケーションにライブテキストを追加するには、ビューコントローラーのサブクラスを変更します。

まず、ImageAnalyzerとImageAnalysisInteractionが必要です。

ここでは、viewDidLoadをオーバーライドし、インタラクションをイメージビューに追加しているだけです。

次に、いつ分析を実行するかを知る必要があります。

新しい画像が設定されると、最初に古い画像の優先InteractionTypesと分析をリセットすることに注意してください。

これで、すべてが新しい分析の準備が整いました。

次に、使用する機能を作成し、画像が存在することを確認します。 画像が存在することを確認します。

もしそうなら、タスクを作成してください。

次に、アナライザに何を探すべきかを伝えるために設定を作成します。

この場合、私はテキストと機械可読コードで行きます。

分析を生成するとスローされる可能性があるので、必要に応じて処理してください。

そして今最後に、分析プロセスを開始するメソッドanalysisImageWithConfigurationを呼び出す準備が整いました。

分析が完了すると、不確定な時間が経過し、アプリケーションの状態が変更された可能性があるため、分析が成功し、表示された画像が変更されていないことを確認します。

これらのチェックがすべて合格した場合、インタラクションの分析を設定し、優先インタラクションタイプを設定するだけです。

ここでは.automaticを使用していますが、デフォルトのシステム動作が表示されます。

これはテストの準備ができていると思います。

ああ、それを見て！

ライブテキストボタンが表示されたので、はい、テキストを選択できるようになりました。

これらのインターフェイス要素が自動的に配置される方法に注目し、画像の境界と可視領域の両方の内部にそれらの位置を保ち、私の側での作業はありません。

OK、ライブテキストボタンをタップすると、選択可能な項目を強調表示し、データ検出器に下線を引いて、クイックアクションが表示されることに注意してください。

このクイックアクションを簡単にタップして電話をかけることができ、長押しでより多くのオプションを見ることができます。

あなたは認めなければなりません、これはかなりクールです。

この数行のコードだけで、私は普通の画像を撮影し、それを生き生きとさせました。

このシンプルなアプリケーションは、画像上のテキストの選択、データ検出器、QRコードの有効化、検索、テキストの翻訳などできるようになりました。

私に言ってくれれば、この数行のコードだけではあまりみすぼらしくありません。

そして、ライブテキストを実装する方法を見たので、採用に役立つヒントやコツをいくつか紹介します。

インタラクションの種類を探ることから始めます。

ほとんどの開発者は、テキスト選択を提供する.automaticを望みますが、ライブテキストボタンがアクティブな場合はデータ検出器も強調表示します。

これにより、該当する検出されたアイテムの下に線が引かれ、ワンタップアクセスでそれらをアクティブにすることができます。

これは、組み込みアプリケーションから見られるのとまったく同じ動作です。

アプリがデータ検出器なしでテキスト選択のみを持つことが理にかなっている場合は、タイプを.textSelectionに設定することができ、ライブテキストボタンの状態では変更されません。

ただし、アプリがテキスト選択なしでデータ検出器のみを持つことが理にかなっている場合は、タイプを.dataDetectorsに設定します。

このモードでは、選択が無効になっているため、ライブテキストボタンは表示されませんが、データ検出器には下線が引かれ、ワンタップアクセスの準備が整うことに注意してください。

preferredInteractionTypesを空のセットに設定すると、インタラクションが無効になります。

また、最後のメモは、テキスト選択または自動モードで、長押しでデータ検出器をアクティブにできることがわかります。

これは、デフォルトであるtrueに設定するとアクティブになるallowLongPressForDataDetectorsIn TextModeプロパティによって制御されます。

必要に応じて、これを無効にするには、falseに設定するだけです。

少し時間を取って、補足インターフェースとして総称される下部にあるこれらのボタンについて話したいと思います。

これは、通常は右下隅にあるライブテキストボタンと、左下に表示されるクイックアクションで構成されています。

クイックアクションは、分析からのデータ検出器を表し、ライブテキストボタンがアクティブなときに表示されます。

サイズ、位置、および可視性は、インタラクションによって制御されます。

また、デフォルトの位置と外観はシステムと一致しますが、アプリにはカスタムインターフェイス要素があり、異なるフォントやシンボルの重みを妨害または利用する可能性があります。

このインターフェースをカスタマイズする方法を見てみましょう。 

まず、isSupplementary InterfaceHiddenプロパティ。

アプリでテキストを選択できるようにしたいが、ライブテキストボタンを表示したくない場合、SupplementaryInterfaceHiddenをtrueに設定すると、ライブテキストボタンやクイックアクションは表示されません。

また、コンテンツ挿入プロパティも利用できます。

補足インターフェイスと重なるインターフェイス要素がある場合は、コンテンツインセットを調整して、ライブテキストボタンとクイックアクションが表示されたときに既存のアプリコンテンツにうまく適応させることができます。

アプリがインターフェイスを採用したいカスタムフォントを使用している場合、補足InterfaceFontを設定すると、ライブテキストボタンとクイックアクションがテキストに指定されたフォントを使用し、シンボルにフォントの重みを使用します。

ボタンのサイズの一貫性のために、ライブテキストはポイントサイズを無視することに注意してください。

しばらくギアを切り替えると、UIImageviewを使用していない場合は、ハイライトが画像と一致しないことに気付くかもしれません。

これは、UIImageViewを使用すると、VisionKitがContentModeプロパティを使用してContentRectを自動的に計算できるためです。

ここでは、インタラクションのビューには、画像コンテンツよりも大きな境界がありますが、単位の長方形であるデフォルトのコンテンツ矩形を使用しています。

これは、デリゲートメソッドcontentsRectForInteractionを実装し、これを修正するために画像コンテンツがインタラクションの境界にどのように関連しているかを説明する単位座標空間に長方形を返すことで簡単に解決できます。

たとえば、これらの値で長方形を返すと問題が修正されますが、アプリの現在のコンテンツとレイアウトに基づいて正しい正規化された長方形を返してください。

contentsRectForInteractionは、インタラクションの境界が変更されるたびに呼び出されますが、contentsRectが変更されたが、インタラクションの境界が変更されていない場合は、setContentsRectNeedsUpdate()を呼び出してインタラクションの更新を依頼できます。

ライブテキストを採用する際のもう1つの質問は、このインタラクションを置くのに最適な場所はどこですか?

理想的には、ライブテキストインタラクションは、画像コンテンツをホストするビューに直接配置されます。

前述のように、UIImageViewはあなたのためにcontentsRect計算を処理し、必要ではありませんが、優先されます。

UIImageviewを使用している場合は、imageViewでインタラクションを設定するだけで、VisionKitが残りを処理します。

ただし、ImageViewがScrollView内にある場合は、ScrollViewにインタラクションを配置したくなる場合がありますが、これは推奨されず、contentRectが継続的に変更されるため、管理が難しい場合があります。

ここでの解決策は同じで、倍率が適用されたScrollView内にある場合でも、画像コンテンツをホストするビューにインタラクションを配置します。

私は少しの間ジェスチャーについて話すつもりです、ライブテキストは、控えめに言っても、ジェスチャーリコグナイザの非常に、非常に豊富なセットを持っています。

アプリの構造によっては、アプリが実際に処理すべきジェスチャーやイベントに応答するインタラクションが見つかる可能性があり、その逆も同様です。

パニックにならないでください。

これらの問題が発生した場合に修正するために使用できるテクニックをいくつか紹介します。

これを修正する一般的な方法の1つは、デリゲートメソッドのinteractionShouldBeginAtPointFor InteractionTypeを実装することです。

Falseを返すと、アクションは実行されません。

始めるのに良い場所は、インタラクションに指定されたポイントにインタラクティブなアイテムがあるかどうか、またはアクティブなテキスト選択があるかどうかを確認することです。

テキスト選択チェックはここで使用されているため、テキストをタップして選択を解除することができます。

一方、インタラクションがジェスチャーに反応しないように見える場合は、代わりにジェスチャーを処理しているジェスチャーリコグナイザがアプリにあるからかもしれません。

この場合、 gestureRecognizer の gestureRecognizerShouldBegin デリゲートメソッドを使用して、同様のソリューションを作成できます。

ここでは、同様のチェックを実行し、その場所にインタラクティブなアイテムがある場合、またはアクティブなテキスト選択がある場合はfalseを返します。

余談ですが。

この例では、まずnilを渡すことで、 gestureRecognizerの位置をウィンドウの座標空間に変換し、それをインタラクションのビューに変換します。

これは、インタラクションが倍率が適用されたScrollView内にある場合に必要になる場合があります。

ポイントが一致しない場合は、このテクニックを試してみてください。

私が役に立つことがわかったもう1つの同様のオプションは、UIViewのhitTest:WithEventをオーバーライドすることです。

ここで、もう一度、同様の話、私は以前と同じ種類のチェックを実行し、この場合、適切なビューを返します。

いつものように、私たちはあなたのアプリができるだけレスポンシブであることを望んでおり、Neural Engineは分析を非常に効率的にしますが、最高のパフォーマンスを得るために共有したいImageAnalyzerのヒントがいくつかあります。

理想的には、アプリで共有したいImageAnalyzerは1つだけです。

また、いくつかの種類の画像もサポートしています。

持っているネイティブタイプを渡すことで、常に画像変換を最小限に抑える必要があります。ただし、CVPixelBufferがある場合、それが最も効率的です。

また、システムリソースを最大限に活用するには、画像が画面に表示される直前または直前に分析を開始する必要があります。

アプリのコンテンツがスクロールする場合(たとえば、タイムラインがある場合)、スクロールが停止した後にのみ分析を開始します。

今、このAPIはあなたがライブテキストを見る唯一の場所ではありません、サポートは、あなたのアプリがすでに使用しているかもしれないシステム全体のいくつかのフレームワークで自動的に提供されます。

たとえば、UITextFieldまたはUITextViewは、キーボード入力にカメラを使用してライブテキストをサポートしています。

また、ライブテキストはWebKitとクイックルックでもサポートされています。

詳細については、これらのセッションをチェックしてください。

今年のiOS 16の新機能は、AVKitにライブテキストサポートを追加しました。

AVPlayerViewとViewControllerは、デフォルトで有効になっているallowsVideoFrameAnalysisプロパティを介して、一時停止したフレーム内のライブテキストを自動的にサポートします。

これは、FairPlayで保護されていないコンテンツでのみ利用可能であることに注意してください。

AVPlayerLayerを使用している場合は、分析とインタラクションを管理する責任がありますが、currentDisplayedPixelBufferプロパティを使用して現在のフレームを取得することは非常に重要です。

これは、適切なフレームが分析されていることを保証する唯一の方法です。

これは、ビデオ再生率がゼロの場合にのみ有効な値が返され、これは浅いコピーであり、書くのは絶対に安全ではありません。

そしてもう一度、FairPlayで保護されていないコンテンツでのみ利用可能です。

私たちはあなたのアプリにライブテキスト機能をもたらすのを手伝うことに興奮しています。

ライブテキストチームの全員を代表して、このセッションにご参加いただきありがとうございます。

アプリの画像にどのように使用するかを見て興奮しています。

そして、いつものように、楽しんでください!

♪