10025

ロン・サントス:ねえ、あなたが元気であることを願っています。入力エンジニアのロン・サントスです。

今日は、ビデオフィードから機械で読み取り可能なコードとテキストをキャプチャすること、または私たちが呼びたいように、データスキャンについてお話しします。

データスキャンとは正確にはどういう意味ですか?

これは単に、カメラのようなセンサーを使用してデータを読み取る方法です。

通常、そのデータはテキストの形で提供されます。

例えば、電話番号、日付、価格などの興味深い情報を含む領収書。

あるいは、データは、ユビキタスQRコードのように、機械で読み取り可能なコードとして来るかもしれません。

おそらく、カメラアプリで、またはiOS 15で導入されたライブテキスト機能を使用して、以前にデータスキャナを使用したことがあるでしょう。

そして、あなたは自分のカスタムスキャン体験で日常生活の中でアプリを使ったことがあるに違いない。

しかし、独自のデータスキャナを構築しなければならない場合はどうなりますか?

どうやってやるの？

iOS SDKには、ニーズに応じて複数のソリューションがあります。

1つのオプションは、AVFoundationフレームワークを使用してカメラグラフを設定し、入力と出力をセッションに接続し、機械読み取り可能なコードのようなAVMetadataObjectsを生成するように設定できることです。

テキストをキャプチャしたい場合、もう1つのオプションは、AVFoundationとVisionフレームワークの両方を組み合わせることです。

この図では、メタデータ出力の代わりに、ビデオデータ出力を作成します。

ビデオデータ出力は、テキストおよびバーコード認識要求で使用するためにビジョンフレームワークに供給できるサンプルバッファの配信をもたらし、その結果、ビジョン観測オブジェクトが得られます。

データスキャンにVisionを使用する方法の詳細については、WWDC21の「Visionを使用してドキュメントデータを抽出する」をご覧ください。

さて、それはデータスキャンにAVFoundationとVisionを使用しています。

iOS 16では、そのすべてをカプセル化する新しいオプションがあります。

VisionKitフレームワークのDataScannerViewControllerを紹介します。

データスキャンの目的で、AVFoundationとVisionの機能を組み合わせています。

DataScannerViewControllerユーザーは、ライブカメラのプレビュー、便利なガイダンスラベル、アイテムの強調表示、選択にも使用されるタップツーフォーカス、そして最後にピンチツーズームなどの機能に扱われます。

そして、あなたのような開発者のための機能について話しましょう。

DataScannerViewControllerは、選択した方法で提示できるUIViewControllerサブクラスです。

認識されたアイテムの座標は常にビュー座標にあり、画像空間からビジョン座標に変換して座標を表示するのを省くことができます。

また、ビュー座標にある関心のある領域を指定することで、ビューのアクティブな部分を制限することもできます。

テキスト認識では、コンテンツタイプを指定して、検索するテキストの種類を制限できます。

また、機械で読み取り可能なコードの場合は、どのシンボルを探すかを正確に指定できます。

わかりました。私はあなたのアプリを使用しており、データスキャンはその機能のほんの一部に過ぎないことを理解しています。

しかし、それは多くのコードを必要とするかもしれません。

DataScannerViewControllerを使用すると、私たちの目標は、あなたが他の場所に時間を集中できるように、あなたのために共通のタスクを実行することです。

次に、アプリに追加する方法を説明します。 アプリに追加します。

プライバシーの使用状況の説明から始めましょう。

アプリがビデオをキャプチャしようとすると、iOSはユーザーにカメラにアクセスする明示的な許可を与えるよう求めます。

あなたのニーズを正当化する説明的なメッセージを提供したいと思うでしょう。

これを行うには、アプリのInfo.plistファイルに「プライバシー - カメラの使用状況の説明」を追加します。

ユーザーが何に同意しているかがわかるように、できるだけ説明的にしてください。

さあ、コードに。

データスキャナを提示したいところはどこでも、まずVisionKitをインポートしてください。

次に、データスキャンはすべてのデバイスでサポートされていないため、isSupportedクラスプロパティを使用して、機能を公開するボタンやメニューを非表示にすると、ユーザーに使用できないものが表示されません。

興味があれば、Apple Neural Engineを搭載した2018年以降のiPhoneおよびiPadデバイスは、データスキャンをサポートしています。

また、空き状況も確認する必要があります。

プライバシーの使用に関する説明を思い出しますか?

スキャンは、ユーザーがカメラアクセスのためにアプリを承認し、デバイスがスクリーンタイムのコンテンツとプライバシー制限でここで設定されたカメラアクセス制限のような制限を受けていない場合に利用できます。

これで、インスタンスを設定する準備が整いました。

これは、最初に関心のあるデータの種類を指定することによって行われます。

たとえば、QRコードとテキストの両方をスキャンできます。

オプションで、テキストリコグナイザが言語修正などのさまざまな処理面のヒントとして使用する言語のリストを渡すことができます。

どの言語が予想されるか考えがあるなら、それらをリストアップしてください。

2つの言語に似た見た目のスクリプトがある場合に特に便利です。

言語を提供しない場合は、デフォルトでユーザーの優先言語が使用されます。

特定のテキストコンテンツタイプをリクエストすることもできます。

この例では、スキャナにURLを探してもらいたい。

認識するデータの種類を述べたので、DataScannerインスタンスを作成できます。

前の例では、バーコードシンボル、認識言語、およびテキストコンテンツタイプを指定しました。

それぞれの他のオプションについて少し説明させてください。

バーコードシンボルについては、ビジョンのバーコード検出器と同じシンボルをすべてサポートしています。

言語の面では、LiveText機能の一部として、DataScannerViewControllerはまったく同じ言語をサポートしています。

そして、iOS 16では、日本語と韓国語のサポートを追加していることを嬉しく思います。

もちろん、これは将来再び変わる可能性があります。

したがって、supportedTextRecognitionLanguagesクラスプロパティを使用して、最新のリストを取得します。

最後に、特定の意味的な意味を持つテキストをスキャンするとき、DataScannerViewControllerはこれらの7つのタイプを見つけることができます。

これで、データスキャナをユーザーに提示する準備が整いました。

他のビューコントローラーと同じように提示したり、フルスクリーンにしたり、シートを使用したり、別のビュー階層に完全に追加したりします。

それはすべてあなた次第です。

その後、プレゼンテーションが完了したら、startScanning()を呼び出してデータの探しを開始します。

だから今、私は一歩下がって、データスキャナの初期化パラメータに時間を費やしたいと思います。

私はここで1つ、認識されたDataTypesを使用しました。

しかし、あなたの経験をカスタマイズするのに役立つ他のものがあります。

それぞれに目を通しましょう。

recognizedDataTypesを使用すると、認識するデータの種類を指定できます。

テキスト、機械で読み取り可能なコード、およびそれぞれのタイプ。

qualityLevelは、バランス、高速、または正確です。

Fastは、看板のテキストなど、大きくて読みやすいアイテムを期待するシナリオでは、スピードを優先して解像度を犠牲にします。

精度は、マイクロQRコードや小さなシリアル番号などの小さなアイテムでも、最高の精度を提供します。

バランスの取れたものから始めることをお勧めします。これはほとんどのケースでうまくいくはずです。

recognizesMultipleItemsは、一度に複数のバーコードをスキャンする場合など、フレーム内の1つ以上のアイテムを探すオプションを提供します。

Falseの場合、ユーザーが他の場所をタップするまで、最中央の項目はデフォルトで認識されます。

ハイライトを描くときに高フレームレートトラッキングを有効にします。

カメラが動いたり、シーンが変わったりすると、ハイライトができるだけ密接にアイテムを追うことができます。

ピンチツーズームを有効にするか、無効にします。

また、ズームレベルを自分で変更する方法もあります。

ガイダンスを有効にすると、ラベルが画面の上部に表示され、ユーザーを指示するのに役立ちます。

そして最後に、必要に応じてシステムハイライトを有効にすることも、それを無効にして独自のカスタムハイライトを描画することもできます。

データスキャナーの提示方法がわかったので、認識されたアイテムをどのように取り込むか、また、独自のカスタムハイライトをどのように描画するかについて話しましょう。

まず、データスキャナにデリゲートを提供します。

デリゲートができたら、ユーザーがアイテムをタップしたときに呼び出されるdataScanner didTapOnメソッドを実装できます。

これにより、この新しいタイプのRecognizeItemのインスタンスを受け取ります。

RecognizedItemは、テキストまたはバーコードを関連する値として保持する列挙型です。

テキストの場合、転写プロパティは認識された文字列を保持します。

バーコードの場合、ペイロードに文字列が含まれている場合は、payloadStringValueで取得できます。

RecognizedItemについて知っておくべき他の2つのこと：まず、認識された各アイテムには、生涯を通じてアイテムを追跡するために使用できる一意の識別子があります。

その寿命は、アイテムが最初に表示されたときに始まり、表示されなくなったときに終了します。

そして第二に、各RecognizedItemにはboundsプロパティがあります。

境界は直線ではありませんが、各コーナーに1つずつ4つのポイントで構成されています。

次に、シーンの変更で認識されたアイテムが呼び出される3つの関連するデリゲートメソッドについて話しましょう。

1つ目はdidAddで、シーン内のアイテムが新たに認識されたときに呼び出されます。

独自のカスタムハイライトを作成したい場合は、新しいアイテムごとにここに1つ作成します。

関連するアイテムのIDを使用して、ハイライトを追跡できます。

また、新しいビューをビュー階層に追加するときは、DataScannerのoverlayContainerViewに追加して、カメラプレビューの上に表示されますが、他の補足クロムの下に表示されます。

次のデリゲートメソッドはdidUpdateで、アイテムが移動したり、カメラが移動したりするときに呼び出されます。

また、認識されたテキストの転写が変更されたときに呼び出すこともできます。

スキャナーがテキストを見る時間が長ければ長いほど、転写がより正確になるため、それらは変わります。

更新されたアイテムのIDを使用して、作成したばかりの辞書からハイライトを取得し、ビューを新しく更新された境界にアニメーション化します。

そして最後に、didRemoveデリゲートメソッドは、アイテムがシーンに表示されなくなったときに呼び出されます。

この方法では、削除されたアイテムに関連付けられたハイライトビューを忘れることができ、ビュー階層から削除できます。

要約すると、アイテムの上に独自のハイライトを描く場合、これらの3つのデリゲートメソッドは、シーンにハイライトをアニメーション化し、その動きをアニメーション化し、それらをアニメーション化するために非常に重要です。

また、以前の3つのデリゲートメソッドのそれぞれについて、現在認識されているすべてのアイテムの配列も与えられます。

これは、アイテムが自然な読み取り順序で配置されるため、テキスト認識に役立つ場合があります。つまり、ユーザーはインデックス1のアイテムの前にインデックス0でアイテムを読みます。

これは、DataScannerViewControllerの使用方法の概要です。

締めくくる前に、写真を撮影するなど、他のいくつかの機能について簡単に言及したかった。

高品質のUIImageを非同期に返すcapturePhotoメソッドを呼び出すことができます。

また、カスタムハイライトを作成していない場合は、これら3つのデリゲートメソッドを必要としないかもしれません。

代わりに、cognizemcogniztItemプロパティを使用できます。

これは、シーンが変化するにつれて継続的に更新されるAsyncStreamです。

ぶらぶらしてくれてありがとう。

iOS SDKは、AVFoundationとVisionフレームワークを使用してコンピュータビジョンワークフローを作成するためのオプションを提供することを忘れないでください。

しかし、ピックアンドパックアプリ、バックオブザウェアハウスアプリ、POSアプリなど、ライブビデオフィードでテキストや機械で読み取り可能なコードをスキャンするアプリを作成しているのかもしれません。

もしそうなら、VisionKitのDataScannerViewControllerを見てみましょう。

今日調べたように、アプリのスタイルとニーズに合ったカスタムエクスペリエンスを提供するために使用できる多くの初期化パラメータとデリゲートメソッドがあります。

そして最後に、静的画像のVisionKitのライブテキスト機能について学ぶことができる「アプリにライブテキストインタラクションを追加する」セッションに叫びたいと思いました。

次回まで、平和。

。