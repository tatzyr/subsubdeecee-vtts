10126

♪ ♪

クリスチャン：こんにちは、私の名前はクリスチャンです。

私はARKitチームのエンジニアであり、私たちのセッション、Discover ARKit 6にあなたを歓迎したいと思います。

拡張現実フレームワークの最新の進歩を活用する方法を学びます。

私たちは、あなたがARKitで過去数年間に作成してきたものを見て喜んでいます。

私たちは、インテリアデザイン、旅行、バーチャル展示会、ゲーム、その他多くの素晴らしいアプリを見ています。

Appleのチームは、あなたのフィードバックに細心の注意を払い、その多くをARKit 6に組み込みました。

見てみましょう。 

これまでで最高の画像解像度でカメラストリームを実行できる新しい4Kビデオモードを導入しています。

その後、ビデオの背景をより詳細に制御できる追加のカメラ機能強化について説明します。

また、プレーンアンカーの動作に関する最新情報、モーションキャプチャAPIの追加、そして最後にロケーションアンカーがサポートされる新しい都市を共有します。

4Kビデオから始めましょう。

過去数年間で、高解像度コンテンツ、特に映画制作に拡張現実の力を活用するアプリに対する多くの需要が、より多くのピクセルに飢えています。

ARKitで画像がどのようにキャプチャされ、処理されるかをお見せしましょう。

これはiPhone 13 Proのカメラモジュールです。

それを開くと、その設定を見ることができます。

ワイドカメラとウルトラワイドカメラについて話しましょう。

これらのカメラは両方とも、世界追跡、モーションキャプチャ、人物セグメンテーションなど、さまざまなコンピュータビジョンタスクに使用できます。

ワイドカメラは、レンダリングの背景のための画像を提供するので、私たちの心の中で特別な場所を持っています。

レンダリングのために画像がどのように処理されるかを理解することが重要なので、センサーレベルにズームインさせてください。

ARKitで画像をキャプチャするときは、イメージセンサーの大部分を使用します。

より正確には、この特定のモデルの3840x2880ピクセルの領域です。

キャプチャ後、ビニングと呼ばれるプロセスを使用します。

次のように動作します。Binningは2x2ピクセルの領域を取り、ピクセル値を平均化し、単一のピクセルを書き込みます。

これには2つの大きな利点があります。

まず、画像の寸法は2倍に縮小され、この場合は1920x1440ピクセルに縮小されます。

その結果、各フレームはより少ないメモリと処理能力を消費します。

これにより、デバイスは毎秒最大60フレームでカメラを実行でき、レンダリング用のリソースが解放されます。

第二に、このプロセスは、ピクセル値の平均化がセンサーノイズの影響を低減する低照度環境で利点を提供します。

私たちは、約17ミリ秒ごとにHD解像度で画像を提供するカメラストリームで終わります。

さまざまなコンピュータビジョンタスクに現在のフレームを使用した後、ARKitはレンダリングのために現在のフレームを表面化します。

独自のMetalレンダラーを作成している場合は、ARSessionのcurrentFrame.capturedImageからアクセスできます。

RealityKitを使用している場合、画像は背景として使用するためにさらに自動的に処理されます。

2532ピクセルの画面幅に合わせてデバイス上でスケーリングされ、ディスプレイのアスペクト比に合わせてトリミングされます。

RealityKitは、この海賊船のような仮想コンテンツをレンダリングして合成するタスクをフレームの上に実行し、最終結果を画面に表示します。

現在、最新のハードウェアのパワーにより、ARKitでフル4Kビデオモードを有効にします。

アプリは、ビニングステップをスキップしてフル4K解像度で直接アクセスすることで、高解像度の画像を利用できるようになりました。

4Kモードでは、3840x2160ピクセルの画像領域が使用され、毎秒30フレームでビデオをキャプチャできます。

これらの変更とは別に、あなたのアプリは以前と同じように動作します。

RealityKitを使用すると、スケーリング、トリミング、レンダリングが実行されます。

いくつかの簡単な手順で4Kモードを有効にできます。

それがコードでどのように見えるか見てみましょう。

「ARConfiguration」には、そのモードがデバイスでサポートされている場合、4Kビデオフォーマットを返す新しい便利な機能「recommendedVideoFormatFor4KResolution」があります。

デバイスまたは構成が4Kをサポートしていない場合、この機能はnilを返します。

次に、このビデオ形式を設定に割り当て、その調整された設定でセッションを実行できます。

4Kビデオモードは、iPhone 11以降、およびM1チップを搭載したiPad Proで利用できます。

解像度は毎秒30フレームで3840x2160ピクセルです。

iPadの場合、アスペクト比は16:9です。つまり、フルスクリーン表示のために画像を側面でトリミングする必要があり、最終的なレンダリングはズームインしているように見えるかもしれません。

ARKitを使用する場合、特に新しい4K解像度では、最適な結果を得るためにいくつかのベストプラクティスに従うことが重要です。

ARFrameをあまり長く保持しないでください。

これにより、システムがメモリを解放する可能性があり、ARKitが新しいフレームを浮上させるのをさらに停止する可能性があります。

これは、レンダリングのフレームドロップで表示されるようになります。

最終的には、ARCameraの追跡状態は制限に戻る可能性があります。

コンソールの警告をチェックして、いつでもあまりにも多くの画像を保持していないことを確認してください。

また、新しい4Kビデオフォーマットが本当にあなたのアプリにとって正しい選択肢であるかどうかも検討してください。

高解像度ビデオの恩恵を受けるアプリは、ビデオ、映画制作、バーチャルプロダクションアプリなど、優れた候補です。

高解像度の画像を扱うことは追加のシステムリソースを占有するので、高いリフレッシュレートに依存するゲームやその他のアプリでは、毎秒60フレームでフルHDビデオを使用することをお勧めします。

新しい4Kモードに加えて、カメラをより詳細に制御できるいくつかの追加機能強化があります。

まず、高解像度の背景写真APIを導入し、新しいHDRモードを有効にする方法を紹介します。

さらに、よりきめ細かい制御のために基礎となるAVCaptureDeviceにアクセスする方法を実演し、ARKitでEXIFタグを読み取る方法を紹介します。

新しい高解像度の背景写真APIに飛び込みましょう。

ARSessionを実行している間も、通常どおりビデオストリームにアクセスできます。

さらに、ARKitを使用すると、ビデオストリームが途切れることなく実行されている間、バックグラウンドでオンデマンドで単一の写真のキャプチャをリクエストできます。

これらの単一のフォトフレームは、カメラセンサーを最大限に活用します。

私のiPhone 13では、それはワイドカメラの完全な12メガピクセルを意味します。

WWDCの準備中、ARKitでは、この新しいAPIが作成に役立つものを強調する写真アプリの楽しいアイデアを思いつきました。

私たちの例では、有名な海賊旗がアップルインフィニットループキャンパスの上に飛んでいた2016年4月1日にあなたを連れ戻します。

私はオリジナルの写真家のトミーに、6年前にその写真をどこで撮ったのか尋ねました。

この座標に基づいて、大きな青い点が示すように、トミーが2016年4月に立っていたのとまったく同じ場所に案内するロケーションアンカーを作成できます。

その場所に到達すると、フォーカススクエアを表示することで、その完璧な画像をフレーミングするのに役立ちます。

最後に、このアプリでは画面をタップして写真を撮ることができます。

その写真は、別のAVCaptureセッションをスピンアップすることなく、現在のARKitセッションの実行中にネイティブカメラ解像度で撮影できます。

ARと写真の力を組み合わせたアイデアを楽しみにしています。私たちは、あなたが持っているものを見て興奮しています。

このAPIによって大きな恩恵を受けるもう1つのユースケースは、オブジェクトキャプチャを使用した3Dモデルの作成です。

オブジェクトキャプチャは、このランニングシューズのような現実世界のオブジェクトの写真を撮り、最新のフォトグラメトリアルゴリズムを使用して、ARアプリに展開する準備ができている3Dモデルに変換します。

ARKitを使用すると、物理的なオブジェクトの上に3D UIをオーバーレイし、より良いキャプチャガイダンスを提供できます。

そして今、新しい高解像度の背景画像APIを使用すると、オブジェクトの高解像度の写真を撮り、さらにリアルな3Dモデルを作成できます。

私はフォトグラメトリの大ファンなので、今年の「Bring your world to augmented reality」セッションをチェックすることを強くお勧めします。

コードで高解像度の写真撮影を有効にする方法をお見せしましょう。

まず、hiResCaptureをサポートするビデオフォーマットを確認します。

そのために、便利な機能「recommendedVideoForForHighResolution FrameCapturing」を使用できます。

フォーマットがサポートされていることを確認したら、新しいビデオフォーマットを設定してセッションを実行できます。

さらに、高解像度画像をキャプチャするタイミングをARKitに伝える必要があります。

以前の例では、写真のキャプチャは画面をタップするとトリガーされます。

独自のアプリケーションでは、高解像度のフレームキャプチャをトリガーするさまざまなイベントに反応したいと思うかもしれません。

それは本当にあなたのユースケースによります。

ARSessionには、captureHighResolutionFrameという新しい関数があります。

この関数を呼び出すと、高解像度画像のアウトオブバンドキャプチャがトリガーされます。

完了ハンドラでは、高解像度画像と他のすべてのフレームプロパティを含むARFrameに非同期にアクセスできます。

コンテンツにアクセスする前に、フレームキャプチャが成功したかどうかを確認する必要があります。

この例では、フレームをディスクに保存します。

また、特にこれらの画像はイメージセンサーのフル解像度を使用しているため、先に述べた画像保持に関するベストプラクティスを覚えておいてください。

次に、HDRについて話しましょう。

ハイダイナミックレンジは、より広い範囲の色をキャプチャし、それらをディスプレイにマッピングします。

これは、コントラストの高い環境で最もよく見られます。

これは私の裏庭の良い例です。

このシーンは、木製のフェンスなど、非常に暗い領域と、空の雲のような非常に明るい領域の両方を特徴としています。

HDRモードをオンにすると、右側のように、雲のふわふわのようなこれらの領域の詳細がHDRではるかによく保存されていることがわかります。

コードでHDRがどのようにオンになっているか見てみましょう。

「isVideoHDRSupported」プロパティを使用して、HDRをサポートしている場合は、任意のビデオフォーマットを照会できます。

現在、バインドされていないビデオフォーマットのみがHDRをサポートしています。

HDRがサポートされている場合は、設定でvideoHDRAllowedをtrueに設定し、その設定でセッションを実行します。

HDRをオンにするとパフォーマンスに影響を与えるので、必要な場合にのみ使用してください。HDRをオンにすると、パフォーマンスに影響します。

露出やホワイトバランスなどの設定を手動で制御することを好むユースケースでは、AVCaptureDeviceに直接アクセスし、その設定を変更する便利な方法があります。

コード例では、設定の「configurableCaptureDevice ForPrimaryCamera」を呼び出すと、基盤となる「AVCaptureDevice」にアクセスできます。

この機能を使用して、ARKitアプリのカスタムルックを作成しますが、画像はレンダリングの背景として使用されるだけでなく、シーンを分析するためにARKitによって積極的に使用されることに注意してください。

したがって、強い露出過剰のような変化は、ARKitの出力品質に悪影響を及ぼす可能性があります。

また、フォーカスイベントのトリガーなど、いくつかの高度な操作を実行することもできます。

AVCaptureSessionsの設定方法の詳細については、developer.apple.comのAVCaptureドキュメントを参照してください。

最後に、ARKitはEXIFタグをアプリに公開します。

それらはすべてのARFrameで利用可能になりました。

EXIFタグには、ホワイトバランス、露出、および後処理に価値のあるその他の設定に関する有用な情報が含まれています。

これで、画像キャプチャパイプラインのすべての更新は終了です。

平面アンカーにどのような変更があるか見てみましょう。

飛行機のアンカーは、ARKitの最初のバージョン以来、人気のある機能です。

あなた方の多くは、平面アンカーと基礎となる平面ジオメトリをよりきれいに分離する必要性を表明しました。

そのため、平面アンカーの挙動と平面のジオメトリに関する最新情報を発表します。

これは、iOS 15の典型的な飛行機アンカーの例です。

ARセッションの開始時に、テーブルの上のこの質感の良いノートブックに平面をフィットさせます。

セッションを実行すると、平面は徐々に更新され、表示されるテーブルの新しい部分が考慮されます。

平面ジオメトリが更新されるたびに、アンカーの回転も更新され、平面の新しい向きが反映されます。

iOS 16では、平面アンカーとその平面ジオメトリをよりクリーンに分離します。

平面アンカーとジオメトリの更新が完全に分離されました。

完全なテーブルが表示されるにつれて、平面がジオメトリを拡張および更新している間、アンカーの回転自体は一定のままです。

左側の古い動作と対比すると、iOS 16の平面アンカーは、ARセッション全体を通して、ノートブックに合わせて同じ向きにとどまっていることがわかります。

平面ジオメトリに関するすべての情報は、ARPlaneExtentというクラスに含まれるようになりました。

回転の更新は、平面アンカー自体を回転させることによって表現されなくなりました。

代わりに、ARPlaneExtentには、回転角度を表す新しいプロパティrotationOnYAxisが含まれています。

この新しいプロパティに加えて、平面は幅と高さ、およびPlaneAnchorの中心座標によって完全に定義されています。

この平面の視覚化をコードで作成する方法をお見せしましょう。

まず、指定された幅と高さに従って平面メッシュに基づいてエンティティを生成します。

次に、y軸の回転に従ってエンティティ変換を設定し、センタープロパティの値でオフセットします。

飛行機が更新されるたびに、幅、高さ、中心座標と新しい回転OnYAxisが変わる可能性があるという事実を考慮する必要があります。

この新しい動作を利用するには、Xcodeの設定で展開ターゲットをiOS 16に設定します。

次のアップデートはMotionCaptureで、私たちの機械学習の首謀者はさらなる改善を行うために懸命に働きました。

2Dスケルトンと3Dジョイントの両方に、一連のアップデートがあります。

2Dスケルトンでは、左耳と右耳の2つの新しい関節を追跡しています。

また、全体的なポーズ検出も改善しました。

iPhone 12以降、およびM1チップを搭載した最新のiPad ProおよびiPad Airモデルでは、赤で示されているように3Dスケルトンが改善されました。

ジッターが少なくなり、全体的に時間的な一貫性が高まるでしょう。

また、人の一部が閉塞している場合や、カメラの近くを歩いているときに、追跡もより安定します。

改良されたMotionCaptureを利用するには、Xcodeの設定で展開ターゲットをiOS 16に設定します。

次に、LocationAnchorsがサポートされる新しい都市や国も発表したいと思います。

小さなリマインダーとして、Apple MapsはLocationAnchor APIを使用して歩行者の歩行指示に電力を供給します。

この例では、LocationAnchorsの力のおかげで、ロンドンの通りを案内できることがわかります。

LocationAnchorsは、米国のますます多くの都市と英国のロンドンですでに利用可能です。

今日から、カナダのバンクーバー、トロント、モントリオールの都市で利用可能になります。

また、シンガポールや東京を含む日本の7つの大都市圏でもそれらを可能にしています。

オーストラリアのメルボルンとシドニーも同様です。

今年後半には、オークランド、ニュージーランド、テルアビブ、イスラエル、パリ、フランスで利用可能にします。LocationAnchorsが特定の座標でサポートされているかどうかを知りたい場合は、ARGeoTrackingConfigurationのcheckAvailabilityメソッドを使用してください。

そして、これらはすべてARKit 6のアップデートでした。

要約すると、私は新しい4KビデオフォーマットでARKitを実行する方法を紹介しました。

高度なユースケースでは、HDRを有効にする方法、またはAVCaptureDeviceを手動で制御する方法を実演しました。

さらに多くのピクセルに飢えたアプリケーションのために、私はARKitセッションから高解像度の写真を取得する方法を実演しました。

プレーンアンカーの新しい動作について話し、新しい耳関節とMotionCaptureの他の改善を発表しました。

また、LocationAnchorsが今年後半にどの国で利用可能になるかを知ることができました。

チューニングしてくれてありがとう。

素晴らしいWWDC 2022をお過ごしください!