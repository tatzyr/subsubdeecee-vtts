10017

♪ ♪

チャオ、私の名前はGeppy Parzialeで、アップルの機械学習エンジニアです。

今日は、機械学習を使用して、通常、非常に専門的な作業を実行するために専門家を必要とする問題を解決するアプリを構築する旅をご案内したいと思います。

この旅は、あなたのアプリにオープンソースの機械学習モデルを追加し、素晴らしい新しい体験を作成する方法を示す機会を与えてくれます。

旅の途中で、機械学習を使用してアプリを構築するために、Apple開発エコシステムで利用可能な多くのツール、フレームワーク、APIのいくつかも強調します。

アプリを構築するとき、開発者であるあなたは、うまくいけばユーザーに最高の体験をもたらす一連の決定を下します。

また、これはアプリケーションに機械学習機能を追加する場合にも当てはまります。

開発中、あなたは尋ねることができます:この機能を構築するために機械学習を使用する必要がありますか?

機械学習モデルを入手するにはどうすればよいですか?

そのモデルをAppleのプラットフォームと互換性を持たせるにはどうすればよいですか?

そのモデルは私の特定のユースケースで機能しますか?

それはApple Neural Engineで動作しますか?

だから、一緒にこの旅をしましょう。

地下室の古い箱で見つけた家族の白黒写真にリアルな色を追加できるアプリを作りたいです。

もちろん、プロの写真家は、写真編集ツールでいくつかの時間を費やす、いくつかの手作業でこれを行うことができます。

代わりに、このプロセスを自動化し、ほんの数秒で着色を適用したい場合はどうなりますか?

これは機械学習にとって完璧な作業のようです。

Appleは、アプリでML機能を構築して統合するのに役立つ膨大な量のフレームワークとツールを提供しています。

データ処理からモデルトレーニングや推論まで、あらゆるものを提供します。

この旅のために、私はそれらのいくつかを使うつもりです。

しかし、開発している特定の機械学習タスクに応じて、多くの選択肢があることを覚えておいてください。

アプリで機械学習機能を開発するときに使用するプロセスは、一連の段階を経ます。

まず、科学出版物または専門的なウェブサイトで適切な機械学習モデルを検索します。

私は写真の着色を検索し、私が必要とするもののために働くかもしれないカラライザーと呼ばれるモデルを見つけました。

以下は、このモデルを使用して取得できる着色の例です。

ここに別のものがあります。

そして、ここに別のものがあります。本当に素晴らしい。

それがどのように機能するかをお見せしましょう。

カラライザーモデルは、入力として白黒画像を期待しています。

私が見つけたPythonのソースコードは、任意のRGB画像をLAB色空間画像に変換します。

この色空間には3つのチャンネルがあります。1つは画像の明度またはLチャンネルを表し、他の2つは色の構成要素を表します。

軽さがカラライザーモデルの入力になる間、カラーコンポーネントは破棄されます。

次に、モデルは、入力Lチャネルと組み合わせて、結果の画像に色を提供する2つの新しいカラーコンポーネントを推定します。

今、このモデルを私のアプリと互換性を持たせる時が来ました。

これを達成するために、coremltoolsを使用して元のPyTorchモデルをCore ML形式に変換できます。

これは、PyTorchモデルをCore MLに変換するために使用したシンプルなPythonスクリプトです。

まず、PyTorchモデルのアーキテクチャと重みをインポートします。

次に、インポートされたモデルを追跡します。

最後に、PyTorchモデルをCore MLに変換して保存します。

モデルがCore ML形式になったら、変換が正しく機能したことを確認する必要があります。

私はcoremltoolsを使って再びPythonで直接それを行うことができます。

そして、これは簡単です。

画像をRGB色空間にインポートし、ラボ色空間に変換します。

次に、カラーチャンネルから明度を分離し、それらを破棄します。

Core MLモデルを使用して予測を実行します。

そして最後に、推定されたカラーコンポーネントで入力の明度を構成し、RGBに変換します。

これにより、変換されたモデルの機能が元のPyTorchモデルの機能と一致することを確認できます。

私はこの段階をモデル検証と呼んでいます。

しかし、もう1つの重要なチェックがあります。

このモデルがターゲットデバイスで十分に速く実行できるかどうかを理解する必要があります。

したがって、デバイス上のモデルを評価し、それが最高のユーザーエクスペリエンスを提供することを確認する必要があります。

Xcode 14で現在利用可能な新しいCore MLパフォーマンスレポートは、Core MLモデルの時間ベースの分析を実行します。

モデルをXcodeにドラッグアンドドロップして、数秒でパフォーマンスレポートを作成するだけです。

このツールを使用すると、M1とiPadOS 16を搭載したiPad Proでは、推定予測時間がほぼ90ミリ秒であることがわかります。

そして、これは私の写真の着色アプリに最適です。

Xcodeのパフォーマンスレポートについてもっと知りたい場合は、今年のセッション「Core MLの使用を最適化する」を見ることをお勧めします。

したがって、パフォーマンスレポートは、モデルを評価し、最高のデバイス上のユーザーエクスペリエンスを提供するのに役立ちます。

モデルの機能とパフォーマンスについて確信が持てたので、アプリに統合させてください。

統合プロセスは、私が今までPythonでやってきたことと同じですが、今回はXcodeとあなたがよく知っている他のすべてのツールを使用して、Swiftでシームレスに行うことができます。

現在Core ML形式のモデルは、その軽さを表す単一チャンネル画像を期待していることを覚えておいてください。

以前にPythonで行ったのと同様に、ラボの色空間を使用してRGB入力画像を画像に変換する必要があります。

私はこの変換を複数の方法で書くことができます。vImageを使用してSwiftで直接、またはMetalを使用します。

ドキュメントをより深く掘り下げてみると、Core Imageフレームワークがこれに役立つ何かを提供していることがわかりました。

では、RGBからLABへの変換を実現し、Core MLモデルを使用して予測を実行する方法をお見せしましょう。

これは、RGB画像から軽さを抽出し、Core MLモデルに渡すSwiftコードです。

まず、RGB画像をLABに変換し、軽さを抽出します。

次に、軽さをCGImageに変換し、Core MLモデルの入力を準備します。

最後に、私は予測を実行します。

入力RGB画像からLチャンネルを抽出するには、まず、新しいCIFilter convertRGBtoLabを使用して、RGB画像をLAB画像に変換します。

軽さの値は0から100の間で設定されます。

次に、ラボ画像にカラーマトリックスを乗算し、カラーチャンネルを破棄し、発信者に明るさを返します。

それでは、モデルの出力で何が起こるかを分析しましょう。

Core MLモデルは、推定カラーコンポーネントを含む2つのMLShapedArrayを返します。

したがって、予測の後、2つのMLShapedArrayを2つのCIImageに変換します。

最後に、私はそれらをモデル入力の軽さと組み合わせます。

これにより、新しいLAB画像が生成され、RGBに変換して返します。

2つのMLShapedArrayを2つのCIImageに変換するには、まず各形状の配列から値を抽出します。

次に、2つのカラーチャンネルを表す2つのコア画像を作成し、それらを返します。

軽さを推定カラーチャンネルと組み合わせるには、3つのチャンネルを入力として受け取り、CIImageを返すカスタムCIKernelを使用します。

次に、新しいCIFilter convertLabToRGBを使用してラボ画像をRGBに変換し、発信者に返します。

これは、単一のCIImageで2つの推定カラーチャンネルと明るさを組み合わせるために使用するカスタムCIKernelのソースコードです。

RGB画像をLAB画像に変換する新しいCIフィルター、またはその逆については、セッション「Core Image、Metal、SwiftUIでEDRコンテンツを表示する」を参照してください。

このML機能の統合をアプリに完了したので、実際に見てみましょう。

でも待って。

アプリケーションで古い家族写真をリアルタイムで色付けするにはどうすればよいですか?

それぞれをデジタル化し、アプリにインポートするのに時間を費やすことができます。

もっといい考えがあると思います。

iPadのカメラを使ってこれらの写真をスキャンし、ライブで色付けするとどうなりますか?

本当に楽しいと思いますし、これを達成するために必要なことはすべて揃っています。

しかし、まず、私は問題を解決しなければなりません。

私のモデルは画像を処理するのに90ミリ秒が必要です。

ビデオを処理したい場合は、もっと速いものが必要です。

スムーズなユーザーエクスペリエンスのために、デバイスカメラを少なくとも30fpsで実行したいと思います。

つまり、カメラは約30ミリ秒ごとにフレームを生成します。

しかし、モデルはビデオフレームを色付けするのに約90ミリ秒を必要とするので、各色付け中に2つまたは3つのフレームを失うことになります。

モデルの合計予測時間は、そのアーキテクチャと、それがマッピングされる計算ユニット操作の両方の関数です。

パフォーマンスレポートをもう一度見ると、私のモデルにはニューラルエンジンとCPUの組み合わせで合計61の操作が実行されていることがわかります。

より速い予測時間が必要な場合は、モデルを変更する必要があります。

私はモデルのアーキテクチャを試して、より速いかもしれないいくつかの選択肢を模索することにしました。

しかし、アーキテクチャの変更は、ネットワークを再訓練する必要があることを意味します。

Appleは、Macで機械学習モデルを直接トレーニングできるさまざまなソリューションを提供しています。

私の場合、元のモデルはPyTorchで開発されたので、Apple Siliconが提供する驚異的なハードウェアアクセラレーションを活用できるように、Metalで新しいPyTorchを使用することにしました。

メタルで加速されたPyTorchについてもっと知りたい場合は、「メタルで機械学習を加速する」というセッションを確認してください。この変更により、私たちの旅は一歩後退する必要があります。

再トレーニング後、結果をCore ML形式に変換し、検証を再度実行する必要があります。

今回、モデル統合は、単に古いモデルを新しいモデルに交換することで構成されています。

いくつかの候補の代替モデルを再訓練した後、私の要件を満たすものを確認しました。

これは、対応するパフォーマンスレポートです。

それは完全にニューラルエンジンで実行され、予測時間は現在約16ミリ秒で、ビデオで機能します。

しかし、パフォーマンスレポートは、私のアプリのパフォーマンスの1つの側面だけを教えてくれます。

確かに、アプリを実行した後、着色が期待するほどスムーズではないことにすぐに気づきました。

では、実行時に私のアプリで何が起こりますか?

それを理解するために、Instrumentsで新しいCore MLテンプレートを使用できます。

Core MLトレースの最初の部分を分析し、モデルをロードした後、アプリが予測を蓄積していることに気づきました。

そして、これは予想外です。

代わりに、フレームごとに1つの予測を期待します。

トレースを拡大して最初の予測をチェックすると、最初の予測が終了する前にアプリが2番目のCore ML予測を要求することがわかります。

ここでは、2番目のリクエストがCore MLに与えられたとき、ニューラルエンジンはまだ最初のリクエストに取り組んでいます。

同様に、3番目の予測は、2番目の予測を処理しながら開始されます。

4つの予測の後でも、要求と実行の間の遅れはすでに約20ミリ秒です。

代わりに、これらのラグのカスケードを避けるために、前の予測が終了した場合にのみ、新しい予測が開始されることを確認する必要があります。

この問題を解決している間に、誤ってカメラのフレームレートを目的の30fpsではなく60fpsに設定してしまったこともわかりました。

以前の予測が完了した後、アプリが新しいフレームを処理することを確認し、カメラのフレームレートを30fpsに設定した後、Core MLが単一の予測をApple Neural Engineに正しくディスパッチし、アプリがスムーズに動作することがわかります。

それで、私たちは旅の終わりに達しました。

私の古い家族写真でアプリをテストしましょう。

これが私の地下室で見つけた白黒写真です。

彼らは私がずっと前に訪れたイタリアの場所のいくつかをキャプチャします。

これはローマのコロッセオの素晴らしい写真です。

壁と空の色はとても現実的です。

これを確認しましょう。

これはイタリア南部のカステル・デル・モンテです。

本当にいいね。

そして、これは私の故郷、グロッタリエです。

これらの画像に色を追加すると、非常に多くの思い出が引き起こされました。

残りのシーンを白黒に保ちながら、写真にのみ色付けを適用していることに注意してください。

ここでは、ビジョンフレームワークで利用可能な長方形検出アルゴリズムを利用しています。

VNDetectRectangleRequestを使用すると、シーン内の写真を分離し、Colorizerモデルへの入力として使用できます。

そして今、要約させてください。

私たちの旅の中で、私はあなたのアプリの機械学習機能を準備、統合、評価するためにAppleが提供する膨大な量のフレームワーク、API、ツールを探求しました。

私は、それを解決するためにオープンソースの機械学習モデルを必要とする問題を特定するこの旅を始めました。

必要な機能を備えたオープンソースモデルを見つけ、Appleプラットフォームと互換性を持たせました。

新しいパフォーマンスレポートを使用して、デバイス上で直接モデルのパフォーマンスを評価しました。

私はあなたがよく知っているツールとフレームワークを使って、私のアプリにモデルを統合しました。

インストゥルメントの新しいCore MLテンプレートを使用してモデルを最適化しました。

Appleのツールとフレームワークを使用すると、データの準備、トレーニング、統合、最適化から、Appleのデバイスとプラットフォームで直接開発プロセスの各段階を処理できます。

今日、私たちは、開発者であるあなたがAppleが提供するフレームワークとツールで達成できることの表面を引っ掻きました。

アプリに機械学習をもたらすための追加の刺激的なアイデアについては、これにリンクされている以前のセッションを参照してください。

フレームワークとツールを探索して試してみてください。

ソフトウェアとハードウェアの大きな相乗効果を活用して、機械学習機能を加速し、アプリのユーザーエクスペリエンスを豊かにします。

素晴らしいWWDCを、そして到着してください。♪ ♪