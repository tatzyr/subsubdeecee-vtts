10027

♪まろやかなインストゥルメンタルヒップホップ音楽♪

♪

こんにちは、私の名前はベンで、コアMLチームのエンジニアです。

今日は、Core MLに追加されるエキサイティングな新機能のいくつかを紹介します。

これらの機能の焦点は、Core MLの使用を最適化することです。

このセッションでは、Core MLを使用する際にモデルのパフォーマンスを理解し、最適化するために必要な情報を提供するために、現在利用可能なパフォーマンスツールについて確認します。

次に、これらの最適化を可能にするいくつかの強化されたAPIについて確認します。

そして最後に、いくつかの追加のCore ML機能と統合オプションの概要を説明します。

パフォーマンスツールから始めましょう。

背景を伝えるために、アプリ内でCore MLを使用する際の標準的なワークフローを要約することから始めます。

最初のステップは、モデルを選択することです。

これは、Core MLツールを使用してPyTorchまたはTensorFlowモデルをCore ML形式に変換する、既存のCore MLモデルを使用する、Create MLを使用してモデルをトレーニングおよびエクスポートするなど、さまざまな方法で行うことができます。

モデル変換の詳細やCreate MLについては、これらのセッションをチェックすることをお勧めします。

次のステップは、そのモデルをアプリに統合することです。

これには、モデルをアプリケーションにバンドルし、Core ML APIを使用して、アプリの実行中にそのモデルの推論をロードして実行することが含まれます。

最後のステップは、Core MLの使用方法を最適化することです。

まず、モデルの選択について確認します。

アプリ内でそのモデルを使用するかどうかを決定する際に考慮すべきモデルの多くの側面があります。

また、選択したいモデルの候補が複数あるかもしれませんが、どちらを使用するかをどのように決めますか?

有効にしたい機能の要件に一致する機能を持つモデルが必要です。

これには、モデルの精度とパフォーマンスを理解することが含まれます。

Core MLモデルについて学ぶ素晴らしい方法は、Xcodeで開くことです。

任意のモデルをダブルクリックするだけで、以下が表示されます。

上部には、モデルタイプ、サイズ、およびオペレーティングシステムの要件があります。

[一般]タブでは、モデルのメタデータ、計算精度とストレージ精度、および予測できるクラスラベルなどの情報でキャプチャされた追加の詳細が表示されます。

プレビュータブは、サンプル入力を提供し、それが何を予測するかを見て、モデルをテストするためのものです。

[予測]タブには、モデルの入力と出力、およびCore MLが実行時に期待するタイプとサイズが表示されます。

そして最後に、ユーティリティタブは、モデルの暗号化と展開タスクに役立ちます。

全体として、これらのビューは、モデルの機能の概要と精度のプレビューを提供します。

しかし、あなたのモデルのパフォーマンスはどうですか?

モデルのロードコスト、単一の予測にかかる時間、または使用するハードウェアは、ユースケースにとって重要な要素である可能性があります。

リアルタイムのストリーミングデータの制約に関連するハードターゲットがある場合や、知覚されるレイテンシに応じて、ユーザーインターフェイスに関する重要な設計決定を行う必要がある場合があります。

モデルのパフォーマンスに関する洞察を得る方法の1つは、アプリへの初期統合を行うか、測定および測定できる小さなプロトタイプを作成することです。

また、パフォーマンスはハードウェアに依存するため、サポートされているさまざまなハードウェアでこれらの測定を行いたいと思うでしょう。

XcodeとCore MLは、1行のコードを書く前でも、このタスクを支援できるようになりました。

Core MLでは、パフォーマンスレポートを作成できるようになりました。

お見せしましょう。

YOLOv3オブジェクト検出モデルのXcodeモデルビューアが開きました。

[予測] タブと [ユーティリティ] タブの間に、[パフォーマンス] タブがあります。

パフォーマンスレポートを生成するには、左下のプラスアイコンを選択し、実行したいデバイスを選択します。これは私のiPhoneです。[次へ]をクリックし、Core MLを使用するコンピューティングユニットを選択します。

Core MLが利用可能なすべてのコンピューティングユニットでレイテンシを最適化できるように、すべてにしておくつもりです。

では、Run Testを押して終了します。

テストを確実に実行できるように、選択したデバイスのロックが解除されていることを確認してください。

パフォーマンスレポートの生成中に回転するアイコンが表示されます。

レポートを作成するには、モデルがデバイスに送信され、モデルで実行されるコンパイル、ロード、および予測のいくつかの反復があります。

これらが完了すると、パフォーマンスレポートのメトリクスが計算されます。

今、それは私のiPhoneでモデルを実行し、パフォーマンスレポートを表示します。

上部には、テストが実行されたデバイスと、どのコンピューティングユニットが選択されたかについての詳細が表示されます。

次に、実行に関する統計が表示されます。

予測時間の中央値は22.19ミリ秒で、ロード時間の中央値は約400ミリ秒でした。

また、デバイス上でモデルをコンパイルする予定の場合、これはコンパイル時間が約940ミリ秒だったことを示しています。

約22ミリ秒の予測時間は、リアルタイムで実行したい場合、このモデルは毎秒約45フレームをサポートできることを示しています。

このモデルにはニューラルネットワークが含まれているため、パフォーマンスレポートの下部にレイヤービューが表示されます。

これは、すべてのレイヤーの名前とタイプ、および各レイヤーが実行された計算ユニットを示します。

塗りつぶされたチェックマークは、その計算ユニットでレイヤーが実行されたことを意味します。

記入されていないチェックマークは、レイヤーがそのコンピューティングユニットでサポートされていることを意味しますが、Core MLはそこで実行することを選択しませんでした。

そして、空のダイヤモンドは、そのコンピューティングユニットでレイヤーがサポートされていないことを意味します。

この場合、54層がGPUで実行され、32層がNeural Engineで実行されました。

クリックして、計算ユニットでレイヤーをフィルタリングすることもできます。

それが、Xcode 14を使用してCore MLモデルのパフォーマンスレポートを生成する方法でした。

これはiPhoneで実行するために示されましたが、1行のコードを書くことなく、複数のオペレーティングシステムとハードウェアの組み合わせでテストすることができます。

モデルを選択したので、次のステップは、このモデルをアプリに統合することです。

これには、モデルをアプリにバンドルし、Core ML APIを使用してモデルをロードし、予測を行うことが含まれます。

この場合、Core MLスタイル転送モデルを使用して、ライブカメラセッションからフレームでスタイル転送を実行するアプリを構築しました。

正常に動作しています。しかし、フレームレートは予想よりも遅く、その理由を理解したいと思います。

これは、Core MLの使用を最適化するためのステップ3に進む場所です。

パフォーマンスレポートを生成すると、モデルがスタンドアロン環境で達成できるパフォーマンスを示すことができます。ただし、アプリでライブで実行されているモデルのパフォーマンスをプロファイリングする方法も必要です。

このため、Xcode 14のInstrumentsアプリにあるCore ML Instrumentを使用できるようになりました。

このインストゥルメントは、アプリでライブで実行されるモデルのパフォーマンスを視覚化し、潜在的なパフォーマンスの問題を特定するのに役立ちます。

それがどのように使用できるかをお見せしましょう。

だから私はスタイル転送アプリのワークスペースを開いた状態でXcodeにいて、アプリをプロファイリングする準備ができています。

実行ボタンを強引にクリックし、プロファイルを選択します。

これにより、最新バージョンのコードがデバイスにインストールされ、ターゲットのデバイスとアプリが選択された状態でInstrumentsが開きます。

Core MLの使用状況をプロファイリングしたいので、Core MLテンプレートを選択します。

このテンプレートには、Core MLインストゥルメントと、Core MLの使用状況をプロファイリングするのに役立つ他のいくつかの便利なインストゥルメントが含まれています。

トレースをキャプチャするには、レコードを押すだけです。

そのアプリは今、私のiPhoneで実行されています。

私はそれを数秒間実行させ、いくつかの異なるスタイルを使用します。

そして今、私は停止ボタンを押してトレースを終了します。

今、私は楽器の痕跡を持っています。

コアMLインストゥルメントに焦点を当てます。

Core ML Instrumentは、トレースでキャプチャされたすべてのCore MLイベントを表示します。

最初のビューは、すべてのイベントをアクティビティ、データ、計算の3つのレーンにグループ化します。

アクティビティレーンは、負荷や予測など、直接呼び出す実際のCore ML APIと1対1の関係を持つトップレベルのCore MLイベントを表示します。

データレーンは、モデルの入出力を安全に操作できることを確認するために、Core MLがデータチェックまたはデータ変換を実行しているイベントを示します。

計算レーンは、Core MLがニューラルエンジンやGPUなどの特定の計算ユニットに計算要求を送信するタイミングを示します。

また、イベントタイプごとに個別のレーンがあるグループ化されていないビューを選択することもできます。

下部には、モデルアクティビティアグリゲーションビューがあります。

このビューは、トレースに表示されるすべてのイベントの集計統計情報を提供します。

たとえば、このトレースでは、平均モデル負荷は17.17ミリ秒かかり、平均予測は7.2ミリ秒かかりました。

もう1つの注意点は、期間でイベントを並べ替えることができるということです。

ここでは、リストは、わずか2.69秒の予測と比較して、合計6.41秒の負荷で、実際に予測を行うよりも多くの時間がモデルのロードに費やされていることを教えてくれています。

おそらく、これは低フレームレートと何か関係があります。

これらすべての負荷がどこから来ているのか調べてみます。

各予測を呼び出す前に、Core MLモデルをリロードしていることに気づいています。

モデルを一度ロードしてメモリに保持できるため、これは一般的に良い習慣ではありません。

私は自分のコードに戻って、これを修正しようとします。

モデルをロードするコードの領域を見つけました。

ここでの問題は、これが適切に計算されていることです。つまり、styleTransferModel変数を参照するたびに、プロパティを再計算します。これは、この場合、モデルをリロードすることを意味します。

これを遅延変数に変更することで、これをすぐに修正できます。

次に、アプリを再プロファイアプ化して、繰り返しロードの問題が解決したかどうかを確認します。

もう一度Core MLテンプレートを選択し、トレースをキャプチャします。

これは私が期待していることとはるかに一致しています。

カウント列は、アプリで使用したスタイルの数と一致する合計5つのロードイベントがあり、ロードの合計期間は予測の合計期間よりもはるかに小さいことを示しています。

また、スクロールしながら...

...それぞれの間に負荷をかけずに繰り返し予測イベントを正しく表示します。

もう1つの注意点は、これまでのところ、すべてのCore MLモデルアクティビティを表示するビューしか見ていないということです。

このアプリでは、スタイルごとに1つのCore MLモデルがあるので、モデルごとにCore MLアクティビティを分解したいと思うかもしれません。

楽器はこれを簡単にします。

メイングラフでは、左上の矢印をクリックすると、トレースで使用されるモデルごとに1つのサブトラックが作成されます。

ここでは、使用されたさまざまなスタイル転送モデルがすべて表示されます。

集計ビューは、統計をモデル別に分解できるようにすることで、同様の機能も提供します。

次に、それがどのように実行されているかをよりよく理解するために、私のモデルの1つの予測に飛び込みたいと思います。

水彩画のモデルをもっと深く見てみます。

この予測では、コンピュートレーンは、私のモデルがニューラルエンジンとGPUの組み合わせで実行されたことを教えてくれます。

Core MLはこれらの計算要求を非同期に送信しているので、これらの計算ユニットがいつモデルを積極的に実行しているかを知りたいなら、Core MLインストゥルメントとGPUインストゥルメントと新しいニューラルエンジンインストゥルメントを組み合わせることができます。

これを行うには、ここに3つの楽器を固定しています。

コアMLインストゥルメントは、モデルが実行された地域全体を示しています。

そして、この領域内では、ニューラルエンジンインストゥルメントは、最初にニューラルエンジンで実行されているコンピューティングを示し、次にGPUインストゥルメントは、モデルがニューラルエンジンから引き継がれ、GPUで実行を終了したことを示しています。

これにより、私のモデルが実際にハードウェアでどのように実行されているかをよりよく知ることができる。

要約すると、Xcode 14のCore ML Instrumentを使用して、アプリでライブで実行しているときのモデルのパフォーマンスについて学びました。

次に、モデルを頻繁にリロードしすぎている問題を特定しました。

コードの問題を修正し、アプリケーションを再プロファイプロファイし、問題が修正されたことを確認しました。

また、Core ML、GPU、新しいNeural Engine Instrumentを組み合わせて、モデルが実際に異なるコンピューティングユニットでどのように実行されているかについての詳細を得ることができました。

これは、パフォーマンスを理解するのに役立つ新しいツールの概要でした。

次に、そのパフォーマンスを最適化するのに役立つ強化されたAPIをいくつか確認します。

まず、Core MLがモデルの入力と出力をどのように処理するかを見てみましょう。

Core MLモデルを作成すると、そのモデルには一連の入出力機能があり、それぞれにタイプとサイズがあります。

実行時に、Core ML APIを使用して、モデルのインターフェイスに準拠した入力を提供し、推論を実行した後に出力を取得します。

もう少し詳細に画像とマルチアレイに集中させてください。

画像の場合、Core MLは、コンポーネントあたり8ビットの8ビットグレースケールと32ビットのカラー画像をサポートしています。

また、多次元配列の場合、Core MLはスカラー型としてInt32、Double、およびFloat32をサポートしています。

アプリがすでにこれらのタイプで動作している場合は、単にモデルに接続するだけです。

しかし、あなたのタイプが異なる場合があります。

例をお見せしましょう。

画像処理とスタイルアプリに新しいフィルターを追加したいのですが。

このフィルターは、単一チャンネルの画像を操作することで画像をシャープにするために機能します。

私のアプリには、GPUでいくつかの前処理と後処理操作があり、この単一のチャネルをFloat16精度で表しています。

これを行うには、coremltoolsを使用して、ここに示すように、画像シャープニングトーチモデルをCore ML形式に変換しました。

このモデルは、Float16の精密計算を使用するように設定されました。

また、画像入力を受け取り、画像出力を生成します。

私はこのようなモデルを手に入れました。

Core MLでは8ビットのグレースケール画像が撮影されることに注意してください。

これを機能させるには、入力をOneComponent16HalfからOneComponent8にダウンキャストし、出力をOneComponent8からOneComponent16Halfにアップキャストするコードを書く必要がありました。

しかし、これは全体の話ではありません。

このモデルはFloat16精度で計算を実行するように設定されているため、ある時点で、Core MLはこれらの8ビット入力をFloat16に変換する必要があります。

変換を効率的に行いますが、アプリを実行しているInstrumentsのトレースを見ると、これが表示されます。

Core MLがNeural Engineの計算の前後に実行しているデータステップに注目してください。

データレーンにズームインすると、Core MLがニューラルエンジンでの計算に備えてデータをコピーしていることを示しています。これは、この場合、Float16に変換することを意味します。

元のデータはすでにFloat16だったので、これは残念なようです。

理想的には、これらのデータ変換は、モデルをFloat16の入力と出力で直接動作させることで、アプリ内とCore ML内の両方で回避できます。

iOS 16とmacOS Ventura以降、Core MLは1つのOneComponent16Halfグレースケール画像とFloat16 MultiArraysをネイティブにサポートします。

coremltools変換メソッドを呼び出しながら、画像の新しいカラーレイアウトまたはMultiArraysの新しいデータタイプを指定することで、Float16の入出力を受け入れるモデルを作成できます。

この場合、モデルの入力と出力をグレースケールのFloat16画像に指定しています。

Float16のサポートはiOS 16とmacOS Venturaから利用可能であるため、これらの機能は最小展開ターゲットがiOS 16として指定されている場合にのみ利用できます。

これがモデルの再変換されたバージョンの外観です。

入力と出力はGrayscale16Halfとしてマークされていることに注意してください。

このFloat16のサポートにより、私のアプリはFloat16画像をCore MLに直接フィードできるため、入力をダウンキャストしたり、アプリ内の出力をアップキャストしたりする必要がなくなります。

これがコードでどのように見えるかです。

OneComponent16Half CVPixelBufferの形式で入力データを持っているので、ピクセルバッファをCore MLに直接送信するだけです。

これにより、データのコピーや変換は発生しません。

次に、出力としてOneComponent16Half CVPixelBufferを取得します。

これにより、コードが簡素化され、データ変換が不要になります。

あなたができるもう一つのクールなことは、Core MLに各予測に新しいバッファを割り当てるのではなく、出力のために事前に割り当てられたバッファを埋めるようにCore MLに依頼することです。

出力バッキングバッファを割り当て、予測オプションに設定することで、これを行うことができます。

私のアプリでは、OneComponent1 HalfCVPixelBufferを返すoutputBackingBufferという関数を書きました。

次に、予測オプションでこれを設定し、最後にそれらの予測オプションを使用してモデルの予測メソッドを呼び出します。

出力バッキングを指定することで、モデル出力のバッファ管理をより適切に制御できます。

したがって、これらの変更を要約すると、8ビットの入力と出力を持つモデルのオリジナルバージョンを使用するときに、Instrumentsトレースに表示されたものは次のとおりです。

そして、新しいFloat16バージョンのモデルにIOSurfaceバックアップのFloat16バッファを提供するためにコードを変更した後、最終的なInstrumentsトレースがどのように見えるかは次のとおりです。

Core MLがそれらを実行する必要がなくなったため、以前にデータレーンに表示されていたデータ変換はなくなりました。

要約すると、Core MLは現在、Float16データをエンドツーエンドのネイティブサポートを持っています。

これは、Float16入力をCore MLに提供し、Core MLにFloat16出力を返すことができることを意味します。

また、新しい出力バッキングAPIを使用して、新しい出力バッファを作成する代わりに、Core MLが事前に割り当てられた出力バッファを埋めることもできます。

そして最後に、Core MLはユニファイドメモリを利用してデータコピーなしで異なるコンピューティングユニット間でデータを転送できるため、可能な限りIOSurfaceバックアップバッファを使用することをお勧めします。

次に、Core MLに追加される追加機能のいくつかの簡単なツアーを見ていきます。

まずは重量圧縮です。

モデルの重みを圧縮すると、モデルを小さくしながら同様の精度を達成できる場合があります。

iOS 12では、Core MLは、Core MLニューラルネットワークモデルのサイズを縮小できるトレーニング後の重量圧縮を導入しました。

現在、16ビットと8ビットのサポートをMLプログラムモデルタイプに拡張し、さらに、まばらな表現で重みを保存する新しいオプションを導入しています。

coremltoolsユーティリティを使用すると、MLプログラムモデルの重みを量子化、パレット化、スパース化できるようになります。

次は新しいコンピューティングユニットオプションです。

Core MLは常に、特定のコンピューティングユニットの好みに対する推論レイテンシを最小限に抑えることを目指しています。

アプリは、MLModelConfiguration computeUnitsプロパティを設定することで、この設定を指定できます。

既存の3つのコンピューティングユニットオプションに加えて、cpuAndNeuralEngineと呼ばれる新しいものがあります。

これは、GPUで計算をディスパッチしないようにCore MLに指示します。これは、アプリが他の計算にGPUを使用する場合に役立ち、したがって、Core MLがCPUとニューラルエンジンにフォーカスを制限することを好みます。

次に、モデルのシリアル化に関して追加の柔軟性を提供するCore MLモデルインスタンスを初期化する新しい方法を追加します。

これにより、カスタム暗号化スキームでモデルデータを暗号化し、読み込み直前に復号化することができます。

これらの新しいAPIを使用すると、コンパイルされたモデルをディスクに必要とせずに、インメモリCore MLモデル仕様をコンパイルしてロードできます。

最後の更新は、Swiftパッケージと、それらがCore MLとどのように連携するかについてです。

パッケージは、再利用可能なコードをバンドルして配布するのに最適な方法です。

Xcode 14を使用すると、SwiftパッケージにCore MLモデルを含めることができ、誰かがパッケージをインポートすると、モデルが機能します。

Xcodeは、Core MLモデルを自動的にコンパイルしてバンドルし、作業に慣れているのと同じコード生成インターフェイスを作成します。

Swiftエコシステムでモデルを配布することがはるかに簡単になるため、この変更に興奮しています。

これで、このセッションは終わりです。

Xcode 14のCore MLパフォーマンスレポートとInstrumentは、アプリのML搭載機能のパフォーマンスを分析して最適化するのに役立ちます。

新しいFloat16サポートと出力バッキングAPIを使用すると、データがCore MLに出入りする方法をより詳細に制御できます。

重量圧縮の拡張サポートは、モデルのサイズを最小限に抑えるのに役立ちます。

また、インメモリモデルとSwiftパッケージのサポートにより、Core MLモデルの表現、統合、共有方法に関しては、さらに多くのオプションがあります。

これはコアMLチームのベンで、WWDCの素晴らしい残りを持っています。

♪