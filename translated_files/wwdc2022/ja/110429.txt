110429

♪インストゥルメンタルヒップホップ音楽♪

こんにちは、「iOSカメラキャプチャの進歩を発見する」へようこそ。

私はカメラソフトウェアチームのニコラス・ゲロで、iOSとiPadOSのエキサイティングな新しいカメラ機能を紹介します。

AVFoundationを使用してLiDARスキャナから深度をストリーミングする方法から始めます。

次に、顔駆動のオートフォーカスと自動露出で、アプリがどのように改善されたフェイスレンダリングを受けるかを見てみましょう。

次に、高度なAVCaptureSessionストリーミング設定を案内します。

そして最後に、あなたのアプリがマルチタスク中にカメラをどのように使用できるかをお見せします。

AVFoundationを使用してLiDARスキャナから深度をストリーミングする方法から始めます。

iPhone 12 Pro、iPhone 13 Pro、iPad Proには、高密度深度マップを出力できるLiDARスキャナーが装備されています。

LiDARスキャナーは、周囲に光を撮影し、シーンの表面から反射した光を収集することで機能します。

深さは、光がLiDARから環境に移動し、スキャナーに反射するのにかかった時間を測定することによって推定されます。

このプロセス全体は毎秒数百万回実行されます。

AVFoundationを使用して、LiDARスキャナーを実際にお見せします。

ここiPhone 13 Pro Maxでは、新しいLiDAR深度カメラAVCaptureDeviceを使用するアプリを実行しています。

このアプリは、ライブカメラフィードの上にストリーミング深度データをレンダリングします。

近い物体には青、遠くにある物体には赤が表示されます。

そして、スライダーを使って、深さの不透明度を調整できます。

このアプリは、高解像度の深度マップでも写真を撮ります。

写真を撮ると、同じ深度オーバーレイが適用されますが、静止画の解像度はさらに高くなります。

このアプリには、もう1つのトリックがあります。

トーチボタンを押すと、アプリはカラー画像付きの高解像度深度マップを使用して、RealityKitを使用してシーンにスポットライトを当てます。

タップして、シーン内のさまざまなオブジェクトにスポットライトを当てることができます。

スポットライトがギターをどのように強調しているかを見てください。

または、壁の隅にある正しい場所をタップすると、スポットライトがハートの形を形成します。

あのギターに戻りましょう。とてもかっこよく見えます。

LiDARスキャナーのAPIは、iPadOS 13.4のARKitで初めて導入されました。

WWDC 2020のプレゼンテーション「Explore ARKit 4」を見ていない場合は、ご覧になることをお勧めします。

iOS 15.4の新機能で、アプリはAVFoundationでLiDARスキャナにアクセスできます。

私たちは、ビデオと深度を提供する内蔵のLiDAR深度カメラである新しいAVCaptureデバイスタイプを導入しました。

高品質で高精度な深度情報を生成します。

この新しいAVCaptureDeviceは、背面の広角カメラを使用して、LiDARスキャナーでビデオを配信し、深度をキャプチャします。

ビデオと奥行きの両方が広角カメラの視野でキャプチャされます。

また、TrueDepth AVCaptureDeviceと同様に、すべてのフォーマットが深度データ配信をサポートしています。

この新しいAVCaptureDeviceは、LiDARスキャナーからのまばらな出力と背面の広角カメラからのカラー画像を融合させることで、高品質の深度データを生成します。

LiDARとカラー入力は、高密度深度マップを出力する機械学習モデルを使用して処理されます。

LiDAR深度カメラは後ろ向きの広角カメラを使用しているため、望遠カメラと超広角カメラはAVCaptureMultiCamSessionに加えて使用できます。

これは、複数のカメラを同時に使用したいアプリに便利です。

LiDAR深度カメラは、640×480のビデオ解像度から4032×3024のフル12メガピクセル画像まで、多くのフォーマットを公開します。

ストリーミング中は、最大320×240の深度マップを出力できます。

また、写真撮影では、768×576の深度マップを受け取ることができます。

深さの解像度は、16×9と4×3のフォーマットでわずかに異なることに注意してください。

これは、ビデオのアスペクト比と一致するためです。

LiDAR深度カメラAVCaptureDeviceは、iPhone 12 Pro、iPhone 13 Pro、iPad Pro第5世代で利用できます。

iPhone 13 Proは、背面カメラの組み合わせを使用して深度データを配信できます。

AVFoundation Capture APIは、これらを物理デバイスで構成される「仮想デバイス」と呼んでいます。

iPhone 13 Proの背面には、使用できる4つの仮想AVCaptureDeviceがあります。新しいLiDAR深度カメラは、広角カメラでLiDARスキャナーを使用します。

デュアルカメラは広角カメラと望遠カメラを使用します。

ワイドカメラとウルトラワイドカメラを使用するデュアルワイドカメラ。

そして、ワイド、超広角、望遠カメラを使用するトリプルカメラ。

これらのデバイスが生み出す深さの種類には違いがあります。

LiDAR深度カメラは「絶対深度」を生み出します。使用される飛行技術の時間により、現実世界のスケールを計算できます。

たとえば、これは測定などのコンピュータビジョンタスクに最適です。

TrueDepth、デュアル、デュアルワイド、トリプルカメラは、相対的な格差ベースの深さを生成します。

これはより少ない電力を使用し、写真の効果をレンダリングするアプリに最適です。

AVFoundationは、AVDepthDataクラスを使用して深さを表します。

このクラスには、深度データ型、精度、フィルタリングされているかどうかなど、それを記述する他のプロパティを含む深度を含むピクセルバッファがあります。

これは、新しいLiDAR深度カメラのように、深度対応のAVCaptureDeviceによって提供されます。

AVCaptureDepthDataOutputから深度をストリーミングしたり、AVCapturePhotoOutputから写真に添付された深度を受信したりできます。

深度データはデフォルトでフィルタリングされます。

フィルタリングはノイズを低減し、深度マップの欠損値または穴を埋めます。

これはビデオや写真アプリに最適なので、深度マップを使用してカラー画像に効果を適用すると、アーティファクトは表示されません。

ただし、コンピュータビジョンアプリは、深度マップの元の値を維持するために、フィルタリングされていない深度データを好むべきです。

フィルタリングが無効になっている場合、LiDAR深度カメラは低い信頼ポイントを除外します。

深度データフィルタリングを無効にするには、AVCaptureDepthDataOutputのisFilteringEnabledプロパティをfalseに設定し、デリゲートコールバックからAVDepthDataオブジェクトを受信すると、フィルタリングされません。

ARKitはすでにLiDARスキャナーへのアクセスを提供しているので、「AVFoundationはどのように比較されますか？」と尋ねるかもしれません。AVFoundationは、ビデオおよび写真アプリ用に設計されています。

AVFoundationを使用すると、LiDARスキャナでキャプチャした深度データを高解像度の写真に埋め込むことができます。

ARKitは、その名前が示すように、拡張現実アプリに最適です。

LiDARスキャナーを使用すると、ARKitはシーンジオメトリやオブジェクトの配置などの機能を提供することができます。

AVFoundationは、映画の録画や写真撮影に最適な高解像度ビデオを提供できます。

AVFoundationのLiDAR深度カメラは、最大768×576の深度を出力できます。

これは、ARKitの深度解像度256×192の2倍以上です。

ARKitは低解像度の深度マップを使用しているため、その機能に拡張現実アルゴリズムを適用できます。

AVFoundationを使用して深度データをキャプチャする方法に関するより「詳細な」情報については、WWDC 2017の以前のセッション「iPhone写真で深度をキャプチャする」をご覧ください。

アプリでLiDAR深度カメラを使用できる興味深い方法を見て興奮しています。

次に、オートフォーカスとオート露出システムの改善が、アプリのシーンでの顔の可視性を向上させるのにどのように役立つかについて説明します。

オートフォーカスとオート露出システムは、シーンを分析して最高の画像をキャプチャします。

オートフォーカスシステムは、被写体にピントを合わせるようにレンズを調整し、自動露光システムは、被写体を見えるようにシーンの最も明るい領域と暗い領域のバランスを取ります。

ただし、自動調整が行われても、被写体の顔にピントが合わないことがあります。

また、明るいバックライト付きのシーンでは、被写体の顔が見えにくい場合があります。

デジタル一眼レフやその他のプロカメラの共通の特徴は、シーン内の顔を追跡して、フォーカスと露出を動的に調整して見えるようにすることです。

iOS 15.4の新機能では、フォーカスシステムと露出システムが顔を優先します。

私たちはこの利点がとても気に入ったので、iOS 15.4以降にリンクされているすべてのアプリでデフォルトで有効にしました。

いくつかの例をお見せします。

顔駆動のオートフォーカスがなければ、カメラは顔に再び焦点を合わせることなく背景に焦点を合わせたままになります。

もう一度見てください。

彼が振り向くとき、彼の顔が焦点から外れたままで、背景の木々が鋭く残っているのを見てください。

顔駆動のオートフォーカスを有効にすると、彼の顔がはっきりと見えます。

そして、彼が背を向けると、カメラは背景に焦点を変えます。

ビデオを並べて比較すると、違いは明らかです。

顔駆動のオートフォーカスが有効になっている右側では、彼のひげの細かい詳細を見ることができます。

明るいバックライト付きのシーンでは、顔をよく露出したままにしておくのは難しいかもしれません。

しかし、顔を優先する自動露出システムで、私たちは簡単に彼を見ることができます。

並んで比較すると、ここでも違いがわかります。

右の写真で彼の顔をよく露出しておくことで、背景の木々が明るく見えることに注目してください。

そして、空もそうです。

顔を優先すると、シーン全体の露出が調整されます。

iOS 15.4では、AVCaptureDeviceには、顔駆動のオートフォーカスと自動露出が有効になっているときに制御するための新しいプロパティがあります。

デバイスがこれらの設定を「自動的に調整」するかどうかを制御し、いつ有効にするかを決定できます。

「isEnabled」プロパティを切り替える前に、まず自動調整を無効にする必要があります。

この動作の自動有効化は、写真アプリに最適です。

それはAppleのカメラアプリで使われています。

また、ビデオ会議アプリが通話中に顔を見えるようにしておくのにも最適です。

FaceTimeはこれを利用しますが、自動フォーカスと自動露出システムが顔によって駆動されるアプリには最適ではない場合があります。

たとえば、アプリがキャプチャした画像をユーザーが手動で制御できるようにする場合は、これをオフにすることを検討してください。

顔駆動のオートフォーカスまたは自動露出がアプリに適切ではないと判断した場合は、この動作をオプトアウトできます。

まず、設定のためにAVCaptureDeviceをロックします。

次に、顔駆動のオートフォーカスまたは自動露出の自動調整をオフにします。

次に、顔駆動のオートフォーカスまたは自動露出を無効にします。

そして最後に、設定のためにデバイスのロックを解除します。

高度なストリーミング構成を使用して、アプリのニーズに合わせてカスタマイズされたオーディオおよびビデオデータを受信する方法について説明します。

AVFoundation Capture APIを使用すると、開発者はカメラを使用して没入型アプリを構築できます。

AVCaptureSessionは、AVCaptureOutputsに接続されているカメラやマイクなどの入力からのデータフローを管理し、ビデオ、オーディオ、写真などを配信できます。

たとえば、一般的なカメラアプリのユースケースを考えてみましょう。録画されたビデオにフィルターやオーバーレイなどのカスタムエフェクトを適用します。

このようなアプリには、カメラとマイクの2つの入力を備えたAVCaptureSessionがあり、1つはビデオデータ用、もう1つはオーディオデータ用の2つの出力に接続されています。

その後、ビデオデータに効果が適用され、処理されたビデオはビデオプレビューと録画用のAVAssetWriterの2つの場所に送信されます。

オーディオデータはAVAssetWriterにも送信されます。

iOS 16とiPadOS 16の新機能で、アプリは複数のAVCaptureVideoDataOutputsを同時に使用できます。

ビデオデータ出力ごとに、解像度、安定化、向き、ピクセルフォーマットをカスタマイズできます。

カメラアプリの例に戻りましょう。

このアプリがバランスをとっている競合するキャプチャ要件があります。

このアプリは、キャプチャされたコンテンツのライブビデオプレビューを表示し、後で再生するために高品質のビデオを録画したいと考えています。

プレビューのために、解像度はデバイスの画面に十分な大きさである必要があります。

そして、処理は低遅延プレビューのために十分に高速である必要があります。

しかし、録画するときは、高品質のエフェクトを適用して高解像度でキャプチャするのが最善です。

2番目のAVCaptureVideoDataOutputを追加する機能により、キャプチャグラフを拡張できます。

これで、ビデオデータ出力を最適化できます。

1つの出力はプレビュー用に小さなバッファを提供でき、もう1つは録画用にフルサイズの4Kバッファを提供できます。

また、このアプリは、より小さなプレビューバッファに、よりシンプルでパフォーマンスの高いバージョンのエフェクトをレンダリングし、録画時にフルサイズのバッファ用に高品質のエフェクトを予約できます。

これで、アプリはプレビューや録画したビデオを危険にさらす必要がなくなりました。

プレビューと録画に別々のビデオデータ出力を使用するもう1つの理由は、異なる安定化モードを適用することです。

ビデオ安定化は、ビデオキャプチャパイプラインに追加のレイテンシをもたらします。

プレビューでは、顕著な遅延によりコンテンツのキャプチャが困難になるため、レイテンシは望ましくありません。

録画のために、後でビデオを見るときにより良い経験のために安定化を適用することができます。

したがって、低遅延プレビューのために1つのビデオデータ出力に安定化を適用せず、後で再生するためにもう1つに安定化を適用することができます。

ビデオデータ出力の解像度を設定する方法はたくさんあります。

フルサイズの出力の場合、まず、出力バッファ寸法の自動設定を無効にします。

次に、プレビューサイズの出力バッファの配信を無効にします。

ただし、ほとんどの場合、ビデオデータ出力はすでにフルサイズの出力用に設定されています。

プレビューサイズの出力の場合、再び自動設定を無効にしますが、代わりにプレビューサイズの出力バッファの配信を有効にします。

これは、写真AVCaptureSessionPresetを使用する場合、デフォルトで有効になります。

カスタム解像度を要求するには、出力のビデオ設定辞書で幅と高さを指定します。

幅と高さのアスペクト比は、ソースデバイスのactiveFormatのアスペクト比と一致する必要があります。

ビデオデータ出力を設定する方法は他にもあります。

安定化を適用するには、好みの安定化をシネマティック拡張のようなモードに設定し、見るのに最適なビデオを生成します。

縦向きのバッファを受信するように方向を変更できます。

また、10ビットのロスレスYUVバッファを受信するピクセル形式を指定できます。

AVCaptureVideoDataOutputのピクセル形式の選択の詳細については、Technote 3121を参照してください。

iOS 16とiPadOS 16以降、複数のビデオデータ出力を使用することに加えて、アプリはAVCaptureVideoDataOutputとAVCaptureAudioDataOutputからデータを受信しながら、AVCaptureMovieFileOutputで録画できます。

セッションに何を追加できるかを判断するには、出力を追加できるかどうかを確認し、セッションのハードウェアコストプロパティを照会して、システムが設定をサポートできるかどうかを判断できます。

ムービーファイル出力でビデオデータを受信することで、録画中にビデオを検査し、シーンを分析することができます。

また、ムービーファイルの出力でオーディオデータを受信すると、録音中にオーディオをサンプリングし、録音されているものを聴くことができます。

このようなキャプチャグラフを使用すると、非圧縮ビデオとオーディオのサンプルを受信しながら、AVCaptureMovieFileOutputに録音の仕組みをオフロードできます。

これらの高度なストリーミング構成を実装するには、新しいAPIを使用する必要がありません。

既存のAPIでより多くのことをできるようにすることで、これを可能にしました。

そして最後に、ユーザーがマルチタスクをしている間、あなたのアプリがどのようにカメラを使用できるかについて話し合います。

iPadでは、ユーザーはさまざまな方法でマルチタスクを実行できます。

たとえば、Split ViewまたはSlide Overでメモを読みながらボイスメモを録音し、Safariの上のフローティングウィンドウでフルスクリーンでメモを書きます。

ピクチャー・イン・ピクチャーを使用すると、より多くのWWDCビデオを見るためのリマインダーを追加しながら、ビデオの再生を続けることができます。

また、iPadOS 16の新しいStage Managerを使用すると、ユーザーはサイズ変更可能なフローティングウィンドウで複数のアプリを開くことができます。

iOS 16以降、AVCaptureSessionsはマルチタスク中にカメラを使用できるようになります。

マルチタスク中にカメラシステムが提供できるサービスの質が懸念されるため、以前はマルチタスク中にカメラへのアクセスを阻止しました。

カメラを使用してアプリと一緒に実行されるゲームのようなリソースを大量に消費するアプリは、フレームドロップやその他のレイテンシを誘発し、カメラのフィードが悪くなる可能性があります。

数ヶ月または数年後に品質の悪いビデオを見ているユーザーは、マルチタスク中に録画したことを覚えていないかもしれません。

良いカメラ体験を提供することが私たちの優先事項です。

システムがマルチタスク中にカメラから録画されたビデオを検出すると、低品質のビデオの可能性についてユーザーに知らせるダイアログが表示されます。

このダイアログは、AVCaptureMovieFileOutputまたはAVAssetWriterで録画が終了した後に表示されます。

すべてのアプリのシステムによって一度だけ表示され、閉じるためのOKボタンがあります。

AVCaptureSessionには、マルチタスクカメラアクセスがサポートされ、有効になっていることを示す2つの新しいプロパティが追加されています。

これを有効にしたキャプチャセッションは、「ビデオデバイスが複数のフォアグラウンドアプリで利用できない」という理由で中断されなくなります。一部のアプリでは、カメラを使用するためにフルスクリーン体験が必要な場合があります。

これは、アプリがシステムリソースのために他のフォアグラウンドアプリと競合しないようにしたい場合に便利です。

たとえば、ARKitはマルチタスク中のカメラの使用をサポートしていません。

他のアプリと一緒に実行するときは、アプリがうまく機能することを確認する必要があります。

通知を監視することで、アプリをシステムプレッシャーの増加に回復力のあるものにし、フレームレートを下げるなど、影響を軽減するための措置を講じます。

低解像度、ビニング、または非HDR形式を要求することで、システム上のアプリのフットプリントを減らすことができます。

パフォーマンスを維持するためのベストプラクティスの詳細については、記事「マルチタスク中にカメラにアクセスする」をお読みください。

また、ビデオ通話やビデオ会議アプリは、システムが提供するピクチャ・イン・ピクチャウィンドウにリモート参加者を表示できます。

これで、アプリのユーザーはiPadでマルチタスクをしながらシームレスにビデオ通話を続けることができます。

AVKitは、アプリがリモートコール参加者を表示するためのビューコントローラーを指定するためのAPIをiOS 15に導入しました。

ビデオ通話ビューコントローラーを使用すると、ウィンドウの内容をカスタマイズできます。

養子縁組の詳細については、「ビデオ通話のためのピクチャー・イン・ピクチャーの採用」の記事を参照してください。

そして、これでiOSカメラキャプチャの進歩が終わります。

AVFoundationを使用してLiDARスキャナーから深度をストリーミングする方法、アプリが改善されたフェイスレンダリングを受け取る方法、アプリに合わせた高度なAVCaptureSessionストリーミング構成、そして最後に、アプリがマルチタスク中にカメラを使用する方法を示しました。

あなたのWWDCがうまくいくことを願っています。

♪ ♪