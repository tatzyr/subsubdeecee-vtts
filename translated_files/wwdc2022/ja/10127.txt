10127

♪(まろやかなインストゥルメンタルヒップホップ音楽) ♪

♪

プラヴィーン・シャルマ:こんにちは。私の名前はプラヴィーンで、ここアップルのプロトタイピングチームの出身です。

カイ・カン:こんにちは。私の名前はカイで、ビデオエンジニアリングチームの出身です。

Praveen: 過去数年間、Appleは人々が自分のアプリに世界を持ち込むための強力な新しい方法を可能にしました。

昨年、現実世界のオブジェクトの写真を撮るObject Captureを導入し、RealityKitのPhotogrammetry APIを使用して、アプリですぐに使用できる3Dモデルに変えました。

オブジェクトキャプチャの前に、スペースの幾何学的構造を大まかに理解し、アプリで新しい拡張現実のユースケースを可能にするScene Reconstruction APIをリリースしました。

今年は、RoomPlanと呼ばれるまったく新しいフレームワークを発表できることを非常に嬉しく思います。

RoomPlanでは、LiDAR対応のiPhoneまたはiPadを使用して部屋をスキャンできます。

アプリで使用できる部屋とその部屋を定義するオブジェクトのパラメトリック3Dモデルを生成します。

RoomPlanのスキャン体験がどのようなものか見てみましょう。

RoomPlanは、ARKitを搭載した洗練された機械学習アルゴリズムを使用して、壁、窓、開口部、ドア、および暖炉、ソファ、テーブル、キャビネットなどの部屋を定義するオブジェクトを検出します。

RealityKitを使用してスキャンの進行状況をリアルタイムでレンダリングするRoomCaptureView APIを使用すると、スキャンエクスペリエンスをアプリに簡単に統合できます。

また、スキャンが完了すると、RoomCaptureViewは最終的な後処理結果を表示しますが、ユースケースに最も適しています。

初めて、機械学習とコンピュータビジョンアルゴリズムを実装する複雑さなしに、人々はまったく新しい方法で自分の部屋と対話できるようになりました。

たとえば、インテリアデザインアプリは、壁の色の変化をプレビューし、部屋の再塗装に必要な塗料の量を正確に計算することができます。

建築アプリにより、部屋のレイアウトの変更をリアルタイムで簡単にプレビューおよび編集できるようになりました。

不動産アプリは、エージェントがリストのフロアプランと3Dモデルをシームレスにキャプチャできるようになりました。

また、eコマースアプリは、物理的な空間での製品の視覚化を通じて顧客を引き付けることができます。

これらは、RoomPlanが有効にするアプリケーションのほんの一例であり、RoomPlanをアプリに統合する方がいかに簡単かに驚かれることでしょう。

見てみましょう。 

RoomPlanを使用するには、主に2つの方法があります。

1つ目は、RoomPlanをアプリにシームレスに統合できる、すぐに使えるスキャン体験です。

2つ目は、アプリがスキャンからのライブパラメトリックデータを使用できるようにするデータAPIですが、ユースケースに最も適しています。

これらのAPIの両方で、可能な限り最高のスキャン結果を達成するのに役立ついくつかのベストプラクティスをお勧めします。このプレゼンテーションの最後のセクションで説明します。

まず、新しいRoomCaptureView APIを使用してアプリに持ち込むことができるスキャン体験について話しましょう。

RoomCaptureViewは、アプリに簡単に配置できるUIViewサブクラスです。

ワールドスペーススキャンフィードバック、リアルタイムルームモデル生成、コーチングとユーザーガイダンスのプレゼンテーションを処理します。

RoomCaptureViewベースのスキャン中に提示されたデザイン要素を詳しく見てみましょう。

アクティブなRoomCaptureViewセッション中に、アニメーションラインアウトラインは、壁、窓、開口部、ドア、および部屋を定義するオブジェクトをリアルタイムで検出しました。

RoomCaptureViewの下部でリアルタイムで生成されたインタラクティブな3Dモデルは、スキャンの進捗状況の概要を一目で確認できます。

最後に、テキストコーチングは、可能な限り最高のスキャン結果にあなたを導きます。

4つの簡単なステップでRoomCaptureViewを使い始める方法を見てみましょう。

まず、ViewControllerでRoomCaptureViewリファレンスを作成します。

次に、RoomCaptureSession設定オブジェクトへの参照を作成します。

第三に、スキャンセッションを開始し、設定をキャプチャセッションの実行機能に渡します。

そして最後に、私たちのアプリケーションはキャプチャセッションにスキャンを停止するように指示します。

オプションで、アプリはRoomCaptureViewDelegateプロトコルに準拠し、後処理された結果とそのプレゼンテーションをオプトアウトしたり、提示された後処理されたスキャン結果を処理したりできます。

たとえば、提供されたCapturedRoomデータ構造体で利用可能なエクスポート関数を呼び出すことで、結果のUSDZをエクスポートできます。

そして、RoomPlanをアプリに統合するのは簡単です。

私たちは、あなたがこのAPIで何を作るかを見てとても興奮しています。

今、私の同僚のカイは、RoomCaptureSessionとRoomPlanのデータAPIについて話します。

カイ:ありがとう、プラヴィーン。

このセクションでは、スキャン中の基礎となるデータ構造へのアクセスを提供し、スキャンエクスペリエンスのカスタム視覚化をゼロから構築するのに役立つデータAPIについて説明します。

基本的なワークフローは、スキャン、プロセス、エクスポートの3つの部分で構成されています。

スキャンについては、キャプチャセッションの設定と開始方法の基本、およびキャプチャプロセスの表示と監視について説明します。

次に、スキャンしたデータがどのように処理され、最終モデルが提示のために受信されるかを見ていきます。

最後に、USDワークフローでも使用できる出力USDファイルを生成およびエクスポートする方法について説明します。

では、スキャンのステップを詳しく見てみましょう。

RoomCaptureSession APIを使用してセッションを設定し、スキャンを続ける際に進捗状況を表示します。

コードでお見せしましょう。

これは例としてシンプルなRealityKitアプリです。

まず、RoomPlanをSwiftプロジェクトにインポートするだけです。

アプリのViewControllerでは、結果を視覚化し、RoomCaptureSessionインスタンスを開始するためのカスタムタイプを持つことができます。

さらに、RoomCaptureSessionは、アプリがARビューで平面とオブジェクトバウンディングボックスを描画できるように、基礎となるARセッションにハンドルを提供します。

RoomCaptureSessionはデリゲートパターンを採用しています。

ViewControllerクラスでは、ViewController自体をcaptureSessionのデリゲートとして割り当てることができます。

これにより、ViewControllerはRoomCaptureSessionからリアルタイムの更新を取得できます。

これらの更新には、キャプチャ中に人々を導くための3Dモデルと指示が含まれています。

これらの更新を取得するには、ViewControllerはRoomCaptureSessionDelegateプロトコルに準拠し、2つのメソッドを実装する必要があります。

1つ目は、リアルタイムのCapturedRoomデータ構造を取得するためのcaptureSession(_ session: didUpdate room:)メソッドです。

ビジュアライザーを使用して、3DモデルのARビューを更新し、進捗状況に関するリアルタイムのフィードバックを人々に提供できます。

トークの後半で、CapturedRoomの構造について詳しく説明します。

このメソッドは、キャプチャされた部屋の更新を検出したときに呼び出されます。

2番目の方法は captureSession(_ session: didProvide instruction:)です。

この方法は、リアルタイムのフィードバックを含む命令構造を提供します。

あなたのビジュアライザーは、スキャン中に人々を導くために指示を使用することができます。

このAPIが提供する手順を見てみましょう。 では、このAPI で説明します。

これらの指示には、オブジェクトまでの距離、スキャン速度、部屋への照明調整、およびより多くのテクスチャを持つ部屋の特定の領域に焦点を当てることが含まれます。

これらの指示は、リアルタイムのフィードバックで人々を導くために、スキャン中に提供されます。

次に、プロセス部分に進みます。

このセクションでは、RoomBuilderクラスを使用してスキャンデータを処理し、最終的な3Dモデルを生成します。

キャプチャされたデータを処理するには、最初のステップは、ViewControllerクラスでRoomBuilderインスタンスを開始することです。

次に、キャプチャプロセス後にセンサーデータを受信するために、アプリはcaptureSession(_ session: didEndWith data: error:)メソッドを実装する必要があります。

RoomCaptureSessionが停止すると、アプリでstop()関数を呼び出すか、エラーが発生すると、この関数はCaptureRoomDataオブジェクトとオプションのエラーを返すために呼び出されます。

最後に、キャプチャされたデータを処理するために、awaitキーワードでroomBuilderのasync roomModel(from:)メソッドを呼び出します。

このメソッドは非同期に実行され、スキャンされたデータを処理し、最終的な3Dモデルを構築します。

昨年のWWDCで導入したSwift async/await機能を利用しています。

ほんの数秒以内に、モデルはアプリの最終プレゼンテーションで利用可能になります。

それでは、CapturedRoomデータ構造の詳細と、それをエクスポートしてアプリで使用する方法を掘り下げてみましょう。

トップレベルには、サーフェスとオブジェクトで構成されるCapturedRoomがあります。

表面には、半径などの曲線を表すユニークな属性が含まれています。開始角度と終了角度。表面の4つの異なるエッジ。壁、開口部、窓、ドアの建築カテゴリ。

オブジェクトには、テーブル、ベッド、ソファなどの家具カテゴリが含まれています。

サーフェスとオブジェクトは、ディメンション、スキャンされたサーフェスまたはオブジェクトの3つのレベルの信頼性、3D変換マトリックス、一意の識別子など、いくつかの共通の属性を共有しています。

それらがコードでどのように表現されているか見てみましょう。

CapturedRoom構造は、部屋内の要素の完全にパラメトリックな表現です。

壁、開口部、ドア、窓、部屋のオブジェクトを含む5つのプロパティが含まれています。

最初の4つの要素については、2D平面建築構造を表す表面構造として表されます。

右側には、先ほど説明したSurfaceのさまざまな特性を見ることができます。

最後のプロパティは、部屋内の3Dオブジェクトの配列であり、それらは立方体として表されます。

右側には、オブジェクトのさまざまなプロパティが表示されます。

以下は、RoomPlanでサポートするオブジェクトタイプのリストです。

これらには、ソファ、テーブル、椅子、ベッドなど、さまざまな一般的な家具タイプが含まれます。

最後に、エクスポート機能を使用すると、このCapturedRoomを既存のワークフローのUSDまたはUSDZデータにエクスポートできます。

以下は、Cinema 4DでUSD出力を直接開いて、部屋の階層データ構造、および各部屋の要素またはオブジェクトの寸法と場所を閲覧および編集する方法を示す例です。

また、既存のUSDおよびUSDZワークフローを活用して、キャプチャされた部屋のレンダリングを不動産、電子商取引、ユーティリティ、インテリアデザインなどのさまざまなアプリケーションに追加することもできます。

これまでのところ、スキャン体験と基礎となるRoomPlan APIを取り上げました。

私たちは今、あなたがRoomPlanで良い結果を得るのを助けるためにいくつかのベストプラクティスを見ていきます。

私たちは、良いスキャンを可能にする推奨条件、部屋を選択する際に注意すべき部屋の機能、および心に留めておくべきいくつかのスキャンと熱的考慮事項をカバーします。

RoomPlan APIは、典型的な家庭で最も一般的な建築構造とオブジェクトをサポートしています。

最大部屋サイズが30フィート×30フィートまたは約9×9メートルのシングル住宅の部屋に最適です。

照明は、APIが鮮明なビデオストリームと優れたARトラッキングパフォーマンスを得るためにも重要です。

夜間の家族のリビングルームに典型的なAPIを使用するには、最低50ルクス以上をお勧めします。

ハードウェアの場合、RoomPlan APIはすべてのLiDAR対応iPhoneおよびiPad Proモデルでサポートされています。

APIに課題をもたらす可能性のある特別な条件がいくつかあります。

たとえば、フルハイトミラーとガラスは、LiDARセンサーが期待される出力を生成するための課題となります。

高い天井でも、LiDARセンサーのスキャン範囲の制限を超える可能性があります。

また、非常に暗い表面は、デバイスがスキャンするのが難しいかもしれません。

より良いスキャン結果を得るためのいくつかの考慮事項があります。

まず、高精度の要件を持つアプリケーションでは、スキャンする前に部屋を準備することで、スキャンの品質を向上させることができます。

たとえば、カーテンを開けると、より多くの自然光が差し込み、窓の閉塞を減らすことができ、昼間のスキャンに最適です。

ドアを閉じると、部屋の外の不要な領域をスキャンする可能性を減らすことができます。

優れたスキャンモーションに従うことは、APIで優れたスキャン結果を達成するためにも非常に重要です。

そのため、スキャン中にテクスチャ、距離、速度、照明条件に関するフィードバックを提供するために、ユーザー命令の委任方法を提供しています。

もう一つ心に留めておくべきことは、デバイスのバッテリーとサーマルです。

優れたスキャン体験を確保するために、RoomPlan APIで多くの最適化を行ってきました。

それにもかかわらず、5分以上の繰り返しスキャンや単一の長いスキャンを避けるのが最善です。

これらは疲労を引き起こすだけでなく、バッテリーを消耗させ、熱の問題を引き起こし、アプリのユーザーエクスペリエンスに影響を与える可能性があります。

今日取り上げたことがたくさんあります。

私たちは真新しいAPI、RoomPlanを導入しました。

それはあなたの部屋をキャプチャするための直感的なスキャン体験、環境を理解するための強力な機械学習モデル、およびアプリに簡単に統合するための完全にパラメトリックなUSD出力フォーマットを提供します。

新しいRoomPlanエクスペリエンスをより適切に設計および実装する方法に関するガイダンスについては、以下の関連講演をご覧ください。

Praveen: アプリでRoomPlanを試す時が来ました。

この新しいAPIで何を作成できるかを見るのが待ちきれません。

カイ：見てくれてありがとう！

♪