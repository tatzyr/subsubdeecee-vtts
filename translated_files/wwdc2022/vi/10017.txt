10017

♪ ♪

Ciao, tên tôi là Geppy Parziale, và tôi là một kỹ sư máy học ở đây tại Apple.

Hôm nay, tôi muốn hướng dẫn bạn hành trình xây dựng một ứng dụng sử dụng máy học để giải quyết một vấn đề thường yêu cầu một chuyên gia thực hiện một số công việc rất chuyên biệt.

Hành trình này cho tôi cơ hội chỉ cho bạn cách thêm các mô hình học máy mã nguồn mở vào ứng dụng của bạn và tạo ra những trải nghiệm mới tuyệt vời.

Trong suốt hành trình, tôi cũng sẽ nêu bật một vài trong số rất nhiều công cụ, khuôn khổ và API có sẵn cho bạn trong hệ sinh thái phát triển của Apple để xây dựng các ứng dụng bằng cách sử dụng máy học.

Khi xây dựng một ứng dụng, bạn, nhà phát triển, thực hiện một loạt các quyết định hy vọng sẽ mang lại trải nghiệm tốt nhất cho người dùng của bạn.

Và điều này cũng đúng khi thêm chức năng học máy vào các ứng dụng.

Trong quá trình phát triển, bạn có thể hỏi: tôi có nên sử dụng máy học để xây dựng tính năng này không?

Làm thế nào tôi có thể có được một mô hình học máy?

Làm cách nào để làm cho mô hình đó tương thích với các nền tảng của Apple?

Mô hình đó có phù hợp với trường hợp sử dụng cụ thể của tôi không?

Nó có chạy trên Apple Neural Engine không?

Vậy chúng ta hãy cùng nhau thực hiện cuộc hành trình này.

Tôi muốn xây dựng một ứng dụng cho phép tôi thêm màu sắc chân thực vào những bức ảnh đen trắng của gia đình mà tôi tìm thấy trong một chiếc hộp cũ ở tầng hầm của mình.

Tất nhiên, một nhiếp ảnh gia chuyên nghiệp có thể làm điều này với một số công việc thủ công, dành thời gian trong công cụ chỉnh sửa ảnh.

Thay vào đó, điều gì sẽ xảy ra nếu tôi muốn tự động hóa quy trình này và áp dụng màu sắc chỉ trong vài giây?

Đây dường như là một nhiệm vụ hoàn hảo cho việc học máy.

Apple cung cấp một lượng lớn các khuôn khổ và công cụ có thể giúp bạn xây dựng và tích hợp chức năng ML trong các ứng dụng của mình.

Họ cung cấp mọi thứ từ xử lý dữ liệu đến đào tạo và suy luận mô hình.

Đối với hành trình này, tôi sẽ sử dụng một vài trong số chúng.

Nhưng hãy nhớ rằng, bạn có nhiều lựa chọn tùy thuộc vào nhiệm vụ học máy cụ thể mà bạn đang phát triển.

Quá trình tôi sử dụng khi phát triển tính năng học máy trong các ứng dụng của mình trải qua một loạt các giai đoạn.

Đầu tiên, tôi tìm kiếm mô hình học máy phù hợp trong ấn phẩm khoa học hoặc trang web chuyên ngành.

Tôi đã tìm kiếm màu ảnh và tôi đã tìm thấy một mô hình có tên Colorizer có thể phù hợp với những gì tôi cần.

Đây là một ví dụ về sự tô màu mà tôi có thể nhận được bằng cách sử dụng mô hình này.

Đây là một cái khác.

Và đây là một cái khác. Thực sự tuyệt vời.

Để tôi chỉ cho bạn cách nó hoạt động.

Mô hình Colorizer mong đợi một hình ảnh đen trắng làm đầu vào.

Mã nguồn Python mà tôi tìm thấy chuyển đổi bất kỳ hình ảnh RGB nào thành hình ảnh không gian màu LAB.

Không gian màu này có 3 kênh: một kênh đại diện cho độ sáng của hình ảnh hoặc kênh L, và hai kênh còn lại đại diện cho các thành phần màu.

Các thành phần màu sắc bị loại bỏ trong khi độ nhẹ trở thành đầu vào của mô hình tạo màu.

Mô hình sau đó ước tính hai thành phần màu mới, kết hợp với kênh L đầu vào, cung cấp màu sắc cho hình ảnh kết quả.

Đã đến lúc làm cho mô hình này tương thích với ứng dụng của tôi.

Để đạt được điều này, tôi có thể chuyển đổi mô hình PyTorch gốc sang định dạng Core ML bằng cách sử dụng coremltools.

Đây là tập lệnh Python đơn giản mà tôi đã sử dụng để chuyển đổi mô hình PyTorch sang Core ML.

Đầu tiên, tôi nhập kiến trúc và trọng lượng mô hình PyTorch.

Sau đó tôi theo dõi mô hình đã nhập.

Cuối cùng, tôi chuyển đổi mô hình PyTorch sang Core ML và lưu nó.

Khi mô hình ở định dạng Core ML, tôi cần xác minh rằng quá trình chuyển đổi hoạt động chính xác.

Tôi có thể làm điều đó trực tiếp trong Python một lần nữa bằng cách sử dụng coremltools.

Và điều này thật dễ dàng.

Tôi nhập hình ảnh vào không gian màu RGB và chuyển đổi nó thành không gian màu Lab.

Sau đó, tôi tách độ nhẹ ra khỏi các kênh màu và loại bỏ chúng.

Tôi chạy dự đoán bằng cách sử dụng mô hình Core ML.

Và cuối cùng, sáng tác độ sáng đầu vào với các thành phần màu ước tính và chuyển đổi sang RGB.

Điều này cho phép tôi xác minh rằng chức năng của mô hình được chuyển đổi phù hợp với chức năng của mô hình PyTorch ban đầu.

Tôi gọi giai đoạn này là Xác minh Mô hình.

Tuy nhiên, có một kiểm tra quan trọng khác cần được thực hiện.

Tôi cần hiểu liệu mô hình này có thể chạy đủ nhanh trên thiết bị mục tiêu của tôi hay không.

Vì vậy, tôi sẽ cần đánh giá mô hình trên thiết bị và đảm bảo rằng nó sẽ cung cấp trải nghiệm người dùng tốt nhất.

Báo cáo Hiệu suất Core ML mới, hiện có sẵn trong Xcode 14, thực hiện phân tích dựa trên thời gian của mô hình Core ML.

Tôi chỉ cần kéo và thả mô hình vào Xcode và tạo báo cáo hiệu suất trong vài giây.

Sử dụng công cụ này, tôi có thể thấy rằng thời gian dự đoán ước tính là gần 90 mili giây trên iPad Pro với M1 và iPadOS 16.

Và điều này là hoàn hảo cho ứng dụng tô màu ảnh của tôi.

Nếu bạn muốn biết thêm về Báo cáo Hiệu suất trong Xcode, tôi khuyên bạn nên xem phiên năm nay "Tối ưu hóa việc sử dụng Core ML của bạn".

Vì vậy, Báo cáo Hiệu suất có thể giúp bạn đánh giá mô hình và đảm bảo nó cung cấp trải nghiệm người dùng trên thiết bị tốt nhất.

Bây giờ tôi chắc chắn về chức năng và hiệu suất của mô hình của mình, hãy để tôi tích hợp nó vào ứng dụng của mình.

Quá trình tích hợp giống hệt với những gì tôi đã làm cho đến bây giờ trong Python, nhưng lần này tôi có thể làm điều đó liền mạch trong Swift bằng Xcode và tất cả các công cụ khác mà bạn quen thuộc.

Hãy nhớ rằng mô hình, bây giờ ở định dạng Core ML, mong đợi một hình ảnh kênh duy nhất đại diện cho sự nhẹ nhàng của nó.

Vì vậy, tương tự như những gì tôi đã làm trước đây trong Python, tôi cần chuyển đổi bất kỳ hình ảnh đầu vào RGB nào thành hình ảnh bằng cách sử dụng không gian màu Lab.

Tôi có thể viết sự biến đổi này theo nhiều cách: trực tiếp bằng Swift với vImage hoặc sử dụng Metal.

Khám phá sâu hơn trong tài liệu, tôi thấy rằng khung Core Image cung cấp thứ gì đó có thể giúp tôi điều này.

Vì vậy, hãy để tôi chỉ cho bạn cách đạt được chuyển đổi RGB sang LAB và chạy dự đoán bằng cách sử dụng mô hình Core ML.

Đây là mã Swift để trích xuất độ nhẹ từ hình ảnh RGB và chuyển nó đến mô hình Core ML.

Đầu tiên, tôi chuyển đổi hình ảnh RGB thành LAB và trích xuất độ nhẹ.

Sau đó, tôi chuyển đổi độ nhẹ thành CGImage và chuẩn bị đầu vào cho mô hình Core ML.

Cuối cùng, tôi chạy dự đoán.

Để trích xuất kênh L từ hình ảnh RGB đầu vào, trước tiên tôi chuyển đổi hình ảnh RGB thành hình ảnh LAB, sử dụng CIFilter convertRGBtoLab mới.

Các giá trị của độ nhẹ được đặt trong khoảng từ 0 đến 100.

Sau đó, tôi nhân hình ảnh Phòng thí nghiệm với ma trận màu và loại bỏ các kênh màu và trả lại độ sáng cho người gọi.

Bây giờ chúng ta hãy phân tích những gì xảy ra ở đầu ra của mô hình.

Mô hình Core ML trả về hai MLShapedArrays chứa các thành phần màu ước tính.

Vì vậy, sau khi dự đoán, tôi chuyển đổi hai MLShapedArray thành hai CIImages.

Cuối cùng, tôi kết hợp chúng với độ nhẹ đầu vào của mô hình.

Điều này tạo ra một hình ảnh LAB mới mà tôi chuyển đổi sang RGB và trả lại nó.

Để chuyển đổi hai MLShapedArrays thành hai CIImages, trước tiên tôi trích xuất các giá trị từ mỗi mảng có hình dạng.

Sau đó, tôi tạo ra hai hình ảnh cốt lõi đại diện cho hai kênh màu và trả lại chúng.

Để kết hợp độ nhẹ với các kênh màu ước tính, tôi sử dụng CIKernel tùy chỉnh lấy ba kênh làm đầu vào và trả về CIImage.

Sau đó, tôi sử dụng CIFilter convertLabToRGB mới để chuyển đổi hình ảnh Lab sang RGB và trả lại cho người gọi.

Đây là mã nguồn cho CIKernel tùy chỉnh mà tôi sử dụng để kết hợp độ nhẹ với hai kênh màu ước tính trong một CIImage duy nhất.

Để biết thêm thông tin về các bộ lọc CI mới để chuyển đổi hình ảnh RGB sang hình ảnh LAB và ngược lại, vui lòng tham khảo phiên "Hiển thị nội dung EDR với Core Image, Metal và SwiftUI."

Bây giờ tôi đã hoàn thành việc tích hợp tính năng ML này trong ứng dụng của mình, hãy xem nó hoạt động.

Nhưng chờ đã.

Tôi sẽ tô màu những bức ảnh gia đình cũ của mình trong thời gian thực như thế nào với ứng dụng của mình?

Tôi có thể dành chút thời gian để số hóa từng cái và nhập chúng vào ứng dụng của mình.

Tôi nghĩ tôi có một ý tưởng hay hơn.

Điều gì sẽ xảy ra nếu tôi sử dụng máy ảnh iPad của mình để quét những bức ảnh này và tô màu trực tiếp chúng?

Tôi nghĩ nó sẽ rất vui, và tôi có mọi thứ tôi cần để hoàn thành việc này.

Nhưng trước tiên, tôi phải giải quyết một vấn đề.

Mô hình của tôi cần 90 mili giây để xử lý một hình ảnh.

Nếu tôi muốn xử lý một video, tôi sẽ cần thứ gì đó nhanh hơn.

Để có trải nghiệm người dùng mượt mà, tôi muốn chạy camera của thiết bị ít nhất 30 khung hình / giây.

Điều đó có nghĩa là máy ảnh sẽ tạo ra một khung hình khoảng 30 mili giây một lần.

Nhưng vì mô hình cần khoảng 90 mili giây để tô màu khung hình video, tôi sẽ mất 2 hoặc 3 khung hình trong mỗi lần tô màu.

Tổng thời gian dự đoán của một mô hình là một hàm của cả kiến trúc của nó cũng như các hoạt động của các đơn vị tính toán mà nó được ánh xạ đến.

Nhìn vào báo cáo hiệu suất một lần nữa, tôi nhận thấy rằng mô hình của tôi có tổng cộng 61 thao tác chạy trên sự kết hợp giữa công cụ thần kinh và CPU.

Nếu tôi muốn thời gian dự đoán nhanh hơn, tôi sẽ phải thay đổi mô hình.

Tôi quyết định thử nghiệm kiến trúc của mô hình và khám phá một số lựa chọn thay thế có thể nhanh hơn.

Tuy nhiên, một sự thay đổi trong kiến trúc có nghĩa là tôi cần phải đào tạo lại mạng.

Apple cung cấp các giải pháp khác nhau cho phép tôi đào tạo các mô hình học máy trực tiếp trên máy Mac của mình.

Trong trường hợp của tôi, vì mô hình ban đầu được phát triển trong PyTorch, tôi đã quyết định sử dụng PyTorch mới trên Metal, vì vậy tôi có thể tận dụng khả năng tăng tốc phần cứng to lớn do Apple Silicon cung cấp.

Nếu bạn muốn biết thêm về PyTorch tăng tốc với Metal, vui lòng kiểm tra phiên, "Tăng tốc học máy với Metal" Do sự thay đổi này, hành trình của chúng ta cần lùi lại một bước.

Sau khi đào tạo lại, tôi sẽ phải chuyển đổi kết quả sang định dạng Core ML và chạy lại xác minh.

Lần này, tích hợp mô hình bao gồm việc hoán đổi mô hình cũ với mô hình mới.

Sau khi đào tạo lại một vài mô hình thay thế ứng cử viên, tôi đã xác minh một mô hình sẽ đáp ứng yêu cầu của tôi.

Đây là báo cáo hiệu suất tương ứng.

Nó chạy hoàn toàn trên công cụ thần kinh và thời gian dự đoán hiện khoảng 16 mili giây, hoạt động cho video.

Nhưng Báo cáo Hiệu suất chỉ cho tôi biết một khía cạnh trong hiệu suất của ứng dụng của tôi.

Thật vậy, sau khi chạy ứng dụng của mình, tôi nhận thấy ngay rằng việc tô màu không mượt mà như tôi mong đợi.

Vậy điều gì xảy ra trong ứng dụng của tôi trong thời gian chạy?

Để hiểu điều đó, tôi có thể sử dụng mẫu Core ML mới trong Instruments.

Phân tích phần ban đầu của dấu vết Core ML, sau khi tải mô hình, tôi nhận thấy rằng ứng dụng tích lũy các dự đoán.

Và điều này thật bất ngờ.

Thay vào đó, tôi mong đợi có một dự đoán duy nhất cho mỗi khung hình.

Phóng to dấu vết và kiểm tra các dự đoán đầu tiên, tôi quan sát thấy rằng ứng dụng yêu cầu dự đoán Core ML thứ hai trước khi dự đoán đầu tiên kết thúc.

Ở đây, Neural Engine vẫn đang làm việc trên yêu cầu đầu tiên khi yêu cầu thứ hai được trao cho Core ML.

Tương tự, dự đoán thứ ba bắt đầu trong khi vẫn đang xử lý dự đoán thứ hai.

Ngay cả sau bốn dự đoán, độ trễ giữa yêu cầu và thực thi đã là khoảng 20 mili giây.

Thay vào đó, tôi cần đảm bảo rằng một dự đoán mới chỉ bắt đầu nếu dự đoán trước đó kết thúc để tránh xếp tầng những độ trễ này.

Trong khi khắc phục sự cố này, tôi cũng phát hiện ra rằng tôi đã vô tình đặt tốc độ khung hình máy ảnh thành 60 khung hình / giây thay vì 30 khung hình / giây mong muốn.

Sau khi đảm bảo rằng các ứng dụng xử lý khung hình mới sau khi dự đoán trước đó hoàn tất và đặt tốc độ khung hình máy ảnh thành 30 khung hình / giây, tôi có thể thấy rằng Core ML gửi chính xác một dự đoán duy nhất đến Apple Neural Engine và bây giờ ứng dụng chạy trơn tru.

Vì vậy, chúng tôi đã kết thúc cuộc hành trình của mình.

Hãy kiểm tra ứng dụng trên những bức ảnh gia đình cũ của tôi.

Đây là những bức ảnh đen trắng của tôi mà tôi tìm thấy trong tầng hầm của mình.

Họ chụp một số địa điểm ở Ý mà tôi đã đến thăm từ lâu.

Đây là một bức ảnh tuyệt vời của Đấu trường La Mã ở Rome.

Màu sắc của những bức tường và bầu trời thật chân thực.

Hãy kiểm tra cái này.

Đây là Castel del Monte ở miền Nam nước Ý.

Thực sự tốt.

Và đây là quê hương của tôi, Grottaglie.

Thêm màu sắc vào những hình ảnh này đã tạo ra rất nhiều kỷ niệm.

Lưu ý rằng tôi chỉ áp dụng màu sắc cho bức ảnh trong khi vẫn giữ phần còn lại của cảnh đen trắng.

Ở đây, tôi đang tận dụng thuật toán phát hiện hình chữ nhật có sẵn trong khung Vision.

Sử dụng VNDetectRectangleRequest, tôi có thể cô lập ảnh trong cảnh và sử dụng nó làm đầu vào cho mô hình Colorizer.

Và bây giờ hãy để tôi tóm tắt lại.

Trong suốt hành trình của chúng tôi, tôi đã khám phá số lượng lớn các khuôn khổ, API và công cụ mà Apple cung cấp để chuẩn bị, tích hợp và đánh giá chức năng học máy cho các ứng dụng của bạn.

Tôi bắt đầu hành trình này để xác định một vấn đề đòi hỏi một mô hình học máy mã nguồn mở để giải quyết nó.

Tôi đã tìm thấy một mô hình mã nguồn mở với chức năng mong muốn và làm cho nó tương thích với các nền tảng của Apple.

Tôi đã đánh giá hiệu suất mô hình trực tiếp trên thiết bị bằng cách sử dụng Báo cáo Hiệu suất mới.

Tôi đã tích hợp mô hình trong ứng dụng của mình bằng cách sử dụng các công cụ và khuôn khổ mà bạn quen thuộc.

Tôi đã tối ưu hóa mô hình bằng cách sử dụng Mẫu Core ML mới trong Instruments.

Với các công cụ và khuôn khổ của Apple, tôi có thể xử lý từng giai đoạn của quá trình phát triển trực tiếp trên các thiết bị và nền tảng của Apple từ việc chuẩn bị dữ liệu, đào tạo, tích hợp và tối ưu hóa.

Hôm nay, chúng tôi vừa làm xước bề mặt của những gì bạn, nhà phát triển, có thể đạt được với các khuôn khổ và công cụ mà Apple cung cấp cho bạn.

Vui lòng tham khảo các phiên trước, được liên kết với phiên này, để có thêm những ý tưởng truyền cảm hứng để đưa máy học vào ứng dụng của bạn.

Khám phá và thử các khuôn khổ và công cụ.

Tận dụng sức mạnh tổng hợp tuyệt vời giữa phần mềm và phần cứng để tăng tốc các tính năng học máy của bạn và làm phong phú thêm trải nghiệm người dùng của các ứng dụng của bạn.

Chúc một WWDC tuyệt vời, và arrivederci. ♪ ♪