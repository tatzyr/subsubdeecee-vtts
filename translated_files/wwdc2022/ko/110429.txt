110429

♪ 기악 힙합 음악 ♪

안녕하세요, "iOS 카메라 캡처의 발전 발견"에 오신 것을 환영합니다.

저는 카메라 소프트웨어 팀의 니콜라스 젤로이며, iOS와 iPadOS에서 몇 가지 흥미로운 새로운 카메라 기능을 선보일 것입니다.

AVFoundation을 사용하여 LiDAR 스캐너에서 깊이를 스트리밍하는 방법부터 시작하겠습니다.

다음으로, 앱이 얼굴 기반 자동 초점과 자동 노출로 개선된 얼굴 렌더링을 어떻게 받을 수 있는지 살펴보세요.

그런 다음, 고급 AVCaptureSession 스트리밍 구성을 안내해 드리겠습니다.

그리고 마지막으로, 멀티태스킹을 하는 동안 앱이 어떻게 카메라를 사용할 수 있는지 보여드리겠습니다.

AVFoundation을 사용하여 LiDAR 스캐너에서 깊이를 스트리밍하는 방법부터 시작하겠습니다.

iPhone 12 Pro, iPhone 13 Pro 및 iPad Pro에는 고밀도 깊이 지도를 출력할 수 있는 LiDAR 스캐너가 장착되어 있습니다.

LiDAR 스캐너는 주변으로 빛을 촬영한 다음, 장면의 표면에서 반사된 빛을 수집하여 작동합니다.

깊이는 빛이 LiDAR에서 환경으로 가서 스캐너로 다시 반사되는 데 걸린 시간을 측정하여 추정됩니다.

이 전체 과정은 초당 수백만 번 실행된다.

AVFoundation을 사용하여 작동하는 LiDAR 스캐너를 보여드리겠습니다.

여기 iPhone 13 Pro Max에서, 저는 새로운 LiDAR 깊이 카메라 AVCaptureDevice를 사용하는 앱을 실행하고 있습니다.

이 앱은 라이브 카메라 피드 위에 스트리밍 깊이 데이터를 렌더링합니다.

가까운 물체는 파란색으로 표시되고 더 멀리 있는 물체는 빨간색으로 표시됩니다.

그리고 슬라이더를 사용하여 깊이의 불투명도를 조정할 수 있습니다.

이 앱은 또한 고해상도 깊이 지도로 사진을 찍는다.

내가 사진을 찍을 때, 동일한 깊이 오버레이가 적용되지만 스틸에 대해 훨씬 더 큰 해상도를 제공합니다.

이 앱에는 속임수가 하나 더 있다.

토치 버튼을 누르면, 앱은 컬러 이미지와 함께 고해상도 심도 맵을 사용하여 RealityKit을 사용하여 장면에 스포트라이트를 렌더링합니다.

나는 그 장면의 다른 물체를 두드리고 스포트라이트를 가리킬 수 있다.

스포트라이트가 기타를 어떻게 강조하는지 보세요.

또는 벽 구석의 오른쪽 지점을 탭하면, 스포트라이트는 심장 모양을 형성한다.

그 기타로 돌아가자. 정말 멋져 보여.

LiDAR 스캐너용 API는 iPadOS 13.4의 ARKit에 처음 도입되었다.

WWDC 2020 프레젠테이션 "Explore ARKit 4"를 본 적이 없다면, 꼭 보시기 바랍니다.

iOS 15.4의 새로운 기능인 앱은 AVFoundation으로 LiDAR 스캐너에 액세스할 수 있습니다.

우리는 비디오와 깊이를 제공하는 내장 LiDAR 깊이 카메라인 새로운 AVCapture 장치 유형을 도입했습니다.

그것은 고품질의, 높은 정확도의 깊이 정보를 생산한다.

이 새로운 AVCaptureDevice는 후면 광각 카메라를 사용하여 LiDAR 스캐너로 비디오를 전달하여 깊이를 캡처합니다.

비디오와 깊이는 모두 광각 카메라의 시야에서 캡처됩니다.

그리고 TrueDepth AVCaptureDevice와 마찬가지로, 모든 형식은 깊이 데이터 전달을 지원합니다.

이 새로운 AVCaptureDevice는 LiDAR 스캐너의 희소 출력과 후면 광각 카메라의 컬러 이미지를 융합하여 고품질 깊이 데이터를 생성합니다.

LiDAR와 색상 입력은 고밀도 깊이 맵을 출력하는 기계 학습 모델을 사용하여 처리됩니다.

LiDAR 심도 카메라는 후면 광각 카메라를 사용하기 때문에, 망원 및 울트라 와이드 카메라는 AVCaptureMultiCamSession과 함께 사용할 수 있습니다.

이것은 동시에 여러 대의 카메라를 사용하고자 하는 앱에 유용합니다.

LiDAR 깊이 카메라는 640 x 480의 비디오 해상도에서 4032 x 3024의 전체 12메가픽셀 이미지에 이르기까지 다양한 형식을 노출합니다.

스트리밍하는 동안, 최대 320 x 240까지 깊이 지도를 출력할 수 있습니다.

그리고 사진 캡처를 위해, 768 x 576의 깊이 지도를 받을 수 있습니다.

참고, 깊이 해상도는 16 x 9 및 4 x 3 형식에서 약간 다릅니다.

이것은 비디오의 종횡비와 일치하기 위한 것이다.

LiDAR 심도 카메라 AVCaptureDevice는 iPhone 12 Pro, iPhone 13 Pro 및 iPad Pro 5세대에서 사용할 수 있습니다.

iPhone 13 Pro는 후면 카메라의 조합을 사용하여 깊이 데이터를 제공할 수 있습니다.

AVFoundation Capture API는 이것들을 물리적 장치로 구성된 "가상 장치"라고 부른다.

iPhone 13 Pro의 뒷면에는 사용할 수 있는 네 개의 가상 AVCaptureDevices가 있습니다: 새로운 LiDAR 깊이 카메라는 광각 카메라와 함께 LiDAR 스캐너를 사용합니다.

듀얼 카메라는 와이드 및 망원 카메라를 사용한다.

와이드 및 울트라 와이드 카메라를 사용하는 듀얼 와이드 카메라.

그리고 와이드, 울트라 와이드 및 망원 카메라를 사용하는 트리플 카메라.

이 장치들이 생산하는 깊이의 유형에는 차이가 있다.

LiDAR 깊이 카메라는 "절대 깊이"를 만들어낸다. 사용된 비행 기술의 시간은 실제 규모를 계산할 수 있게 해준다.

예를 들어, 이것은 측정과 같은 컴퓨터 비전 작업에 적합합니다.

TrueDepth, 듀얼, 듀얼 와이드 및 트리플 카메라는 상대적인 불균형 기반 깊이를 생성합니다.

이것은 더 적은 전력을 사용하며 사진 효과를 렌더링하는 앱에 적합합니다.

AVFoundation은 AVDepthData 클래스를 사용하여 깊이를 나타냅니다.

이 클래스에는 깊이 데이터 유형, 정확도 및 필터링 여부를 포함하여 그것을 설명하는 다른 속성과 함께 깊이를 포함하는 픽셀 버퍼가 있습니다.

그것은 새로운 LiDAR 심도 카메라와 같은 심도 지원 AVCaptureDevice에 의해 제공됩니다.

AVCaptureDepthDataOutput에서 깊이를 스트리밍하거나 AVCapturePhotoOutput에서 사진에 첨부된 깊이를 받을 수 있습니다.

깊이 데이터는 기본적으로 필터링됩니다.

필터링은 소음을 줄이고 깊이 맵에서 누락된 값이나 구멍을 채웁니다.

이것은 비디오 및 사진 앱에 적합하므로, 깊이 맵을 사용하여 컬러 이미지에 효과를 적용할 때 아티팩트가 나타나지 않습니다.

그러나, 컴퓨터 비전 앱은 깊이 맵의 원래 값을 보존하기 위해 필터링되지 않은 깊이 데이터를 선호해야 한다.

필터링이 비활성화되면, LiDAR 심도 카메라는 낮은 신뢰 지점을 제외합니다.

깊이 데이터 필터링을 비활성화하려면 AVCaptureDepthDataOutput의 isFilteringEnabled 속성을 false로 설정하면 대리자 콜백에서 AVDepthData 객체를 받으면 필터링되지 않습니다.

ARKit은 이미 LiDAR 스캐너에 대한 액세스를 제공했기 때문에, "AVFoundation은 어떻게 비교됩니까?"라고 물어볼 수 있습니다. AVFoundation은 비디오 및 사진 앱을 위해 설계되었습니다.

AVFoundation을 사용하면 LiDAR 스캐너로 캡처한 깊이 데이터를 고해상도 사진에 삽입할 수 있습니다.

ARKit은 이름에서 알 수 있듯이 증강 현실 앱에 가장 적합합니다.

LiDAR 스캐너를 통해 ARKit은 장면 형상 및 물체 배치와 같은 기능을 제공할 수 있습니다.

AVFoundation은 영화를 녹화하고 사진을 찍기에 좋은 고해상도 비디오를 제공할 수 있습니다.

AVFoundation의 LiDAR 심도 카메라는 최대 768 x 576의 깊이를 출력할 수 있습니다.

이것은 ARKit의 깊이 해상도인 256 x 192보다 두 배 이상 크다.

ARKit은 저해상도 깊이 맵을 사용하므로, 기능에 증강 현실 알고리즘을 적용할 수 있습니다.

AVFoundation을 사용하여 깊이 데이터를 캡처하는 방법에 대한 더 많은 "심층" 정보는 WWDC 2017의 이전 세션 "iPhone 사진의 깊이 캡처"를 시청하십시오.

앱에서 LiDAR 깊이 카메라를 사용할 수 있는 흥미로운 방법을 보게 되어 기쁩니다.

다음으로, 자동 초점 및 자동 노출 시스템의 개선이 앱의 장면에서 얼굴의 가시성을 개선하는 데 어떻게 도움이 되는지 논의하겠습니다.

자동 초점과 자동 노출 시스템은 최고의 이미지를 포착하기 위해 장면을 분석한다.

자동 초점 시스템은 피사체에 초점을 맞추기 위해 렌즈를 조정하고, 자동 노출 시스템은 피사체를 볼 수 있도록 장면의 가장 밝고 어두운 영역의 균형을 맞춘다.

그러나, 때때로 자동 조정은 피사체의 얼굴에 초점을 맞추지 않습니다.

그리고 다른 때에는, 밝은 백라이트 장면으로 피사체의 얼굴을 보기 어려울 수 있다.

DSLR 및 기타 프로 카메라의 일반적인 특징은 장면에서 얼굴을 추적하여 초점과 노출을 동적으로 조정하여 볼 수 있도록 하는 것입니다.

iOS 15.4의 새로운 기능인 초점과 노출 시스템은 얼굴을 우선시할 것이다.

우리는 이것의 이점이 너무 좋아서 iOS 15.4 이상에 연결된 모든 앱에 대해 기본적으로 활성화했습니다.

몇 가지 예를 보여줄게.

얼굴 기반 자동 초점이 없으면, 카메라는 얼굴에 다시 초점을 맞추지 않고 배경에 초점을 맞춘다.

다시 봐.

그가 돌아서면서 그의 얼굴이 어떻게 초점이 맞지 않고 배경의 나무들이 날카롭게 유지되는지 보세요.

얼굴 구동 자동 초점이 활성화되면, 그의 얼굴을 명확하게 볼 수 있습니다.

그리고 그가 돌아섰을 때, 카메라는 초점이 배경으로 바뀝니다.

우리가 비디오를 나란히 비교할 때, 그 차이는 분명하다.

얼굴 구동 자동 초점이 활성화된 오른쪽에서, 당신은 그의 수염에서 더 미세한 디테일을 볼 수 있습니다.

밝은 백라이트 장면으로, 얼굴을 잘 노출시키는 것은 어려울 수 있다.

하지만 얼굴을 우선시하는 자동 노출 시스템으로, 우리는 그를 쉽게 볼 수 있다.

나란히 비교하면, 우리는 여기서 그 차이를 다시 볼 수 있다.

오른쪽 사진에서 그의 얼굴을 잘 노출시킴으로써, 배경의 나무들이 더 밝게 보인다는 것을 주목하세요.

그리고 하늘도 그래.

얼굴의 우선순위를 정할 때 전체 장면의 노출이 조정된다.

iOS 15.4에는 얼굴 기반 자동 초점과 자동 노출이 활성화될 때 제어할 수 있는 AVCaptureDevice의 새로운 속성이 있습니다.

장치가 이러한 설정을 "자동으로 조정"할지 여부를 제어하고 언제 활성화해야 하는지 결정할 수 있습니다.

"isEnabled" 속성을 전환하기 전에, 먼저 자동 조정을 비활성화해야 합니다.

이 행동의 자동 활성화는 사진 앱에 좋다.

그것은 애플의 카메라 앱에서 사용된다.

화상 회의 앱이 통화 중에 얼굴을 볼 수 있도록 하는 것도 좋습니다.

페이스타임은 이것을 활용하지만, 때로는 자동 초점과 자동 노출 시스템이 얼굴에 의해 구동되는 앱에 가장 적합하지 않습니다.

예를 들어, 앱이 캡처된 이미지에 대한 사용자 수동 제어를 제공하기를 원한다면, 이것을 끄는 것을 고려할 수 있습니다.

얼굴 기반 자동 초점이나 자동 노출이 앱에 맞지 않다고 결정하면, 이 동작을 선택 해제할 수 있습니다.

먼저, 구성을 위해 AVCaptureDevice를 잠그세요.

그런 다음, 얼굴 구동 자동 초점 또는 자동 노출의 자동 조정을 끄세요.

다음으로, 얼굴 구동 자동 초점 또는 자동 노출을 비활성화하세요.

그리고 마지막으로, 구성을 위해 장치의 잠금을 해제하세요.

고급 스트리밍 구성을 사용하여 앱의 요구에 맞는 오디오 및 비디오 데이터를 수신하는 방법에 대해 이야기하겠습니다.

AVFoundation Capture API를 통해 개발자는 카메라를 사용하여 몰입형 앱을 만들 수 있습니다.

AVCaptureSession은 비디오, 오디오, 사진 등을 제공할 수 있는 AVCaptureOutputs에 연결된 카메라 및 마이크와 같은 입력의 데이터 흐름을 관리합니다.

일반적인 카메라 앱 사용 사례를 예로 들어 보겠습니다: 녹화된 비디오에 필터나 오버레이와 같은 사용자 지정 효과를 적용합니다.

이와 같은 앱은 다음과 같습니다: 두 개의 입력, 카메라와 마이크가 있는 AVCaptureSession은 두 개의 출력에 연결되어 있으며, 하나는 비디오 데이터용이고 다른 하나는 오디오 데이터용입니다.

그런 다음 비디오 데이터에 효과가 적용되고, 처리된 비디오는 비디오 미리보기와 녹화를 위한 AVAssetWriter로 두 곳으로 전송됩니다.

오디오 데이터는 또한 AVAssetWriter로 전송됩니다.

iOS 16 및 iPadOS 16의 새로운 기능, 앱은 여러 AVCaptureVideoDataOutputs를 동시에 사용할 수 있습니다.

각 비디오 데이터 출력에 대해 해상도, 안정화, 방향 및 픽셀 형식을 사용자 정의할 수 있습니다.

예시 카메라 앱으로 돌아가자.

이 앱이 균형을 이루고 있는 경쟁적인 캡처 요구 사항이 있다.

이 앱은 캡처된 콘텐츠의 라이브 비디오 미리보기를 보여주고 나중에 재생할 수 있도록 고품질 비디오를 녹화하고자 합니다.

미리보기를 위해, 해상도는 장치의 화면에 충분히 커야 합니다.

그리고 처리는 지연 시간이 짧은 미리보기를 위해 충분히 빨라야 한다.

하지만 녹음할 때, 고품질 효과가 적용된 고해상도로 캡처하는 것이 가장 좋습니다.

두 번째 AVCaptureVideoDataOutput을 추가할 수 있는 기능으로 캡처 그래프를 확장할 수 있습니다.

이제 비디오 데이터 출력을 최적화할 수 있습니다.

하나의 출력은 미리보기를 위한 더 작은 버퍼를 제공할 수 있으며, 다른 하나는 녹화를 위한 풀사이즈 4K 버퍼를 제공할 수 있다.

또한, 이 앱은 더 작은 미리보기 버퍼에 더 간단하고 성능이 뛰어난 버전의 효과를 렌더링하고 녹화할 때 풀 사이즈 버퍼를 위한 고품질 효과를 예약할 수 있습니다.

이제 앱은 더 이상 미리보기나 녹화된 비디오를 손상시킬 필요가 없습니다.

미리보기와 녹화를 위해 별도의 비디오 데이터 출력을 사용하는 또 다른 이유는 다른 안정화 모드를 적용하는 것이다.

비디오 안정화는 비디오 캡처 파이프라인에 추가 대기 시간을 도입한다.

미리보기의 경우, 눈에 띄는 지연으로 인해 콘텐츠를 캡처하기가 어렵기 때문에 대기 시간이 바람직하지 않습니다.

녹화를 위해, 나중에 비디오를 볼 때 더 나은 경험을 위해 안정화를 적용할 수 있습니다.

따라서 저지연 미리보기를 위해 하나의 비디오 데이터 출력에 안정화를 적용하지 않고 나중에 재생하기 위해 다른 비디오에 안정화를 적용할 수 있습니다.

비디오 데이터 출력의 해상도를 구성하는 방법에는 여러 가지가 있습니다.

풀 사이즈 출력의 경우, 먼저 출력 버퍼 치수의 자동 구성을 비활성화하십시오.

그런 다음 미리보기 크기의 출력 버퍼의 전달을 비활성화하세요.

그러나, 대부분의 경우, 비디오 데이터 출력은 이미 풀사이즈 출력을 위해 구성되어 있다.

미리보기 크기의 출력의 경우, 다시 자동 구성을 비활성화하고, 대신 미리보기 크기의 출력 버퍼를 전달할 수 있습니다.

이것은 사진 AVCaptureSessionPreset을 사용할 때 기본적으로 활성화됩니다.

사용자 지정 해상도를 요청하려면, 출력의 비디오 설정 사전에서 너비와 높이를 지정하십시오.

너비와 높이의 종횡비는 소스 장치의 activeFormat의 종횡비와 일치해야 합니다.

비디오 데이터 출력을 구성하는 더 많은 방법이 있습니다.

안정화를 적용하려면, 보기 좋은 비디오를 생성하는 시네마틱 확장과 같은 모드로 선호하는 안정화를 설정하세요.

세로로 버퍼를 받기 위해 방향을 변경할 수 있습니다.

그리고 10비트 무손실 YUV 버퍼를 받기 위해 픽셀 형식을 지정할 수 있습니다.

AVCaptureVideoDataOutput의 픽셀 형식을 선택하는 방법에 대한 자세한 내용은 Technote 3121을 참조하십시오.

iOS 16 및 iPadOS 16부터 여러 비디오 데이터 출력을 사용하는 것 외에도 앱은 AVCaptureVideoDataOutput 및 AVCaptureAudioDataOutput에서 데이터를 수신하는 동안 AVCaptureMovieFileOutput으로 녹화할 수 있습니다.

세션에 무엇을 추가할 수 있는지 결정하려면, 출력을 추가할 수 있는지 확인하고 세션의 hardwareCost 속성을 쿼리하여 시스템이 구성을 지원할 수 있는지 여부를 결정할 수 있습니다.

동영상 파일 출력으로 비디오 데이터를 수신함으로써, 장면을 녹화하고 분석하는 동안 비디오를 검사할 수 있습니다.

그리고 영화 파일 출력으로 오디오 데이터를 수신하면, 녹음하는 동안 오디오를 샘플링하고 녹음된 것을 들을 수 있습니다.

이와 같은 캡처 그래프를 사용하면 압축되지 않은 비디오 및 오디오 샘플을 받는 동안 AVCaptureMovieFileOutput에 녹음 메커니즘을 오프로드할 수 있습니다.

이러한 고급 스트리밍 구성을 구현하려면 새로운 API를 사용할 필요가 없습니다.

우리는 당신이 기존 API로 더 많은 것을 할 수 있도록 함으로써 이것을 활성화했습니다.

그리고 마지막으로, 사용자가 멀티태스킹을 하는 동안 앱이 카메라를 어떻게 사용할 수 있는지 논의하겠습니다.

iPad에서, 사용자는 다양한 방법으로 멀티태스킹을 할 수 있다.

예를 들어, Split View 또는 Slide Over에서 메모를 읽는 동안 음성 메모를 녹음하고, 전체 화면으로 Safari 위의 떠 있는 창에 메모를 작성하세요.

Picture in Picture를 사용하면 더 많은 WWDC 비디오를 볼 수 있는 알림을 추가하면서 비디오 재생을 계속할 수 있습니다.

그리고 iPadOS 16의 새로운 Stage Manager를 사용하면, 사용자는 크기를 조정할 수 있는 플로팅 창에서 여러 앱을 열 수 있습니다.

iOS 16부터 AVCaptureSessions는 멀티태스킹하는 동안 카메라를 사용할 수 있습니다.

우리는 카메라 시스템이 멀티태스킹하는 동안 제공할 수 있는 서비스 품질에 대한 우려 때문에 이전에 멀티태스킹하는 동안 카메라 액세스를 차단했습니다.

카메라를 사용하는 앱과 함께 실행되는 게임과 같은 자원 집약적인 앱은 프레임 드롭과 기타 대기 시간을 유발하여 카메라 피드가 좋지 않습니다.

몇 달 또는 몇 년 후에 품질이 좋지 않은 비디오를 보는 사용자는 멀티태스킹 중에 녹화했다는 것을 기억하지 못할 수 있습니다.

좋은 카메라 경험을 제공하는 것이 우리의 우선순위이다.

시스템이 멀티태스킹하는 동안 카메라의 비디오가 녹화된 것을 감지하면, 사용자에게 저품질 비디오의 가능성을 알리는 대화상자가 표시됩니다.

이 대화상자는 AVCaptureMovieFileOutput 또는 AVAssetWriter로 녹화가 완료된 후에 표시됩니다.

모든 앱에 대해 시스템에서 한 번만 표시되며 해제할 수 있는 확인 버튼이 있습니다.

멀티태스킹 카메라 액세스가 지원되고 활성화될 때를 나타내기 위해 AVCaptureSession에 두 가지 새로운 속성이 추가되었습니다.

이를 활성화한 캡처 세션은 "여러 포그라운드 앱에서 사용할 수 없는 비디오 장치"라는 이유로 더 이상 중단되지 않습니다. 일부 앱은 카메라를 사용하기 위해 전체 화면 경험이 필요할 수 있습니다.

이것은 당신의 앱이 시스템 리소스를 위해 다른 포그라운드 앱과 경쟁하지 않기를 원한다면 유용할 수 있습니다.

예를 들어, ARKit은 멀티태스킹 동안 카메라 사용을 지원하지 않습니다.

다른 앱과 함께 실행할 때 앱이 잘 작동하는지 확인해야 합니다.

알림을 모니터링하여 증가하는 시스템 압력에 대해 앱을 탄력적으로 만들고, 프레임 속도를 낮추는 것과 같은 영향을 줄이기 위한 조치를 취하십시오.

저해상도, binned 또는 비HDR 형식을 요청하여 시스템에서 앱의 풋프린트를 줄일 수 있습니다.

성능 유지 모범 사례에 대한 자세한 내용은 "멀티태스킹 중 카메라에 액세스하기" 기사를 읽어보세요.

또한, 화상 통화 및 화상 회의 앱은 시스템에서 제공하는 Picture in Picture 창에 원격 참가자를 표시할 수 있습니다.

이제 앱 사용자는 iPad에서 멀티태스킹하는 동안 화상 통화를 원활하게 계속할 수 있습니다.

AVKit은 앱이 원격 통화 참가자를 표시하기 위한 뷰 컨트롤러를 지정할 수 있도록 iOS 15에 API를 도입했습니다.

화상 통화 보기 컨트롤러를 사용하면 창의 내용을 사용자 정의할 수 있습니다.

입양에 대해 자세히 알아보려면, "비디오 통화를 위한 사진 속 사진 채택" 기사를 참조하십시오.

그리고 이것은 iOS 카메라 캡처의 발전을 끝낸다.

AVFoundation을 사용하여 LiDAR 스캐너에서 깊이를 스트리밍하는 방법, 앱이 향상된 얼굴 렌더링을 받는 방법, 앱에 맞는 고급 AVCaptureSession 스트리밍 구성, 그리고 마지막으로 앱이 멀티태스킹하는 동안 카메라를 사용할 수 있는 방법을 보여주었습니다.

네 WWDC가 잘 되길 바라.

♪ ♪