10126

♪ ♪

크리스찬: 안녕하세요, 제 이름은 크리스찬입니다.

저는 ARKit 팀의 엔지니어이며, Discover ARKit 6 세션에 오신 것을 환영합니다.

증강 현실 프레임워크의 최신 발전을 활용하는 방법을 배우게 될 것입니다.

우리는 당신이 ARKit으로 지난 몇 년 동안 무엇을 만들어왔는지 보게 되어 기쁩니다.

우리는 인테리어 디자인, 여행, 가상 전시회, 게임 등에서 놀라운 앱을 보고 있습니다.

Apple의 우리 팀은 당신의 피드백에 세심한 주의를 기울였고, 우리는 많은 것을 ARKit 6에 통합했습니다.

한 번 보자.

우리는 당신이 가장 높은 이미지 해상도로 카메라 스트림을 실행할 수 있는 새로운 4K 비디오 모드를 도입하고 있습니다.

그 후, 비디오 배경을 더 잘 제어할 수 있도록 우리가 만든 몇 가지 추가 카메라 개선 사항에 대해 이야기하겠습니다.

우리는 또한 비행기 앵커의 동작에 대한 업데이트, 모션 캡처 API에 추가, 그리고 마지막으로 위치 앵커가 지원될 새로운 도시를 공유합니다.

4K 비디오부터 시작합시다.

지난 몇 년 동안, 우리는 고해상도 콘텐츠, 특히 영화 제작을 위해 증강 현실의 힘을 활용하는 앱에 대한 많은 수요가 더 많은 픽셀에 굶주려 있는 것을 보았습니다.

ARKit을 위해 이미지가 어떻게 캡처되고 처리되는지 보여드리겠습니다.

이것은 iPhone 13 Pro의 카메라 모듈입니다.

우리가 그것을 열면, 우리는 그것의 설정을 볼 수 있다.

와이드와 울트라와이드 카메라에 대해 이야기해 봅시다.

이 두 카메라 모두 세계 추적, 모션 캡처 또는 사람 세분화와 같은 다른 컴퓨터 비전 작업에 사용할 수 있습니다.

와이드 카메라는 렌더링 배경을 위한 이미지를 제공하기 때문에 우리 마음에 특별한 자리를 차지하고 있다.

렌더링을 위해 이미지가 어떻게 처리되는지 이해하는 것이 중요하므로, 센서 레벨로 확대하겠습니다.

ARKit용 이미지를 캡처할 때, 우리는 이미지 센서의 많은 부분을 사용합니다.

더 정확하게 말하자면, 이 특정 모델의 면적은 3840x2880 픽셀이다.

캡처 후, 우리는 비닝이라는 프로세스를 사용합니다.

그것은 다음과 같이 작동합니다: Binning은 2x2 픽셀의 영역을 취하고, 픽셀 값을 평균하고, 단일 픽셀을 다시 씁니다.

이것은 두 가지 중요한 장점이 있다.

첫째, 이미지 크기는 두 배로 줄어들며, 이 경우 1920x1440 픽셀로 축소됩니다.

그 결과, 각 프레임은 훨씬 적은 메모리와 처리 능력을 소비한다.

이를 통해 장치는 초당 최대 60프레임으로 카메라를 실행할 수 있으며 렌더링을 위한 리소스를 확보할 수 있습니다.

둘째, 이 과정은 픽셀 값의 평균이 센서 노이즈의 영향을 줄이는 저조도 환경에서 이점을 제공한다.

우리는 대략 17밀리초마다 HD 해상도로 이미지를 제공하는 카메라 스트림으로 끝납니다.

다양한 컴퓨터 비전 작업에 현재 프레임을 사용한 후, ARKit은 렌더링을 위해 현재 프레임을 표면화합니다.

자신만의 메탈 렌더러를 작성하는 경우, ARSession의 currentFrame.capturedImage를 통해 액세스할 수 있습니다.

RealityKit을 사용하는 경우, 이미지는 배경으로 사용하기 위해 자동으로 추가로 처리됩니다.

그것은 2532 픽셀의 화면 너비와 일치하도록 장치 내에서 조정되며 디스플레이 종횡비에 맞게 잘립니다.

RealityKit은 이 해적선과 같은 가상 콘텐츠를 프레임 위에 렌더링하고 합성하는 작업을 수행하고 화면에 최종 결과를 표시합니다.

이제, 최신 하드웨어의 힘으로, 우리는 ARKit에서 전체 4K 비디오 모드를 활성화합니다.

당신의 앱은 이제 비닝 단계를 건너뛰고 전체 4K 해상도로 직접 액세스하여 고해상도 이미지를 활용할 수 있습니다.

4K 모드에서는 3840x2160 픽셀의 이미지 영역이 사용되며 초당 30프레임으로 비디오를 캡처할 수 있습니다.

이러한 변경 사항 외에도, 당신의 앱은 이전과 같은 방식으로 작동할 것입니다.

RealityKit을 사용하면 스케일링, 자르기 및 렌더링을 수행합니다.

몇 가지 간단한 단계를 사용하여 4K 모드를 활성화할 수 있습니다.

그게 코드에서 어떻게 보이는지 보자.

'ARConfiguration'에는 장치에서 지원되는 경우 4K 비디오 형식을 반환하는 새로운 편의 기능 'recommendedVideoFormatFor4KResolution'이 있습니다.

장치나 구성이 4K를 지원하지 않으면, 이 기능은 nil을 반환합니다.

그런 다음 이 비디오 형식을 구성에 할당한 다음, 조정된 구성으로 세션을 실행할 수 있습니다.

4K 비디오 모드는 iPhone 11 이상과 M1 칩이 장착된 모든 iPad Pro에서 사용할 수 있습니다.

해상도는 초당 30프레임에서 3840x2160 픽셀이다.

종횡비는 16:9이며, iPad의 경우 전체 화면 디스플레이를 위해 이미지를 측면에서 잘라야 하며 최종 렌더링이 확대된 것처럼 보일 수 있습니다.

ARKit을 사용할 때, 특히 새로운 4K 해상도에서, 최적의 결과를 위해 몇 가지 모범 사례를 따르는 것이 중요합니다.

ARFrame을 너무 오래 붙잡지 마세요.

이것은 시스템이 메모리를 확보하는 것을 막을 수 있으며, 이는 ARKit이 새로운 프레임을 당신에게 표면화하는 것을 더 막을 수 있습니다.

이것은 렌더링에서 프레임 드롭을 통해 보일 것입니다.

궁극적으로, ARCamera의 추적 상태는 제한적으로 떨어질 수 있다.

주어진 시간에 너무 많은 이미지를 보유하지 않도록 콘솔 경고를 확인하세요.

또한 새로운 4K 비디오 포맷이 실제로 앱에 적합한 옵션인지 고려하십시오.

고해상도 비디오의 혜택을 받는 앱은 비디오, 영화 제작 및 가상 제작 앱과 같은 좋은 후보입니다.

고해상도 이미지를 다루는 것은 추가 시스템 리소스를 차지하므로, 높은 재생률에 의존하는 게임 및 기타 앱의 경우, 여전히 초당 60프레임의 풀 HD 비디오를 사용하는 것이 좋습니다.

새로운 4K 모드 외에도, 카메라를 더 잘 제어할 수 있는 몇 가지 추가 개선 사항이 있습니다.

고해상도 배경 사진 API를 소개하고 새로운 HDR 모드를 활성화하는 방법을 보여드리겠습니다.

또한, 더 세밀한 제어를 위해 기본 AVCaptureDevice에 액세스하는 방법을 시연하고 ARKit에서 EXIF 태그를 읽는 방법을 보여 드리겠습니다.

새로운 고해상도 배경 사진 API로 넘어갑시다.

ARSession을 실행하는 동안, 당신은 여전히 평소와 같이 비디오 스트림에 접근할 수 있습니다.

또한, ARKit을 사용하면 비디오 스트림이 중단 없이 실행되는 동안 백그라운드에서 주문형 단일 사진 캡처를 요청할 수 있습니다.

그 단일 사진 프레임은 카메라 센서를 최대한 활용합니다.

내 아이폰 13에서 그것은 와이드 카메라의 전체 12메가픽셀을 의미한다.

WWDC를 준비할 때, ARKit에서 우리는 이 새로운 API가 당신이 만드는 데 도움이 될 수 있는 것을 강조하는 사진 앱에 대한 재미있는 아이디어를 가지고 있었습니다.

우리의 예에서, 우리는 유명한 해적 깃발이 애플 인피니트 루프 캠퍼스 위로 날고 있던 2016년 4월 1일로 거슬러 올라갑니다.

나는 원래 사진작가인 토미에게 6년 전에 정확히 어디서 그 사진을 찍었는지 물었다.

이 좌표를 바탕으로, 우리는 큰 파란색 점과 같이 토미가 2016년 4월에 서 있던 곳과 정확히 같은 장소로 안내하는 위치 앵커를 만들 수 있습니다.

그 지점에 도달하면, 초점 사각형을 보여줌으로써 완벽한 그림을 짜맞추는 데 도움이 됩니다.

마지막으로, 이 앱을 사용하면 화면을 탭하여 사진을 찍을 수 있습니다.

그 사진은 다른 AVCapture 세션을 스핀업할 필요 없이 현재 ARKit 세션이 실행되는 동안 네이티브 카메라 해상도로 찍을 수 있습니다.

우리는 당신이 AR과 사진의 힘을 결합한 어떤 아이디어를 가지고 있는지 보게 되어 기쁩니다.

이 API로 큰 도움이 될 또 다른 사용 사례는 오브젝트 캡처를 사용하여 3D 모델을 만드는 것이다.

물체 캡처는 이 러닝화와 같은 실제 물체의 사진을 찍고, 최신 사진 측량 알고리즘을 사용하여 AR 앱에서 배포할 수 있는 3D 모델로 바꿉니다.

ARKit을 사용하면 물리적 개체 위에 3D UI를 오버레이하고 더 나은 캡처 지침을 제공할 수 있습니다.

그리고 이제 새로운 고해상도 배경 이미지 API를 사용하면 물체의 고해상도 사진을 찍고 훨씬 더 사실적인 3D 모델을 만들 수 있습니다.

저는 사진 측량의 열렬한 팬이므로, 올해의 "당신의 세계를 증강 현실로 가져오세요" 세션을 확인하는 것을 강력히 추천합니다.

코드에서 고해상도 사진 캡처를 활성화하는 방법을 보여드리겠습니다.

먼저, 우리는 hiResCapture를 지원하는 비디오 형식을 확인합니다.

우리는 편의 기능 'recommendedVideoFormatForHighResolution FrameCapturing'을 사용할 수 있습니다.

형식이 지원되는지 확인한 후, 새로운 비디오 형식을 설정하고 세션을 실행할 수 있습니다.

우리는 또한 ARKit에게 언제 고해상도 이미지를 캡처해야 하는지 말해야 한다.

이전 예에서, 사진 캡처는 화면을 탭하여 트리거됩니다.

자신의 애플리케이션에서 고해상도 프레임 캡처를 트리거하는 다양한 이벤트에 반응하고 싶을 수도 있습니다.

그건 정말 네 사용 사례에 달렸어.

ARSession에는 captureHighResolutionFrame이라는 새로운 기능이 있다.

이 기능을 호출하면 고해상도 이미지의 대역외 캡처가 트리거됩니다.

완료 핸들러에서 고해상도 이미지와 다른 모든 프레임 속성을 비동기적으로 포함하는 ARFrame에 액세스할 수 있습니다.

콘텐츠에 액세스하기 전에 프레임 캡처가 성공했는지 확인해야 합니다.

이 예시에서 우리는 프레임을 디스크에 저장한다.

또한 특히 이러한 이미지는 이미지 센서의 전체 해상도를 사용하기 때문에 앞서 언급한 이미지 보존에 대한 모범 사례를 명심하십시오.

다음으로, HDR에 대해 이야기해 봅시다.

하이 다이내믹 레인지(High Dynamic Range)는 더 넓은 범위의 색상을 캡처하고 디스플레이에 매핑합니다.

이것은 대비가 높은 환경에서 가장 잘 보인다.

여기 내 뒷마당의 좋은 예가 있어.

이 장면은 나무 울타리와 같은 매우 어두운 영역과 하늘의 구름과 같은 매우 밝은 영역을 특징으로 한다.

오른쪽과 같이 HDR 모드를 켤 때, 구름의 플러피와 같은 이 지역의 세부 사항이 HDR에서 훨씬 더 잘 보존된다는 것을 알 수 있습니다.

코드에서 HDR이 어떻게 켜지는지 봅시다.

'isVideoHDRSupported' 속성을 통해 HDR을 지원하는 경우 모든 비디오 형식을 쿼리할 수 있습니다.

현재, 비바인 비디오 포맷만 HDR을 지원합니다.

HDR이 지원되는 경우, 설정에서 videoHDRAllowed를 true로 설정하고 해당 구성으로 세션을 실행하십시오.

HDR을 켜면 성능에 영향을 미칠 것이므로, 필요할 때만 사용하세요.

노출이나 화이트 밸런스와 같은 설정에 대한 수동 제어를 선호하는 경우, 이제 AVCaptureDevice에 직접 액세스하고 설정을 변경할 수 있는 편리한 방법이 있습니다.

코드 예제에서, 기본 'AVCaptureDevice'에 액세스하려면 구성의 'configurableCaptureDevice ForPrimaryCamera'를 호출하십시오.

이 기능을 사용하여 ARKit 앱의 사용자 지정 모양을 만들 수 있지만, 이미지는 렌더링 배경으로 사용될 뿐만 아니라 ARKit에서 장면을 분석하기 위해 적극적으로 사용된다는 것을 명심하세요.

따라서 강한 과다 노출과 같은 변화는 ARKit의 출력 품질에 부정적인 영향을 미칠 수 있습니다.

포커스 이벤트를 트리거하는 것과 같은 몇 가지 고급 작업을 수행할 수도 있습니다.

AVCaptureSessions를 구성하는 방법에 대한 자세한 내용은 developer.apple.com의 AVCapture 문서를 참조하십시오.

마지막으로, ARKit은 EXIF 태그를 앱에 노출합니다.

그것들은 이제 모든 ARFrame에서 사용할 수 있습니다.

EXIF 태그에는 후처리에 유용할 수 있는 화이트 밸런스, 노출 및 기타 설정에 대한 유용한 정보가 포함되어 있습니다.

이것으로 이미지 캡처 파이프라인에 대한 모든 업데이트를 마칩니다.

비행기 앵커에 어떤 변화가 있는지 봅시다.

비행기 앵커는 ARKit의 첫 번째 버전부터 인기 있는 기능이었다.

여러분 중 많은 사람들이 평면 앵커와 기본 평면 형상을 더 깨끗하게 분리할 필요성을 표현했습니다.

그런 이유로, 우리는 비행기 앵커의 동작과 비행기의 기하학에 대한 업데이트를 발표하고 있다.

이것은 iOS 15의 전형적인 비행기 앵커의 예이다.

AR 세션의 시작 부분에서, 그것은 테이블 위의 질감이 좋은 노트북에 비행기에 맞는다.

세션을 실행할 때, 비행기는 시야에 들어오는 테이블의 새로운 부분을 설명하기 위해 점차 업데이트됩니다.

비행기 기하학이 업데이트될 때마다, 비행기의 새로운 방향을 반영하기 위해 앵커 회전도 업데이트됩니다.

iOS 16을 통해, 우리는 평면 앵커와 평면 형상 사이의 더 깨끗한 분리를 소개합니다.

평면 앵커와 기하학 업데이트는 이제 완전히 분리되었다.

전체 테이블이 시야에 따라 비행기가 확장되고 기하학을 업데이트하는 동안, 앵커 회전 자체는 일정하게 유지된다.

왼쪽의 오래된 동작과 대조할 때, iOS 16의 비행기 앵커가 전체 AR 세션 동안 노트북과 정렬된 동일한 방향에 머물렀다는 것을 알 수 있습니다.

평면 기하학에 대한 모든 정보는 이제 ARPlaneExtent라는 클래스에 포함되어 있다.

회전 업데이트는 더 이상 평면 앵커 자체를 회전함으로써 표현되지 않는다.

대신, ARPlaneExtent는 회전 각도를 나타내는 새로운 속성인 rotationOnYAxis를 포함한다.

이 새로운 속성 외에도, 평면은 폭과 높이뿐만 아니라 PlaneAnchor의 중심 좌표로 완전히 정의된다.

코드로 이 비행기 시각화를 만드는 방법을 보여드리겠습니다.

먼저, 우리는 지정된 너비와 높이에 따라 평면 메쉬를 기반으로 엔티티를 생성합니다.

그런 다음 우리는 y축의 회전에 따라 엔티티 변환을 설정하고 중앙 속성의 값으로 오프셋합니다.

비행기가 업데이트될 때마다, 우리는 너비, 높이, 중심 좌표와 새로운 rotationOnYAxis가 바뀔 수 있다는 사실을 설명해야 한다.

이 새로운 동작을 사용하려면, Xcode 설정에서 배포 대상을 iOS 16으로 설정하십시오.

다음 업데이트는 MotionCapture에 있으며, 우리의 기계 학습 지도자는 더 많은 개선을 위해 열심히 노력했습니다.

2D 골격과 3D 관절 모두에 대한 전체 업데이트가 있습니다.

2D 골격을 위해 우리는 두 개의 새로운 관절을 추적하고 있다: 왼쪽 귀와 오른쪽 귀.

우리는 또한 전반적인 포즈 감지를 개선했다.

iPhone 12 이상과 M1 칩이 장착된 최신 iPad Pro 및 iPad Air 모델에서는 빨간색으로 표시된 3D 스켈레톤이 개선되었습니다.

당신은 덜 불안하고 전반적으로 더 시간적인 일관성을 경험하게 될 것입니다.

사람의 일부가 막혀 있거나 카메라 가까이 걸어갈 때 추적이 더 안정적이다.

개선된 MotionCapture를 사용하려면, Xcode 설정에서 배포 대상을 iOS 16으로 설정하십시오.

다음으로, 저는 또한 LocationAnchors가 지원될 새로운 도시와 국가를 발표하고 싶습니다.

다시 한 번 말씀드리자면, Apple Maps는 LocationAnchor API를 사용하여 보행자 보행 지침에 전원을 공급합니다.

이 예에서 당신은 LocationAnchors의 힘 덕분에 런던의 거리를 안내할 수 있다는 것을 알 수 있습니다.

LocationAnchors는 이미 미국과 영국 런던의 점점 더 많은 도시에서 사용할 수 있다.

오늘부터, 그것들은 캐나다 도시인 밴쿠버, 토론토, 몬트리올에서 이용할 수 있을 것이다.

우리는 또한 싱가포르와 도쿄를 포함한 일본의 7개 대도시 지역에서 그것들을 가능하게 하고 있다.

호주 멜버른과 시드니뿐만 아니라.

올해 말에, 우리는 오클랜드, 뉴질랜드, 텔아비브, 이스라엘, 프랑스 파리에서 사용할 수 있도록 할 것입니다. LocationAnchors가 특정 좌표에서 지원되는지 알고 싶다면, ARGeoTrackingConfiguration의 checkAvailability 방법을 사용하세요.

그리고 그것들은 모두 ARKit 6에 대한 업데이트였다.

요약하자면, 나는 새로운 4K 비디오 형식으로 ARKit을 실행하는 방법을 제시했다.

고급 사용 사례를 위해, 저는 HDR을 활성화하거나 AVCaptureDevice를 수동으로 제어하는 방법을 시연했습니다.

더 많은 픽셀에 굶주린 애플리케이션을 위해, 나는 ARKit 세션에서 고해상도 사진을 얻는 방법을 시연했다.

우리는 비행기 앵커의 새로운 행동에 대해 이야기했고, 나는 새로운 귀 관절과 MotionCapture의 다른 개선 사항을 제시했다.

당신은 또한 올해 말에 LocationAnchors가 어느 국가에서 이용 가능한지 알게 될 것입니다.

튜닝해줘서 고마워.

멋진 WWDC 2022 되세요!