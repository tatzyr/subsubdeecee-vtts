[
  {
    "text": "♪ ♪ ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=0"
  },
  {
    "text": "Hi, my name is Brett Keating, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=9"
  },
  {
    "text": "and it's my pleasure to be introducing you to what is new in the Vision framework. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=11"
  },
  {
    "text": "You may be new to Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=15"
  },
  {
    "text": "Perhaps this is the first session you've seen about the Vision framework. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=17"
  },
  {
    "text": "If so, welcome. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=21"
  },
  {
    "text": "For your benefit, let's briefly recap some highlights about the Vision framework. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=24"
  },
  {
    "text": "Some Vision framework facts for you. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=28"
  },
  {
    "text": "Vision was first introduced in 2017, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=31"
  },
  {
    "text": "and since then, many thousands of great apps have been developed ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=34"
  },
  {
    "text": "with the technology Vision provides. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=37"
  },
  {
    "text": "Vision is a collection of computer vision algorithms ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=40"
  },
  {
    "text": "that continues to grow over time ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=43"
  },
  {
    "text": "and includes such things as face detection, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=45"
  },
  {
    "text": "image classification, and contour detection to name a few. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=47"
  },
  {
    "text": "Each of these algorithms is made available through an easy-to-use, consistent API. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=52"
  },
  {
    "text": "If you know how to run one algorithm in the Vision framework, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=56"
  },
  {
    "text": "you know how to run them all. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=58"
  },
  {
    "text": "And Vision takes full advantage of Apple Silicon ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=61"
  },
  {
    "text": "on all of the platforms it supports, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=64"
  },
  {
    "text": "to power the machine learning at the core of many of Vision's algorithms. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=66"
  },
  {
    "text": "Vision is available on tvOS, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=70"
  },
  {
    "text": "iOS, and macOS; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=73"
  },
  {
    "text": "and will fully leverage Apple Silicon on the Mac.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=75"
  },
  {
    "text": "Some recent additions to the Vision framework include Person segmentation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=80"
  },
  {
    "text": "shown here.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=83"
  },
  {
    "text": "Also hand pose estimation, shown in this demo.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=87"
  },
  {
    "text": "And here is our Action and Vision sample app, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=94"
  },
  {
    "text": "which uses body pose estimation and trajectory analysis.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=97"
  },
  {
    "text": "Our agenda today begins with an overview of some new revisions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=102"
  },
  {
    "text": "which are updates to existing requests ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=106"
  },
  {
    "text": "that may provide increased functionality, improve performance, or improve accuracy.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=109"
  },
  {
    "text": "First, we have a new revision for text recognition. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=117"
  },
  {
    "text": "This is the third revision, given by VNRecognizeTextRequestRevision3. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=120"
  },
  {
    "text": "This is the text recognizer that powers the amazing Live Text feature. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=126"
  },
  {
    "text": "The text recognizer supports several languages, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=130"
  },
  {
    "text": "and you may discover which languages are supported ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=133"
  },
  {
    "text": "by calling supportedRecognitionLanguages. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=136"
  },
  {
    "text": "We have now added a few new languages, and I'll show you a couple examples. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=139"
  },
  {
    "text": "We are now supporting the Korean language in Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=143"
  },
  {
    "text": "Here is an example of Vision at work transcribing a Korean receipt. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=147"
  },
  {
    "text": "And here is a corresponding example for Japanese, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=151"
  },
  {
    "text": "also showing the results of Vision's text recognition ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=155"
  },
  {
    "text": "on this now-supported language. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=158"
  },
  {
    "text": "For text recognition, we have a new automatic language identification. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=160"
  },
  {
    "text": "You may still specify the recognition languages to use ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=166"
  },
  {
    "text": "using the recognitionLanguages property. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=170"
  },
  {
    "text": "But suppose you don't know ahead of time which languages ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=173"
  },
  {
    "text": "your app user might be trying to recognize. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=176"
  },
  {
    "text": "Now, but only for accurate recognition mode, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=179"
  },
  {
    "text": "you may ask the text recognizer to automatically detect the language ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=183"
  },
  {
    "text": "by setting automaticallyDetectsLanguage to true.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=187"
  },
  {
    "text": "It's best to use this just for such a situation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=192"
  },
  {
    "text": "where you don't know which language to recognize, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=195"
  },
  {
    "text": "because the language detection can occasionally get this wrong. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=198"
  },
  {
    "text": "If you have the prior knowledge about which language to recognize, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=201"
  },
  {
    "text": "it's still best to specify these languages to Vision ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=205"
  },
  {
    "text": "and leave automaticallyDetectsLanguage turned off.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=208"
  },
  {
    "text": "Next, we have a new third revision for our barcode detection, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=214"
  },
  {
    "text": "called VNDetectBarcodesRequestRevision3. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=219"
  },
  {
    "text": "This revision leverages modern machine learning under the hood, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=223"
  },
  {
    "text": "which is a departure from prior revisions. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=226"
  },
  {
    "text": "Barcodes come in a variety of symbologies, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=229"
  },
  {
    "text": "from barcodes often seen on products in stores, to QR codes, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=231"
  },
  {
    "text": "to specialty codes used in healthcare applications. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=235"
  },
  {
    "text": "In order to know which symbologies Vision supports, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=239"
  },
  {
    "text": "you may call supportedSymbologies.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=241"
  },
  {
    "text": "Let's talk about performance. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=246"
  },
  {
    "text": "Partly because we are using ML, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=248"
  },
  {
    "text": "we are detecting multiple codes in one shot rather than one at a time, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=250"
  },
  {
    "text": "so the request will be faster for images containing multiple codes. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=255"
  },
  {
    "text": "Also, more codes are detected in a given image containing many codes, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=260"
  },
  {
    "text": "due to increased accuracy. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=265"
  },
  {
    "text": "And furthermore, there are few, if any, duplicate detections. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=267"
  },
  {
    "text": "The bounding boxes are improved for some codes, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=271"
  },
  {
    "text": "particularly linear codes such as ean13, for which a line was formerly returned. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=274"
  },
  {
    "text": "Now, the bounding box surrounds the entire visible code.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=279"
  },
  {
    "text": "Finally, the ML model is more able to ignore such things as curved surfaces, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=285"
  },
  {
    "text": "reflections, and other artifacts that have hindered detection accuracy in the past.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=289"
  },
  {
    "text": "Both of these new revisions, for text recognition ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=296"
  },
  {
    "text": "and for barcode detection, form the technological foundations ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=299"
  },
  {
    "text": "for the VisionKit Data Scanner API, which is a drop-in UI element ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=303"
  },
  {
    "text": "that sets up the camera stream to scan and return barcodes and text. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=308"
  },
  {
    "text": "It's really a fantastic addition to our SDK, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=311"
  },
  {
    "text": "and I highly recommend you check out the session about it to learn more. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=314"
  },
  {
    "text": "The final new revision I'll tell you about today is a new revision ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=319"
  },
  {
    "text": "for our optical flow request called ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=322"
  },
  {
    "text": "VNGenerateOpticalFlowRequestRevision2. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=325"
  },
  {
    "text": "Like the barcode detector, this new revision also uses ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=329"
  },
  {
    "text": "modern machine learning under the hood.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=332"
  },
  {
    "text": "Although optical flow is one of the longest studied computer vision problems, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=337"
  },
  {
    "text": "you might not be aware of what it does, compared to detection of things ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=341"
  },
  {
    "text": "which form part of all of our daily lives, like text and barcodes.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=344"
  },
  {
    "text": "Optical flow analyzes two consecutive images, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=348"
  },
  {
    "text": "typically frames from a video. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=352"
  },
  {
    "text": "Depending on your use case, you might look at motion ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=354"
  },
  {
    "text": "between two adjacent frames, or skip a few frames in between, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=356"
  },
  {
    "text": "but in any case, the two images should be in chronological order.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=360"
  },
  {
    "text": "The analysis provides an estimate of the direction and magnitude of the motion, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=364"
  },
  {
    "text": "or by how much parts of the first image need to \"move,\" so to speak, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=369"
  },
  {
    "text": "to be positioned correctly in the second image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=373"
  },
  {
    "text": "A VNPixelBufferObservation is the result, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=377"
  },
  {
    "text": "which represents this motion at all places in the image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=380"
  },
  {
    "text": "It is a two-channel image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=383"
  },
  {
    "text": "One channel contains the X magnitude, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=385"
  },
  {
    "text": "and the other contains the Y magnitude. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=388"
  },
  {
    "text": "Together, these form 2D vectors at each pixel ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=390"
  },
  {
    "text": "arranged in this 2D image so that their locations map ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=394"
  },
  {
    "text": "to corresponding locations in the images that were provided as input. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=397"
  },
  {
    "text": "Let's have a look at this visually. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=401"
  },
  {
    "text": "Suppose you have an incoming video and several frames are coming in, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=403"
  },
  {
    "text": "but let's look at these two images in particular. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=407"
  },
  {
    "text": "Here we have a dog running on the beach. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=410"
  },
  {
    "text": "From the left image to the right image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=412"
  },
  {
    "text": "it appears the dog has moved a bit to the left. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=414"
  },
  {
    "text": "How would you estimate and represent this motion? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=417"
  },
  {
    "text": "Well, you would run optical flow ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=421"
  },
  {
    "text": "and arrive at something akin to the image below. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=423"
  },
  {
    "text": "The darker areas are where motion has been found, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=425"
  },
  {
    "text": "and notice that it does indeed look just like the shape of the dog. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=428"
  },
  {
    "text": "That's because only the dog is really moving in this scene. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=432"
  },
  {
    "text": "We are showing the motion vectors in this image by using \"false color,\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=436"
  },
  {
    "text": "which maps the x,y from the vectors into a color palette. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=439"
  },
  {
    "text": "In this false color representation, \"red\" hues happen to indicate ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=443"
  },
  {
    "text": "movement primarily to the left. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=447"
  },
  {
    "text": "Now that you've seen an example from one frame, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=450"
  },
  {
    "text": "let's see how it looks for a whole video clip. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=453"
  },
  {
    "text": "Here we compute optical flow for a short clip of this dog ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=456"
  },
  {
    "text": "fetching a water bottle on a beach. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=459"
  },
  {
    "text": "On the left is the result from revision 1. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=462"
  },
  {
    "text": "On the right is the result from our new ML-based revision 2. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=465"
  },
  {
    "text": "Hopefully some of the improvements in revision 2 are clear to see. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=469"
  },
  {
    "text": "For one thing, perhaps most obviously, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=472"
  },
  {
    "text": "the water bottle's motion is captured much more accurately. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=475"
  },
  {
    "text": "You might also notice improvements in some of the estimated motion of the dog. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=478"
  },
  {
    "text": "I notice improvements in the tail most clearly ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=483"
  },
  {
    "text": "but also can see the motion of his ears flapping in the new revision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=485"
  },
  {
    "text": "The first revision also contains a bit of background noise motions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=490"
  },
  {
    "text": "while the second revision more coherently represents the backgrounds as not moving. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=493"
  },
  {
    "text": "Hopefully that example gave you a good idea what this technology does. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=497"
  },
  {
    "text": "Now let's dive in a bit on how you might use it in your app. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=501"
  },
  {
    "text": "Clearly the primary use case is to discover local motion in a video. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=505"
  },
  {
    "text": "This feeds directly into security video use cases, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=509"
  },
  {
    "text": "where it's most important to identify and localize motions ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=513"
  },
  {
    "text": "that deviate from the background, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=515"
  },
  {
    "text": "and it should be mentioned that optical flow does work best ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=517"
  },
  {
    "text": "for stationary cameras, such as most security cameras. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=520"
  },
  {
    "text": "You might want to use Vision's object tracker ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=524"
  },
  {
    "text": "to track objects that are moving in a video, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=526"
  },
  {
    "text": "but need to know where to initialize a tracker. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=528"
  },
  {
    "text": "Optical flow can help you there as well. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=531"
  },
  {
    "text": "If you have some computer vision or image processing savvy of your own, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=534"
  },
  {
    "text": "you might leverage our optical flow results ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=538"
  },
  {
    "text": "to enable further video processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=540"
  },
  {
    "text": "Video interpolation, or video action analysis, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=543"
  },
  {
    "text": "can greatly benefit from the information optical flow provides. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=546"
  },
  {
    "text": "Let's now dig into some important additional differences ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=550"
  },
  {
    "text": "between revision 1 and revision 2.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=553"
  },
  {
    "text": "Revision 1 always returns optical flow fields ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=556"
  },
  {
    "text": "that have the same resolution as the input. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=559"
  },
  {
    "text": "Revision 2 will also do this by default. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=562"
  },
  {
    "text": "However, there is a tiny wrinkle: partially due to the fact ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=565"
  },
  {
    "text": "that revision 2 is ML-based, the output of the underlying model ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=568"
  },
  {
    "text": "is relatively low resolution compared to most input image resolutions. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=572"
  },
  {
    "text": "Therefore, to match revision 1 default behavior, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=576"
  },
  {
    "text": "some upsampling must be done, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=580"
  },
  {
    "text": "and we are using bilinear upsampling to do this. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=582"
  },
  {
    "text": "Here is a visual example explaining what upsampling does. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=585"
  },
  {
    "text": "On the left, we have a zoomed-in portion of the network output, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=588"
  },
  {
    "text": "which is low resolution and therefore appears pixelated. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=592"
  },
  {
    "text": "The overall flow field might have an aspect ratio of 7:5. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=595"
  },
  {
    "text": "On the right, we have a similar region taken from the same field, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=600"
  },
  {
    "text": "upsampled to the original image resolution. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=603"
  },
  {
    "text": "Perhaps that image also has a different aspect ratio, let's say 16:9. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=606"
  },
  {
    "text": "You will notice that the edges of the flow field are smoothed out ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=611"
  },
  {
    "text": "by the bilinear upsampling. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=614"
  },
  {
    "text": "Due to the potential for the aspect ratios to differ, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=618"
  },
  {
    "text": "keep in mind that as part of the upsampling process, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=620"
  },
  {
    "text": "the flow image will be stretched ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=623"
  },
  {
    "text": "in order to properly correspond the flow field ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=625"
  },
  {
    "text": "to what is happening in the image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=628"
  },
  {
    "text": "When working with the network output directly, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=630"
  },
  {
    "text": "you should account for resolution and aspect ratio in a similar fashion ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=632"
  },
  {
    "text": "when mapping flow results to the original images.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=636"
  },
  {
    "text": "You have the option to skip the upsampling ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=641"
  },
  {
    "text": "by turning on keepNetworkOutput on the request. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=643"
  },
  {
    "text": "This will give you the raw model output. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=647"
  },
  {
    "text": "There are four computationAccuracy settings you may apply to the request ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=650"
  },
  {
    "text": "in order to choose an available output resolution. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=654"
  },
  {
    "text": "You can see the resolutions for each accuracy setting in this table, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=657"
  },
  {
    "text": "but be sure to always check the width and height of the pixel buffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=660"
  },
  {
    "text": "contained in the observation.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=663"
  },
  {
    "text": "When should you use network output, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=666"
  },
  {
    "text": "and when should you allow Vision to upsample? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=667"
  },
  {
    "text": "The default behavior is best if you already are using optical flow ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=670"
  },
  {
    "text": "and want the behavior to remain backward compatible. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=674"
  },
  {
    "text": "It's also a good option if you want upsampled output, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=677"
  },
  {
    "text": "and bilinear is acceptable to you and worth the additional memory and latency. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=680"
  },
  {
    "text": "Network output is best if you don't need full resolution ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=684"
  },
  {
    "text": "and can form correspondences on the fly or just want to initialize a tracker. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=688"
  },
  {
    "text": "Network output may also be the right choice ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=693"
  },
  {
    "text": "if you do need a full resolution flow, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=695"
  },
  {
    "text": "but would prefer to use your own upsampling methods. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=697"
  },
  {
    "text": "That covers the new algorithm revisions for this session. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=700"
  },
  {
    "text": "Let's move on to discuss some spring cleaning we are doing ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=704"
  },
  {
    "text": "in the Vision framework and how it might impact you. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=707"
  },
  {
    "text": "We first introduced face detection and face landmarks ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=710"
  },
  {
    "text": "when Vision was initially released five years ago, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=713"
  },
  {
    "text": "as \"revision 1\" for each algorithm. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=716"
  },
  {
    "text": "Since that time we've released two newer revisions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=719"
  },
  {
    "text": "which use more efficient and more accurate technologies. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=723"
  },
  {
    "text": "Therefore, we are removing the first revisions of these algorithms ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=726"
  },
  {
    "text": "from Vision framework, while keeping the second and third revisions only. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=730"
  },
  {
    "text": "However, if you use revision 1, never fear. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=735"
  },
  {
    "text": "We will continue to support code that specifies revision 1 ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=738"
  },
  {
    "text": "or code that has been compiled against SDKs which only contained revision 1. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=741"
  },
  {
    "text": "How is that possible, you may ask? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=746"
  },
  {
    "text": "Revision 1 executes an algorithm under the hood ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=748"
  },
  {
    "text": "that I have called \"the revision 1 detector\" in this diagram. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=752"
  },
  {
    "text": "In the same way, revision 2 uses the revision 2 detector. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=756"
  },
  {
    "text": "What we have done for this release of Vision ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=760"
  },
  {
    "text": "is to satisfy revision 1 requests ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=762"
  },
  {
    "text": "with the output of the revision 2 detector. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=765"
  },
  {
    "text": "Additionally, the revision 1 request will be marked as deprecated. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=768"
  },
  {
    "text": "This allows us to remove the old revision 1 detector completely, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=772"
  },
  {
    "text": "allowing the Vision framework to remain streamlined. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=776"
  },
  {
    "text": "This has several benefits, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=779"
  },
  {
    "text": "not the least of which is to save space on disk, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=781"
  },
  {
    "text": "which makes our OS releases and SDKs less expensive to download and install. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=784"
  },
  {
    "text": "All you Vision experts out there might be saying to yourselves, \"But wait a minute, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=789"
  },
  {
    "text": "\"revision 2 returns upside down faces while revision 1 does not. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=793"
  },
  {
    "text": "Couldn't this behavior difference have an impact on some apps?\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=798"
  },
  {
    "text": "It certainly would, except we will be taking precautions ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=801"
  },
  {
    "text": "to preserve revision 1 behavior. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=804"
  },
  {
    "text": "We will not be returning upside-down faces from the revision 2 detector. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=807"
  },
  {
    "text": "Similarly, the revision 2 landmark detector ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=812"
  },
  {
    "text": "will return results that match the revision 1 landmark constellation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=815"
  },
  {
    "text": "The execution time is on par, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=820"
  },
  {
    "text": "and you ought to experience a boost in accuracy. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=822"
  },
  {
    "text": "In any case, this change will not require any apps ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=825"
  },
  {
    "text": "to make any modifications to their code, and things will continue to work.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=828"
  },
  {
    "text": "Still, we have a call to action for you. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=834"
  },
  {
    "text": "You shouldn't be satisfied with using revision 1 ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=837"
  },
  {
    "text": "when we have much better options available. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=839"
  },
  {
    "text": "We always recommend using the latest revisions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=842"
  },
  {
    "text": "and for these requests, that would be revision 3.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=844"
  },
  {
    "text": "Of course the main reason for this recommendation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=848"
  },
  {
    "text": "is to use the latest technology, which provides the highest level ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=851"
  },
  {
    "text": "of accuracy and performance available, and who doesn't want that? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=854"
  },
  {
    "text": "Furthermore, we have established and communicated several times, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=858"
  },
  {
    "text": "and we reiterate again here, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=862"
  },
  {
    "text": "the best practice of always explicitly specifying your revisions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=864"
  },
  {
    "text": "rather than relying upon default behaviors. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=868"
  },
  {
    "text": "And that's what we've done for our spring cleaning. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=871"
  },
  {
    "text": "Now let's talk about how we've made it easier to debug apps ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=874"
  },
  {
    "text": "that use the Vision framework. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=876"
  },
  {
    "text": "We've added Quick Look Preview support to Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=878"
  },
  {
    "text": "What does this mean for Vision in particular? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=881"
  },
  {
    "text": "Well, now you can mouse over VNObservations in the debugger, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=884"
  },
  {
    "text": "and with one click, you can visualize the result on your input image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=888"
  },
  {
    "text": "We've also made this available in Xcode Playgrounds. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=892"
  },
  {
    "text": "I think the only way to really explain how this can benefit your debugging ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=895"
  },
  {
    "text": "is to show you. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=899"
  },
  {
    "text": "Let's move to an Xcode demo.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=900"
  },
  {
    "text": "Here we have a simple routine that will detect face landmarks ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=904"
  },
  {
    "text": "and return the face observations. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=908"
  },
  {
    "text": "First, we set up a face landmarks request. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=911"
  },
  {
    "text": "Then, if we have an image ready to go in our class, we display it. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=915"
  },
  {
    "text": "Then, we declare an array to hold our results.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=920"
  },
  {
    "text": "Inside the autoreleasepool, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=926"
  },
  {
    "text": "we instantiate a request handler with that image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=928"
  },
  {
    "text": "and then perform our request. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=931"
  },
  {
    "text": "Assuming all went well, we can retrieve the results from the request.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=934"
  },
  {
    "text": "I will run it and get to a breakpoint after we retrieve the results. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=939"
  },
  {
    "text": "So now I'm in the debugger. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=944"
  },
  {
    "text": "When I mouse over the results, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=945"
  },
  {
    "text": "the overlay shows I've detected three faces. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=947"
  },
  {
    "text": "That's great. I do have three faces in my input image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=950"
  },
  {
    "text": "But how do I know which observation is which face? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=953"
  },
  {
    "text": "That's where the Quick Look Preview support comes in. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=956"
  },
  {
    "text": "As I go into this request, I can click on each \"eye\" icon ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=959"
  },
  {
    "text": "in order to visualize the result. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=964"
  },
  {
    "text": "The image appears with overlays drawn for the landmarks constellation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=967"
  },
  {
    "text": "and for the face bounding box.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=972"
  },
  {
    "text": "Now you know where the first observation is in the image.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=975"
  },
  {
    "text": "I can click on the next one to draw overlays for the second observation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=979"
  },
  {
    "text": "and for the third observation.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=983"
  },
  {
    "text": "Continuing to the next breakpoint, we run some code ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=987"
  },
  {
    "text": "that prints the face observations to the debug console. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=990"
  },
  {
    "text": "As you can imagine, here in the debug console ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=994"
  },
  {
    "text": "where the face information is printed, it's pretty hard ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=997"
  },
  {
    "text": "to immediately visualize in your mind which face is which ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1000"
  },
  {
    "text": "or whether the results look correct just from these printed coordinates.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1003"
  },
  {
    "text": "But there is one more thing to point out here. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1008"
  },
  {
    "text": "Notice that I've somewhat artificially forced the request handler ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1011"
  },
  {
    "text": "out of scope by introducing an autoreleasepool. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1014"
  },
  {
    "text": "Now that the request handler is out of scope, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1017"
  },
  {
    "text": "let's use the Quick Look Preview support again on the results. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1020"
  },
  {
    "text": "Well, what do you know, the overlays are still drawn, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1023"
  },
  {
    "text": "but the image is not available.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1026"
  },
  {
    "text": "This is something to keep in mind: the image request handler that was used ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1029"
  },
  {
    "text": "to generate the observations must still be in scope somewhere ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1033"
  },
  {
    "text": "in order for Quick Look Preview support to use the original image for display. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1036"
  },
  {
    "text": "That is because the image request handler is where your input image resides. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1041"
  },
  {
    "text": "Things will continue to work, but the image will not be available. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1045"
  },
  {
    "text": "This Quick Look preview support can be especially useful ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1048"
  },
  {
    "text": "in an Xcode Playgrounds session, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1051"
  },
  {
    "text": "while doing quick experiments to see how things work. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1054"
  },
  {
    "text": "Let's have a look at that now. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1057"
  },
  {
    "text": "Here we have a simple Playground set up to analyze images for barcodes. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1060"
  },
  {
    "text": "Rather than go through this code, let's just make a couple modifications ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1064"
  },
  {
    "text": "and check out how it impacts the results. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1067"
  },
  {
    "text": "We'll start off by using revision 2 ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1070"
  },
  {
    "text": "on an image with two barcodes of different symbologies. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1072"
  },
  {
    "text": "All the results at once are displayed if we ask for all the results, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1076"
  },
  {
    "text": "and just the first result is also displayed at the end.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1080"
  },
  {
    "text": "Notice that revision 2 has a couple issues. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1084"
  },
  {
    "text": "First, it missed the first barcode. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1087"
  },
  {
    "text": "Also, it detected the second barcode twice. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1090"
  },
  {
    "text": "And it gives you a line through the barcode ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1093"
  },
  {
    "text": "rather than a complete bounding box.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1095"
  },
  {
    "text": "What happens if we change to revision 3 now, instead of revision 2? ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1099"
  },
  {
    "text": "First of all, we detect both barcodes. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1106"
  },
  {
    "text": "And, instead of a line, we are given complete bounding boxes.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1108"
  },
  {
    "text": "What is great about this Quick Look Preview support ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1113"
  },
  {
    "text": "is that we've removed the need for you to write a variety of utility functions ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1115"
  },
  {
    "text": "to visualize the results. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1119"
  },
  {
    "text": "They can be overlaid directly on your images in the debugger ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1121"
  },
  {
    "text": "or in an Xcode Playground.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1124"
  },
  {
    "text": "So that is Quick Look Preview support in Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1129"
  },
  {
    "text": "Now you can more easily know which observation is which. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1134"
  },
  {
    "text": "Just be sure to keep the image request handler in scope ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1138"
  },
  {
    "text": "in order to use it with your input image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1140"
  },
  {
    "text": "and hopefully the Xcode Playground support will make live tuning ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1143"
  },
  {
    "text": "of your Vision framework code much easier. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1146"
  },
  {
    "text": "We've covered some important updates to Vision today. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1148"
  },
  {
    "text": "To quickly review, we've added some great new revisions ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1151"
  },
  {
    "text": "to text recognition, barcode detection, and optical flow.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1154"
  },
  {
    "text": "As we continue to add updated revisions, we will also be removing older ones, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1161"
  },
  {
    "text": "so keep your revisions up-to-date ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1166"
  },
  {
    "text": "and use the latest and greatest technology. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1167"
  },
  {
    "text": "We've also made debugging Vision applications much easier this year ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1170"
  },
  {
    "text": "with Quick Look Preview support. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1174"
  },
  {
    "text": "I hope you enjoyed this session, and have a wonderful WWDC.  ♪ ♪",
    "link": "https://developer.apple.com/videos/play/wwdc2022-10024/?time=1176"
  }
]