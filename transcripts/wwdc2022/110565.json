[
  {
    "text": "♪ ♪ ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=0"
  },
  {
    "text": "Ken Greenebaum: Hi everyone! Welcome to WWDC 2022. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=9"
  },
  {
    "text": "My name is Ken Greenebaum, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=13"
  },
  {
    "text": "and I'm with the Color and Display Technologies team at Apple. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=15"
  },
  {
    "text": "We are thrilled to have three EDR talks this year. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=18"
  },
  {
    "text": "Hope you've had an opportunity to watch \"Explore EDR on iOS,\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=21"
  },
  {
    "text": "where we announced EDR API support for iOS, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=25"
  },
  {
    "text": "as well as \"Display EDR content with Core Image, Metal, and SwiftUI.\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=29"
  },
  {
    "text": "Some of you may have also watched my EDR talk last year, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=33"
  },
  {
    "text": "where we demonstrated how to use AVPlayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=37"
  },
  {
    "text": "to play back HDR video, using EDR.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=40"
  },
  {
    "text": "In this talk we're gonna go deeper, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=44"
  },
  {
    "text": "and explore how to use Core Media interfaces to provide, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=46"
  },
  {
    "text": "not only EDR playback, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=49"
  },
  {
    "text": "but also how to decode and playback HDR video, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=51"
  },
  {
    "text": "into your own EDR layers or views.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=54"
  },
  {
    "text": "Then we'll continue beyond simply playing back content, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=59"
  },
  {
    "text": "to show how to access the decoded video frames in real time, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=62"
  },
  {
    "text": "via Core Video's display link, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=65"
  },
  {
    "text": "send those frames to CoreImage Filters, or a Metal Shader, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=68"
  },
  {
    "text": "to add color management, visual effects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=71"
  },
  {
    "text": "or apply other signal processing, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=73"
  },
  {
    "text": "and finally, plumb the resulting frames to Metal to render. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=75"
  },
  {
    "text": "We're going to start by reviewing the EDR compatible video media frameworks, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=80"
  },
  {
    "text": "to help you decide which best matches your application's requirements.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=84"
  },
  {
    "text": "Next we will briefly discuss the high level AVKit ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=90"
  },
  {
    "text": "and AVFoundation frameworks, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=92"
  },
  {
    "text": "that can do all of the work of playing HDR video, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=95"
  },
  {
    "text": "if your application requires straight forward playback.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=97"
  },
  {
    "text": "And finally, we'll discuss best practices ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=102"
  },
  {
    "text": "for using decoded video frames, with Core Video and Metal, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=104"
  },
  {
    "text": "in your EDR playback, editing, or image processing engine.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=108"
  },
  {
    "text": "Let's begin by taking a quick survey of Apple's video frameworks; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=114"
  },
  {
    "text": "Starting with the highest level interfaces; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=118"
  },
  {
    "text": "which are the easiest to use; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=121"
  },
  {
    "text": "and continuing to lower level frameworks that offer more opportunities, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=123"
  },
  {
    "text": "at the expense of adding complexity to your code. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=126"
  },
  {
    "text": "It is best to use the highest level framework possible ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=130"
  },
  {
    "text": "to take advantage of the optimizations provided automatically for you. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=133"
  },
  {
    "text": "This will get us ready to dive into the body of the talk, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=137"
  },
  {
    "text": "where we will be exploring a number of scenarios, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=140"
  },
  {
    "text": "from simple EDR playback ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=143"
  },
  {
    "text": "to more sophisticated plumbing of decoded video frames ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=144"
  },
  {
    "text": "to CoreImage or Metal for real time processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=148"
  },
  {
    "text": "At the highest level, there is AVKit. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=151"
  },
  {
    "text": "With AVKit you can create user interfaces for media playback; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=154"
  },
  {
    "text": "complete with transport controls, chapter navigation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=158"
  },
  {
    "text": "Picture in Picture support, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=161"
  },
  {
    "text": "and display of subtitles and closed captions. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=163"
  },
  {
    "text": "AVKit can playback HDR content as EDR, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=165"
  },
  {
    "text": "as we will demonstrate using AVPlayerViewController. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=169"
  },
  {
    "text": "However, if your application requires further processing of video frames, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=172"
  },
  {
    "text": "you will have to use a media framework ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=177"
  },
  {
    "text": "that can give you more control over your pipeline. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=179"
  },
  {
    "text": "Next there is AVFoundation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=181"
  },
  {
    "text": "AVFoundation is the full-featured framework ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=184"
  },
  {
    "text": "for working with time based audio visual media on Apple Platforms. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=187"
  },
  {
    "text": "Using AVFoundation, you can easily play, create, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=192"
  },
  {
    "text": "and edit QuickTime movies and MPEG 4 files, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=196"
  },
  {
    "text": "play HLS streams, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=199"
  },
  {
    "text": "and build powerful media functionality into your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=200"
  },
  {
    "text": "We'll be exploring use of AVPlayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=204"
  },
  {
    "text": "and the related AVPlayerLayer interface in this talk. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=206"
  },
  {
    "text": "Core Video is a framework that provides a pipeline model for digital video. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=210"
  },
  {
    "text": "It simplifies the way you work with videos ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=215"
  },
  {
    "text": "by partitioning the process into discrete steps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=217"
  },
  {
    "text": "Core Video also makes it easier for you to access and manipulate ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=220"
  },
  {
    "text": "individual frames, without having to worry about ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=223"
  },
  {
    "text": "translating between data types ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=226"
  },
  {
    "text": "or worrying about display synchronization. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=228"
  },
  {
    "text": "We'll be demonstrating use of DisplayLink, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=231"
  },
  {
    "text": "and CVPixelBuffer's with Core Image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=233"
  },
  {
    "text": "And CVMetalTextureCache, with Metal. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=236"
  },
  {
    "text": "Next there is Video Toolbox. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=239"
  },
  {
    "text": "This is a low-level framework that provides direct access ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=241"
  },
  {
    "text": "to hardware encoders and decoders. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=244"
  },
  {
    "text": "Video Toolbox provides services for video compression and decompression, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=246"
  },
  {
    "text": "and for conversion between raster image formats ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=250"
  },
  {
    "text": "stored in Core Video pixel buffers. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=253"
  },
  {
    "text": "VTDecompressionSession is a powerful low-level interface ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=255"
  },
  {
    "text": "that is outside of the scope of this talk, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=259"
  },
  {
    "text": "but advanced developers might want to investigate further. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=261"
  },
  {
    "text": "And finally, there is Core Media. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=264"
  },
  {
    "text": "This framework defines the media pipeline used by AVFoundation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=266"
  },
  {
    "text": "and the other high-level media frameworks. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=270"
  },
  {
    "text": "You can always use Core Media's low-level data types and interfaces ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=273"
  },
  {
    "text": "to efficiently process media samples ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=277"
  },
  {
    "text": "and manage queues of media data. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=279"
  },
  {
    "text": "In the remainder of this talk we will demonstrate how and when ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=281"
  },
  {
    "text": "to use these frameworks in your app. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=285"
  },
  {
    "text": "First, how to use AVKit and AVFoundation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=287"
  },
  {
    "text": "to easily playback HDR video rendered as EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=291"
  },
  {
    "text": "Then a series of more sophisticated applications of AVPlayer: ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=295"
  },
  {
    "text": "to render to your own layer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=299"
  },
  {
    "text": "to access individually decoded frames via CADisplayLink ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=301"
  },
  {
    "text": "and send the resulting CVPixelBuffers to Core Image for processing, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=305"
  },
  {
    "text": "and finally, accessing the decoded frames as Metal textures ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=309"
  },
  {
    "text": "via the CVMetalTextureCache ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=313"
  },
  {
    "text": "for processing and rendering in Metal. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=315"
  },
  {
    "text": "Now that we have an overview of the video media layer on Apple platforms, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=317"
  },
  {
    "text": "we'll focus on AVKit and AVFoundation frameworks. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=322"
  },
  {
    "text": "Let's get things started by first discussing playback ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=326"
  },
  {
    "text": "of your HDR video content ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=328"
  },
  {
    "text": "using AVFoundation's AVPlayer interface. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=330"
  },
  {
    "text": "An AVPlayer is a controller object, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=333"
  },
  {
    "text": "used to manage the playback and timing of a media asset. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=336"
  },
  {
    "text": "The AVPlayer interface can be used for high-performance playback of HDR video, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=339"
  },
  {
    "text": "automatically rendering the result as EDR when possible.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=344"
  },
  {
    "text": "With AVPlayer, you can play local, and remote file based media, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=348"
  },
  {
    "text": "such as QuickTime movies; ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=351"
  },
  {
    "text": "as well as streaming media, served using HLS. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=353"
  },
  {
    "text": "Essentially, AVPlayer is used to play one media asset at a time. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=358"
  },
  {
    "text": "You can reuse the player instance to serially play additional media assets, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=362"
  },
  {
    "text": "or even create multiple instances to play more than one asset simultaneously, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=367"
  },
  {
    "text": "but AVPlayer manages the playback of only a single media asset at a time. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=372"
  },
  {
    "text": "AVFoundation framework also provides a subclass of AVPlayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=377"
  },
  {
    "text": "called AVQueuePlayer that you can use to create ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=381"
  },
  {
    "text": "and manage the queuing and playing of sequential HDR media assets. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=384"
  },
  {
    "text": "If your application requires simple playback of HDR video media ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=389"
  },
  {
    "text": "rendered to EDR, then AVPlayer with AVPlayerViewController, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=393"
  },
  {
    "text": "may be the best approach. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=397"
  },
  {
    "text": "Use AVPlayer with AVPlayerLayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=399"
  },
  {
    "text": "to playback your own views on iOS or macOS.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=401"
  },
  {
    "text": "These are the most straightforward ways of using AVPlayer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=406"
  },
  {
    "text": "Let's look at examples of both. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=409"
  },
  {
    "text": "First we will look how you can use AVFoundation's AVPlayer interface, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=411"
  },
  {
    "text": "in conjunction with AVKit's AVPlayer View Controller. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=415"
  },
  {
    "text": "Here, we start by instantiating AVPlayer from the media's URL.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=419"
  },
  {
    "text": "Next we create an AVPlayerViewController, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=426"
  },
  {
    "text": "then set the player property of our viewer controller ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=429"
  },
  {
    "text": "to the player we just created from the media's URL.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=432"
  },
  {
    "text": "And present the view controller modally to start playback of the video. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=438"
  },
  {
    "text": "AVKit manages all the details for you ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=443"
  },
  {
    "text": "and will automatically play back HDR Video as EDR ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=445"
  },
  {
    "text": "on displays supporting EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=449"
  },
  {
    "text": "As I mentioned, some applications will need to play back HDR video media ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=451"
  },
  {
    "text": "into their own view. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=455"
  },
  {
    "text": "Let's look at how to accomplish this using AVPlayer with AVPlayerLayer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=457"
  },
  {
    "text": "To play HDR video media as EDR in your own view, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=462"
  },
  {
    "text": "we again start by creating an AVPlayer with the media's URL. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=466"
  },
  {
    "text": "However this time we instantiate an AVPlayerLayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=471"
  },
  {
    "text": "with the player we just created. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=474"
  },
  {
    "text": "Next we need to set the bounds on the player layer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=477"
  },
  {
    "text": "which we get from the view. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=480"
  },
  {
    "text": "Now that the player layer has the bounds from the view, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=482"
  },
  {
    "text": "we can add the player layer as a sublayer to the view. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=485"
  },
  {
    "text": "Finally, to play back the HDR video media, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=490"
  },
  {
    "text": "we call AVPlayer's play method. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=492"
  },
  {
    "text": "That's all that is needed to play back HDR video media as EDR ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=495"
  },
  {
    "text": "in your own layer using AVPlayer and AVPlayerLayer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=499"
  },
  {
    "text": "We just explored the two most straightforward ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=504"
  },
  {
    "text": "HDR video playback workflows using AVPlayer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=506"
  },
  {
    "text": "However, many applications require more than simple media playback.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=509"
  },
  {
    "text": "For example, an application might require image processing, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=515"
  },
  {
    "text": "such as color grading or chroma keying to be applied to the video. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=518"
  },
  {
    "text": "Let's explore a workflow that gets decoded video frames from AVPlayer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=523"
  },
  {
    "text": "applies Core Image filters or Metal shaders in real time, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=528"
  },
  {
    "text": "and renders the results as EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=531"
  },
  {
    "text": "We will be demonstrating how to use AVPlayer and the AVPlayerItem ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=535"
  },
  {
    "text": "to decode EDR frames from your HDR video media, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=539"
  },
  {
    "text": "access the decoded frames from the Core Video display link, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=543"
  },
  {
    "text": "send the resulting pixel buffers to Core Image or Metal for processing, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=546"
  },
  {
    "text": "then render the results in a CAMetalLayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=551"
  },
  {
    "text": "as EDR on displays with EDR support. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=554"
  },
  {
    "text": "With this in mind, let's first demonstrate ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=557"
  },
  {
    "text": "setting a few key properties on the CAMetalLayer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=560"
  },
  {
    "text": "which are required to ensure HDR media will render correctly as EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=563"
  },
  {
    "text": "First we need to get the CAMetalLayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=568"
  },
  {
    "text": "that we will be rendering the HDR video content to. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=570"
  },
  {
    "text": "On that layer we opt into EDR by setting ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=574"
  },
  {
    "text": "the wantsExtendedDynamicRangeContent flag to true.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=577"
  },
  {
    "text": "Please be sure to use a pixel format that supports Extended Dynamic Range content. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=582"
  },
  {
    "text": "For the AVPlayer example that follows, we will set the CAMetalLayer to use ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=588"
  },
  {
    "text": "a half float pixel format, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=592"
  },
  {
    "text": "however a ten bit format used in conjunction with a PQ ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=594"
  },
  {
    "text": "or HLG transfer function would also work. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=598"
  },
  {
    "text": "To avoid limiting the result to SDR, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=601"
  },
  {
    "text": "we also need to set the layer to an EDR compatible ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=603"
  },
  {
    "text": "extended range color space.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=606"
  },
  {
    "text": "In our examples we will be setting the half float metal texture ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=610"
  },
  {
    "text": "to the extended linear display P3 color space. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=613"
  },
  {
    "text": "We just scratched the surface regarding EDR, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=618"
  },
  {
    "text": "color spaces, and pixel buffer formats. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=620"
  },
  {
    "text": "You might want to check out my session from last year, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=623"
  },
  {
    "text": "\"HDR rendering with EDR,\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=625"
  },
  {
    "text": "as well as this year's \"EDR on iOS,\" for more details.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=627"
  },
  {
    "text": "Now that we have set the basic properties on the CAMetalLayer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=633"
  },
  {
    "text": "let's continue the demonstration by adding real time image processing ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=636"
  },
  {
    "text": "using a Core Image, or Metal shader. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=639"
  },
  {
    "text": "We'll be using a display link in conjunction with AVPlayer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=642"
  },
  {
    "text": "to access the decoded video frames in real time.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=645"
  },
  {
    "text": "For this workflow, you start by creating an AVPlayer from an AVPlayerItem. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=649"
  },
  {
    "text": "Next, you instantiate an AVPlayerItemVideoOutput, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=653"
  },
  {
    "text": "configured with appropriate pixel buffer format and color space for EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=657"
  },
  {
    "text": "Then you create and configure a Display link. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=662"
  },
  {
    "text": "And lastly, you run the Display link to get the pixel buffers ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=665"
  },
  {
    "text": "to Core Image or Metal for processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=668"
  },
  {
    "text": "We will demonstrate a CADisplayLink as is used on iOS. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=671"
  },
  {
    "text": "Please use the equivalent CVDisplayLink interface when developing for macOS. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=676"
  },
  {
    "text": "This time we choose to create an AVPlayerItem ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=681"
  },
  {
    "text": "from our media's URL, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=684"
  },
  {
    "text": "and instantiate an AVPlayer with the AVPlayerItem that we just created. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=686"
  },
  {
    "text": "Now we create a pair of dictionaries to specify the color space ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=692"
  },
  {
    "text": "and pixel buffer format of the decoded frames. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=695"
  },
  {
    "text": "The first dictionary, videoColorProperties, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=699"
  },
  {
    "text": "is where the color space and transfer function are specified. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=701"
  },
  {
    "text": "In this example we request the Display P3 colorspace, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=705"
  },
  {
    "text": "which corresponds to the color space of most Apple displays, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=708"
  },
  {
    "text": "and the linear transfer function which allows AVFoundation ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=711"
  },
  {
    "text": "to maintain the extended range values required for EDR.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=715"
  },
  {
    "text": "The second dictionary, outputVideoSettings, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=720"
  },
  {
    "text": "specifies the characteristics of the pixel buffer format ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=722"
  },
  {
    "text": "and also provides a reference to the videoColorProperties dictionary ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=726"
  },
  {
    "text": "we just created. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=728"
  },
  {
    "text": "In this example, we request wide color and the half float pixel buffer format. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=731"
  },
  {
    "text": "It is very helpful that AVPlayerItemVideoOutput, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=737"
  },
  {
    "text": "not only decodes video into the pixel buffer format we specify ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=740"
  },
  {
    "text": "in the output settings dictionary, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=744"
  },
  {
    "text": "but also automatically performs any color conversion required ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=746"
  },
  {
    "text": "via a pixel transfer session. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=750"
  },
  {
    "text": "Recall, a video might contain multiple clips, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=754"
  },
  {
    "text": "potentially with different colorspaces. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=756"
  },
  {
    "text": "AVFoundation automatically manages these for us, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=759"
  },
  {
    "text": "and as we'll soon be demonstrating, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=762"
  },
  {
    "text": "this behavior also allows the resulting decoded video frames ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=765"
  },
  {
    "text": "to be sent to low level frameworks like Metal ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=768"
  },
  {
    "text": "that don't themselves provide automatic colorspace conversion ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=771"
  },
  {
    "text": "to the display's colorspace.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=774"
  },
  {
    "text": "Now we create the AVPlayerItemVideoOutput ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=777"
  },
  {
    "text": "with the outputVideoSettings dictionary. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=780"
  },
  {
    "text": "As the third step, we setup the Display link, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=783"
  },
  {
    "text": "which will be used to access the decoded frames in real time. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=786"
  },
  {
    "text": "CADisplayLink takes a call back that is run on each display update. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=790"
  },
  {
    "text": "In our example we call a local function that we will explore in a moment ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=795"
  },
  {
    "text": "to get the CVPixelBuffers that we will be sending to Core Image for processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=799"
  },
  {
    "text": "Next we create a video player item observer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=804"
  },
  {
    "text": "to allow us to handle changes to specified player Item properties.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=807"
  },
  {
    "text": "Our example will execute this code ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=813"
  },
  {
    "text": "every time for the player item's status changes.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=815"
  },
  {
    "text": "When the player item's status changes to readyToPlay, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=821"
  },
  {
    "text": "we add our AVPlayerItemVideoOutput ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=824"
  },
  {
    "text": "to the new AVPlayerItem that was just returned, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=827"
  },
  {
    "text": "register CADisplayLink with the main run loop set to common mode, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=832"
  },
  {
    "text": "and start the real time decoding of the HDR video ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=837"
  },
  {
    "text": "by calling play on the video player.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=839"
  },
  {
    "text": "Finally, we will take a look at an example CADisplayLink callback implementation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=844"
  },
  {
    "text": "which we referred to earlier as the `displayLinkCopyPixelBuffers` ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=848"
  },
  {
    "text": "local function. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=852"
  },
  {
    "text": "Once the HDR video begins to play, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=855"
  },
  {
    "text": "the CADisplayLink callback function is called on each display refresh. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=857"
  },
  {
    "text": "For instance it might be called 60 times a second for a typical display. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=862"
  },
  {
    "text": "This is our code's opportunity to update the frame displayed ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=867"
  },
  {
    "text": "if there is a new CVPixelBuffer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=870"
  },
  {
    "text": "On each display callback, we attempt to copy a CVPixelBuffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=874"
  },
  {
    "text": "containing the decoded video frame to be displayed ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=877"
  },
  {
    "text": "at the current wall clock time. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=880"
  },
  {
    "text": "However, the `copyPixelBuffer` call might fail, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=883"
  },
  {
    "text": "as there won't always be a new CVPixelBuffer available ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=887"
  },
  {
    "text": "at every display refresh, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=890"
  },
  {
    "text": "especially when the screen refresh rate exceeds that of the video being played. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=892"
  },
  {
    "text": "If there is not a new CVPixelBuffer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=896"
  },
  {
    "text": "then the call fails and we skip the render. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=899"
  },
  {
    "text": "This causes the preceding frame to remain on-screen for another display refresh. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=901"
  },
  {
    "text": "But if the copy succeeds, then we have a fresh frame of video ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=906"
  },
  {
    "text": "in a CVPixelBuffer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=910"
  },
  {
    "text": "There are a number of ways that we might process and render this new frame. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=912"
  },
  {
    "text": "One opportunity is to send the CVPixelBuffer to Core Image ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=916"
  },
  {
    "text": "for processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=919"
  },
  {
    "text": "Core Image can string together one or more CIFilters ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=921"
  },
  {
    "text": "to provide GPU accelerated image processing to the video frame.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=924"
  },
  {
    "text": "Please note that not all CIFilters are compatible with EDR ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=929"
  },
  {
    "text": "and might have trouble with HDR content, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=933"
  },
  {
    "text": "including clamping to SDR or worse. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=935"
  },
  {
    "text": "Core Image provides many EDR compatible Filters. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=938"
  },
  {
    "text": "Use filter names with CICategoryHighDynamicRange, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=942"
  },
  {
    "text": "to enumerate EDR compatible Core Image filters. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=946"
  },
  {
    "text": "In our example, we will be adding a simple sepia tone effect. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=949"
  },
  {
    "text": "Now let's return to our example and integrate Core Image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=953"
  },
  {
    "text": "On each display link callback that yields a fresh CVPixelBuffer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=958"
  },
  {
    "text": "create a CIImage from that pixel buffer.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=961"
  },
  {
    "text": "Instance the CIFilter to implement the desired effect. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=966"
  },
  {
    "text": "I am using the sepia tone filter because of its parameter-less simplicity, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=969"
  },
  {
    "text": "however there are many CIFilters built into the system, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=973"
  },
  {
    "text": "and it is straightforward to write your own, too. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=976"
  },
  {
    "text": "Set the CIFilter's inputImage to the CIImage we just created.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=980"
  },
  {
    "text": "And the processed video result will be available ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=986"
  },
  {
    "text": "in the filter's outputImage. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=989"
  },
  {
    "text": "Chain as many CIFilters together as are required to achieve your desired effect. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=992"
  },
  {
    "text": "Then use a CIRenderDestination ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=997"
  },
  {
    "text": "to render the resulting image to your application's view code.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=999"
  },
  {
    "text": "Please refer to the WWDC 2020 talk \"Optimize the Core Image pipeline for your video app\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1004"
  },
  {
    "text": "to learn more about this workflow. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1010"
  },
  {
    "text": "Another opportunity, is to process and render the fresh CVPixelBuffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1011"
  },
  {
    "text": "using Metal and custom Metal shaders. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1015"
  },
  {
    "text": "We will briefly describe the process of converting the CVPixelBuffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1019"
  },
  {
    "text": "to a Metal texture. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1022"
  },
  {
    "text": "However, implementing this conversion maintaining best performance ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1024"
  },
  {
    "text": "is a deep topic best left for another talk. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1027"
  },
  {
    "text": "We instead recommend deriving the Metal texture ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1030"
  },
  {
    "text": "from the CoreVideo Metal texture cache, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1033"
  },
  {
    "text": "and will walk through that process as the final example in this talk. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1035"
  },
  {
    "text": "Generally speaking, the process is to get the IOSurface from the CVPixelBuffer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1039"
  },
  {
    "text": "create a MetalTextureDescriptor, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1044"
  },
  {
    "text": "and then create a MetalTexture from the MetalDevice, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1047"
  },
  {
    "text": "using `newTextureWithDescriptor`.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1049"
  },
  {
    "text": "However, there is a danger that the textures may be re-used, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1053"
  },
  {
    "text": "and over-drawn, if careful locking is not applied. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1057"
  },
  {
    "text": "Further, not all PixelBuffer formats are natively supported by MetalTexture, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1061"
  },
  {
    "text": "which is why we use half float in this example. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1065"
  },
  {
    "text": "Because of these complications, we instead recommend ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1069"
  },
  {
    "text": "directly accessing Metal textures from Core Video, as we will now demonstrate. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1072"
  },
  {
    "text": "Let's further explore Core Video and Metal. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1076"
  },
  {
    "text": "As mentioned, CVMetalTextureCache is both a straightforward ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1080"
  },
  {
    "text": "and efficient way to use CVPixelBuffers with Metal. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1083"
  },
  {
    "text": "CVMetalTextureCache is handy because you get a Metal texture ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1087"
  },
  {
    "text": "directly from the cache without need for further conversion. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1090"
  },
  {
    "text": "CVMetalTextureCache automatically bridges between ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1094"
  },
  {
    "text": "CVPixelBuffer's, and MetalTexture's, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1098"
  },
  {
    "text": "thereby both simplifying your code and keeping you on the fast path. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1101"
  },
  {
    "text": "In conjunction with CVPixelBufferPools, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1106"
  },
  {
    "text": "CVMetalTextureCache also provides performance benefits, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1108"
  },
  {
    "text": "by keeping MTLTexture to IOSurface mapping alive.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1112"
  },
  {
    "text": "Finally, using CVMetalTextureCache removes the need ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1117"
  },
  {
    "text": "to manually track IOSurfaces. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1121"
  },
  {
    "text": "Now the final example in our talk: ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1123"
  },
  {
    "text": "how to extract Metal textures directly from Core Video ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1126"
  },
  {
    "text": "using CVMetalTextureCache.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1129"
  },
  {
    "text": "Here, we start by getting the system default Metal device. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1132"
  },
  {
    "text": "We use that to create a Metal Texture Cache, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1135"
  },
  {
    "text": "and then instantiate a Core Video Metal Texture Cache ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1138"
  },
  {
    "text": "associated with the Metal Texture Cache. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1141"
  },
  {
    "text": "That can then be used to access decoded video frames as Metal Textures, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1144"
  },
  {
    "text": "which conveniently, can be directly used in our Metal engine. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1148"
  },
  {
    "text": "In this example, we create and use the Metal system default device. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1153"
  },
  {
    "text": "Next we create the CVMetalTextureCache ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1158"
  },
  {
    "text": "with CVMetalTextureCacheCreate, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1161"
  },
  {
    "text": "specifying the Metal device we just created. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1163"
  },
  {
    "text": "We get the height and width of the CVPixelBuffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1167"
  },
  {
    "text": "needed to create the Core Video Metal texture. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1170"
  },
  {
    "text": "Then we call `CVMetalTextureCacheCreateTextureFromImage`, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1173"
  },
  {
    "text": "to instantiate a CVMetalTexture object ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1177"
  },
  {
    "text": "and associate that with the CVPixelBuffer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1180"
  },
  {
    "text": "Finally we call `CVMetalTextureGetTexture`, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1183"
  },
  {
    "text": "to get the desired Metal texture. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1186"
  },
  {
    "text": "Swift applications should use a strong reference for CVMetalTexture, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1190"
  },
  {
    "text": "however, when using Objective-C, you must ensure that Metal is done ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1194"
  },
  {
    "text": "with your texture before you release the CVMetalTextureRef. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1197"
  },
  {
    "text": "This may be accomplished using metal command buffer completion handlers.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1201"
  },
  {
    "text": "And that's all, folks! ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1207"
  },
  {
    "text": "To review, we explored a number of workflows ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1209"
  },
  {
    "text": "that will render your HDR video media to EDR, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1211"
  },
  {
    "text": "for playback, editing, or image processing.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1214"
  },
  {
    "text": "You learned how to go from AVPlayer to AVKit's AVPlayerViewController, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1218"
  },
  {
    "text": "for playback of HDR media. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1223"
  },
  {
    "text": "You also learned how use AVPlayer, along with AVPlayerLayer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1226"
  },
  {
    "text": "to display HDR media on your own view. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1230"
  },
  {
    "text": "And finally, we explored how to add real time effects during playback. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1234"
  },
  {
    "text": "Connecting AVFoundation's AVPlayer to CoreVideo ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1238"
  },
  {
    "text": "and then to Metal for rendering. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1242"
  },
  {
    "text": "And applying real time effects using CoreImage filters, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1244"
  },
  {
    "text": "as well as Metal shaders.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1246"
  },
  {
    "text": "If you want to dig deeper, I recommend a few WWDC sessions ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1251"
  },
  {
    "text": "related to creating video workflows, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1255"
  },
  {
    "text": "as well as integrating HDR media with EDR. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1258"
  },
  {
    "text": "I especially want to call out the session ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1262"
  },
  {
    "text": "\"Edit and play back HDR video with AVFoundation\". ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1264"
  },
  {
    "text": "This session explores use of AVVideoComposition ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1268"
  },
  {
    "text": "with `applyingCIFiltersWithHandler` for applying effects to your HDR media. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1271"
  },
  {
    "text": "In this session you'll also learn how to use custom compositor, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1277"
  },
  {
    "text": "which can then be used with a CVPixelBuffer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1280"
  },
  {
    "text": "when each video frame becomes available for processing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1283"
  },
  {
    "text": "As I mentioned at the beginning, this year we're also presenting ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1286"
  },
  {
    "text": "two other sessions on EDR: \"EDR on iOS,\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1289"
  },
  {
    "text": "where we announced EDR API support has expanded to include iOS, ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1293"
  },
  {
    "text": "and \"HDR content display with EDR using CoreImage, Metal and SwiftUI,\" ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1298"
  },
  {
    "text": "where we further explore integrating EDR with other media frameworks. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1304"
  },
  {
    "text": "Hope you incorporate HDR video into your EDR enabled applications ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1309"
  },
  {
    "text": "on both macOS and now iOS. ",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1312"
  },
  {
    "text": "Thanks for watching.",
    "link": "https://developer.apple.com/videos/play/wwdc2022-110565/?time=1316"
  }
]