[
  {
    "text": "♪ ♪ ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=0"
  },
  {
    "text": "Lizzy: Hi! I'm Lizzy, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=10"
  },
  {
    "text": "and I'm an engineer working on VisionKit here at Apple. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=12"
  },
  {
    "text": "I'm excited to talk to you today ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=15"
  },
  {
    "text": "about how to bring subject lifting into your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=16"
  },
  {
    "text": "Subject lifting was introduced in iOS 16, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=20"
  },
  {
    "text": "allowing users to select, lift, and share image subjects. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=22"
  },
  {
    "text": "First, I'll go over the basics of what subject lifting is. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=28"
  },
  {
    "text": "Then, I'll walk you through how to add subject lifting using new VisionKit API. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=32"
  },
  {
    "text": "Finally, my colleague Saumitro will dive deeper ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=39"
  },
  {
    "text": "and introduce new underlying Vision API.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=41"
  },
  {
    "text": "So what exactly is a subject? ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=46"
  },
  {
    "text": "A subject is the foreground object, or objects, of a photo. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=49"
  },
  {
    "text": "This is not always a person or a pet. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=54"
  },
  {
    "text": "It can be anything from a building, a plate of food, or some pairs of shoes.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=57"
  },
  {
    "text": "Images can have multiple subjects, like these three cups of coffee here. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=63"
  },
  {
    "text": "It's important to note that subjects are not always an individual object. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=68"
  },
  {
    "text": "In this example, the man and his dog together ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=73"
  },
  {
    "text": "are the focal point of the image, making them one combined subject. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=76"
  },
  {
    "text": "So how can you get this into an app? ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=80"
  },
  {
    "text": "There are two separate APIs available to help you add subject lifting to your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=83"
  },
  {
    "text": "VisionKit. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=88"
  },
  {
    "text": "And Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=90"
  },
  {
    "text": "VisionKit allows you to very easily adopt system-like subject lifting behavior, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=91"
  },
  {
    "text": "right out of the box. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=96"
  },
  {
    "text": "You can easily recreate the subject lifting UI ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=98"
  },
  {
    "text": "that we all know and love, with just a few lines of code. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=101"
  },
  {
    "text": "VisionKit also exposes some basic information about these subjects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=105"
  },
  {
    "text": "so you can give people new ways to interact with image subjects.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=109"
  },
  {
    "text": "This all happens out-of-process, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=114"
  },
  {
    "text": "which has performance benefits ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=116"
  },
  {
    "text": "but means the image size is limited. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=117"
  },
  {
    "text": "Vision is a lower-level framework and doesn't have out-of-the-box UI. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=120"
  },
  {
    "text": "This means it's not tied to a view, giving you more flexibility.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=125"
  },
  {
    "text": "Image analysis happens in-process, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=130"
  },
  {
    "text": "and isn't as limited in image resolution as VisionKit. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=132"
  },
  {
    "text": "Finally, this API can be part of more advanced image editing pipelines, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=137"
  },
  {
    "text": "such as those using CoreImage. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=141"
  },
  {
    "text": "First, let's dive into subject lifting API in VisionKit. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=144"
  },
  {
    "text": "To add subject lifting with VisionKit, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=148"
  },
  {
    "text": "simply initialize an ImageAnalysisInteraction ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=151"
  },
  {
    "text": "and add it to a view containing the image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=154"
  },
  {
    "text": "This can be a UIImageView, but doesn't need to be. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=156"
  },
  {
    "text": "It's that simple. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=160"
  },
  {
    "text": "Now, your image will have system subject lifting interactions. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=161"
  },
  {
    "text": "Similarly, on macOS, create a ImageAnalysisOverlayView, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=165"
  },
  {
    "text": "and add it as a subview of the NSView that contains your image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=168"
  },
  {
    "text": "You can set the preferred interaction types ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=172"
  },
  {
    "text": "of your ImageAnalysisInteraction ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=175"
  },
  {
    "text": "or ImageAnalysisOverlayView ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=177"
  },
  {
    "text": "to choose which types of VisionKit interactions to support. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=179"
  },
  {
    "text": "The default interaction type is .automatic, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=183"
  },
  {
    "text": "which mirrors system behavior. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=185"
  },
  {
    "text": "Use this type if you want subject lifting, live text, and data detectors. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=187"
  },
  {
    "text": "The new imageSubject type only includes subject lifting, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=192"
  },
  {
    "text": "for cases when you don't want the text to be interactive. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=196"
  },
  {
    "text": "In addition to these UI interactions, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=199"
  },
  {
    "text": "VisionKit also allows you to programmatically access ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=201"
  },
  {
    "text": "the subjects of an image using an ImageAnalysis. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=204"
  },
  {
    "text": "To generate an image analysis, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=207"
  },
  {
    "text": "simply create an ImageAnalyzer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=210"
  },
  {
    "text": "then call the analyze function. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=212"
  },
  {
    "text": "Pass in the desired image and analyzer configuration. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=214"
  },
  {
    "text": "You can asynchronously access a list of all of the image's subjects ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=219"
  },
  {
    "text": "using the subjects property of an ImageAnalysis. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=222"
  },
  {
    "text": "This uses the new Subject struct, which contains an image and its bounds. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=226"
  },
  {
    "text": "The highlighted subjects property returns a set of the highlighted subjects. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=231"
  },
  {
    "text": "In this example, the bottom two subjects have been highlighted. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=237"
  },
  {
    "text": "Users can highlight a subject by long-pressing on it, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=241"
  },
  {
    "text": "but you can also change the selection state ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=244"
  },
  {
    "text": "by updating the highlightedSubjects set in code. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=246"
  },
  {
    "text": "You can look up a subject by point, using the async subject(at:) method. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=249"
  },
  {
    "text": "In this example, tapping here would return the middle subject. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=254"
  },
  {
    "text": "If there is no subject at that point, this method will return nil. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=259"
  },
  {
    "text": "Finally, you can generate subject images in two ways. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=263"
  },
  {
    "text": "For a single subject, just access the subject's image property. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=267"
  },
  {
    "text": "If you need an image composed of multiple subjects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=271"
  },
  {
    "text": "use the async image(for:) method, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=274"
  },
  {
    "text": "and pass along the subjects that you'd like to include. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=276"
  },
  {
    "text": "In this example, if I want an image of just the bottom two subjects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=280"
  },
  {
    "text": "I can use this method to generate this image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=285"
  },
  {
    "text": "Let's see this all come together in a demo. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=288"
  },
  {
    "text": "I'm working on a puzzle app. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=290"
  },
  {
    "text": "I want to drag the pieces to the puzzle, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=293"
  },
  {
    "text": "but I can't lift any of them yet. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=295"
  },
  {
    "text": "Let's fix that. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=297"
  },
  {
    "text": "First, I'll need to enable subject lifting interactions in this image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=298"
  },
  {
    "text": "so the pieces can be interacted with. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=302"
  },
  {
    "text": "I can do this by creating an ImageAnalysisInteraction... ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=305"
  },
  {
    "text": "...and simply adding it to my view.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=310"
  },
  {
    "text": "I used the imageSubject interaction type here, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=314"
  },
  {
    "text": "since I don't need to include live text.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=317"
  },
  {
    "text": "Awesome! ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=326"
  },
  {
    "text": "Now I can select puzzle pieces and interact with them like this. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=327"
  },
  {
    "text": "This image wasn't pre-processed in any way. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=330"
  },
  {
    "text": "This is done with just subject lifting. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=333"
  },
  {
    "text": "I added some code to handle dropping puzzle pieces into the puzzle, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=336"
  },
  {
    "text": "and even adjusts them into place.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=339"
  },
  {
    "text": "It's looking pretty cool, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=348"
  },
  {
    "text": "but I want to make my app feel even more engaging. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=349"
  },
  {
    "text": "I'm thinking of adding a drop shadow below each puzzle piece ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=352"
  },
  {
    "text": "as I hover over it, to give a slight 3D effect. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=355"
  },
  {
    "text": "I already have a hover gesture handler here, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=359"
  },
  {
    "text": "I just need to add the shadow. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=362"
  },
  {
    "text": "I can't easily edit the image, so instead, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=365"
  },
  {
    "text": "I'm going to do it with an image layering trick. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=367"
  },
  {
    "text": "First, I check that I'm hovering over a subject ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=370"
  },
  {
    "text": "by calling imageAnalysis.subject(at point:). ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=373"
  },
  {
    "text": "I have a method addShadow(for subject:), ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=378"
  },
  {
    "text": "which inserts a copy of the subject image, turns it grey, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=382"
  },
  {
    "text": "and offsets it slightly from the original subject position. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=386"
  },
  {
    "text": "Then, I add a copy of the subject image on top of the shadow ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=389"
  },
  {
    "text": "so it looks three dimensional. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=393"
  },
  {
    "text": "Finally, if the hover point didn't intersect with a subject, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=396"
  },
  {
    "text": "I clear the shadow.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=399"
  },
  {
    "text": "Let's try it out.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=404"
  },
  {
    "text": "Awesome. The pieces now get a shadow effect ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=409"
  },
  {
    "text": "when I hover over them.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=411"
  },
  {
    "text": "Using VisionKit, I was able to set up subject lifting in my app, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=420"
  },
  {
    "text": "and even add a fun subject effect with just a few lines of code.",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=424"
  },
  {
    "text": "Next, I'll pass things along to my colleague Saumitro, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=430"
  },
  {
    "text": "who will talk about some new Vision API and how to integrate it into your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=433"
  },
  {
    "text": "Saumitro: Thanks, Lizzy! ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=437"
  },
  {
    "text": "Hi, I'm Saumitro, and I'm an engineer on the Vision Team. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=439"
  },
  {
    "text": "VisionKit's API is the easiest way to get started with subject lifting. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=443"
  },
  {
    "text": "For applications that need more advanced features, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=448"
  },
  {
    "text": "Vision has you covered. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=450"
  },
  {
    "text": "Subject lifting joins Vision's existing collection of segmentation APIs ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=452"
  },
  {
    "text": "like saliency and person segmentation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=457"
  },
  {
    "text": "Let's quickly review each of their strengths ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=460"
  },
  {
    "text": "and see how subject lifting fits in. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=462"
  },
  {
    "text": "Saliency requests, like the ones for attention and objectness, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=465"
  },
  {
    "text": "are best used for coarse, region-based analysis. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=468"
  },
  {
    "text": "Notice that the generated saliency maps are at a fairly low resolution ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=472"
  },
  {
    "text": "and as such, not suitable for segmentation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=476"
  },
  {
    "text": "Instead, you could use the salient regions for tasks like auto-cropping an image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=479"
  },
  {
    "text": "The person segmentation API shines at producing detailed segmentation masks ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=484"
  },
  {
    "text": "for people in the scene. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=489"
  },
  {
    "text": "Use this if you specifically want to focus on segmenting people. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=491"
  },
  {
    "text": "The new person instance segmentation API takes things further ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=496"
  },
  {
    "text": "by providing a separate mask for each person in the scene. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=500"
  },
  {
    "text": "To learn more, check out this session on person segmentation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=504"
  },
  {
    "text": "In contrast to person segmentation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=508"
  },
  {
    "text": "the newly introduced subject lifting API is \"class agnostic\". ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=510"
  },
  {
    "text": "Any foreground object, regardless of its semantic class, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=515"
  },
  {
    "text": "can be potentially segmented. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=518"
  },
  {
    "text": "For instance, notice how it picks up the car ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=520"
  },
  {
    "text": "in addition to the people in this image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=523"
  },
  {
    "text": "Now let's take a look at some of the key concepts involved. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=526"
  },
  {
    "text": "You start with an input image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=529"
  },
  {
    "text": "The subject lifting request processes this image ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=531"
  },
  {
    "text": "and produces a soft segmentation mask at the same resolution. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=535"
  },
  {
    "text": "Taking this mask and applying it to the source image ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=539"
  },
  {
    "text": "results in the masked image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=542"
  },
  {
    "text": "Each distinct segmented object is referred to as an instance. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=544"
  },
  {
    "text": "Vision also provides you with pixelwise information ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=549"
  },
  {
    "text": "about these instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=552"
  },
  {
    "text": "This instance mask maps pixels in the source image to their instance index. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=554"
  },
  {
    "text": "The zero index is reserved for the background, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=559"
  },
  {
    "text": "and then each foreground instance is labeled sequentially, starting at 1. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=562"
  },
  {
    "text": "Beyond being contiguously labeled, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=567"
  },
  {
    "text": "the ordering of these IDs is not guaranteed. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=569"
  },
  {
    "text": "You can use these indices to segment a subset of the foreground objects ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=573"
  },
  {
    "text": "in the source image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=577"
  },
  {
    "text": "If you're designing an interactive app, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=579"
  },
  {
    "text": "this instance mask is also useful for hit testing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=581"
  },
  {
    "text": "I'll demonstrate how to do both of these tasks in a bit. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=584"
  },
  {
    "text": "Let's dive into the API. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=587"
  },
  {
    "text": "Subject lifting follows the familiar pattern ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=590"
  },
  {
    "text": "of image-based requests in Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=592"
  },
  {
    "text": "You start by instantiating the foreground instance mask request, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=594"
  },
  {
    "text": "followed by an image request handler with your input image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=598"
  },
  {
    "text": "You then perform the request. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=603"
  },
  {
    "text": "Under the hood, this is when Vision analyzes the image ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=605"
  },
  {
    "text": "to figure out the subject. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=608"
  },
  {
    "text": "While it's optimized to take advantage of Apple's hardware for efficiency, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=611"
  },
  {
    "text": "it's still a resource intensive task ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=614"
  },
  {
    "text": "and best deferred to a background thread so as not to block the UI. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=617"
  },
  {
    "text": "One common way to do this is to perform this step asynchronously ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=621"
  },
  {
    "text": "on a separate DispatchQueue. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=624"
  },
  {
    "text": "If one or more subjects are detected in the input image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=627"
  },
  {
    "text": "the results array will be populated with a single observation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=631"
  },
  {
    "text": "From here on, you can query the observation for masks ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=635"
  },
  {
    "text": "and segmented images. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=638"
  },
  {
    "text": "Let's take a closer look at two of the parameters ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=640"
  },
  {
    "text": "that control which instances are segmented ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=642"
  },
  {
    "text": "and how the results are cropped. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=645"
  },
  {
    "text": "The instances parameter is an IndexSet that controls ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=647"
  },
  {
    "text": "which objects are extracted in the final segmented image or mask. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=650"
  },
  {
    "text": "As an example, this image contains two foreground instances, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=655"
  },
  {
    "text": "not including the background instance. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=659"
  },
  {
    "text": "Since segmenting all detected foreground instances is a very common operation, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=661"
  },
  {
    "text": "Vision provides a handy allInstances property that returns ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=667"
  },
  {
    "text": "an IndexSet containing all foreground instance indices. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=671"
  },
  {
    "text": "For the image here, this includes indices 1 and 2. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=675"
  },
  {
    "text": "Note that the background instance 0 is not included. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=679"
  },
  {
    "text": "You can also provide just a subset of those indices. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=683"
  },
  {
    "text": "Here's just instance 1. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=686"
  },
  {
    "text": "And just instance 2. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=689"
  },
  {
    "text": "You can also control how the final masked image is cropped. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=691"
  },
  {
    "text": "If this parameter is set to false, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=694"
  },
  {
    "text": "the output image resolution matches the input image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=697"
  },
  {
    "text": "This is nice when you want to preserve the relative location ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=700"
  },
  {
    "text": "of the segmented objects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=703"
  },
  {
    "text": "say, for downstream compositing operations. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=705"
  },
  {
    "text": "If you set it to true, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=708"
  },
  {
    "text": "you get a tight crop for the selected instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=710"
  },
  {
    "text": "In the examples so far, I've been working with the fully masked image outputs. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=713"
  },
  {
    "text": "However, for some operations, like applying masked effects, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=718"
  },
  {
    "text": "it can be more convenient to work with just the segmentation masks instead. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=722"
  },
  {
    "text": "You can generate these masks by invoking the createScaledMask method ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=727"
  },
  {
    "text": "on the observation. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=730"
  },
  {
    "text": "The parameters behave the same as before. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=733"
  },
  {
    "text": "The output is a single channel floating point pixel buffer ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=736"
  },
  {
    "text": "containing the soft segmentation mask. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=739"
  },
  {
    "text": "The mask I just generated is perfectly suited for use with CoreImage. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=743"
  },
  {
    "text": "Vision, much like VisionKit, produces SDR outputs. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=747"
  },
  {
    "text": "Performing the masking in CoreImage, however, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=752"
  },
  {
    "text": "preserves the high dynamic range of the input. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=755"
  },
  {
    "text": "To learn more about this, consider checking out the session ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=758"
  },
  {
    "text": "on adding HDR to your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=761"
  },
  {
    "text": "One way to perform this masking ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=763"
  },
  {
    "text": "is to use the CIBlendWithMask filter. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=765"
  },
  {
    "text": "I will start with the source image that needs to be masked. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=767"
  },
  {
    "text": "This will typically be the same image you passed in to Vision. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=770"
  },
  {
    "text": "The mask obtained from Vision's createScaledMask call. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=774"
  },
  {
    "text": "And finally, the new background image ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=778"
  },
  {
    "text": "that the subject will be composited on top of. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=780"
  },
  {
    "text": "Using an empty image for this will result in a transparent background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=783"
  },
  {
    "text": "Alternatively, if you plan to composite the result on top of a new background, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=787"
  },
  {
    "text": "you can directly pass it in here. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=792"
  },
  {
    "text": "And that's pretty much it. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=794"
  },
  {
    "text": "The output will be an HDR-preserved masked and composited image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=795"
  },
  {
    "text": "Now let's put everything together to build a cool subject lifting visual effects app. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=801"
  },
  {
    "text": "You can remove the background and show the views underneath it, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=806"
  },
  {
    "text": "or replace it with something else. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=809"
  },
  {
    "text": "On top of that, you can apply one of the preset effects. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=812"
  },
  {
    "text": "And the effects compose with the selected background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=816"
  },
  {
    "text": "You can even tap on a foreground instance to selectively lift it. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=820"
  },
  {
    "text": "Let's look at an outline of how I will approach the creation of this app. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=825"
  },
  {
    "text": "The core of our app relies on an effects pipeline that accepts inputs from the UI ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=828"
  },
  {
    "text": "and performs all the work necessary for generating the final output. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=833"
  },
  {
    "text": "I'll start by performing subject lifting on the source image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=838"
  },
  {
    "text": "An optional tap will allow the selection of individual instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=842"
  },
  {
    "text": "The resulting mask will be applied to the source image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=846"
  },
  {
    "text": "And finally, the selected background and visual effects ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=850"
  },
  {
    "text": "will be applied and composited to produce the final output image. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=853"
  },
  {
    "text": "These last two steps will be accomplished using CoreImage. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=858"
  },
  {
    "text": "Our top level function takes in the input image, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=861"
  },
  {
    "text": "the selected background image and effect, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=864"
  },
  {
    "text": "and potentially a tap position from the user for selecting one of the instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=866"
  },
  {
    "text": "The Effect type here is just a simple enum for our presets. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=871"
  },
  {
    "text": "Its output is the final composited image, ready for display in the UI. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=875"
  },
  {
    "text": "This task can be deconstructed into two stages. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=879"
  },
  {
    "text": "First, generating the subject mask for the selected instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=883"
  },
  {
    "text": "And second, using that mask to apply the selected effect. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=888"
  },
  {
    "text": "Let's start with the first stage. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=892"
  },
  {
    "text": "The input to this stage is the source image and the optional tap position. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=894"
  },
  {
    "text": "We have already encountered most of the code here, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=899"
  },
  {
    "text": "which simply performs the Vision request and returns the mask. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=901"
  },
  {
    "text": "The interesting bit is this line which maps the tap position ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=906"
  },
  {
    "text": "to a set of indices using the label mask. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=909"
  },
  {
    "text": "Let's take a closer look. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=912"
  },
  {
    "text": "If the tap is missing, I'll default to using all instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=915"
  },
  {
    "text": "I want to map the tap position to a pixel in the instance mask. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=919"
  },
  {
    "text": "There are two pieces of information that are relevant here. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=924"
  },
  {
    "text": "First, the UI normalizes the tap position to be in 0, 1 before passing it in. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=927"
  },
  {
    "text": "This is nice because I don't have to worry about details ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=933"
  },
  {
    "text": "like display resolutions and scaling factors. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=935"
  },
  {
    "text": "Second, it uses the default UIKit coordinate system ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=938"
  },
  {
    "text": "which has the origin at the top-left. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=942"
  },
  {
    "text": "This is aligned with our image-space coordinate of the pixel buffer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=944"
  },
  {
    "text": "So I can perform this transformation ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=948"
  },
  {
    "text": "using this existing Vision helper function. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=949"
  },
  {
    "text": "I now have all the information necessary to look up the tapped instance label. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=953"
  },
  {
    "text": "This involves directly accessing the pixel buffer's data, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=958"
  },
  {
    "text": "and I'll show you how to do that next. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=961"
  },
  {
    "text": "Once I have the label, I check to see if it's zero. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=964"
  },
  {
    "text": "Recall that a zero label implies that the user tapped on a background pixel. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=968"
  },
  {
    "text": "In this case, I'll fall back to selecting all instances. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=972"
  },
  {
    "text": "Otherwise, I'll return a singleton set with just the selected label. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=975"
  },
  {
    "text": "This bit of code fills in how the instance label lookup is performed. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=980"
  },
  {
    "text": "As with any pixel buffer, I first need to lock it before accessing its data. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=984"
  },
  {
    "text": "Read-only access is sufficient for our purposes. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=989"
  },
  {
    "text": "The rows of a pixel buffer may be padded for alignment, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=993"
  },
  {
    "text": "so the most robust way to compute the byte offset ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=996"
  },
  {
    "text": "for a pixel is to use its bytesPerRow value. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=998"
  },
  {
    "text": "Since the instanceMask is a single channel UInt8 buffer, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1002"
  },
  {
    "text": "I don't have to worry about any further scaling. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1006"
  },
  {
    "text": "I'm done reading from the instance mask, so I can unlock the buffer. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1010"
  },
  {
    "text": "And with that wrapped up, I have the mask with the selected instance isolated. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1013"
  },
  {
    "text": "I can now move on to applying the effects. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1018"
  },
  {
    "text": "The first step here is to apply the selected effect to the background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1021"
  },
  {
    "text": "Once that's done, I will use CoreImage to composite the masked subject ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1024"
  },
  {
    "text": "on top of the transformed background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1029"
  },
  {
    "text": "The first few effects are quite straightforward ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1031"
  },
  {
    "text": "and direct applications of existing CoreImage filters. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1034"
  },
  {
    "text": "For instance, for highlighting the subject, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1037"
  },
  {
    "text": "I used the exposure adjustment filter to dim the background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1040"
  },
  {
    "text": "The bokeh effect is slightly more involved. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1043"
  },
  {
    "text": "In addition to blurring the background, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1046"
  },
  {
    "text": "I want a halo highlighting our selected subject. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1048"
  },
  {
    "text": "A white cut out for the subject before blurring would do the trick. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1051"
  },
  {
    "text": "A quick way to accomplish this is to reuse our current function ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1055"
  },
  {
    "text": "and pass in a solid white image for the subject. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1058"
  },
  {
    "text": "And with that, I have the base layer for compositing. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1063"
  },
  {
    "text": "Finally, I will drop in the CoreImage blending snippet from earlier. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1066"
  },
  {
    "text": "This composites the lifted subject on top of the newly transformed background. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1071"
  },
  {
    "text": "And with that final piece of the effects pipeline in place, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1077"
  },
  {
    "text": "the app is now complete. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1080"
  },
  {
    "text": "I hope it gave you a taste of what's possible with the new subject lifting API. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1082"
  },
  {
    "text": "In summary, VisionKit is the fastest way ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1087"
  },
  {
    "text": "to incorporate subject lifting into your apps. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1090"
  },
  {
    "text": "For more advanced applications, you can drop down to Vision's API. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1093"
  },
  {
    "text": "And finally, CoreImage is the perfect companion ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1098"
  },
  {
    "text": "for performing HDR-enabled image processing with subject lifting. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1101"
  },
  {
    "text": "Lizzy and I hope you enjoyed this video, ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1105"
  },
  {
    "text": "and we're very excited to see what you build. ",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1108"
  },
  {
    "text": "♪ ♪",
    "link": "https://developer.apple.com/videos/play/wwdc2023-10176/?time=1110"
  }
]