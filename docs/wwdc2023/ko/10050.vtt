WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:18.000
데니스: 안녕하세요, 제 이름은 데니스 비에류이고, 저는 애플의 GPU, 그래픽 및 디스플레이 소프트웨어 그룹의 소프트웨어 엔지니어입니다.

00:00:18.000 --> 00:00:25.000
오늘 저는 올해 메탈에서 기계 학습에 도입된 모든 새로운 기능과 개선 사항을 소개할 것입니다.

00:00:25.000 --> 00:00:28.000
먼저 기존 기계 학습 백엔드를 요약하겠습니다.

00:00:28.000 --> 00:00:34.000
금속 기계 학습 API는 금속 성능 셰이더 프레임워크를 통해 노출된다.

00:00:34.000 --> 00:00:43.000
MPS는 이미지 처리, 선형 대수학 및 기계 학습과 같은 다양한 분야를 위한 고성능 GPU 프리미티브 모음입니다.

00:00:43.000 --> 00:00:53.000
MPSGraph는 MPS 프레임워크 위에 있으며 다차원 텐서에 대한 지원을 확장하는 범용 컴퓨팅 그래프입니다.

00:00:53.000 --> 00:00:59.000
CoreML과 같은 기계 학습 추론 프레임워크는 MPSGraph 백엔드 위에 구축된다.

00:00:59.000 --> 00:01:05.000
MPSGraph는 또한 TensorFlow와 PyTorch와 같은 교육 프레임워크를 지원합니다.

00:01:05.000 --> 00:01:14.000
MPSGraph와 ML 프레임워크에 대해 자세히 알아보려면, 여기에 나열된 이전 Metal WWDC 회담을 참조하십시오.

00:01:14.000 --> 00:01:29.000
이 세션은 PyTorch 및 TensorFlow Metal 백엔드에 추가된 업데이트 및 개선 사항, JAX의 새로운 GPU 가속, 올해 ML 추론을 위한 MPSGraph에 추가된 기능에 중점을 둡니다.

00:01:29.000 --> 00:01:36.000
PyTorch와 TensorFlow Metal 가속을 사용하면 MPS의 고효율 커널을 사용하여 Mac에서 최고의 성능을 얻을 수 있습니다.

00:01:36.000 --> 00:01:42.000
PyTorch Metal 가속은 MPS 백엔드를 통해 버전 1.12부터 사용할 수 있습니다.

00:01:42.000 --> 00:01:52.000
이것은 작년에 PyTorch 생태계에 도입되었고, 그 이후로 메모리 사용과 뷰 텐서를 최적화하기 위해 여러 가지 개선이 이루어졌다.

00:01:52.000 --> 00:02:00.000
올해, PyTorch 2.0 MPS 백엔드는 큰 도약을 했고 베타 스테이지 자격을 얻었다.

00:02:00.000 --> 00:02:03.000
하지만 이것들이 모두 개선된 것은 아니었다.

00:02:03.000 --> 00:02:12.000
최신 PyTorch 빌드에는 MPS 작업 프로파일링, 사용자 지정 커널 및 자동 혼합 정밀도 지원과 같은 많은 새로운 업데이트가 포함되어 있습니다.

00:02:12.000 --> 00:02:20.000
모든 야간 빌드 기능을 다루기 전에, PyTorch 2.0의 새로운 기능부터 시작하겠습니다.

00:02:20.000 --> 00:02:32.000
그리드 샘플러, 삼각형 솔브, topk 등과 같은 작전을 포함하여 가장 많이 사용되는 상위 60개의 토치 운영자에 대한 지원이 있습니다.

00:02:32.000 --> 00:02:35.000
테스트 범위가 크게 개선되었다.

00:02:35.000 --> 00:02:43.000
여기에는 대부분의 토치 운영자에 대한 테스트, 그라디언트 테스트 및 ModuleInfo 기반 테스트가 포함됩니다.

00:02:43.000 --> 00:02:51.000
출시 이후, 여러 인기 모델이 MPS를 macOS의 공식 백엔드로 채택함에 따라 네트워크 커버리지가 확장되었다.

00:02:51.000 --> 00:03:01.000
여기에는 WhisperAI와 같은 기초 모델, YOLO와 같은 물체 감지 모델, 안정적인 확산 모델 등이 포함됩니다.

00:03:01.000 --> 00:03:06.000
최신 PyTorch 2.0을 사용하여 작동 중인 이 모델 중 하나를 확인해 봅시다.

00:03:06.000 --> 00:03:13.000
이 예에서, 저는 M2 Max에서 실행되는 물체 감지 네트워크인 YoloV5를 사용하고 있습니다.

00:03:13.000 --> 00:03:24.000
왼쪽에는 PyTorch MPS 백엔드를 사용하여 라이브 이미지를 실행하고 생성하는 네트워크가 있으며, 오른쪽에는 정확히 같은 모델을 가지고 있지만 CPU에서 실행됩니다.

00:03:24.000 --> 00:03:31.000
MPS 백엔드를 사용하는 왼쪽은 눈에 띄게 높은 프레임률로 실행되고 있다.

00:03:31.000 --> 00:03:47.000
게다가, 개발자들은 외부 네트워크에서 PyTorch MPS 백엔드를 채택했을 뿐만 아니라 히스토그램, group_norm, signbit 등을 포함한 여러 새로운 운영자를 위한 코드를 기여했다.

00:03:47.000 --> 00:03:55.000
다음으로, MPS 작업에 대한 프로파일링 지원을 시작으로 최신 PyTorch 빌드에서 사용할 수 있는 새로운 기능을 다룰 것입니다.

00:03:55.000 --> 00:04:09.000
PyTorch 야간 빌드에는 OS 표지판을 사용하여 작업 실행의 정확한 실행 시간, CPU와 GPU 간의 복사본, 지원되지 않는 운영자로 인한 CPU로의 대체를 보여주는 프로파일링 지원이 있습니다.

00:04:09.000 --> 00:04:17.000
Instruments의 일부인 매우 친숙한 도구인 Metal System Trace에서 프로파일링 데이터를 시각화할 수 있습니다.

00:04:17.000 --> 00:04:27.000
Metal System Trace를 사용하는 ML 애플리케이션 프로파일링에 대해 자세히 알아보려면, 작년의 "Accelerate machine learning with Metal" 세션을 시청하는 것이 좋습니다.

00:04:27.000 --> 00:04:31.000
프로파일러를 사용하는 것은 매우 간단한 과정이다.

00:04:31.000 --> 00:04:40.000
MPS 프로파일러 패키지의 시작 메소드를 호출하여 추적을 활성화하고, 스크립트가 끝날 때 stop 메소드를 사용하여 프로파일링을 종료하십시오.

00:04:40.000 --> 00:04:45.000
이제 나는 예시를 디버깅하기 위해 프로파일러를 살펴볼 것이다.

00:04:45.000 --> 00:04:56.000
이 샘플 네트워크는 모델에서 총 7개의 레이어가 있는 선형 변환과 Softshrink 활성화 기능으로 구성된 순차 모델을 사용합니다.

00:04:56.000 --> 00:05:00.000
이 모델의 현재 성능은 만족스럽지 않다.

00:05:00.000 --> 00:05:05.000
이 경우, 프로파일러는 병목 현상을 찾는 데 사용될 수 있다.

00:05:05.000 --> 00:05:10.000
금속 시스템 추적에서, 먼저, os_signpost를 활성화해야 합니다.

00:05:10.000 --> 00:05:14.000
이를 통해 PyTorch 운영자 정보를 캡처할 수 있습니다.

00:05:14.000 --> 00:05:21.000
다음으로, 장치와 올바른 실행 파일이 설정되어 있는지, 이 경우 파이썬 바이너리가 설정되어 있는지 확인하세요.

00:05:21.000 --> 00:05:24.000
그런 다음 녹화 버튼을 클릭하세요.

00:05:24.000 --> 00:05:28.000
악기는 이제 PyTorch 실행을 기록하고 있다.

00:05:28.000 --> 00:05:32.000
충분한 데이터를 캡처할 수 있도록 몇 초 동안 실행되도록 놔둘게.

00:05:32.000 --> 00:05:36.000
그리고 나서 나는 정지를 클릭한다.

00:05:36.000 --> 00:05:42.000
Os_signpost 탭에서 PyTorch 간격 타임라인을 공개하세요.

00:05:42.000 --> 00:05:53.000
이 타임라인은 문자열 식별자, 데이터 유형 및 복사 길이와 같은 PyTorch 메타데이터와 함께 연산자의 실행 시간을 표시합니다.

00:05:53.000 --> 00:05:58.000
타임라인을 확대하면 이 예제에서 사용되는 PyTorch 연산자가 드러난다.

00:05:58.000 --> 00:06:05.000
이 추적의 패턴은 7개의 레이어로 구성된 사용자 지정 순차 모델로 쉽게 식별할 수 있습니다.

00:06:05.000 --> 00:06:11.000
추적에서, 병목 현상이 CPU로의 Softshrink 폴백에 있다는 것은 분명하다.

00:06:11.000 --> 00:06:13.000
이 과정은 매우 비효율적이다.

00:06:13.000 --> 00:06:22.000
이 모델은 GPU가 부족한 동안 Softshrink 운영자의 CPU 실행과 추가 복사본으로 인한 오버헤드가 발생한다.

00:06:22.000 --> 00:06:29.000
GPU 타임라인의 대부분의 격차는 CPU로 돌아가는 Softshrink 활성화 기능에서 비롯된다.

00:06:29.000 --> 00:06:34.000
이것을 고치기 위해, 나는 성능을 향상시키기 위해 사용자 지정 커널을 작성할 것이다.

00:06:34.000 --> 00:06:37.000
사용자 지정 작업을 작성하는 데는 네 단계가 있습니다.

00:06:37.000 --> 00:06:42.000
먼저, Objective-C와 Metal에서 작업을 실행하세요.

00:06:42.000 --> 00:06:48.000
다음으로, Objective-C 코드에 대한 파이썬 바인딩을 만들고 확장을 컴파일하세요.

00:06:48.000 --> 00:06:55.000
마지막으로, 확장이 구축되면, 작업을 교육 스크립트로 가져오고 사용을 시작하세요.

00:06:55.000 --> 00:07:00.000
나는 운영 실행부터 시작할 것이다.

00:07:00.000 --> 00:07:03.000
토치 확장 헤더를 가져오는 것으로 시작하세요.

00:07:03.000 --> 00:07:09.000
여기에는 C++ 확장을 작성하는 데 필요한 모든 PyTorch 비트가 포함됩니다.

00:07:09.000 --> 00:07:20.000
그런 다음 컴퓨팅 함수를 정의하고 get_command_buffer MPS 백엔드 API를 사용하여 MPSStream 명령 버퍼에 대한 참조를 얻으십시오.

00:07:20.000 --> 00:07:26.000
마찬가지로, get_dispatch_queue API를 사용하여 직렬 대기열에 대한 참조를 얻으십시오.

00:07:26.000 --> 00:07:33.000
다음으로, 명령 버퍼를 사용하여 인코더를 만들고 사용자 지정 GPU 커널을 정의하십시오.

00:07:33.000 --> 00:07:41.000
여러 스레드의 제출이 직렬화되도록 디스패치 대기열 내부의 커널을 인코딩합니다.

00:07:41.000 --> 00:07:50.000
모든 작업이 인코딩된 후, 동기화 API를 사용하여 현재 명령 버퍼가 실행될 때까지 기다리면 직렬화된 제출을 관찰할 수 있습니다.

00:07:50.000 --> 00:07:55.000
또는 직렬화가 필요하지 않다면, 커밋 API를 사용하세요.

00:07:55.000 --> 00:07:58.000
다음으로, 사용자 지정 기능을 바인딩하세요.

00:07:58.000 --> 00:08:04.000
PYBIND11을 사용하여 매우 간단한 방식으로 Objective-C 함수를 파이썬에 바인딩할 수 있습니다.

00:08:04.000 --> 00:08:10.000
이 확장의 경우, 필요한 바인딩 코드는 두 줄에 불과하다.

00:08:10.000 --> 00:08:14.000
바인딩 후, 확장을 컴파일하세요.

00:08:14.000 --> 00:08:18.000
첫 번째 가져오기 torch.utils.cpp_extension.

00:08:18.000 --> 00:08:23.000
이것은 확장을 컴파일하는 데 사용할 수 있는 로드 기능을 제공합니다.

00:08:23.000 --> 00:08:31.000
다음으로, 빌드할 확장자의 이름을 전달한 다음, 소스 코드 파일에 대한 상대 또는 절대 경로 목록을 전달하십시오.

00:08:31.000 --> 00:08:37.000
선택적으로, 빌드로 전달할 추가 컴파일러 플래그를 나열할 수 있습니다.

00:08:37.000 --> 00:08:46.000
로드 함수는 소스 파일을 공유 라이브러리로 컴파일하며, 이후 현재 파이썬 프로세스에 모듈로 로드됩니다.

00:08:46.000 --> 00:08:52.000
마지막으로, 연산자를 스크립트로 가져와서 사용을 시작하세요.

00:08:52.000 --> 00:09:01.000
컴파일된 라이브러리를 가져오는 것으로 시작하고 사용자 지정 Softshrink 커널을 사용하기 위해 이전 순차 모델을 변경하십시오.

00:09:01.000 --> 00:09:06.000
같은 모델을 다시 실행하고 결과를 확인해 봅시다.

00:09:06.000 --> 00:09:12.000
새로 추가된 사용자 지정 운영자와 함께, 모델은 훨씬 더 효율적으로 실행된다.

00:09:12.000 --> 00:09:20.000
CPU로의 폴백에 의해 생성된 모든 복사본과 중간 텐서는 사라졌고, 순차 모델은 훨씬 더 빨리 실행된다.

00:09:20.000 --> 00:09:26.000
이제 당신의 네트워크를 더 개선할 수 있는 더 많은 방법을 살펴봅시다.

00:09:26.000 --> 00:09:35.000
PyTorch MPS 백엔드는 이제 자동 혼합 정밀도를 지원하므로 더 적은 메모리를 사용하여 품질 손실 없이 더 빠르게 훈련할 수 있습니다.

00:09:35.000 --> 00:09:40.000
혼합 정밀도를 이해하기 위해, 나는 먼저 지원되는 데이터 유형을 검토할 것이다.

00:09:40.000 --> 00:09:50.000
혼합 정밀 교육은 단일 정밀 부동 소수점과 반 정밀 부동 소수점이 혼합된 딥 러닝 모델을 훈련할 수 있는 모드입니다.

00:09:50.000 --> 00:09:57.000
macOS 소노마를 시작으로, MPSGraph는 새로운 데이터 유형인 bfloat16에 대한 지원을 추가합니다.

00:09:57.000 --> 00:10:02.000
Bfloat16은 딥 러닝을 위한 16비트 부동 소수점 형식이다.

00:10:02.000 --> 00:10:08.000
그것은 1개의 기호 비트, 8개의 지수 비트, 그리고 7개의 맨티사 비트로 구성되어 있다.

00:10:08.000 --> 00:10:16.000
이것은 딥 러닝 애플리케이션을 염두에 두고 설계되지 않은 표준 IEEE 16비트 부동 소수점 형식과 다릅니다.

00:10:16.000 --> 00:10:23.000
자동 혼합 정밀도는 float16과 bfloat16 모두에서 활성화될 것이다.

00:10:23.000 --> 00:10:38.000
자동 혼합 정밀도는 기본 정밀도로 네트워크의 성능을 측정하여 레이어당 올바른 정밀도를 선택한 다음, 정확도에 영향을 미치지 않고 성능을 최적화하기 위해 혼합 정밀도 설정으로 다시 실행됩니다.

00:10:38.000 --> 00:10:45.000
신경망의 일부 계층은 컨볼루션 또는 선형 계층과 같은 낮은 정밀도로 실행될 수 있다.

00:10:45.000 --> 00:10:52.000
감소와 같은 다른 층은 종종 더 높은 정밀도 수준을 요구할 것이다.

00:10:52.000 --> 00:10:57.000
네트워크에 자동 혼합 정밀도 지원을 추가하는 것은 매우 쉬운 과정입니다.

00:10:57.000 --> 00:10:59.000
먼저, 오토캐스트를 추가하세요.

00:10:59.000 --> 00:11:04.000
Float16과 bfloat16 모두 지원됩니다.

00:11:04.000 --> 00:11:12.000
오토캐스트는 스크립트의 영역을 혼합 정밀도로 실행할 수 있는 컨텍스트 관리자 역할을 한다.

00:11:12.000 --> 00:11:21.000
이 지역에서 MPS ops는 정확성을 유지하면서 성능을 향상시키기 위해 오토캐스트가 선택한 데이터 유형으로 실행됩니다.

00:11:21.000 --> 00:11:24.000
MPS 백엔드 또한 상당히 최적화되었다.

00:11:24.000 --> 00:11:32.000
PyTorch 2.0과 macOS Sonoma를 사용하면 MPS 백엔드가 이전 릴리스에 비해 최대 5배 더 빠릅니다.

00:11:32.000 --> 00:11:36.000
그게 파이토치를 위한 거야. 이제 TensorFlow로 넘어가자.

00:11:36.000 --> 00:11:41.000
텐서플로우 메탈 백엔드는 안정적인 1.0 릴리스 버전으로 성숙했다.

00:11:41.000 --> 00:11:47.000
이 릴리스에서, 그래플러 리매핑 옵티마이저 패스가 플러그인에 추가되었습니다.

00:11:47.000 --> 00:11:55.000
메탈 플러그인은 또한 혼합 정밀 지원을 받으며, 설치 과정은 이제 이전보다 간단하다.

00:11:55.000 --> 00:12:04.000
TensorFlow Metal 백엔드의 성능은 인식된 계산 패턴의 자동 융합을 추가하여 향상되었습니다.

00:12:04.000 --> 00:12:12.000
이러한 계산에는 융합된 컨벌루션과 행렬 곱셈, 옵티마이저 연산 및 RNN 셀이 포함됩니다.

00:12:12.000 --> 00:12:20.000
이 최적화는 계산 그래프가 생성될 때 그래플러 패스를 통해 자동으로 발생합니다.

00:12:20.000 --> 00:12:25.000
여기 2차원 컨벌루션 연산의 일반적인 계산의 예가 있습니다.

00:12:25.000 --> 00:12:32.000
컨벌루션은 종종 컨벌루션 신경망의 일반적인 패턴인 덧셈 함수가 뒤따른다.

00:12:32.000 --> 00:12:39.000
이 패턴을 식별함으로써, 그래플러 패스는 계산을 다시 매핑할 수 있다.

00:12:39.000 --> 00:12:45.000
이를 통해 더 최적화된 커널을 사용하여 동일한 출력을 달성하여 더 나은 성능을 낼 수 있습니다.

00:12:45.000 --> 00:12:50.000
PyTorch와 마찬가지로, TensorFlow도 혼합 정밀도 지원을 받는다.

00:12:50.000 --> 00:12:53.000
TensorFlow는 전 세계적으로 혼합 정밀도를 설정할 수 있게 해준다.

00:12:53.000 --> 00:13:06.000
이를 통해 요청된 데이터 유형 정책으로 모든 네트워크 레이어를 자동으로 만들 수 있으므로 표준 워크플로우에서 이러한 변경을 활성화하려면 기존 코드를 최소한으로 변경해야 합니다.

00:13:06.000 --> 00:13:14.000
글로벌 정책은 Float16 또는 BFloat16을 사용하도록 설정할 수 있습니다.

00:13:14.000 --> 00:13:21.000
성능 향상 외에도, 금속 가속을 가능하게 하는 사용자 경험이 간소화되었다.

00:13:21.000 --> 00:13:32.000
이제부터 패키지 관리자를 통해 TensorFlow 휠과 TensorFlow-Metal 플러그인을 설치하는 일반적인 경로를 따르기만 하면 Metal 가속을 가능하게 할 수 있습니다.

00:13:32.000 --> 00:13:41.000
TensorFlow 개발의 최첨단에 머물고 싶은 사람들을 위해, 금속 가속 지원은 이제 TensorFlow의 야간 릴리스에서도 사용할 수 있습니다.

00:13:41.000 --> 00:13:46.000
이제 JAX의 새로운 GPU 가속에 대해 이야기해 봅시다.

00:13:46.000 --> 00:13:53.000
올해 JAX GPU 가속은 PyTorch 및 TensorFlow와 유사한 Metal 백엔드를 통해 지원될 것이다.

00:13:53.000 --> 00:14:00.000
JAX는 고성능 수치 컴퓨팅과 기계 학습 연구를 위한 파이썬 라이브러리이다.

00:14:00.000 --> 00:14:09.000
그것은 기계 학습 연구를 위한 세 가지 주요 확장과 함께 큰 배열로 작업하기 위한 인기 있는 NumPy 프레임워크를 기반으로 한다.

00:14:09.000 --> 00:14:14.000
첫째, Grad 함수를 사용하여 자동 차별화를 지원합니다.

00:14:14.000 --> 00:14:21.000
그것은 파이썬 기능의 큰 하위 집합을 통해 차별화할 수 있으며, 심지어 높은 주문 파생물을 취할 수도 있다.

00:14:21.000 --> 00:14:25.000
JAX는 또한 빠르고 효율적인 벡터화를 지원합니다.

00:14:25.000 --> 00:14:33.000
함수 apply_matrix를 감안할 때, 파이썬에서 배치 차원을 반복할 수 있지만, 최적 이하의 성능으로 실행될 수 있습니다.

00:14:33.000 --> 00:14:38.000
이 경우, vmap을 사용하여 일괄 처리 지원을 자동으로 추가할 수 있습니다.

00:14:38.000 --> 00:14:45.000
게다가, JAX를 사용하면 jit이라는 API를 사용하여 기능을 최적화된 커널로 컴파일할 수 있습니다.

00:14:45.000 --> 00:14:53.000
같은 경우, jit은 더 빠르게 실행되도록 vmap 위에 있는 기능을 변환하는 데 사용됩니다.

00:14:53.000 --> 00:15:03.000
M2 Max가 장착된 MacBook Pro에서 JAX Metal 가속은 이러한 네트워크에서 CPU보다 평균 10배 빠른 놀라운 속도를 제공합니다.

00:15:03.000 --> 00:15:12.000
JAX의 환경 설정 및 설치에 대한 자세한 내용은 Metal Developer Resources 웹 페이지를 참조하십시오.

00:15:12.000 --> 00:15:15.000
기어를 바꾸고 ML 추론으로 넘어갑시다.

00:15:15.000 --> 00:15:22.000
로드 시간을 최적화하는 데 사용하는 MPSGraph의 새로운 직렬화 형식을 도입하는 것으로 시작하겠습니다.

00:15:22.000 --> 00:15:28.000
이 새로운 직렬화 형식은 다른 프레임워크의 기존 직렬화 네트워크에서 생성할 수 있습니다.

00:15:28.000 --> 00:15:36.000
마지막으로, 8비트 정수 양자화를 활용하여 네트워크의 메모리 풋프린트를 최적화하는 방법을 보여드리겠습니다.

00:15:36.000 --> 00:15:38.000
시작하자.

00:15:38.000 --> 00:15:44.000
MPSGraph는 레이어별로 완전한 유연성을 갖춘 높은 수준의 API를 사용하여 만들 수 있습니다.

00:15:44.000 --> 00:15:51.000
자세한 내용은 금속 성능 셰이더 그래프로 맞춤형 ML 모델을 구축하는 비디오를 참조하십시오.

00:15:51.000 --> 00:15:59.000
사용자 지정 그래프를 정의하고 컴파일한 후, 결과를 얻기 위해 MPSGraphExecutable을 통해 실행됩니다.

00:15:59.000 --> 00:16:02.000
보통, 이 과정은 잘 작동한다.

00:16:02.000 --> 00:16:10.000
그러나, 많은 레이어가 있는 복잡한 그래프에서, 이 초기 컴파일은 높은 애플리케이션 실행 시간으로 이어질 수 있다.

00:16:10.000 --> 00:16:17.000
MPSGraph는 이 문제를 정확히 해결하기 위해 MPSGraphPackage라는 새로운 직렬화 형식을 가지고 있다.

00:16:17.000 --> 00:16:23.000
이 새로운 직렬화 형식을 사용하면 MPSGraphExecutable을 미리 만들 수 있습니다.

00:16:23.000 --> 00:16:30.000
일단 생성되면, 최적화된 MPSGraphExecutable은 MPSGraphPackage 파일에서 직접 로드할 수 있습니다.

00:16:30.000 --> 00:16:34.000
MPSGraphPackage를 만드는 것은 매우 간단합니다.

00:16:34.000 --> 00:16:42.000
직렬화 설명자를 만들고 직렬화하려는 MPSGraphExecutable의 직렬화 기능에 전달하기만 하면 됩니다.

00:16:42.000 --> 00:16:46.000
당신은 또한 그것을 저장하기 위해 길을 통과해야 할 것입니다.

00:16:46.000 --> 00:16:51.000
패키지를 만든 후, 이것이 앱에 그래프를 로드하는 방법입니다.

00:16:51.000 --> 00:16:55.000
컴파일 설명자와 저장된 패키지의 경로가 필요합니다.

00:16:55.000 --> 00:16:59.000
그런 다음 그것들을 사용하여 MPSGraphExecutable을 초기화하세요.

00:16:59.000 --> 00:17:07.000
이미 MPSGraph를 사용하고 있다면, 우리가 제시한 API를 사용하여 새로운 직렬화 형식을 쉽게 채택할 수 있습니다.

00:17:07.000 --> 00:17:14.000
하지만 다른 프레임워크에서 왔다면, 이제 새로운 MPSGraphTool을 사용하여 MPSGraphPackage로 쉽게 마이그레이션할 수 있습니다.

00:17:14.000 --> 00:17:22.000
CoreML 사용자의 경우, MPSGraphPackage를 만들 MPSGraphTool에서 ML 프로그램을 통과할 수 있습니다.

00:17:22.000 --> 00:17:26.000
ONNX 파일을 입력으로 사용할 수 있는 ONNX도 마찬가지입니다.

00:17:26.000 --> 00:17:35.000
이 새로운 도구를 사용하면 추론 모델을 수동으로 인코딩할 필요 없이 기존 모델을 MPSGraph 애플리케이션에 빠르게 포함할 수 있습니다.

00:17:35.000 --> 00:17:38.000
명령줄 도구를 사용하는 방법은 다음과 같습니다.

00:17:38.000 --> 00:17:44.000
MPSGraphTool에 입력 모델 유형, 이 경우 CoreML 패키지를 선언하는 플래그를 부여합니다.

00:17:44.000 --> 00:17:50.000
또한 출력 목적지의 경로와 출력 모델의 이름을 제공합니다.

00:17:50.000 --> 00:17:55.000
또한, 대상 플랫폼과 최소 OS 버전을 정의합니다.

00:17:55.000 --> 00:18:02.000
변환 후, 생성된 MPSGraphPackages를 앱에 로드하고 직접 실행할 수 있습니다.

00:18:02.000 --> 00:18:10.000
다음으로, 8비트 정수 양자화를 사용하여 계산의 효율성을 향상시킬 수 있는 방법에 대해 논의해 봅시다.

00:18:10.000 --> 00:18:16.000
16비트 부동 소수점 형식과 같은 훈련과 추론을 하기 위해 부동 소수점 형식을 사용하는 것이 일반적이다.

00:18:16.000 --> 00:18:23.000
그러나, 추론에서, 이 모델들은 결과를 예측하는 데 더 오랜 시간이 걸릴 수 있다.

00:18:23.000 --> 00:18:29.000
대신, 많은 경우에 감소된 정밀도 또는 8비트 정수를 사용하는 것이 좋습니다.

00:18:29.000 --> 00:18:36.000
이것은 메모리 밴드를 저장하고 모델의 메모리 풋프린트를 줄이는 데 도움이 될 것입니다.

00:18:36.000 --> 00:18:42.000
8비트 정수 형식의 경우, 양자화에는 두 가지 유형이 있다: 대칭과 비대칭.

00:18:42.000 --> 00:18:46.000
MPSGraph는 이제 둘 다에 대한 API를 지원합니다.

00:18:46.000 --> 00:18:55.000
대칭 양자화와 비교할 때, 비대칭 양자화를 사용하면 여기서 제로포인트로 표시된 양자화 편향을 지정할 수 있습니다.

00:18:55.000 --> 00:19:05.000
이제 입력으로 Int8 형식의 활성화와 가중치부터 시작하여 예를 통해 양자화된 계산을 사용하는 방법을 탐구해 봅시다.

00:19:05.000 --> 00:19:12.000
이러한 입력은 MPSGraph의 dequantizeTensor op를 사용하여 부동 소수점 형식으로 디퀀트화됩니다.

00:19:12.000 --> 00:19:17.000
이제 부동 소수점 입력을 컨벌루션 작업에 공급할 수 있습니다.

00:19:17.000 --> 00:19:23.000
그 결과 부동 소수점 텐서는 quantizeTensor op를 사용하여 Int8로 다시 양자화할 수 있다.

00:19:23.000 --> 00:19:33.000
MPSGraph는 이러한 모든 커널을 단일 작업으로 자동으로 융합하여 메모리 대역폭을 절약하고 잠재적으로 성능을 향상시킬 수 있습니다.

00:19:33.000 --> 00:19:39.000
그리고 이것이 MPSGraph에서 양자화 지원을 사용할 수 있는 방법입니다.

00:19:39.000 --> 00:19:45.000
이전의 새로운 기능 외에도, MPSGraph는 더 많은 기계 학습 연산자를 지원합니다.

00:19:45.000 --> 00:19:50.000
올해부터, 대부분의 그래프 연산에서 복잡한 유형이 지원된다.

00:19:50.000 --> 00:19:56.000
단일 정밀도 또는 절반 정밀도 부동 소수점 형식으로 복소수를 사용할 수 있습니다.

00:19:56.000 --> 00:20:02.000
복잡한 데이터 유형을 기반으로, MPSGraph는 빠른 푸리에 변환을 계산하기 위한 연산자를 추가합니다.

00:20:02.000 --> 00:20:09.000
복잡한에서 복잡한, 복잡한에서 실제, 그리고 복잡한 변환에 최대 4차원까지 적용할 수 있습니다.

00:20:09.000 --> 00:20:15.000
이것들은 오디오, 비디오 및 이미지 처리 응용 프로그램에서 매우 일반적이다.

00:20:15.000 --> 00:20:30.000
또한, MPSGraph를 사용하면 이제 3차원 컨벌루션, 그리드 샘플링, 정렬 및 ArgSort, 합계, 제품, 최소값 및 최대치를 포함한 누적 작업을 수행할 수 있습니다.

00:20:30.000 --> 00:20:35.000
그리고 이것은 MPSGraph의 새로운 기능에 대한 논의를 마칩니다.

00:20:35.000 --> 00:20:38.000
오늘 이 세션에서 발표된 것을 검토해 봅시다.

00:20:38.000 --> 00:20:45.000
나는 Metal을 통해 PyTorch와 TensorFlow와 같은 인기 있는 ML 프레임워크를 가속화하는 개선 사항을 검토했다.

00:20:45.000 --> 00:20:51.000
이제 새로운 금속 가속 JAX 프레임워크를 활용할 수도 있습니다.

00:20:51.000 --> 00:20:59.000
우리는 또한 새로운 직렬화 도구를 사용하여 다른 프레임워크에서 MPSGraph로 기존 모델을 원활하게 통합하는 방법에 대해 논의했습니다.

00:20:59.000 --> 00:21:01.000
그리고 이것으로 우리의 이야기를 마칩니다.

00:21:01.000 --> 00:21:05.000
우리는 당신이 이 모든 기능을 사용하여 만들 놀라운 콘텐츠를 빨리 보고 싶습니다.

00:21:05.000 --> 00:21:07.000
봐줘서 고마워.

00:21:07.000 --> 23:59:59.000
♪ ♪

