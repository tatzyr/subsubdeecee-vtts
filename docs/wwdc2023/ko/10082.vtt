WEBVTT

00:00:00.000 --> 00:00:02.000
♪ 부드러운 기악 힙합 ♪

00:00:02.000 --> 00:00:10.000
♪

00:00:10.000 --> 00:00:12.000
라이언 테일러: 안녕! 내 이름은 라이언이야.

00:00:12.000 --> 00:00:13.000
코너 브룩스: 그리고 저는 코너입니다.

00:00:13.000 --> 00:00:18.000
라이언: 이 세션에서, 우리는 당신에게 공간 컴퓨팅을 위한 ARKit을 소개할 것입니다.

00:00:18.000 --> 00:00:25.000
우리는 이 새로운 플랫폼에서 하는 중요한 역할과 차세대 앱을 구축하기 위해 그것을 어떻게 활용할 수 있는지에 대해 논의할 것입니다.

00:00:25.000 --> 00:00:32.000
ARKit은 정교한 컴퓨터 비전 알고리즘을 사용하여 주변 세계와 움직임에 대한 이해를 구축합니다.

00:00:32.000 --> 00:00:42.000
우리는 개발자들이 손바닥에서 사용할 수 있는 놀라운 증강 현실 경험을 만들 수 있는 방법으로 iOS 11에서 이 기술을 처음 도입했습니다.

00:00:42.000 --> 00:00:50.000
이 플랫폼에서 ARKit은 본격적인 시스템 서비스로 성숙했으며, 새로운 실시간 기반으로 처음부터 재건되었습니다.

00:00:50.000 --> 00:01:00.000
ARKit은 전체 운영 체제의 구조에 깊이 짜여져 있으며, 창과의 상호 작용부터 몰입형 게임에 이르기까지 모든 것을 구동합니다.

00:01:00.000 --> 00:01:04.000
이 여정의 일환으로, 우리는 또한 API를 완전히 점검했습니다.

00:01:04.000 --> 00:01:12.000
새로운 디자인은 우리가 iOS에서 배운 모든 것과 공간 컴퓨팅의 독특한 요구의 결과이며, 우리는 당신이 그것을 좋아할 것이라고 생각합니다.

00:01:12.000 --> 00:01:21.000
ARKit은 가상 콘텐츠를 테이블에 놓는 것과 같은 놀라운 일을 하기 위해 결합할 수 있는 다양한 강력한 기능을 제공합니다.

00:01:21.000 --> 00:01:27.000
마치 실제로 거기에 있는 것처럼 콘텐츠를 터치한 다음, 콘텐츠가 현실 세계와 상호 작용하는 것을 볼 수 있습니다.

00:01:27.000 --> 00:01:30.000
그건 정말 마법 같은 경험이야.

00:01:30.000 --> 00:01:38.000
이제 이 새로운 플랫폼에서 ARKit을 사용하여 무엇을 성취할 수 있는지 엿볼 수 있게 되었으니, 우리의 의제를 안내해드리겠습니다.

00:01:38.000 --> 00:01:43.000
우리는 API를 구성하는 기본 개념과 빌딩 블록에 대한 개요로 시작할 것입니다.

00:01:43.000 --> 00:01:50.000
다음으로, 우리는 현실 세계와 관련된 가상 콘텐츠를 배치하는 데 필수적인 세계 추적에 뛰어들 것이다.

00:01:50.000 --> 00:01:57.000
그런 다음, 우리는 당신의 주변 환경에 대한 유용한 정보를 제공하는 장면 이해 기능을 탐구할 것입니다.

00:01:57.000 --> 00:02:10.000
그 후, 우리는 당신에게 우리의 최신 기능인 손 추적, 당신의 손과 관련된 가상 콘텐츠를 배치하거나 다른 유형의 맞춤형 상호 작용을 구축하는 데 활용할 수 있는 흥미진진한 새로운 추가 기능을 소개할 것입니다.

00:02:10.000 --> 00:02:20.000
그리고 마지막으로, 우리는 잠시 전에 보여드린 비디오의 코드를 검토하여 이러한 기능 중 일부의 실제 적용을 살펴볼 것입니다.

00:02:20.000 --> 00:02:22.000
좋아, 시작하자!

00:02:22.000 --> 00:02:31.000
우리의 새로운 API는 두 가지 상쾌한 맛, 현대적인 스위프트와 클래식 C로 꼼꼼하게 제작되었습니다.

00:02:31.000 --> 00:02:35.000
모든 ARKit 기능은 이제 단품으로 제공됩니다.

00:02:35.000 --> 00:02:43.000
우리는 개발자들이 가능한 한 많은 유연성을 갖기를 원했기 때문에, 경험을 구축하는 데 필요한 것을 선택하고 선택할 수 있기를 원했습니다.

00:02:43.000 --> 00:02:47.000
ARKit 데이터에 대한 액세스는 개인 정보 보호 우선 접근 방식으로 설계되었습니다.

00:02:47.000 --> 00:02:54.000
우리는 개발자를 위한 단순성을 유지하면서 사람들의 사생활을 보호하기 위해 안전 장치를 마련했습니다.

00:02:54.000 --> 00:03:03.000
API는 세 가지 기본 구성 요소로 구성되어 있다: 세션, 데이터 제공자 및 앵커.

00:03:03.000 --> 00:03:08.000
앵커로 시작한 다음 세션으로 돌아가자.

00:03:08.000 --> 00:03:12.000
앵커는 현실 세계에서의 위치와 방향을 나타낸다.

00:03:12.000 --> 00:03:17.000
모든 앵커는 고유 식별자와 변형을 포함한다.

00:03:17.000 --> 00:03:20.000
몇몇 유형의 앵커도 추적할 수 있다.

00:03:20.000 --> 00:03:28.000
추적 가능한 앵커가 추적되지 않을 때, 앵커된 가상 콘텐츠를 숨겨야 합니다.

00:03:28.000 --> 00:03:32.000
데이터 제공자는 개별 ARKit 기능을 나타냅니다.

00:03:32.000 --> 00:03:37.000
데이터 공급자를 사용하면 앵커 변경과 같은 데이터 업데이트를 폴링하거나 관찰할 수 있습니다.

00:03:37.000 --> 00:03:42.000
다른 유형의 데이터 제공자는 다른 유형의 데이터를 제공한다.

00:03:42.000 --> 00:03:48.000
세션은 특정 경험을 위해 함께 사용하고 싶은 ARKit 기능의 결합된 세트를 나타냅니다.

00:03:48.000 --> 00:03:53.000
일련의 데이터 제공자를 제공하여 세션을 실행합니다.

00:03:53.000 --> 00:03:58.000
세션이 실행되면, 데이터 제공자는 데이터를 받기 시작할 것이다.

00:03:58.000 --> 00:04:03.000
업데이트는 데이터 유형에 따라 비동기적으로 다른 주파수로 도착합니다.

00:04:03.000 --> 00:04:08.000
이제 개인 정보 보호와 앱이 ARKit 데이터에 액세스하는 방법에 대해 이야기해 봅시다.

00:04:08.000 --> 00:04:11.000
사생활은 기본적인 인권이다.

00:04:11.000 --> 00:04:14.000
그것은 또한 우리의 핵심 가치 중 하나이다.

00:04:14.000 --> 00:04:18.000
ARKit의 아키텍처와 API는 사람들의 사생활을 보호하기 위해 신중하게 설계되었습니다.

00:04:18.000 --> 00:04:25.000
ARKit이 당신 주변의 세계에 대한 이해를 구축하기 위해, 이 장치에는 많은 카메라와 다른 유형의 센서가 있습니다.

00:04:25.000 --> 00:04:30.000
카메라 프레임과 같은 이러한 센서의 데이터는 고객 공간으로 전송되지 않습니다.

00:04:30.000 --> 00:04:36.000
대신, 센서 데이터는 알고리즘에 의한 안전한 처리를 위해 ARKit의 데몬으로 전송됩니다.

00:04:36.000 --> 00:04:45.000
이러한 알고리즘에 의해 생성된 결과 데이터는 앱과 같은 데이터를 요청하는 고객에게 전달되기 전에 신중하게 큐레이팅됩니다.

00:04:45.000 --> 00:04:49.000
ARKit 데이터에 접근하기 위한 몇 가지 전제 조건이 있다.

00:04:49.000 --> 00:04:52.000
먼저, 앱은 전체 공간에 들어가야 합니다.

00:04:52.000 --> 00:04:57.000
ARKit은 공유 공간에 있는 앱에 데이터를 보내지 않습니다.

00:04:57.000 --> 00:05:01.000
둘째, 일부 유형의 ARKit 데이터는 액세스 권한이 필요합니다.

00:05:01.000 --> 00:05:07.000
그 사람이 권한을 부여하지 않으면, 우리는 그 유형의 데이터를 당신의 앱에 보내지 않을 것입니다.

00:05:07.000 --> 00:05:15.000
이를 용이하게 하기 위해, ARKit은 권한을 처리하기 위한 편리한 인증 API를 제공합니다.

00:05:15.000 --> 00:05:21.000
세션을 사용하여 액세스하려는 데이터 유형에 대한 승인을 요청할 수 있습니다.

00:05:21.000 --> 00:05:29.000
이렇게 하지 않으면, 필요한 경우, ARKit은 세션을 실행할 때 자동으로 그 사람에게 권한을 요청합니다.

00:05:29.000 --> 00:05:32.000
여기서, 우리는 손 추적 데이터에 대한 접근을 요청하고 있습니다.

00:05:32.000 --> 00:05:39.000
필요한 모든 인증 유형을 하나의 요청으로 일괄 처리할 수 있습니다.

00:05:39.000 --> 00:05:46.000
승인 결과가 나오면, 우리는 그것들을 반복하고 각 승인 유형의 상태를 확인합니다.

00:05:46.000 --> 00:05:51.000
그 사람이 허가를 받았다면, 그 상태는 허용될 것이다.

00:05:51.000 --> 00:05:59.000
그 사람이 접근을 거부한 데이터를 제공하는 데이터 제공자와 세션을 실행하려고 하면 세션이 실패할 것이다.

00:05:59.000 --> 00:06:06.000
이제, 세계 추적을 시작으로 ARKit이 이 플랫폼에서 지원하는 각 기능을 자세히 살펴봅시다.

00:06:06.000 --> 00:06:10.000
세계 추적을 사용하면 현실 세계에서 가상 콘텐츠를 고정할 수 있습니다.

00:06:10.000 --> 00:06:19.000
ARKit은 장치의 움직임을 6도의 자유도로 추적하고 각 앵커를 업데이트하여 주변 환경에 비해 같은 장소에 머물도록 합니다.

00:06:19.000 --> 00:06:26.000
세계 추적이 사용하는 DataProvider의 유형은 WorldTrackingProvider라고 불리며, 몇 가지 중요한 기능을 제공합니다.

00:06:26.000 --> 00:06:34.000
WorldAnchors를 추가할 수 있으며, ARKit은 장치가 움직일 때 사람들의 주변 환경에 비해 고정된 상태로 유지되도록 업데이트됩니다.

00:06:34.000 --> 00:06:38.000
WorldAnchors는 가상 콘텐츠 배치에 필수적인 도구입니다.

00:06:38.000 --> 00:06:43.000
추가하는 모든 WorldAnchors는 앱 실행 및 재부팅 시 자동으로 유지됩니다.

00:06:43.000 --> 00:06:52.000
만약 이 행동이 당신이 만들고 있는 경험에 바람직하지 않다면, 당신은 단순히 앵커를 제거할 수 있으며, 그것들은 더 이상 지속되지 않을 것입니다.

00:06:52.000 --> 00:06:57.000
지속성을 사용할 수 없는 경우가 있다는 점에 유의하는 것이 중요합니다.

00:06:57.000 --> 00:07:06.000
또한 WorldTrackingProvider를 사용하여 앱 출처와 관련된 장치의 포즈를 얻을 수 있으며, 이는 Metal을 사용하여 자체 렌더링을 수행하는 경우 필요합니다.

00:07:06.000 --> 00:07:11.000
WorldAnchor가 무엇이고 왜 사용하고 싶은지 자세히 살펴보는 것으로 시작합시다.

00:07:11.000 --> 00:07:22.000
WorldAnchor는 앱의 원점과 관련하여 앵커를 배치하고 싶은 위치와 방향인 변환을 취하는 이니셜라이저가 있는 TrackableAnchor입니다.

00:07:22.000 --> 00:07:29.000
우리는 고정되지 않은 가상 콘텐츠와 고정된 콘텐츠의 차이점을 시각화하는 데 도움이 되는 예를 준비했습니다.

00:07:29.000 --> 00:07:32.000
여기 큐브가 두 개 있어.

00:07:32.000 --> 00:07:40.000
왼쪽의 파란색 큐브는 WorldAnchor에 의해 업데이트되지 않고, 오른쪽의 빨간색 큐브는 WorldAnchor에 의해 업데이트되고 있다.

00:07:40.000 --> 00:07:45.000
두 큐브 모두 앱이 실행되었을 때 앱의 원점을 기준으로 배치되었다.

00:07:45.000 --> 00:07:50.000
장치가 움직일 때, 두 큐브 모두 그들이 배치된 곳에 남아 있다.

00:07:50.000 --> 00:07:54.000
앱을 업데이트하려면 왕관을 길게 누를 수 있습니다.

00:07:54.000 --> 00:07:59.000
최근 발생이 발생하면, 앱의 출처는 현재 위치로 이동됩니다.

00:07:59.000 --> 00:08:11.000
고정되지 않은 파란색 큐브는 앱의 원점에 대한 상대적인 위치를 유지하기 위해 재배치된다는 점에 유의하십시오. 고정되어 있는 빨간색 큐브는 현실 세계에 대해 고정되어 있습니다.

00:08:11.000 --> 00:08:14.000
WorldAnchor의 지속성이 어떻게 작동하는지 살펴봅시다.

00:08:14.000 --> 00:08:19.000
장치가 이동함에 따라, ARKit은 주변 지도를 구축합니다.

00:08:19.000 --> 00:08:25.000
WorldAnchors를 추가하면, 우리는 그것들을 지도에 삽입하고 자동으로 당신을 위해 유지합니다.

00:08:25.000 --> 00:08:29.000
WorldAnchor 식별자와 변환만 지속됩니다.

00:08:29.000 --> 00:08:33.000
가상 콘텐츠와 같은 다른 데이터는 포함되지 않습니다.

00:08:33.000 --> 00:08:41.000
WorldAnchor 식별자를 당신이 그들과 연관시키는 모든 가상 콘텐츠에 매핑하는 것은 당신에게 달려 있습니다.

00:08:41.000 --> 00:08:53.000
지도는 위치 기반이므로, 예를 들어 집에서 사무실로 장치를 새로운 위치로 가져갈 때 집의 지도가 언로드되고 다른 지도가 사무실을 위해 현지화됩니다.

00:08:53.000 --> 00:08:59.000
이 새로운 위치에 추가하는 앵커는 그 지도에 들어갈 것이다.

00:08:59.000 --> 00:09:09.000
하루가 끝날 때 사무실을 떠나 집으로 향할 때, ARKit이 사무실에 짓고 있는 지도와 거기에 배치한 앵커가 내려질 것입니다.

00:09:09.000 --> 00:09:14.000
하지만 다시 한 번, 우리는 당신의 앵커와 함께 지도를 자동으로 유지해 왔습니다.

00:09:14.000 --> 00:09:26.000
집으로 돌아오면, ARKit은 위치가 변경되었음을 인식할 것이며, 우리는 이 위치에 대한 기존 지도를 확인하여 재배치 과정을 시작할 것입니다.

00:09:26.000 --> 00:09:35.000
우리가 하나를 찾으면, 우리는 그것으로 현지화할 것이고, 당신이 이전에 집에서 추가한 모든 앵커는 다시 한 번 추적될 것입니다.

00:09:35.000 --> 00:09:38.000
장치 포즈로 넘어가자.

00:09:38.000 --> 00:09:45.000
WorldAnchors를 추가 및 제거하는 것과 함께, WorldTrackingProvider를 사용하여 장치의 포즈를 취할 수도 있습니다.

00:09:45.000 --> 00:09:50.000
포즈는 앱의 원점에 대한 장치의 위치와 방향이다.

00:09:50.000 --> 00:09:58.000
완전히 몰입형 환경에서 Metal 및 CompositorServices로 자신만의 렌더링을 하는 경우 포즈를 쿼리해야 합니다.

00:09:58.000 --> 00:10:00.000
이 질문은 상대적으로 비싸다.

00:10:00.000 --> 00:10:07.000
콘텐츠 배치와 같은 다른 유형의 앱 로직에 대한 장치 포즈를 쿼리할 때 주의하십시오.

00:10:07.000 --> 00:10:16.000
ARKit에서 CompositorServices로 장치 포즈를 제공하는 방법을 보여주기 위해 단순화된 렌더링 예제를 빠르게 살펴보겠습니다.

00:10:16.000 --> 00:10:23.000
우리는 세션, 세계 추적 공급자 및 최신 포즈를 개최할 렌더러 구조체가 있습니다.

00:10:23.000 --> 00:10:28.000
렌더러를 초기화할 때, 우리는 세션을 만드는 것으로 시작합니다.

00:10:28.000 --> 00:10:36.000
다음으로, 우리는 각 프레임을 렌더링할 때 장치 포즈를 쿼리하는 데 사용할 세계 추적 공급자를 만듭니다.

00:10:36.000 --> 00:10:41.000
이제, 우리는 필요한 모든 데이터 공급자와 함께 세션을 진행할 수 있습니다.

00:10:41.000 --> 00:10:45.000
이 경우, 우리는 세계 추적 공급자만 사용하고 있습니다.

00:10:45.000 --> 00:10:49.000
우리는 또한 렌더링 함수에 할당하는 것을 피하기 위해 포즈를 만듭니다.

00:10:49.000 --> 00:10:55.000
이제 프레임 속도로 호출할 렌더링 함수로 넘어갑시다.

00:10:55.000 --> 00:11:00.000
CompositorServices의 드로어블을 사용하여 대상 렌더링 시간을 가져옵니다.

00:11:00.000 --> 00:11:06.000
다음으로, 우리는 대상 렌더링 시간을 사용하여 장치의 포즈를 쿼리합니다.

00:11:06.000 --> 00:11:11.000
성공하면, 우리는 앱의 기원과 관련된 포즈의 변형을 추출할 수 있다.

00:11:11.000 --> 00:11:15.000
이것은 당신의 콘텐츠를 렌더링하는 데 사용할 변환입니다.

00:11:15.000 --> 00:11:25.000
마지막으로, 합성을 위해 프레임을 제출하기 전에, 우리는 드로어블에 포즈를 설정하여, 컴포지터는 우리가 프레임의 콘텐츠를 렌더링하는 데 사용한 포즈를 알 수 있도록 합니다.

00:11:25.000 --> 00:11:32.000
자체 렌더링에 대한 자세한 내용은 Metal을 사용하여 몰입형 앱을 만드는 전용 세션을 참조하십시오.

00:11:32.000 --> 00:11:39.000
또한, 공간 컴퓨팅 성능 고려 사항에 대한 훌륭한 세션이 있으며, 또한 확인하시기 바랍니다.

00:11:39.000 --> 00:11:43.000
다음으로, 현장 이해를 살펴봅시다.

00:11:43.000 --> 00:11:48.000
장면 이해는 다양한 방식으로 주변 환경에 대해 알려주는 기능의 범주입니다.

00:11:48.000 --> 00:11:51.000
비행기 탐지부터 시작합시다.

00:11:51.000 --> 00:11:58.000
평면 감지는 ARKit이 현실 세계에서 감지하는 수평 및 수직 표면의 앵커를 제공합니다.

00:11:58.000 --> 00:12:03.000
평면 감지가 사용하는 DataProvider의 유형은 PlaneDetectionProvider라고 불린다.

00:12:03.000 --> 00:12:09.000
비행기가 당신의 주변에서 감지됨에 따라, 그것들은 PlaneAnchors의 형태로 당신에게 제공됩니다.

00:12:09.000 --> 00:12:16.000
PlaneAnchors는 테이블 위에 가상 물체를 배치하는 것과 같은 콘텐츠 배치를 용이하게 하는 데 사용될 수 있다.

00:12:16.000 --> 00:12:24.000
또한, 바닥이나 벽과 같은 기본적이고 평평한 기하학이 충분한 물리학 시뮬레이션을 위해 평면을 사용할 수 있습니다.

00:12:24.000 --> 00:12:35.000
각 PlaneAnchor는 수평 또는 수직인 정렬, 평면의 기하학, 그리고 의미 분류를 포함한다.

00:12:35.000 --> 00:12:40.000
비행기는 바닥이나 테이블과 같은 다양한 유형의 표면으로 분류될 수 있다.

00:12:40.000 --> 00:12:51.000
특정 표면을 식별할 수 없는 경우, 제공된 분류는 상황에 따라 알 수 없거나, 결정되지 않았거나, 사용할 수 없는 것으로 표시됩니다.

00:12:51.000 --> 00:12:55.000
이제, 장면 기하학으로 넘어가자.

00:12:55.000 --> 00:13:02.000
장면 기하학은 현실 세계의 모양을 추정하는 다각형 메쉬를 포함하는 앵커를 제공한다.

00:13:02.000 --> 00:13:08.000
장면 지오메트리가 사용하는 DataProvider의 유형은 SceneReconstructionProvider라고 불린다.

00:13:08.000 --> 00:13:18.000
ARKit이 당신 주변의 세계를 스캔할 때, 우리는 당신의 주변을 세분화된 메쉬로 재구성한 다음 MeshAnchors의 형태로 제공됩니다.

00:13:18.000 --> 00:13:23.000
PlaneAnchors와 마찬가지로, MeshAnchors는 콘텐츠 배치를 용이하게 하는 데 사용될 수 있다.

00:13:23.000 --> 00:13:33.000
또한 단순하고 평평한 표면이 아닌 물체와 상호 작용하기 위해 가상 콘텐츠가 필요한 경우 고충실도 물리 시뮬레이션을 달성할 수 있습니다.

00:13:33.000 --> 00:13:37.000
각 MeshAnchor는 메쉬의 형상을 포함한다.

00:13:37.000 --> 00:13:47.000
이 기하학은 정점, 법선, 면 및 면당 의미 분류를 포함한다.

00:13:47.000 --> 00:13:51.000
메쉬 면은 다양한 유형의 물체로 분류될 수 있다.

00:13:51.000 --> 00:13:58.000
만약 우리가 특정 물체를 식별할 수 없다면, 제공된 분류는 없을 것이다.

00:13:58.000 --> 00:14:02.000
마지막으로, 이미지 추적을 살펴봅시다.

00:14:02.000 --> 00:14:07.000
이미지 추적을 사용하면 현실 세계에서 2D 이미지를 감지할 수 있습니다.

00:14:07.000 --> 00:14:13.000
이미지 추적이 사용하는 DataProvider의 유형은 ImageTrackingProvider라고 불린다.

00:14:13.000 --> 00:14:18.000
감지하려는 참조 이미지 세트로 ImageTrackingProvider를 구성합니다.

00:14:18.000 --> 00:14:22.000
이 참조 이미지는 몇 가지 다른 방법으로 만들 수 있다.

00:14:22.000 --> 00:14:28.000
한 가지 옵션은 프로젝트의 자산 카탈로그의 AR 리소스 그룹에서 그것들을 로드하는 것입니다.

00:14:28.000 --> 00:14:36.000
또는 CVPixelBuffer 또는 CGImage를 제공하여 ReferenceImage를 직접 초기화할 수도 있습니다.

00:14:36.000 --> 00:14:41.000
이미지가 감지되면, ARKit은 ImageAnchor를 제공합니다.

00:14:41.000 --> 00:14:46.000
ImageAnchors는 알려진 정적으로 배치된 이미지에 콘텐츠를 배치하는 데 사용할 수 있습니다.

00:14:46.000 --> 00:14:52.000
예를 들어, 영화 포스터 옆에 영화에 대한 정보를 표시할 수 있습니다.

00:14:52.000 --> 00:15:05.000
ImageAnchors는 예상 스케일 계수를 포함하는 TrackableAnchors로, 감지된 이미지의 크기가 지정한 물리적 크기와 앵커가 대응하는 ReferenceImage와 어떻게 비교되는지 나타냅니다.

00:15:05.000 --> 00:15:12.000
이제, 우리의 새로운 기능인 손 추적에 대해 말하고, 예를 살펴보기 위해, 여기 코너가 있습니다.

00:15:12.000 --> 00:15:16.000
코너: 안녕. ARKit에 새로 추가된 손 추적을 살펴봅시다.

00:15:16.000 --> 00:15:21.000
손 추적은 각 손에 대한 골격 데이터가 포함된 앵커를 제공합니다.

00:15:21.000 --> 00:15:26.000
핸드 트래킹이 사용하는 DataProvider의 유형은 HandTrackingProvider라고 불린다.

00:15:26.000 --> 00:15:30.000
당신의 손이 감지되면, HandAnchors의 형태로 제공됩니다.

00:15:30.000 --> 00:15:33.000
핸드앵커는 추적 가능한 앵커이다.

00:15:33.000 --> 00:15:37.000
핸드앵커는 골격과 키랄성을 포함한다.

00:15:37.000 --> 00:15:41.000
키랄성은 이것이 왼손인지 오른손인지를 알려준다.

00:15:41.000 --> 00:15:47.000
HandAnchor의 변형은 앱 원점에 대한 손목의 변형이다.

00:15:47.000 --> 00:15:51.000
골격은 이름으로 질문할 수 있는 관절로 구성되어 있다.

00:15:51.000 --> 00:16:07.000
관절은 부모 관절을 포함한다; 그 이름; 부모 관절과 관련된 localTransform; 뿌리 관절과 관련된 rootTransform; 그리고 마지막으로, 각 관절은 이 관절이 추적되는지 여부를 나타내는 bool을 포함한다.

00:16:07.000 --> 00:16:11.000
여기서 우리는 손 골격에서 사용 가능한 모든 관절을 열거합니다.

00:16:11.000 --> 00:16:14.000
관절 계층 구조의 하위 집합을 살펴보자.

00:16:14.000 --> 00:16:17.000
손목은 손의 근관절이다.

00:16:17.000 --> 00:16:25.000
각 손가락에 대해, 첫 번째 관절은 손목에 부모로 되어 있다; 예를 들어, 1은 0으로 부모로 되어 있다.

00:16:25.000 --> 00:16:33.000
후속 손가락 관절은 이전 관절로 양육된다; 예를 들어, 2는 1로 양육된다.

00:16:33.000 --> 00:16:38.000
핸드앵커는 손과 관련된 콘텐츠를 배치하거나 사용자 지정 제스처를 감지하는 데 사용할 수 있습니다.

00:16:38.000 --> 00:16:46.000
HandAnchors를 받기 위한 두 가지 옵션이 있습니다. 업데이트를 위해 투표하거나 앵커를 사용할 수 있을 때 비동기적으로 받을 수 있습니다.

00:16:46.000 --> 00:16:54.000
나중에 Swift 예제에서 비동기 업데이트를 살펴볼 것이므로, 이전부터 렌더러에 핸드 앵커 폴링을 추가합시다.

00:16:54.000 --> 00:16:56.000
여기 우리의 업데이트된 구조 정의가 있습니다.

00:16:56.000 --> 00:17:01.000
우리는 왼손과 오른손 앵커와 함께 손 추적 공급자를 추가했습니다.

00:17:01.000 --> 00:17:12.000
업데이트된 초기화 기능에서, 우리는 새로운 손 추적 공급자를 만들고 우리가 운영하는 공급자 목록에 추가합니다; 그런 다음 투표할 때 필요한 왼쪽과 오른쪽 앵커를 만듭니다.

00:17:12.000 --> 00:17:17.000
참고로, 우리는 렌더링 루프에서 할당하는 것을 피하기 위해 이것들을 미리 만듭니다.

00:17:17.000 --> 00:17:23.000
구조체가 업데이트되고 초기화되면, 렌더링 함수에서 get_latest_anchors를 호출할 수 있습니다.

00:17:23.000 --> 00:17:27.000
우리는 공급자와 사전 할당된 핸드 앵커를 통과합니다.

00:17:27.000 --> 00:17:32.000
우리의 앵커는 사용 가능한 최신 데이터로 채워질 것이다.

00:17:32.000 --> 00:17:36.000
우리의 최신 앵커가 채워지면, 우리는 이제 우리의 경험에서 그들의 데이터를 사용할 수 있다.

00:17:36.000 --> 00:17:38.000
정말 멋져.

00:17:38.000 --> 00:17:41.000
이제 우리가 전에 보여준 예시를 다시 살펴봐야 할 때입니다.

00:17:41.000 --> 00:17:45.000
우리는 이 경험을 구축하기 위해 ARKit과 RealityKit 기능의 조합을 사용했습니다.

00:17:45.000 --> 00:17:52.000
장면 기하학은 물리학과 제스처의 충돌기로 사용되었고, 손 추적은 큐브 엔티티와 직접 상호 작용하는 데 사용되었다.

00:17:52.000 --> 00:17:55.000
우리가 이 예시를 어떻게 만들었는지 살펴봅시다.

00:17:55.000 --> 00:17:59.000
먼저, 우리는 앱 구조와 보기 모델을 확인할 것입니다.

00:17:59.000 --> 00:18:02.000
다음으로, 우리는 ARKit 세션을 초기화할 것이다.

00:18:02.000 --> 00:18:07.000
그런 다음 우리는 현장 재구성에서 손끝과 충돌기를 위한 충돌기를 추가할 것입니다.

00:18:07.000 --> 00:18:10.000
마지막으로, 우리는 제스처로 큐브를 추가하는 방법을 살펴볼 것이다.

00:18:10.000 --> 00:18:13.000
그냥 바로 뛰어들자.

00:18:13.000 --> 00:18:17.000
여기 우리의 앱인 TimeForCube가 있습니다.

00:18:17.000 --> 00:18:21.000
우리는 비교적 표준인 SwiftUI 앱과 장면 설정을 가지고 있습니다.

00:18:21.000 --> 00:18:24.000
우리 장면에서, 우리는 ImmersiveSpace를 선언한다.

00:18:24.000 --> 00:18:29.000
ARKit 데이터에 액세스하려면 전체 공간으로 이동해야 하기 때문에 IimmersiveSpace가 필요합니다.

00:18:29.000 --> 00:18:35.000
ImmersiveSpace 내에서, 우리는 뷰 모델의 콘텐츠를 제시할 RealityView를 정의합니다.

00:18:35.000 --> 00:18:38.000
뷰 모델은 우리 앱의 논리의 대부분이 살 수 있는 곳이다.

00:18:38.000 --> 00:18:41.000
잠깐 살펴봅시다.

00:18:41.000 --> 00:18:55.000
뷰 모델은 ARKit 세션, 우리가 사용할 데이터 공급자, 우리가 만드는 다른 모든 엔티티를 포함할 콘텐츠 엔티티, 그리고 장면과 핸드 충돌기 맵을 모두 유지합니다.

00:18:55.000 --> 00:18:59.000
우리의 뷰 모델은 또한 우리가 앱에서 호출할 다양한 기능을 제공합니다.

00:18:59.000 --> 00:19:02.000
우리는 앱의 맥락에서 이것들 각각을 살펴볼 것이다.

00:19:02.000 --> 00:19:09.000
우리가 호출할 첫 번째 기능은 ContentEntity를 설정하기 위한 RealityView의 make closure 내에 있습니다.

00:19:09.000 --> 00:19:17.000
우리는 이 엔티티를 RealityView의 콘텐츠에 추가하여 뷰 모델이 뷰의 콘텐츠에 엔티티를 추가할 수 있도록 할 것입니다.

00:19:17.000 --> 00:19:24.000
setupContentEntity는 지도의 모든 손가락 엔티티를 contentEntity의 자식으로 추가한 다음 반환합니다.

00:19:24.000 --> 00:19:25.000
좋아!

00:19:25.000 --> 00:19:28.000
세션 초기화로 넘어갑시다.

00:19:28.000 --> 00:19:31.000
우리의 세션 초기화는 세 가지 작업 중 하나로 실행됩니다.

00:19:31.000 --> 00:19:34.000
우리의 첫 번째 작업은 runSession 함수를 호출한다.

00:19:34.000 --> 00:19:39.000
이 기능은 단순히 우리의 두 공급자와의 세션을 실행합니다.

00:19:39.000 --> 00:19:42.000
세션이 실행되면, 우리는 앵커 업데이트를 받기 시작할 수 있습니다.

00:19:42.000 --> 00:19:47.000
큐브와 상호 작용하는 데 사용할 손가락 끝 충돌기를 만들고 업데이트합시다.

00:19:47.000 --> 00:19:51.000
여기 손 업데이트를 처리하기 위한 우리의 작업이 있습니다.

00:19:51.000 --> 00:19:56.000
그 기능은 공급자의 앵커 업데이트의 비동기 시퀀스를 반복한다.

00:19:56.000 --> 00:20:06.000
우리는 핸드 앵커가 추적되고, 집게 손가락 관절을 얻고, 관절 자체도 추적되는지 확인합니다.

00:20:06.000 --> 00:20:12.000
그런 다음 앱 원점에 대한 집게 손가락 끝의 변환을 계산합니다.

00:20:12.000 --> 00:20:19.000
마지막으로, 우리는 어떤 손가락 엔티티를 업데이트하고 변환을 설정해야 하는지 찾아본다.

00:20:19.000 --> 00:20:21.000
우리의 손가락 엔티티 지도를 다시 살펴봅시다.

00:20:21.000 --> 00:20:31.000
우리는 ModelEntity의 확장을 통해 손당 엔티티를 만듭니다. 이 확장은 충돌 모양의 5mm 구를 만듭니다.

00:20:31.000 --> 00:20:37.000
우리는 운동 물리학 신체 구성 요소를 추가하고 불투명도 구성 요소를 추가하여 이 개체를 숨깁니다.

00:20:37.000 --> 00:20:43.000
사용 사례를 위해 이것들을 숨길 것이지만, 모든 것이 예상대로 작동하는지 확인하기 위해 손가락 끝 엔티티를 시각화하는 것이 좋을 것입니다.

00:20:43.000 --> 00:20:48.000
일시적으로 우리의 불투명도를 하나로 설정하고 우리의 실체가 올바른 위치에 있는지 확인합시다.

00:20:48.000 --> 00:20:49.000
좋아!

00:20:49.000 --> 00:20:51.000
우리는 손끝이 있는 곳에서 구체를 볼 수 있다!

00:20:51.000 --> 00:20:54.000
주목하세요, 우리의 손은 부분적으로 구체를 덮고 있습니다.

00:20:54.000 --> 00:21:01.000
이것은 사람이 가상 콘텐츠 위에 있는 손을 볼 수 있는 시스템 기능인 손 폐색이라고 불린다.

00:21:01.000 --> 00:21:11.000
이것은 기본적으로 활성화되어 있지만, 구를 좀 더 명확하게 보고 싶다면, 장면에서 upperLimbVisibility 세터를 사용하여 손 폐색 가시성을 구성할 수 있습니다.

00:21:11.000 --> 00:21:18.000
사지 가시성을 숨겨진 것으로 설정하면, 손이 어디에 있는지에 관계없이 전체 구체를 볼 수 있습니다.

00:21:18.000 --> 00:21:23.000
예를 들어, 우리는 상지 가시성을 기본값으로 남겨두고 불투명도를 다시 0으로 설정할 것입니다.

00:21:23.000 --> 00:21:29.000
깔끔해! 이제 장면 충돌기를 추가합시다 - 우리는 이것들을 물리학과 제스처 표적으로 사용할 것입니다.

00:21:29.000 --> 00:21:32.000
여기 우리 모델의 기능을 호출하는 작업이 있습니다.

00:21:32.000 --> 00:21:44.000
우리는 공급자에 대한 앵커 업데이트의 비동기 시퀀스를 반복하고, MeshAnchor에서 ShapeResource를 생성하려고 시도한 다음, 앵커 업데이트 이벤트를 켭니다.

00:21:44.000 --> 00:21:56.000
앵커를 추가하는 경우, 새로운 엔티티를 만들고, 변환을 설정하고, 충돌 및 물리 바디 구성 요소를 추가한 다음, 이 충돌기가 제스처의 대상이 될 수 있도록 입력 대상 구성 요소를 추가합니다.

00:21:56.000 --> 00:22:02.000
마지막으로, 우리는 지도에 새로운 엔티티를 추가하고 콘텐츠 엔티티의 자식으로 추가합니다.

00:22:02.000 --> 00:22:09.000
엔티티를 업데이트하려면 지도에서 검색한 다음 변환 및 충돌 구성 요소 모양을 업데이트합니다.

00:22:09.000 --> 00:22:15.000
제거를 위해, 우리는 부모와 지도에서 해당 엔티티를 제거합니다.

00:22:15.000 --> 00:22:19.000
이제 손과 장면 충돌기가 있으므로, 제스처를 사용하여 큐브를 추가할 수 있습니다.

00:22:19.000 --> 00:22:27.000
우리는 모든 엔티티를 대상으로 하는 SpatialTapGesture를 추가하여 누군가가 RealityView의 콘텐츠에서 어떤 엔티티를 탭했는지 알려줍니다.

00:22:27.000 --> 00:22:33.000
그 탭이 끝나면, 우리는 글로벌 좌표에서 장면 좌표로 변환하는 3D 위치를 받습니다.

00:22:33.000 --> 00:22:35.000
이 위치를 시각화해 봅시다.

00:22:35.000 --> 00:22:40.000
탭 위치에 구체를 추가하면 볼 수 있는 것은 다음과 같습니다.

00:22:40.000 --> 00:22:44.000
이제, 우리는 뷰 모델에 이 위치에 대한 큐브를 추가하라고 말한다.

00:22:44.000 --> 00:22:51.000
큐브를 추가하려면, 먼저 탭 위치보다 20센티미터 높은 배치 위치를 계산합니다.

00:22:51.000 --> 00:22:55.000
그런 다음 큐브를 만들고 계산된 배치 위치로 위치를 설정합니다.

00:22:55.000 --> 00:23:01.000
우리는 엔티티가 응답할 제스처 유형을 설정할 수 있는 InputTargetComponent를 추가합니다.

00:23:01.000 --> 00:23:09.000
우리의 사용 사례를 위해, 우리는 손가락 끝 충돌기가 직접적인 상호 작용을 제공하기 때문에 이러한 큐브에 대한 간접 입력 유형만 허용할 것입니다.

00:23:09.000 --> 00:23:14.000
우리는 물리 상호 작용을 조금 더 좋게 만들기 위해 사용자 지정 매개 변수가 있는 PhysicsBodyComponent를 추가합니다.

00:23:14.000 --> 00:23:20.000
마지막으로, 우리는 콘텐츠 엔티티에 큐브를 추가합니다. 즉, 마침내 큐브를 위한 시간입니다.

00:23:20.000 --> 00:23:24.000
우리의 예를 마지막으로 한 번 살펴봅시다, 처음부터 끝까지.

00:23:24.000 --> 00:23:29.000
장면 충돌기나 큐브를 탭할 때마다, 탭 위치 위에 새로운 큐브가 추가됩니다.

00:23:29.000 --> 00:23:36.000
물리 시스템은 큐브가 현장 충돌기에 떨어지게 하고, 우리의 손 충돌기는 우리가 큐브와 상호 작용할 수 있게 해준다.

00:23:36.000 --> 00:23:42.000
RealityKit에 대한 자세한 내용은 공간 컴퓨팅을 위해 RealityKit을 사용하는 입문 세션을 확인하세요.

00:23:42.000 --> 00:23:51.000
그리고, 이미 iOS에서 이 플랫폼으로 가져오는 데 관심이 있는 기존 ARKit 경험이 있다면, 추가 지침을 위해 이 주제에 대한 전용 세션을 시청하세요.

00:23:51.000 --> 00:23:56.000
우리 팀 전체는 당신이 새로운 버전의 ARKit을 손에 넣을 수 있게 되어 매우 기쁩니다.

00:23:56.000 --> 00:24:01.000
우리는 당신이 이 흥미진진한 새로운 플랫폼을 위해 만들 모든 획기적인 앱을 빨리 보고 싶습니다.

00:24:01.000 --> 00:24:03.000
라이언: 봐줘서 고마워!

00:24:03.000 --> 23:59:59.000
♪

