WEBVTT

00:00:00.000 --> 00:00:11.000
롭: 안녕.

00:00:11.000 --> 00:00:20.000
저는 사진 팀의 세바스찬 메디나와 함께 카메라 소프트웨어 팀의 롭 시무티스입니다. "더 반응이 빠른 카메라 경험 만들기" 세션에 오신 것을 환영합니다.

00:00:20.000 --> 00:00:28.000
우리는 AVFoundation 캡처 클래스와 PhotoKit 프레임워크에서 많은 새로운 강력한 API를 선보일 것입니다.

00:00:28.000 --> 00:00:31.000
먼저, 우리는 지연된 사진 처리에 대해 이야기할 것이다.

00:00:31.000 --> 00:00:37.000
그럼, 셔터 지연 없이 어떻게 "순간을 포착"할 수 있는지 보여줄게.

00:00:37.000 --> 00:00:41.000
셋째, 나는 우리의 새로운 반응형 캡처 API를 다룰 것이다.

00:00:41.000 --> 00:00:47.000
그리고 마지막으로, 나는 업데이트된 비디오 효과로 반응할 수 있는 다른 방법을 살펴볼 것이다.

00:00:47.000 --> 00:01:02.000
iOS 13부터 AVCapturePhotoSetting의 사진 품질 우선 순위 열거형 값을 사용하여 응답성을 변경하거나, 얼마나 빨리 캡처하고 처리된 사진을 다시 받은 다음 다음 사진을 찍을 수 있습니다.

00:01:02.000 --> 00:01:09.000
앱은 올바른 응답성을 달성하기 위해 다양한 열거형 값 중에서 선택할 수 있지만, 이미지 품질에 영향을 미쳤습니다.

00:01:09.000 --> 00:01:13.000
또는 항상 품질 가치를 사용하고 싶다면, 샷 투 샷 시간에 영향을 미칠 수 있습니다.

00:01:13.000 --> 00:01:26.000
iOS 17에서는 여전히 이 API를 사용할 수 있지만, 고품질의 사진을 얻는 동시에 원하는 샷을 얻을 수 있는 기회를 높일 수 있도록 새롭고 보완적인 API를 제공합니다.

00:01:26.000 --> 00:01:29.000
우리 팀이 우리 세션을 위해 만든 이 앱을 안내해 줄게.

00:01:29.000 --> 00:01:39.000
각 토글 스위치를 "on"으로 전환함으로써, 우리는 올해 새로운 기능을 활성화할 수 있으며, 더 반응이 빠른 사진 경험을 만들기 위해 하나씩 개념을 구축할 것입니다.

00:01:39.000 --> 00:01:43.000
그러니 연기된 사진 처리부터 시작해서 움직이자.

00:01:43.000 --> 00:01:54.000
오늘날, AVCapturePhotoOutput에서 최고 품질의 사진을 얻으려면, 사진을 캡처할 때 설정에서 "품질"의 사진 품질 우선 순위 열거형 값을 사용합니다.

00:01:54.000 --> 00:02:00.000
우리의 "균형"과 "품질" 값의 경우, 이것은 일반적으로 일종의 다중 프레임 융합과 소음 감소를 포함한다.

00:02:00.000 --> 00:02:05.000
아이폰 11 프로와 최신 모델에서, 우리의 가장 진보된 기술 중 하나는 "딥 퓨전"이라고 불린다.

00:02:05.000 --> 00:02:09.000
이것은 고해상도 사진에서 놀랍고 선명한 디테일을 제공한다.

00:02:09.000 --> 00:02:14.000
이 딥 퓨전 샷에서, 앵무새의 깃털은 매우 날카롭고, 정말 눈에 띈다.

00:02:14.000 --> 00:02:16.000
하지만 그건 대가를 치러야 해.

00:02:16.000 --> 00:02:22.000
이 처리는 다음 캡처 요청이 시작되기 전에 완료되어야 하며, 완료하는 데 시간이 좀 걸릴 수 있습니다.

00:02:22.000 --> 00:02:23.000
실제 사례를 살펴봅시다.

00:02:23.000 --> 00:02:29.000
나는 카메라 소프트웨어 팀이 어떻게 사무실에 가서 일을 끝내는지에 대한 프레젠테이션을 준비하고 있다.

00:02:29.000 --> 00:02:34.000
여기 Apple Park를 돌아다니기 위해 최신 자전거 기술을 사용하는 제 동료 Devin이 있습니다.

00:02:34.000 --> 00:02:41.000
내가 탭할 때, 나는 다음 샷을 찍기 전에 셔터 버튼이 한 샷에서 회전을 끝내기를 기다리고 있다.

00:02:41.000 --> 00:02:44.000
최종 결과는 훌륭한 딥 퓨전 사진이다.

00:02:44.000 --> 00:02:46.000
그냥 그 수염 디테일을 확인해봐!

00:02:46.000 --> 00:02:52.000
하지만 내가 두드리는 동안, 처리는 동기적으로 실행되며, 샷 투 샷 시간은 약간 느리게 느껴진다.

00:02:52.000 --> 00:02:59.000
그래서 나는 선명한 디테일로 좋은 사진을 찍었을 수도 있지만, 정확히 내가 찾고 있던 사진은 아닐 수도 있다.

00:02:59.000 --> 00:03:02.000
이벤트 다이어그램을 확인해 봅시다.

00:03:02.000 --> 00:03:10.000
설정으로 AVCapturePhotoOutput의 capturePhoto 메소드를 호출하면 대리인은 프로세스의 다양한 지점에서 콜백을 받습니다...

00:03:10.000 --> 00:03:13.000
resolvedSettings를 위한 willBeginCapture와 같은.

00:03:13.000 --> 00:03:21.000
카메라 소프트웨어 스택은 센서에서 프레임을 잡고, 우리의 처리 기술을 사용하여 딥 퓨전 이미지로 융합합니다...

00:03:21.000 --> 00:03:26.000
그런 다음 didFinishProcessingPhoto 대리 콜백을 통해 사진을 다시 보냅니다.

00:03:26.000 --> 00:03:32.000
이 처리는 다음 캡처가 일어나기 전에 완료되어야 하며, 시간이 좀 걸릴 수 있습니다.

00:03:32.000 --> 00:03:41.000
didFinishProcessingPhoto 콜백이 발생하기 전에 capturePhoto를 호출할 수도 있지만, 이전 사진의 처리가 완료될 때까지 시작되지 않습니다.

00:03:41.000 --> 00:03:44.000
지연된 사진 처리로, 이 타임라인은 줄어든다.

00:03:44.000 --> 00:03:55.000
사진을 요청하면, 적절한 경우, 카메라 파이프라인은 새로운 didFinishCapturingDeferredPhotoProxy 대리 콜백을 통해 가볍게 처리된 "프록시" 사진을 전달할 것입니다.

00:03:55.000 --> 00:03:59.000
이 프록시 사진을 자리 표시자로 도서관에 저장합니다.

00:03:59.000 --> 00:04:02.000
그리고 다음 사진은 즉시 찍을 수 있다.

00:04:02.000 --> 00:04:09.000
시스템은 카메라 세션이 해제되면 최종 사진을 얻기 위해 나중에 처리를 실행할 것이다.

00:04:09.000 --> 00:04:20.000
그래서 이제 앱의 설정에서 지연된 사진 처리를 켜면, 캡처 세션은 적절한 경우 캡처 시점에 프록시 사진을 전달하기 위해 재구성됩니다.

00:04:20.000 --> 00:04:29.000
그리고 나는 이전과 같은 날카롭고 매우 상세한 사진을 얻을 수 있지만, 최종 처리를 나중으로 연기함으로써 그 순간에 더 많은 사진을 찍을 수 있다.

00:04:29.000 --> 00:04:34.000
멋진 휠리. 그 자전거들은 단단해.

00:04:34.000 --> 00:04:39.000
우리가 간다. 그건 내 발표, 수염 그리고 모든 것을 위한 멋진 사진이야.

00:04:39.000 --> 00:04:44.000
그래서 당신에게 지연 처리된 사진을 주기 위해 상호 작용하는 모든 부분을 살펴봅시다.

00:04:44.000 --> 00:04:58.000
이전 WW 프레젠테이션의 간단한 재교육 과정으로, 사진을 캡처하기 위해 AVCaptureSession을 구성할 때, 카메라인 AVCaptureDevice와 함께 AVCaptureDeviceInput을 세션에 추가합니다.

00:04:58.000 --> 00:05:10.000
그런 다음 세션에 AVCapturePhotoOutput을 추가하고, 앱이 photoOutput에서 capturePhoto를 호출할 때 특정 형식이나 세션 프리셋, 일반적으로 "Photo" 세션 프리셋을 선택합니다.

00:05:10.000 --> 00:05:18.000
지연된 사진 처리에 더 적합한 사진 유형이라면, didFinishCapturing Deferred Photo Proxy로 다시 전화드리겠습니다.

00:05:18.000 --> 00:05:22.000
거기에서, 당신은 프록시 데이터를 사진 라이브러리로 보냅니다.

00:05:22.000 --> 00:05:30.000
그래서 지금 도서관에 있는 것은 프록시 사진이지만, 결국 최종 이미지를 사용하거나 공유하고 싶을 것입니다.

00:05:30.000 --> 00:05:42.000
최종 사진 처리는 라이브러리에서 이미지 데이터를 다시 요청할 때 주문형으로 이루어지거나, 장치가 유휴 상태인 것과 같이 시스템이 그렇게 하기에 좋다고 결정할 때 백그라운드에서 이루어집니다.

00:05:42.000 --> 00:05:46.000
그리고 이제 내 동료 세바스찬이 너에게 이걸 코딩하는 방법을 보여주도록 할게.

00:05:46.000 --> 00:05:48.000
너에게, 세바스찬.

00:05:48.000 --> 00:05:49.000
세바스찬: 고마워, 롭.

00:05:49.000 --> 00:05:52.000
안녕하세요, 제 이름은 세바스찬 메디나이고 저는 사진 팀의 엔지니어입니다.

00:05:52.000 --> 00:05:58.000
오늘 저는 지연 처리를 트리거하기 위해 PhotoKit을 통해 최근에 캡처한 이미지를 찍을 것입니다.

00:05:58.000 --> 00:06:05.000
그런 다음 PHImageManager 요청에서 이미지를 받는 새로운 기능을 보여주기 위해 동일한 이미지를 요청할 것입니다.

00:06:05.000 --> 00:06:13.000
PhotoKit을 통해 자산을 처리하기 전에 지연 처리를 위한 새로운 카메라 API가 설정되어 있는지 확인해야 합니다.

00:06:13.000 --> 00:06:18.000
이를 통해 내 앱은 PhotoKit을 통해 보낼 수 있는 지연된 사진 프록시 이미지를 수락할 수 있습니다.

00:06:18.000 --> 00:06:22.000
이제, 나는 이것을 활용하기 위해 코드를 작성할 것이다.

00:06:22.000 --> 00:06:27.000
여기서 AVCapturePhotoOutput과 AVCaptureSession 객체를 설정했습니다.

00:06:27.000 --> 00:06:31.000
이제, 나는 우리 세션을 구성하기 시작할 수 있다.

00:06:31.000 --> 00:06:38.000
이 경우, 저는 세션이 지연된 처리를 활용할 수 있도록 유형 사진의 사전 설정을 갖기를 원합니다.

00:06:38.000 --> 00:06:44.000
이제 캡처 장치를 잡고 장치 입력을 설정할 것입니다.

00:06:44.000 --> 00:06:50.000
그런 다음, 가능하다면, 장치 입력을 추가하겠습니다.

00:06:50.000 --> 00:06:55.000
다음으로, photoOutput을 추가할 수 있는지 확인하고 싶습니다.

00:06:55.000 --> 00:06:59.000
그리고 만약 그렇다면, 추가하세요.

00:06:59.000 --> 00:07:00.000
이제 새로운 것을 위해.

00:07:00.000 --> 00:07:10.000
지연 처리를 통해 캡처한 사진을 보낼 수 있도록 새로운 autoDeferredPhotoDeliverySupported 값이 사실인지 확인하겠습니다.

00:07:10.000 --> 00:07:22.000
이것이 통과되면, 나는 계속해서 재산 autoDeferredPhotoDeliveryEnabled와 함께 새로운 지연된 사진 배달을 선택할 수 있다.

00:07:22.000 --> 00:07:29.000
이 지연된 사진 배송 확인 및 활성화는 지연된 사진을 활성화하기 위해 카메라 코드에 추가해야 하는 모든 것입니다.

00:07:29.000 --> 00:07:34.000
마지막으로, 나는 우리의 세션 구성을 커밋할 것이다.

00:07:34.000 --> 00:07:42.000
그래서 이제 capturePhoto 메소드에 대한 호출이 이루어질 때, 우리가 받는 대리자 콜백은 지연된 프록시 객체를 보유할 것이다.

00:07:42.000 --> 00:07:50.000
이 콜백 중 하나의 예를 확인해 봅시다.

00:07:50.000 --> 00:08:01.000
이 사진 캡처 콜백에서 저는 최근에 캡처한 이미지와 관련된 카메라에서 AVCapturePhotoOutput 및 AVCaptureDeferredPhotoProxy 객체를 받고 있습니다.

00:08:01.000 --> 00:08:11.000
먼저, 우리가 적절한 사진 출력 값을 받고 있는지 확인하는 것이 좋기 때문에, 오류 매개 변수의 값을 확인하겠습니다.

00:08:11.000 --> 00:08:16.000
이제, 우리는 PhotoKit을 사용하여 포토 라이브러리에 이미지를 저장하기 시작할 것입니다.

00:08:16.000 --> 00:08:20.000
나는 공유 PHPhotoLibrary에서 변경을 수행할 것이다.

00:08:20.000 --> 00:08:26.000
하지만, 참고로, 사진 라이브러리에 쓰기 액세스만 하면 됩니다.

00:08:26.000 --> 00:08:36.000
그런 다음 AVCaptureDeferredPhotoProxy 객체에서 사진 데이터를 캡처할 것입니다.

00:08:36.000 --> 00:08:44.000
사진 라이브러리를 변경할 것이기 때문에, 관련 performChanges 인스턴스 방법을 설정해야 합니다.

00:08:44.000 --> 00:08:50.000
어떤 자산을 저장하는 것과 마찬가지로, 나는 PHAssetCreationRequest를 사용할 것이다.

00:08:50.000 --> 00:08:56.000
그런 다음 요청에 따라 'addResource' 방법을 호출하겠습니다.

00:08:56.000 --> 00:09:01.000
매개 변수를 위해, 나는 새로운 PHAssetResourceType '.photoProxy'를 사용할 것이다.

00:09:01.000 --> 00:09:05.000
이것은 PhotoKit이 이미지에서 지연 처리를 트리거하도록 지시하는 것이다.

00:09:05.000 --> 00:09:10.000
그런 다음 이전에 캡처한 프록시 이미지 데이터를 추가할 수 있습니다.

00:09:10.000 --> 00:09:15.000
그리고 이 경우 나는 어떤 옵션도 사용하지 않을 것이다.

00:09:15.000 --> 00:09:22.000
여기서, 지연 처리가 필요하지 않은 이미지 데이터에 이 새로운 리소스 유형을 사용하면 오류가 발생한다는 것을 아는 것이 중요합니다.

00:09:22.000 --> 00:09:28.000
그리고 오류에 대해 말하자면, 나는 완료 핸들러 내에서 그것들을 확인할 것이다.

00:09:28.000 --> 00:09:29.000
그리고 그건 그것만큼 쉬워.

00:09:29.000 --> 00:09:35.000
애플리케이션이 적합하다고 생각하는 대로 완료 핸들러 내에서 성공과 오류를 처리하세요.

00:09:35.000 --> 00:09:38.000
이제, 내가 우리의 자산을 되찾고 싶다고 말해.

00:09:38.000 --> 00:09:43.000
나는 PHImageManager 요청을 통해 그것을 달성할 수 있으므로, 그것을 하기 위해 코드를 살펴볼 것이다.

00:09:43.000 --> 00:09:52.000
매개 변수의 경우 PhotoKit을 통해 방금 보낸 이미지에 대한 PHAsset 개체, 반환할 이미지의 대상 크기 및 콘텐츠 모드가 있습니다.

00:09:52.000 --> 00:10:05.000
기본 PHImageManager 객체를 가져올게요. 그런 다음 imageManager 객체를 사용하여 requestImageForAsset 메소드에 대한 요청 이미지 자산을 호출할 수 있습니다.

00:10:05.000 --> 00:10:16.000
매개 변수의 경우, 이전에 가져온 자산, 목표 크기, 콘텐츠 모드를 사용할 것이며, 이 경우 어떤 옵션도 사용하지 않을 것입니다.

00:10:16.000 --> 00:10:25.000
이제 resultImage가 UIImage이고 정보가 이미지와 관련된 사전인 resultHandler를 통해 콜백을 처리할 수 있습니다.

00:10:25.000 --> 00:10:36.000
오늘, 첫 번째 콜백은 정보 dicitionary 키 PHImageResultIsDegradedKey로 저해상도 이미지를 보유할 것이지만 최종 이미지 콜백은 그렇지 않습니다.

00:10:36.000 --> 00:10:41.000
그래서, 나는 여기에 있는 사람들을 위해 수표를 만들 수 있다.

00:10:41.000 --> 00:10:53.000
PhotoKit을 통해 처리된 이미지를 만드는 추가는 개발자가 requestImageForAsset 방법에서 보조 이미지를 받을 수 있는 새로운 API를 불러올 수 있는 좋은 기회를 제공합니다.

00:10:53.000 --> 00:11:01.000
지연된 처리를 거치는 이미지가 마무리되는 데 더 오래 걸릴 수 있기 때문에, 이 새로운 보조 이미지는 그 동안 표시될 수 있습니다.

00:11:01.000 --> 00:11:08.000
이 새로운 이미지를 받으려면, PHImageRequestOptions에서 새로운 allowSecondaryDegradedImage를 사용해야 합니다.

00:11:08.000 --> 00:11:14.000
이 새로운 이미지는 requestImageForAsset 메소드의 현재 두 콜백 사이에 자리잡을 것이다.

00:11:14.000 --> 00:11:23.000
그리고 이미지와 관련된 정보 사전에는 오늘날 첫 번째 이미지 콜백에서 사용되는 PHImageResultIsDegradedKey에 대한 항목이 있을 것이다.

00:11:23.000 --> 00:11:29.000
무슨 일이 일어나고 있는지 더 잘 설명하기 위해, 오늘, requestImageForAsset 방법은 두 개의 이미지를 제공한다.

00:11:29.000 --> 00:11:36.000
첫 번째는 최종 고품질 이미지를 준비하는 동안 일시적으로 표시하기에 적합한 저품질 이미지입니다.

00:11:36.000 --> 00:11:44.000
이 새로운 옵션을 사용하면, 현재 두 가지 사이에, 최종이 처리되는 동안 표시할 새로운 고해상도 이미지가 제공됩니다.

00:11:44.000 --> 00:11:51.000
이 새로운 이미지를 표시하면 최종 이미지가 처리가 완료되기를 기다리는 동안 사용자에게 더 즐거운 시각적 경험을 제공할 것입니다.

00:11:51.000 --> 00:11:55.000
이제, 이것을 활용하기 위해 코드를 작성해 봅시다.

00:11:55.000 --> 00:12:02.000
하지만, 이번에는 PHImageRequestOptions 객체를 만들 것이다.

00:12:02.000 --> 00:12:06.000
그런 다음 새로운 allowSecondaryDegradedImage 옵션을 true로 설정하겠습니다.

00:12:06.000 --> 00:12:11.000
이렇게 하면 요청이 새로운 보조 이미지 콜백을 다시 보내는 것을 알 수 있습니다.

00:12:11.000 --> 00:12:22.000
여기서, 저는 이전에 작성한 requestImageForAsset 메소드를 재사용할 수 있지만, 이제 방금 만든 이미지 요청 옵션 객체를 추가할 것입니다.

00:12:22.000 --> 00:12:33.000
새로운 보조 이미지 정보 사전은 첫 번째 콜백과 마찬가지로 PHImageResultIsDegradedKey에 대한 진정한 값을 보유할 것이기 때문에, 여기에서 확인하겠습니다.

00:12:33.000 --> 00:12:37.000
그리고 그것은 새로운 2차 이미지 표현을 받기 위한 것이다.

00:12:37.000 --> 00:12:41.000
앱을 가장 잘 지원하기 위해 결과 핸들러 내에서 이미지를 처리하는 것을 잊지 마세요.

00:12:41.000 --> 00:12:54.000
이제 지연된 처리를 통해 사진 라이브러리에 이미지를 추가하는 방법과 최종 이미지가 처리가 완료되기를 기다리는 동안 앱에 표시할 이미지 요청에서 2차 고품질 이미지를 받는 방법을 알 수 있습니다.

00:12:54.000 --> 00:13:04.000
이러한 변경 사항은 새로운 지연 처리 PhotoKit 변경 사항과 함께 iOS 17, tvOS 17 및 macOS Sonoma부터 사용할 수 있습니다.

00:13:04.000 --> 00:13:10.000
이제, 나는 더 반응이 빠른 카메라를 만들기 위한 새로운 도구에 대한 더 많은 것을 위해 롭에게 돌려줄 것이다.

00:13:10.000 --> 00:13:12.000
롭: 멋져. 고마워, 세바스찬!

00:13:12.000 --> 00:13:17.000
지연된 사진 처리에 대한 훌륭한 경험을 할 수 있도록 세부 사항에 대해 자세히 알아봅시다.

00:13:17.000 --> 00:13:19.000
우리는 사진 라이브러리부터 시작할 거야.

00:13:19.000 --> 00:13:31.000
지연된 사진 처리를 사용하려면, 프록시 사진을 저장하기 위해 사진 라이브러리에 쓰기 권한이 있어야 하며, 앱이 최종 사진을 표시해야 하거나 어떤 식으로든 수정하려는 경우 읽기 권한이 필요합니다.

00:13:31.000 --> 00:13:38.000
하지만 기억하세요, 당신은 고객을 대신하여 가장 많은 프라이버시와 신뢰를 유지하기 위해 고객에게 필요한 최소한의 도서관 접근 권한만 요청해야 합니다.

00:13:38.000 --> 00:13:46.000
그리고, 프록시를 받으면, 가능한 한 빨리 fileDataRepresentation을 라이브러리로 가져오는 것이 좋습니다.

00:13:46.000 --> 00:13:51.000
앱이 백그라운드가 되면, 시스템이 중단하기 전에 실행할 수 있는 시간이 제한되어 있습니다.

00:13:51.000 --> 00:13:58.000
메모리 압력이 너무 커지면, 앱은 배경 설정 기간 동안 시스템에 의해 자동으로 강제 해제될 수 있습니다.

00:13:58.000 --> 00:14:05.000
가능한 한 빨리 프록시를 라이브러리에 가져오면 고객의 데이터 손실 가능성을 최소화할 수 있습니다.

00:14:05.000 --> 00:14:22.000
다음으로, 일반적으로 필터 적용과 같은 사진의 픽셀 버퍼를 변경하거나 AVCapturePhoto 파일 데이터 표현 사용자 지정자를 사용하여 AVCapturePhoto의 메타데이터 또는 기타 속성을 변경하는 경우, 처리가 완료되면 라이브러리에서 최종 사진에 적용되지 않습니다.

00:14:22.000 --> 00:14:28.000
나중에 PhotoKit API를 사용하여 사진을 조정해야 합니다.

00:14:28.000 --> 00:14:34.000
또한, 당신의 코드는 같은 세션에서 지연된 프록시와 지연되지 않은 사진을 모두 처리할 수 있어야 합니다.

00:14:34.000 --> 00:14:39.000
이것은 모든 사진이 필요한 추가 단계를 처리하는 것이 타당한 것은 아니기 때문이다.

00:14:39.000 --> 00:14:50.000
예를 들어, "품질" 사진 품질 우선 순위 지정 열거형 값으로 찍은 플래시 캡처는 딥 퓨전 사진처럼 샷 투 샷 절약의 혜택을 받는 방식으로 처리되지 않습니다.

00:14:50.000 --> 00:14:55.000
또한 AVCapturePhotoSettings에 옵트인 또는 옵트아웃 속성이 없다는 것을 알 수 있습니다.

00:14:55.000 --> 00:14:58.000
그것은 지연된 사진 처리가 자동이기 때문이다.

00:14:58.000 --> 00:15:05.000
만약 당신이 선택하고 카메라 파이프라인이 더 긴 처리 시간이 필요한 사진을 찍는다면, 그것은 당신에게 프록시를 다시 보낼 것입니다.

00:15:05.000 --> 00:15:11.000
적합하지 않다면, 그것은 당신에게 최종 사진을 보낼 것이므로, 샷당 옵트인 또는 옵트아웃할 필요가 없습니다.

00:15:11.000 --> 00:15:19.000
캡처 세션을 시작하기 전에 AVCapturePhotoOutput에 isAutoDeferredPhotoProcessingEnabled를 true로 원한다고 말하면 됩니다.

00:15:19.000 --> 00:15:22.000
마지막으로, 사용자 경험에 대해 이야기해 봅시다.

00:15:22.000 --> 00:15:31.000
지연된 사진 처리는 빠른 샷 투 샷 시간으로 최고의 이미지 품질을 제공하지만, 나중에 최종 처리를 지연시킬 뿐입니다.

00:15:31.000 --> 00:15:42.000
앱이 사용자가 공유 또는 편집을 위해 즉시 이미지를 원할 수 있고 우리가 제공하는 최고 품질의 사진에 관심이 없다면, 지연된 사진 처리를 사용하지 않는 것이 합리적일 수 있습니다.

00:15:42.000 --> 00:15:49.000
이 기능은 iPhone 11 Pro와 11 Pro Max 및 최신 iPhone부터 사용할 수 있습니다.

00:15:49.000 --> 00:15:57.000
그리고 여기 AVCapturePhotoOutput 작업과 라이브러리 권한 처리에 대한 훌륭한 관련 비디오가 있습니다.

00:15:57.000 --> 00:16:03.000
그리고 이제, 제로 셔터 지연으로 돌아서서 스케이트보드에 대해 이야기해 봅시다.

00:16:03.000 --> 00:16:10.000
카메라 소프트웨어 팀의 교통 수단에 대한 다가오는 프레젠테이션을 위해, 우리는 영상을 찍기 위해 스케이트 공원에 갔다.

00:16:10.000 --> 00:16:20.000
저는 iPhone 14 Pro에서 액션 모드로 동료를 촬영하고 있으며 슬라이드를 위한 고품질 영웅 액션 샷을 얻고 싶습니다.

00:16:20.000 --> 00:16:25.000
하지만, 스포일러 경고, 나는 스케이트보드를 타지 않을 거야.

00:16:25.000 --> 00:16:29.000
나는 내 동료 토모의 사진을 찍기 위해 셔터 버튼을 누른다.

00:16:29.000 --> 00:16:35.000
내가 사진을 검사하기 위해 카메라 롤에 갔을 때, 이것이 내가 얻은 것이었다.

00:16:35.000 --> 00:16:40.000
나는 그가 점프의 높이에 있을 때 셔터 버튼을 두드렸지만, 사진은 그의 착륙 사진이다.

00:16:40.000 --> 00:16:42.000
그건 정확히 내가 원했던 게 아니야.

00:16:42.000 --> 00:16:46.000
그래서 무슨 일이야? 셔터 지연.

00:16:46.000 --> 00:16:49.000
셔터 지연이 발생했다.

00:16:49.000 --> 00:16:58.000
"셔터 지연"은 캡처를 요청할 때부터 센서에서 하나 이상의 프레임을 판독하여 사진으로 융합하여 전달하기까지의 지연으로 생각할 수 있습니다.

00:16:58.000 --> 00:17:04.000
여기서, 시간은 왼쪽에서 오른쪽으로 가고, 왼쪽은 오래된 프레임이고, 오른쪽은 새로운 프레임이다.

00:17:04.000 --> 00:17:07.000
프레임 5가 카메라 뷰파인더에 있는 것이라고 말하세요.

00:17:07.000 --> 00:17:17.000
오늘날, capturePhoto를 호출할 때: AVCapturePhotoOutput의 설정으로, 카메라 파이프라인은 센서에서 프레임을 잡기 시작하고 처리 기술을 적용합니다.

00:17:17.000 --> 00:17:22.000
하지만 캡처된 프레임의 브래킷은 터치다운 후, 프레임 5 이후에 시작됩니다.

00:17:22.000 --> 00:17:26.000
당신이 얻는 것은 6에서 9 또는 그 이후의 프레임을 기반으로 한 사진입니다.

00:17:26.000 --> 00:17:31.000
초당 30프레임에서, 각 프레임은 33밀리초 동안 뷰파인더에 있다.

00:17:31.000 --> 00:17:35.000
그렇게 많이 들리지는 않지만, 행동이 끝나는 데는 정말 오래 걸리지 않는다.

00:17:35.000 --> 00:17:40.000
그것은 토모가 착륙하기에 충분히 길었고, 나는 그 영웅 샷을 놓쳤다.

00:17:40.000 --> 00:17:47.000
제로 셔터 지연이 활성화되면, 카메라 파이프라인은 과거의 프레임의 롤링 링 버퍼를 유지합니다.

00:17:47.000 --> 00:18:03.000
이제, 프레임 5는 뷰파인더에서 볼 수 있는 것이며, 탭하여 캡처하고, 카메라 파이프라인은 약간의 시간 여행을 하고, 링 버퍼에서 프레임을 잡고, 함께 융합하여 원하는 사진을 얻을 수 있습니다.

00:18:03.000 --> 00:18:18.000
그래서 이제 앱의 설정 창에서 두 번째 토글을 사용하여 제로 셔터 지연을 활성화하면, 토모가 공기를 잡을 때 셔터 버튼을 탭하면 프레젠테이션을 위해 원했던 "영웅" 샷 중 하나를 얻었습니다.

00:18:18.000 --> 00:18:24.000
앱에서 제로 셔터 지연을 얻기 위해 무엇을 해야 하는지 이야기해 봅시다.

00:18:24.000 --> 00:18:27.000
아무것도 아니야!

00:18:27.000 --> 00:18:39.000
우리는 지원되는 가장 높은 사진 품질이 사실인 AVCaptureSessionPresets 및 AVCaptureDeviceFormats를 위해 iOS 17 또는 그 이후에 연결되는 앱에서 제로 셔터 지연을 활성화했습니다.

00:18:39.000 --> 00:18:49.000
하지만, 테스트 중에 원하는 결과를 얻지 못했다면, AVCapturePhotoOutput.isZeroShutter LagEnabled를 false로 설정하여 옵트아웃할 수 있습니다.

00:18:49.000 --> 00:19:00.000
그리고 출력이 세션에 연결되면 isZeroShutterLagSupported가 참인지 확인하여 photoOutput이 구성된 사전 설정 또는 형식에 대해 제로 셔터 지연을 지원하는지 확인할 수 있습니다.

00:19:00.000 --> 00:19:17.000
플래시 캡처, 수동 노출을 위한 AVCaptureDevice 구성, 괄호 캡처 및 여러 카메라의 동기화된 프레임인 구성 사진 전달과 같은 특정 유형의 스틸 이미지 캡처는 제로 셔터 지연을 얻지 못합니다.

00:19:17.000 --> 00:19:30.000
카메라 파이프라인이 링 버퍼에서 프레임을 잡기 위해 시간을 거슬러 올라가기 때문에, 사용자는 캡처를 시작하는 제스처와 사진 출력을 보낼 때 사진 설정 사이에 긴 지연이 있는 경우 카메라 흔들림을 사진으로 유도할 수 있습니다.

00:19:30.000 --> 00:19:36.000
따라서 탭 이벤트와 사진 출력에 대한 capturePhoto API 호출 사이에 하는 모든 작업을 최소화하고 싶을 것입니다.

00:19:36.000 --> 00:19:44.000
더 반응이 빠른 사진 경험을 만들기 위한 기능을 마무리하면서, 이제 반응형 캡처 API를 다루겠습니다.

00:19:44.000 --> 00:19:56.000
이것은 고객이 겹치는 캡처를 찍고, 사진 품질을 조정하여 촬영 시간을 우선시하며, 다음 사진을 찍을 수 있을 때 훌륭한 UI 피드백을 제공할 수 있는 API 그룹입니다.

00:19:56.000 --> 00:19:59.000
먼저, 주요 API, 반응형 캡처.

00:19:59.000 --> 00:20:05.000
스케이트 공원으로 돌아가서, 이전에 두 가지 기능을 활성화한 상태에서, 나는 초당 약 두 장의 사진을 찍을 수 있다.

00:20:05.000 --> 00:20:09.000
우리는 그것을 명확하게 하기 위해 영상을 늦췄다.

00:20:09.000 --> 00:20:18.000
초당 두 프레임에서, 당신은 공중에서 토모의 행동을 많이 볼 수 없으며 이것은 내가 끝낸 최고의 사진이었다.

00:20:18.000 --> 00:20:21.000
꽤 좋아, 하지만 우리가 더 잘할 수 있는지 보자.

00:20:21.000 --> 00:20:26.000
이제 반응형 캡처 기능을 활성화하기 위해 세 번째와 네 번째 스위치를 켤 것입니다.

00:20:26.000 --> 00:20:30.000
나는 잠시 후에 빠른 캡처 우선 순위를 검토할 것이다.

00:20:30.000 --> 00:20:32.000
하지만 먼저, 공원으로 돌아가!

00:20:32.000 --> 00:20:36.000
그리고 다시 해보자.

00:20:36.000 --> 00:20:44.000
반응형 캡처를 사용하면 같은 시간에 더 많은 사진을 찍을 수 있으며, 올바른 사진을 찍을 수 있는 기회를 높일 수 있습니다.

00:20:44.000 --> 00:20:47.000
그리고 내 발표를 시작하기 위한 "영웅" 샷이 있어.

00:20:47.000 --> 00:20:50.000
그 팀은 정말 좋아할 거야!

00:20:50.000 --> 00:21:05.000
설정 방법으로 AVCapturePhotoOutput.capturePhoto에 대한 호출을 세 가지 단계를 거치는 것으로 생각할 수 있습니다: 센서에서 프레임을 캡처하고, 그 프레임을 최종 압축되지 않은 이미지로 처리한 다음, 사진을 HEIC 또는 JPEG로 인코딩합니다.

00:21:05.000 --> 00:21:20.000
인코딩이 완료되면, 사진 출력은 대리인의 didFinishProcessingPhoto 콜백을 호출하거나, 지연된 사진 처리 API를 선택한 경우, 적절한 샷인 경우 didFinishCapturing Deferred Photo Proxy를 호출할 수 있습니다.

00:21:20.000 --> 00:21:26.000
하지만 일단 "캡처" 단계가 완료되고 "처리"가 시작되면, 사진 출력은 이론적으로 또 다른 캡처를 시작할 수 있다.

00:21:26.000 --> 00:21:31.000
그리고 이제, 그 이론은 현실이며, 당신의 앱에서 사용할 수 있습니다.

00:21:31.000 --> 00:21:45.000
주요 반응형 캡처 API를 선택함으로써, 사진 출력은 이러한 단계와 겹치므로 다른 요청이 처리 단계에 있는 동안 새로운 사진 캡처 요청을 시작할 수 있으며, 고객에게 더 빠르고 일관된 연속 촬영을 제공합니다.

00:21:45.000 --> 00:21:57.000
이것은 사진 출력에 사용되는 피크 메모리를 증가시킬 것이므로, 앱이 많은 메모리를 사용하는 경우 시스템에 압력을 가할 것이며, 이 경우 선호하거나 거부해야 할 수도 있습니다.

00:21:57.000 --> 00:22:02.000
우리의 타임라인 다이어그램으로 돌아가서, 당신은 두 장의 사진을 연속으로 찍습니다.

00:22:02.000 --> 00:22:11.000
당신의 대리인은 willBeginCaptureFor resolvedSettings와 사진 A의 didFinishCaptureFor resolvedSettings에 대해 다시 호출될 것입니다.

00:22:11.000 --> 00:22:24.000
하지만 사진이 인코딩되어 당신에게 전달되는 사진 A에 대한 didFinish 처리 사진 콜백을 받는 대신, 사진 B에 대한 해결된 설정에 대한 첫 번째 willBeginCapture를 얻을 수 있습니다.

00:22:24.000 --> 00:22:32.000
이제 두 개의 기내 사진 요청이 있으므로, 코드가 인터리브된 사진에 대한 콜백을 제대로 처리하는지 확인해야 합니다.

00:22:32.000 --> 00:22:38.000
겹치는 반응형 캡처를 얻으려면, 먼저 지원될 때 제로 셔터 지연을 활성화하세요.

00:22:38.000 --> 00:22:42.000
반응형 캡처 지원을 받으려면 켜져 있어야 합니다.

00:22:42.000 --> 00:22:53.000
그런 다음 AVCapturePhotoOutput isResponsiveCaptureSupported API를 사용하여 사진 출력이 사전 설정 또는 형식에 대해 지원하는지 확인한 다음 AVCapturePhotoOutput을 설정하여 켜십시오.

00:22:53.000 --> 00:22:56.000
.isResponsiveCaptureEnabled to true.

00:22:56.000 --> 00:23:02.000
이전에, 우리는 "빠른 캡처 우선 순위 지정"을 활성화했기 때문에, 이제 간단히 검토하겠습니다.

00:23:02.000 --> 00:23:17.000
사진 출력을 위해 켜지면, 짧은 기간 동안 여러 캡처가 촬영될 때를 감지하고, 이에 대응하여 사진 품질을 최고 품질 설정에서 더 많은 "균형" 품질 설정으로 조정하여 샷 투 샷 시간을 유지합니다.

00:23:17.000 --> 00:23:22.000
하지만, 이것은 사진 품질에 영향을 미칠 수 있기 때문에, 기본적으로 꺼져 있습니다.

00:23:22.000 --> 00:23:26.000
Camera.app의 설정 창에서, 이것은 "더 빠른 촬영 우선 순위 지정"이라고 불린다.

00:23:26.000 --> 00:23:38.000
우리는 일관된 샷 투 샷 시간이 기본적으로 더 중요하다고 생각하기 때문에 Camera.app에 대해 기본적으로 켜기로 선택했지만, 앱과 고객을 위해 다르게 선택할 수 있습니다.

00:23:38.000 --> 00:23:53.000
지금까지 예상할 수 있듯이, 지원될 때 사진 출력에서 "빠른 캡처 우선 순위 지정이 지원되는" 속성을 확인할 수 있으며, 지원될 때, 귀하 또는 귀하의 고객이 이 기능을 사용하려는 경우 "빠른 캡처 우선 순위 지정이 활성화됨"을 true로 설정할 수 있습니다.

00:23:53.000 --> 00:23:57.000
이제, 버튼 상태와 모양 관리에 대해 이야기해 봅시다.

00:23:57.000 --> 00:24:06.000
사진 출력은 다음 캡처를 시작할 준비가 되었는지 또는 처리 중일 때의 지표를 제공할 수 있으며, 사진 캡처 버튼을 적절하게 업데이트할 수 있습니다.

00:24:06.000 --> 00:24:11.000
이것은 AVCapturePhotoOutput CaptureReadiness라는 값의 열거형을 통해 이루어집니다.

00:24:11.000 --> 00:24:21.000
사진 출력은 "실행되지 않음", "준비됨" 및 세 가지 "준비되지 않음" 상태일 수 있습니다: "잠시", "캡쳐 대기 중" 또는 "처리 대기 중".

00:24:21.000 --> 00:24:33.000
"준비되지 않음" 열거형은 설정으로 capturePhoto를 호출하면 캡처와 사진 전송 사이에 더 긴 대기 시간이 발생하여 이전에 이야기했던 셔터 지연이 증가한다는 것을 나타냅니다.

00:24:33.000 --> 00:24:40.000
앱은 새로운 클래스인 AVCapturePhotoOutputReadinessCoordinator를 사용하여 이 상태 변경을 들을 수 있습니다.

00:24:40.000 --> 00:24:45.000
이것은 사진 출력의 준비 상태가 변경될 때 제공하는 대리자 객체에 대한 콜백을 만듭니다.

00:24:45.000 --> 00:24:52.000
반응형 캡처 또는 빠른 캡처 우선 순위 API를 사용하지 않더라도 이 클래스를 사용할 수 있습니다.

00:24:52.000 --> 00:25:01.000
준비 코디네이터와 준비 열거형을 사용하여 셔터 가용성을 전달하고 버튼 모양을 수정하는 방법은 다음과 같습니다.

00:25:01.000 --> 00:25:14.000
우리 세션용 앱은 "준비되지 않은" 열거형 값을 처리할 때 캡처 버튼의 사용자 상호 작용 이벤트를 끄고 추가 요청이 실수로 여러 탭으로 인해 긴 셔터 지연이 발생하는 것을 방지합니다.

00:25:14.000 --> 00:25:24.000
탭하고 설정 요청이 있는 capturePhoto가 queueed된 후, captureReadiness 상태는 .ready와 .notReadyMomentarily 열거형 값 사이에 있습니다.

00:25:24.000 --> 00:25:28.000
플래시 캡처는 .notReadyWaitingForCapture 상태를 강타한다.

00:25:28.000 --> 00:25:34.000
플래시가 발사될 때까지, 사진 출력은 센서에서 프레임을 얻지 못했기 때문에 버튼이 흐리게 표시됩니다.

00:25:34.000 --> 00:25:49.000
마지막으로, 올해 셔터 지연이 없고 다른 기능을 사용하지 않는다면, 각 사진의 캡처 및 처리가 완료됨에 따라 .notReadyWaitingForProcessing 열거형 값이 현재 준비 상태인 동안 스피너를 보여줄 수 있습니다.

00:25:49.000 --> 00:25:53.000
그래서 코드에서 준비 코디네이터를 사용하는 방법은 다음과 같습니다.

00:25:53.000 --> 00:26:02.000
먼저, 사진 출력을 위한 준비 코디네이터를 만들고, 준비 상태에 대한 콜백을 받기 위해 적절한 대리 객체를 설정하십시오.

00:26:02.000 --> 00:26:07.000
그런 다음, 각 캡처할 때, 평소처럼 사진 설정을 설정하세요.

00:26:07.000 --> 00:26:14.000
그런 다음, 준비 코디네이터에게 해당 설정에 대한 캡처 요청의 준비 상태를 추적하기 시작하라고 말하세요.

00:26:14.000 --> 00:26:18.000
그런 다음 사진 출력에서 capturePhoto를 호출하세요.

00:26:18.000 --> 00:26:23.000
준비 코디네이터는 captureReadinessDidChange 대리 콜백을 호출할 것이다.

00:26:23.000 --> 00:26:34.000
받은 준비 열거 값을 기반으로 캡처 버튼의 상태와 모양을 업데이트하여 고객이 다음에 캡처할 수 있는 시기에 대한 최상의 피드백을 제공합니다.

00:26:34.000 --> 00:26:47.000
반응형 캡처 및 빠른 캡처 우선 순위 API는 A12 Bionic 칩 이상이 탑재된 iPhone에서 사용할 수 있으며, AVCapturePhotoOutput이 지원되는 모든 곳에서 준비 코디네이터를 사용할 수 있습니다.

00:26:47.000 --> 00:26:56.000
그리고 이제 저는 매우 선명하고 고품질의 사진을 제공하는 가장 반응이 빠른 카메라 경험을 통해 앱에서 모든 새로운 기능을 활성화했습니다.

00:26:56.000 --> 00:27:00.000
하지만, 더 나은 경험을 하기 위해 그것들을 모두 사용할 필요는 없습니다.

00:27:00.000 --> 00:27:04.000
앱에 적합한 것만 사용할 수 있습니다.

00:27:04.000 --> 00:27:07.000
우리는 오늘 업데이트된 비디오 효과로 세션을 마칠 것입니다.

00:27:07.000 --> 00:27:19.000
이전에 macOS의 제어 센터는 센터 스테이지, 초상화 및 스튜디오 라이트와 같은 카메라 스트리밍 기능에 대한 옵션을 제공했습니다.

00:27:19.000 --> 00:27:25.000
macOS 소노마를 사용하면 비디오 효과를 제어 센터에서 자체 메뉴로 옮겼습니다.

00:27:25.000 --> 00:27:32.000
카메라 또는 화면 공유의 미리보기를 볼 수 있으며, 인물 사진 모드와 스튜디오 라이트와 같은 비디오 효과를 활성화할 수 있습니다.

00:27:32.000 --> 00:27:39.000
초상화와 스튜디오 라이트 효과는 이제 강도를 조절할 수 있으며, 스튜디오 라이트는 더 많은 장치에서 사용할 수 있습니다.

00:27:39.000 --> 00:27:43.000
그리고 우리는 "반응"이라고 불리는 새로운 효과 유형을 가지고 있다.

00:27:43.000 --> 00:27:52.000
화상 통화를 할 때, 당신은 당신이 아이디어를 좋아한다고 표현하거나, 좋은 소식에 대해 엄지손가락을 올리고 싶을 수도 있습니다.

00:27:52.000 --> 00:27:58.000
반응은 당신의 비디오를 풍선, 색종이 조각 등과 매끄럽게 혼합합니다.

00:27:58.000 --> 00:28:09.000
반응은 초상화 및 스튜디오 조명 효과의 템플릿을 따르며, 앱에서 코드 변경 없이 즉시 사용할 수 있는 시스템 수준의 카메라 기능입니다.

00:28:09.000 --> 00:28:17.000
인물 사진 및 스튜디오 조명 효과에 대한 자세한 내용은 2021년 세션인 "카메라 캡처의 새로운 기능"을 확인하세요.

00:28:17.000 --> 00:28:27.000
비디오 스트림에서 반응을 표시하는 세 가지 방법이 있습니다. 먼저, macOS의 새로운 비디오 효과 메뉴의 하단 창에서 반응 효과를 클릭할 수 있습니다.

00:28:27.000 --> 00:28:39.000
둘째, 앱은 반응 유형에 대해 AVCaptureDevice.performEffect를 호출할 수 있습니다. 예를 들어, 참가자가 반응을 수행하기 위해 클릭할 수 있는 앱의 보기 중 하나에 반응 버튼 세트가 있을 수 있습니다.

00:28:39.000 --> 00:28:44.000
그리고 셋째, 반응이 활성화되면, 제스처를 통해 보낼 수 있다.

00:28:44.000 --> 00:28:47.000
이거 확인해 보자.

00:28:47.000 --> 00:29:14.000
엄지손가락을 올리고, 엄지손가락을 내리고, 두 개의 엄지손가락을 올린 불꽃놀이, 심장, 하나의 승리 기호가 있는 풍선, 두 개의 엄지손가락을 내린 비, 두 개의 승리 기호가 있는 색종이, 그리고 내가 개인적으로 가장 좋아하는 레이저를 할 수 있습니다.

00:29:14.000 --> 00:29:18.000
이봐, 그건 좋은 효과들이야.

00:29:18.000 --> 00:29:27.000
캡처 세션에서 사용하려는 AVCaptureDeviceFormat의 reactionEffectsSupported 속성을 보고 반응 효과 지원을 확인할 수 있습니다.

00:29:27.000 --> 00:29:36.000
AVCaptureDevice에는 제스처 인식이 켜져 있고 반응 효과가 활성화되었을 때를 알기 위해 읽거나 키 값을 관찰할 수 있는 속성이 있습니다.

00:29:36.000 --> 00:29:42.000
이것들은 사용자의 통제 하에 있기 때문에, 당신의 앱은 그것들을 켜거나 끌 수 없습니다.

00:29:42.000 --> 00:29:44.000
iOS에서, 그것은 같은 생각이다.

00:29:44.000 --> 00:29:51.000
참가자는 제스처 인식을 켜거나 끄기 위해 제어 센터로 가고, 이런 일이 일어날 때 키 값을 관찰할 수 있습니다.

00:29:51.000 --> 00:29:55.000
그러나, iOS에서 앱에서 효과를 트리거하려면, 프로그래밍 방식으로 해야 합니다.

00:29:55.000 --> 00:29:59.000
그러니 지금 네가 그걸 어떻게 할 수 있는지 살펴보자.

00:29:59.000 --> 00:30:07.000
"canPerformReactionEffects" 속성이 참이면, reactionType 메소드에 대한 performEffect를 호출하면 반응이 비디오 피드로 렌더링됩니다.

00:30:07.000 --> 00:30:10.000
당신의 앱은 효과를 트리거하기 위한 버튼을 제공해야 합니다.

00:30:10.000 --> 00:30:19.000
제스처를 통해 들어오는 반응은 감지에 사용되는 단서에 따라 performEffect\를 호출할 때와 비디오의 다른 위치에서 렌더링될 수 있습니다.

00:30:19.000 --> 00:30:32.000
AVCaptureDevice가 캡처 세션에서 인식하고 비디오 콘텐츠로 렌더링할 수 있는 엄지손가락이나 풍선과 같은 모든 다양한 반응 효과에 대해 AVCaptureReactionType이라는 새로운 열거형이 있습니다.

00:30:32.000 --> 00:30:41.000
그리고 "AVCaptureDevice.availableReactionTypes" 속성은 구성된 형식이나 세션 프리셋을 기반으로 AVCaptureReactionTypes 세트를 반환합니다.

00:30:41.000 --> 00:30:47.000
이러한 효과에는 또한 자신의 보기에 배치할 수 있는 시스템 UIImages가 내장되어 있습니다.

00:30:47.000 --> 00:31:02.000
AVCaptureReactionType을 받아들이고 UIImagesystemName 생성자와 함께 사용할 적절한 문자열을 반환하는 새로운 함수 AVCaptureReactionType.systemImageName에서 반응에 대한 systemName을 얻을 수 있습니다.

00:31:02.000 --> 00:31:10.000
그리고 반응 효과가 진행 중일 때 적절한 이름의 AVCaptureDevice.reactionEffectsInProgress를 알려주는 API가 있습니다.

00:31:10.000 --> 00:31:18.000
사용자가 여러 반응 효과를 순서대로 수행할 때, 그들은 잠시 겹칠 수 있으므로, 이것은 상태 객체의 배열을 반환합니다.

00:31:18.000 --> 00:31:22.000
키-값 관찰을 사용하여 언제 시작하고 끝나는지 알 수 있습니다.

00:31:22.000 --> 00:31:33.000
Voice-over-IP 회의 앱인 경우, 이 정보를 사용하여 효과에 대한 메타데이터를 원격 보기로 보낼 수도 있습니다. 특히 발신자가 대역폭 이유로 비디오를 껐을 때 더욱 그렇습니다.

00:31:33.000 --> 00:31:38.000
예를 들어, 다른 발신자를 대신하여 UI에 효과 아이콘을 표시할 수 있습니다.

00:31:38.000 --> 00:31:43.000
비디오 스트림에 효과 애니메이션을 렌더링하는 것은 비디오 인코더에 어려울 수 있습니다.

00:31:43.000 --> 00:31:49.000
그들은 콘텐츠의 복잡성을 증가시키고, 그것을 인코딩하기 위해 더 큰 비트레이트 예산이 필요할 수 있다.

00:31:49.000 --> 00:31:57.000
key-Value를 관찰하여 reactionEffectsInProgress를 통해 렌더링이 진행되는 동안 인코더를 조정할 수 있습니다.

00:31:57.000 --> 00:32:02.000
앱이 가능하다면, 효과가 렌더링되는 동안 인코더의 비트 전송률을 높일 수 있습니다.

00:32:02.000 --> 00:32:27.000
또는 VideoToolbox를 통해 저지연 비디오 인코더를 사용하고 MaxAllowedFrameQP VTCompressionPropertyKey를 설정하는 경우, 지원되는 해상도, 프레임 속도 및 비트 전송률 계층을 포함한 다양한 비디오 구성을 사용하여 앱에서 테스트를 실행하고 효과가 진행되는 동안 최대 허용 FrameQP를 조정하는 것이 좋습니다.

00:32:27.000 --> 00:32:35.000
MaxAllowedFrameQP 값이 낮으면 효과의 프레임 속도가 손상될 수 있으며 비디오 프레임 속도가 낮아질 수 있습니다.

00:32:35.000 --> 00:32:44.000
2021년 세션 "VideoToolbox로 저지연 비디오 인코딩 탐색"에는 이 기능 작업에 대한 더 많은 정보가 있습니다.

00:32:44.000 --> 00:32:49.000
또한 효과가 진행 중일 때 비디오 프레임 속도가 바뀔 수 있다는 것을 알아야 합니다.

00:32:49.000 --> 00:32:58.000
예를 들어, AVCaptureSession을 초당 60프레임으로 실행하도록 구성한 경우, 효과가 실행되지 않는 동안 초당 60프레임을 얻을 수 있습니다.

00:32:58.000 --> 00:33:04.000
하지만 효과가 진행되는 동안, 초당 30프레임과 같은 다른 프레임 속도를 얻을 수 있습니다.

00:33:04.000 --> 00:33:11.000
이것은 최종 프레임 속도가 지정한 것보다 낮을 수 있는 초상화 및 스튜디오 조명 효과의 모델을 따릅니다.

00:33:11.000 --> 00:33:22.000
프레임 속도를 보려면, 장치에서 구성 중인 형식에 대한 AVCaptureDeviceFormat.videoFrameRateRange ForReactionEffectsInProgress를 확인하세요.

00:33:22.000 --> 00:33:29.000
다른 AVCaptureDeviceFormat 속성과 마찬가지로, 이것은 당신이 제어할 수 있는 것이 아니라 앱에 대한 정보입니다.

00:33:29.000 --> 00:33:35.000
macOS와 연속성 카메라를 사용하는 tvOS 앱에서는 반응 효과가 항상 활성화됩니다.

00:33:35.000 --> 00:33:41.000
iOS와 iPad OS에서, 애플리케이션은 Info.plist의 변경을 통해 옵트인할 수 있습니다.

00:33:41.000 --> 00:33:54.000
UIBackgroundModes 배열에서 VoIP 애플리케이션 카테고리라고 광고하거나 YES 값으로 NSCameraReactionEffectsEnabled를 추가하여 선택합니다.

00:33:54.000 --> 00:34:18.000
반응 효과와 제스처 인식은 연속성 카메라 장치를 사용하는 iPhone 12, Apple Silicon Mac 및 Intel Mac 및 Apple TV, USB-C iPad 또는 Apple Silicon Mac에 연결된 Apple Studio 디스플레이, USB-C iPad 또는 Apple Silicon Mac에 연결된 타사 카메라와 같은 A14 칩 또는 그 이상의 iPhone 및 iPad에서 사용할 수 있습니다.

00:34:18.000 --> 00:34:23.000
그리고 그것은 올해 새로운 API로 반응형 카메라 경험에 대한 세션을 마무리합니다.

00:34:23.000 --> 00:34:44.000
우리는 향상된 이미지 품질로 가장 반응이 빠른 사진 앱을 만들 수 있는 새로운 가능성을 제공하기 위해 지연된 사진 처리, 제로 셔터 지연 및 반응형 캡처 API에 대해 이야기했으며, 또한 사용자가 새로운 "반응"을 포함하여 업데이트된 비디오 효과로 자신을 표현할 수 있는 방법을 다루었습니다.

00:34:44.000 --> 00:34:48.000
나는 네가 모든 훌륭한 새로운 기능에 어떻게 반응하는지 빨리 보고 싶어.

00:34:48.000 --> 00:34:49.000
봐줘서 고마워.

00:34:49.000 --> 23:59:59.000
.

