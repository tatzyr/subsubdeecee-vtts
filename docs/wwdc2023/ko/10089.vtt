WEBVTT

00:00:00.000 --> 00:00:04.000
♪ 부드러운 기악 힙합 ♪

00:00:04.000 --> 00:00:10.000
♪

00:00:10.000 --> 00:00:14.000
안녕하세요, 저는 Apple의 소프트웨어 엔지니어인 Pau Sastre Miguel입니다.

00:00:14.000 --> 00:00:21.000
오늘 저는 xrOS에서 메탈로 몰입형 경험을 만드는 방법에 대해 이야기할 것입니다.

00:00:21.000 --> 00:00:29.000
올해, xrOS의 출시와 함께, 당신은 Apple 생태계에서 친숙한 기술로 몰입형 경험을 만들 수 있을 것입니다.

00:00:29.000 --> 00:00:35.000
RealityKit을 사용하면 가상 콘텐츠와 현실 세계를 혼합하는 경험을 만들 수 있습니다.

00:00:35.000 --> 00:00:47.000
반면에, 애플리케이션이 사용자를 완전히 몰입형으로 데려갈 경우, xrOS를 사용하면 실제 콘텐츠를 자신의 가상 콘텐츠로 완전히 대체할 수 있습니다.

00:00:47.000 --> 00:00:52.000
완전히 몰입감 있는 경험을 만들 때, 렌더링 방법과 관련하여 몇 가지 선택지가 있습니다.

00:00:52.000 --> 00:00:58.000
당신은 여전히 RealityKit을 사용할 수 있으며, 원한다면 Metal과 ARKit API를 선택할 수 있습니다.

00:00:58.000 --> 00:01:12.000
RecRoom은 CompositorServices를 사용하여 렌더링 세션을 만들고, Metal API를 사용하여 프레임을 렌더링하고, ARKit을 사용하여 세계와 손 추적을 얻는 완전한 몰입형 경험을 제공하는 응용 프로그램의 좋은 예입니다.

00:01:12.000 --> 00:01:18.000
그들은 유니티 편집기 덕분에 이 모든 기술을 지원할 수 있었다.

00:01:18.000 --> 00:01:25.000
자신만의 엔진을 작성하고 싶다면, CompositorServices API를 사용하면 xrOS에서 Metal 렌더링에 액세스할 수 있습니다.

00:01:25.000 --> 00:01:32.000
세계 추적과 손 추적을 추가하는 ARKit과 결합하여 완전히 몰입할 수 있는 경험을 만들 수 있습니다.

00:01:32.000 --> 00:01:37.000
CompositorServices는 xrOS에서 작동하도록 엔진을 구성하는 열쇠입니다.

00:01:37.000 --> 00:01:42.000
렌더링 루프를 설정하는 방법과 한 프레임을 렌더링하는 방법을 보여드리겠습니다.

00:01:42.000 --> 00:01:47.000
마지막으로, ARKit을 사용하여 대화형 경험을 만드는 방법을 보여드리겠습니다.

00:01:47.000 --> 00:01:52.000
xrOS 앱의 아키텍처로 시작하세요.

00:01:52.000 --> 00:01:59.000
메탈 API와 메탈 렌더링 기술에 대한 이전 경험이 있다면 오늘의 세션을 최대한 활용할 수 있습니다.

00:01:59.000 --> 00:02:07.000
이전에 Metal을 사용해 본 적이 없다면, developer.apple.com/Metal에서 코드 샘플과 문서를 확인하세요.

00:02:07.000 --> 00:02:15.000
Metal로 xrOS에서 몰입형 경험을 만들 때, SwiftUI로 시작하여 애플리케이션과 렌더링 세션을 만들 수 있습니다.

00:02:15.000 --> 00:02:25.000
렌더링 세션을 만든 후, C 또는 C++와 같이 더 익숙한 언어로 전환하여 엔진의 내부 부분을 정의할 수 있습니다.

00:02:25.000 --> 00:02:29.000
SwiftUI 앱 프로토콜을 준수하는 유형을 만드는 것으로 시작합니다.

00:02:29.000 --> 00:02:34.000
이 프로토콜을 준수하려면, 앱에서 장면 목록을 정의할 것입니다.

00:02:34.000 --> 00:02:37.000
xrOS에는 세 가지 주요 장면 유형이 있다.

00:02:37.000 --> 00:02:42.000
창 유형은 macOS와 같은 2D 플랫폼과 유사한 경험을 제공한다.

00:02:42.000 --> 00:02:48.000
볼륨 유형은 범위 내에서 콘텐츠를 렌더링하고 공유 공간에서 다른 응용 프로그램과 공존합니다.

00:02:48.000 --> 00:02:53.000
그리고 ImmersiveSpace를 사용하면 어디서나 콘텐츠를 렌더링할 수 있습니다.

00:02:53.000 --> 00:03:00.000
메탈로 완전히 몰입형 경험을 할 때마다, 당신은 ImmersiveSpace 유형을 선택할 것입니다.

00:03:00.000 --> 00:03:05.000
ImmersiveSpace는 xrOS에서 사용할 수 있는 새로운 SwiftUI 장면 유형입니다.

00:03:05.000 --> 00:03:10.000
그것은 완전히 몰입할 수 있는 경험을 위한 용기 역할을 한다.

00:03:10.000 --> 00:03:16.000
ImmersiveSpace를 사용하는 방법을 배우려면, "SwiftUI로 창 너머로 이동" 세션을 확인하세요.

00:03:16.000 --> 00:03:26.000
ImmersiveSpace 장면을 만들 때, 애플리케이션은 ImmersiveSpaceContent 프로토콜을 준수하는 유형을 사용하여 콘텐츠를 제공합니다.

00:03:26.000 --> 00:03:32.000
종종, 몰입형 공간 장면을 위한 콘텐츠를 만들 때, 애플리케이션은 RealityKit을 사용합니다.

00:03:32.000 --> 00:03:35.000
그것은 후드 아래에서 CoreAnimation과 MaterialX를 사용한다.

00:03:35.000 --> 00:03:42.000
하지만 대신, 메탈의 힘을 사용하여 애플리케이션의 콘텐츠를 렌더링하고 싶다면, 다른 옵션이 있습니다.

00:03:42.000 --> 00:03:50.000
CompositorServices API는 Metal과 ARKit을 사용하여 애플리케이션에 몰입형 렌더링 기능을 제공합니다.

00:03:50.000 --> 00:04:00.000
xrOS에 도입된 새로운 CompositorServices API는 ImmersiveSpace의 내용을 렌더링할 수 있는 Metal 렌더링 인터페이스를 제공합니다.

00:04:00.000 --> 00:04:05.000
CompositorServices를 사용하면 응용 프로그램이 compositor 서버로 직접 렌더링할 수 있습니다.

00:04:05.000 --> 00:04:14.000
대기 시간을 최소화하기 위해 IPC 오버헤드가 낮으며, C와 Swift API를 모두 지원하기 위해 처음부터 구축되었습니다.

00:04:14.000 --> 00:04:20.000
CompositorServices를 사용할 때, ImmersiveSpaceContent는 CompositorLayer라고 불린다.

00:04:20.000 --> 00:04:25.000
CompositorLayer를 만들려면 두 개의 매개 변수를 제공해야 합니다.

00:04:25.000 --> 00:04:29.000
첫 번째는 CompositorLayerConfiguration 프로토콜이다.

00:04:29.000 --> 00:04:34.000
이 프로토콜은 렌더링 세션의 동작과 기능을 정의합니다.

00:04:34.000 --> 00:04:37.000
두 번째는 레이어렌더러이다.

00:04:37.000 --> 00:04:40.000
이것은 레이어 렌더링 세션의 인터페이스입니다.

00:04:40.000 --> 00:04:45.000
당신의 애플리케이션은 이 객체를 사용하여 새 프레임을 예약하고 렌더링할 것입니다.

00:04:45.000 --> 00:04:51.000
메탈로 몰입형 경험을 작성할 때, 앱 유형을 정의하는 것부터 시작하세요.

00:04:51.000 --> 00:04:54.000
장면 유형으로, ImmersiveSpace를 사용하세요.

00:04:54.000 --> 00:04:58.000
콘텐츠 유형의 경우, CompositorLayer를 사용하세요.

00:04:58.000 --> 00:05:06.000
CompositorLayer가 콘텐츠를 렌더링할 준비가 되면, 시스템은 렌더링 세션의 인스턴스로 애플리케이션을 호출할 것이다.

00:05:06.000 --> 00:05:11.000
여기 사용자 지정 엔진의 인스턴스를 만들 수 있는 좋은 장소가 있습니다.

00:05:11.000 --> 00:05:20.000
이제 엔진 인스턴스가 있으므로, 렌더링 스레드를 만들고 start를 호출하여 렌더링 루프를 실행할 수 있습니다.

00:05:20.000 --> 00:05:31.000
애플리케이션에서 장면 목록을 정의할 때 고려해야 할 한 가지는 기본적으로 SwiftUI가 앱의 첫 번째 장면이 ImmersiveSpace인 경우에도 창 장면을 만든다는 것입니다.

00:05:31.000 --> 00:05:35.000
기본 동작을 변경하려면, 앱의 정보 plist를 수정할 수 있습니다.

00:05:35.000 --> 00:05:44.000
UIApplicationPreferred DefaultSceneSessionRole 키를 애플리케이션 장면 매니페스트에 추가하여 애플리케이션의 기본 장면 유형을 변경할 수 있습니다.

00:05:44.000 --> 00:05:52.000
Compositor SpaceContent와 함께 공간을 사용하는 경우, CPSceneSessionRole ImmersiveSpaceApplication을 사용할 것입니다.

00:05:52.000 --> 00:06:01.000
애플리케이션을 설정한 후, 그리고 렌더링 루프에 들어가기 전에, 당신은 CompositorServices에게 LayerRenderer를 구성하는 방법을 알려줄 것입니다.

00:06:01.000 --> 00:06:08.000
CompositorLayer에 구성을 제공하기 위해, CompositorLayerConfiguration 프로토콜을 준수하는 새로운 유형을 만들 것입니다.

00:06:08.000 --> 00:06:15.000
이 프로토콜을 사용하면 설정과 렌더링 세션의 일부 동작을 수정할 수 있습니다.

00:06:15.000 --> 00:06:18.000
CompositorLayerConfiguration은 두 가지 매개 변수를 제공합니다.

00:06:18.000 --> 00:06:21.000
첫 번째는 레이어 기능이다.

00:06:21.000 --> 00:06:25.000
장치에서 어떤 기능을 사용할 수 있는지 쿼리할 수 있습니다.

00:06:25.000 --> 00:06:29.000
기능을 사용하여 유효한 구성을 만드세요.

00:06:29.000 --> 00:06:32.000
그리고 두 번째 매개 변수는 LayerRenderer 구성이다.

00:06:32.000 --> 00:06:36.000
이 유형은 렌더링 세션의 구성을 정의합니다.

00:06:36.000 --> 00:06:46.000
구성을 사용하면 엔진이 콘텐츠를 레이어에 매핑하는 방법을 정의하고, foveated 렌더링을 활성화하고, 파이프라인의 색상 관리를 정의할 수 있습니다.

00:06:46.000 --> 00:06:51.000
이제, 이 속성들 각각이 당신의 엔진에 어떤 영향을 미칠지에 대해 이야기하겠습니다.

00:06:51.000 --> 00:06:54.000
첫 번째는 foveated 렌더링이다.

00:06:54.000 --> 00:07:02.000
이 기능의 주요 목표는 더 큰 텍스처 크기를 사용하지 않고도 더 높은 픽셀 단위 밀도로 콘텐츠를 렌더링할 수 있도록 하는 것입니다.

00:07:02.000 --> 00:07:08.000
일반 디스플레이 파이프라인에서, 픽셀은 텍스처에 선형으로 분포되어 있다.

00:07:08.000 --> 00:07:16.000
xrOS는 디스플레이의 어떤 영역이 더 낮은 샘플링 속도를 사용할 수 있는지 정의하는 지도를 만들어 이 워크플로우를 최적화합니다.

00:07:16.000 --> 00:07:22.000
이것은 디스플레이의 시각적 충실도를 유지하면서 프레임을 렌더링하는 데 필요한 전력을 줄이는 데 도움이 됩니다.

00:07:22.000 --> 00:07:28.000
더 나은 시각적 경험을 얻을 수 있기 때문에 가능할 때마다 foveation을 사용하는 것이 중요하다.

00:07:28.000 --> 00:07:35.000
Foveation이 렌더링 파이프라인에 어떤 영향을 미치는지 시각화하는 좋은 방법은 Xcode의 Metal Debugger를 사용하는 것입니다.

00:07:35.000 --> 00:07:43.000
메탈 디버거를 사용하면 렌더링 파이프라인에서 사용되는 대상 텍스처와 래스터화 속도 맵을 검사할 수 있습니다.

00:07:43.000 --> 00:07:49.000
이 캡처는 래스터화 속도 맵의 스케일링 없이 텍스처의 내용을 보여줍니다.

00:07:49.000 --> 00:07:55.000
더 압축된 텍스처 영역에 초점을 맞추면 다른 샘플 속도를 알 수 있습니다.

00:07:55.000 --> 00:08:04.000
메탈 디버거의 첨부 뷰어 옵션을 사용하면 이미지를 조정하여 디스플레이에 표시할 최종 결과를 시각화할 수 있습니다.

00:08:04.000 --> 00:08:11.000
컴포지터는 각 프레임에 대해 MTLRasterizationRateMap을 사용하여 foveation 맵을 제공합니다.

00:08:11.000 --> 00:08:15.000
Foveation이 지원되는지 항상 확인하는 것이 좋습니다.

00:08:15.000 --> 00:08:17.000
이것은 플랫폼에 따라 바뀔 것이다.

00:08:17.000 --> 00:08:22.000
예를 들어, xrOS 시뮬레이터에서는 foveation을 사용할 수 없습니다.

00:08:22.000 --> 00:08:28.000
Foveation을 활성화하려면, 구성에서 isFoveationEnabled를 설정할 수 있습니다.

00:08:28.000 --> 00:08:31.000
두 번째 속성은 레이어렌더러 레이아웃이다.

00:08:31.000 --> 00:08:36.000
이 속성은 당신의 엔진에 가장 중요한 구성 중 하나입니다.

00:08:36.000 --> 00:08:43.000
헤드셋의 각 디스플레이가 애플리케이션의 렌더링된 콘텐츠에 매핑되는 방법을 정의합니다.

00:08:43.000 --> 00:08:48.000
각 눈은 먼저 컴포지터가 제공하는 금속 텍스처로 매핑됩니다.

00:08:48.000 --> 00:08:53.000
그런 다음 컴포지터는 그 텍스처 내에서 사용할 슬라이스의 인덱스를 제공합니다.

00:08:53.000 --> 00:08:59.000
그리고 마지막으로, 컴포지터는 텍스처 슬라이스 내에서 사용할 뷰포트를 제공합니다.

00:08:59.000 --> 00:09:04.000
LayerRenderer 레이아웃을 사용하면 텍스처 슬라이스와 뷰포트 사이에서 다른 매핑을 선택할 수 있습니다.

00:09:04.000 --> 00:09:10.000
계층화된 컴포지터는 두 개의 슬라이스와 두 개의 뷰포트가 있는 하나의 텍스처를 사용할 것이다.

00:09:10.000 --> 00:09:16.000
전용으로, 컴포지터는 각각 하나의 슬라이스와 하나의 뷰포트가 있는 두 개의 텍스처를 사용할 것이다.

00:09:16.000 --> 00:09:24.000
그리고 마지막으로 공유와 함께, 컴포지터는 그 슬라이스에 대해 하나의 텍스처, 하나의 슬라이스, 그리고 두 개의 다른 뷰포트를 사용할 것이다.

00:09:24.000 --> 00:09:29.000
사용할 레이아웃을 선택하는 것은 렌더링 파이프라인을 설정하는 방법에 따라 다릅니다.

00:09:29.000 --> 00:09:38.000
예를 들어, 계층화 및 공유를 사용하면 한 번의 패스로 렌더링을 수행할 수 있으므로 렌더링 파이프라인을 최적화할 수 있습니다.

00:09:38.000 --> 00:09:44.000
공유 레이아웃을 사용하면, foveated 렌더링이 옵션이 아닌 기존 코드 베이스를 포팅하는 것이 더 쉬울 수 있습니다.

00:09:44.000 --> 00:09:53.000
계층화된 레이아웃은 포브 렌더링을 유지하면서 한 번에 장면을 렌더링할 수 있기 때문에 최적의 레이아웃입니다.

00:09:53.000 --> 00:09:57.000
논의할 마지막 구성 속성은 색상 관리입니다.

00:09:57.000 --> 00:10:03.000
컴포지터는 콘텐츠가 확장된 선형 디스플레이 P3 색 공간으로 렌더링될 것으로 예상한다.

00:10:03.000 --> 00:10:06.000
xrOS는 2.0의 EDR 헤드룸을 지원합니다.

00:10:06.000 --> 00:10:09.000
그것은 SDR 범위의 두 배이다.

00:10:09.000 --> 00:10:21.000
기본적으로 컴포지터는 HDR 렌더링 가능한 픽셀 형식을 사용하지 않지만, 애플리케이션이 HDR을 지원하는 경우 레이어 구성에서 rgba16Float를 지정할 수 있습니다.

00:10:21.000 --> 00:10:29.000
EDR로 HDR을 렌더링하는 방법에 대해 더 알고 싶다면, "EDR로 HDR 렌더링 탐색" 세션을 확인하세요.

00:10:29.000 --> 00:10:37.000
애플리케이션에서 사용자 지정 구성을 만들려면, CompositorLayerConfiguration 프로토콜을 준수하는 새로운 유형을 정의하는 것으로 시작하세요.

00:10:37.000 --> 00:10:42.000
이 프로토콜을 준수하려면, makeConfiguration 방법을 추가하세요.

00:10:42.000 --> 00:10:47.000
이 방법은 레이어 기능과 수정할 수 있는 구성을 제공합니다.

00:10:47.000 --> 00:10:52.000
내가 전에 언급한 세 가지 속성을 활성화하려면, 먼저 foveation이 지원되는지 확인하세요.

00:10:52.000 --> 00:10:57.000
그런 다음 이 장치에서 어떤 레이아웃이 지원되는지 확인하세요.

00:10:57.000 --> 00:11:01.000
이 정보를 사용하면 구성에서 유효한 레이아웃을 설정할 수 있습니다.

00:11:01.000 --> 00:11:08.000
컴포지터가 하나의 보기만 렌더링하는 시뮬레이터와 같은 일부 장치에서는 레이어드를 사용할 수 없습니다.

00:11:08.000 --> 00:11:12.000
Foveation의 경우, 장치가 지원하는 경우 true로 설정하세요.

00:11:12.000 --> 00:11:19.000
그리고 마지막으로, HDR 콘텐츠를 렌더링할 수 있도록 colorFormat을 rgba16Float로 설정하세요.

00:11:19.000 --> 00:11:26.000
컴포지터 레이어를 만든 코드로 돌아가서, 이제 방금 만든 구성 유형을 추가할 수 있습니다.

00:11:26.000 --> 00:11:31.000
이제 렌더링 세션이 구성되었으므로, 렌더링 루프를 설정할 수 있습니다.

00:11:31.000 --> 00:11:35.000
CompositorLayer의 LayerRenderer 객체를 사용하는 것으로 시작할 것입니다.

00:11:35.000 --> 00:11:42.000
먼저, 리소스를 로드하고 엔진이 프레임을 렌더링하는 데 필요한 모든 개체를 초기화합니다.

00:11:42.000 --> 00:11:44.000
그런 다음 레이어의 상태를 확인하세요.

00:11:44.000 --> 00:11:48.000
레이어가 일시 중지되면, 레이어가 실행될 때까지 기다리세요.

00:11:48.000 --> 00:11:51.000
대기에서 레이어가 차단 해제되면, 레이어 상태를 다시 확인하세요.

00:11:51.000 --> 00:11:55.000
레이어가 실행 중이라면, 프레임을 렌더링할 수 있습니다.

00:11:55.000 --> 00:12:00.000
그리고 그 프레임이 렌더링되면, 다음 프레임을 렌더링하기 전에 레이어 상태를 다시 확인하세요.

00:12:00.000 --> 00:12:06.000
레이어 상태가 무효화되면, 렌더링 루프를 위해 만든 리소스를 해제하세요.

00:12:06.000 --> 00:12:10.000
이제, render_loop의 주요 기능을 정의할 때입니다.

00:12:10.000 --> 00:12:15.000
지금까지 저는 ImmersiveSpace API가 Swift에서만 사용할 수 있기 때문에 Swift를 사용해 왔습니다.

00:12:15.000 --> 00:12:20.000
하지만 여기서부터 나는 렌더링 루프를 쓰기 위해 C로 전환할 것이다.

00:12:20.000 --> 00:12:27.000
내가 언급했듯이, 렌더링 루프의 첫 번째 단계는 프레임을 렌더링하는 데 필요한 모든 객체를 할당하고 초기화하는 것이다.

00:12:27.000 --> 00:12:31.000
사용자 지정 엔진의 설정 기능을 호출하여 이를 수행할 수 있습니다.

00:12:31.000 --> 00:12:35.000
다음은 루프의 주요 부분이다.

00:12:35.000 --> 00:12:38.000
첫 번째 단계는 레이어렌더러 상태를 확인하는 것이다.

00:12:38.000 --> 00:12:43.000
상태가 일시 중지되면, 레이어렌더러가 실행될 때까지 스레드가 잠자기 모드로 전환됩니다.

00:12:43.000 --> 00:12:48.000
레이어 상태가 실행 중이라면, 엔진은 하나의 프레임을 렌더링할 것이다.

00:12:48.000 --> 00:12:53.000
그리고 마지막으로, 레이어가 무효화되면, 렌더링 루프가 끝날 것이다.

00:12:53.000 --> 00:12:58.000
Render_loop 함수의 마지막 단계는 사용된 자원을 지우는 것이다.

00:12:58.000 --> 00:13:03.000
이제 앱이 렌더링 루프를 거치고 있으니, 한 프레임을 렌더링하는 방법을 설명하겠습니다.

00:13:03.000 --> 00:13:08.000
xrOS에서 콘텐츠를 렌더링하는 것은 항상 장치의 관점에서이다.

00:13:08.000 --> 00:13:12.000
ARKit을 사용하여 장치 방향과 번역을 얻을 수 있습니다.

00:13:12.000 --> 00:13:23.000
ARKit은 이미 iOS에서 사용할 수 있으며, 이제 xrOS는 몰입형 경험을 만드는 데 도움이 되는 추가 기능을 갖춘 완전히 새로운 API를 도입하고 있습니다.

00:13:23.000 --> 00:13:31.000
ARKit을 사용하면 세계 추적, 손 추적 및 기타 세계 감지 기능을 애플리케이션에 추가할 수 있습니다.

00:13:31.000 --> 00:13:41.000
새로운 ARKit API는 또한 C 및 Swift API를 지원하기 위해 처음부터 구축되어 기존 렌더링 엔진과 더 쉽게 통합할 수 있습니다.

00:13:41.000 --> 00:13:47.000
xrOS에서 ARKit에 대해 자세히 알아보려면, "공간 컴퓨팅을 위한 ARKit을 만나보세요"를 확인하세요.

00:13:47.000 --> 00:13:52.000
렌더링 루프 내에서, 하나의 프레임을 렌더링할 시간이다.

00:13:52.000 --> 00:13:56.000
프레임을 렌더링할 때, 컴포지터는 두 개의 주요 섹션을 정의한다.

00:13:56.000 --> 00:13:58.000
첫 번째는 업데이트입니다.

00:13:58.000 --> 00:14:02.000
여기가 당신이 입력 지연에 중요하지 않은 일을 할 수 있는 곳입니다.

00:14:02.000 --> 00:14:10.000
이것은 장면의 애니메이션을 업데이트하거나, 캐릭터를 업데이트하거나, 핸드 스켈레톤 포즈와 같은 시스템에서 입력을 수집하는 것과 같은 것일 수 있습니다.

00:14:10.000 --> 00:14:13.000
프레임의 두 번째 섹션은 제출 섹션이다.

00:14:13.000 --> 00:14:16.000
여기가 당신이 대기 시간에 중요한 작업을 수행할 곳입니다.

00:14:16.000 --> 00:14:21.000
또한 헤드셋 포즈에 의존하는 모든 콘텐츠를 렌더링할 수 있습니다.

00:14:21.000 --> 00:14:27.000
각 섹션의 타이밍을 정의하기 위해, 컴포지터는 타이밍 객체를 제공한다.

00:14:27.000 --> 00:14:32.000
이 다이어그램은 타이밍이 다른 프레임 섹션에 어떤 영향을 미치는지 정의한다.

00:14:32.000 --> 00:14:37.000
CPU와 GPU 트랙은 응용 프로그램에서 수행 중인 작업을 나타냅니다.

00:14:37.000 --> 00:14:43.000
그리고 컴포지터 트랙은 컴포지터 서버가 프레임을 표시하기 위해 수행한 작업을 나타냅니다.

00:14:43.000 --> 00:14:48.000
컴포지터 서비스의 타이밍 유형은 세 가지 주요 시간 값을 정의합니다.

00:14:48.000 --> 00:14:50.000
첫 번째는 최적의 입력 시간이다.

00:14:50.000 --> 00:14:57.000
대기 시간에 중요한 입력을 쿼리하고 프레임 렌더링을 시작하기에 가장 좋은 시기입니다.

00:14:57.000 --> 00:14:59.000
두 번째는 렌더링 마감일이다.

00:14:59.000 --> 00:15:05.000
그것은 CPU와 GPU가 프레임을 렌더링하기 위해 작업을 완료해야 하는 시간이다.

00:15:05.000 --> 00:15:07.000
그리고 세 번째는 발표 시간이다.

00:15:07.000 --> 00:15:11.000
그때가 당신의 프레임이 전시될 때입니다.

00:15:11.000 --> 00:15:18.000
프레임의 두 섹션에서, 업데이트 섹션은 최적의 입력 시간 전에 발생해야 합니다.

00:15:18.000 --> 00:15:23.000
업데이트 후, 프레임 제출을 시작하기 전에 최적의 입력 시간을 기다릴 것입니다.

00:15:23.000 --> 00:15:29.000
그런 다음 렌더링 작업을 GPU에 제출할 프레임 제출을 수행할 것입니다.

00:15:29.000 --> 00:15:42.000
CPU와 GPU 작업이 렌더링 마감일 전에 완료되어야 한다는 점에 유의하는 것이 중요합니다. 그렇지 않으면 컴포지터 서버는 이 프레임을 사용할 수 없고 대신 이전 프레임을 사용할 것입니다.

00:15:42.000 --> 00:15:49.000
마지막으로, 렌더링 마감일에, 컴포지터 서버는 이 프레임을 시스템의 다른 레이어와 합성할 것이다.

00:15:49.000 --> 00:15:56.000
렌더링 루프 코드로 돌아가서, render_new_frame 함수를 정의할 때입니다.

00:15:56.000 --> 00:16:02.000
엔진의 render_new_frame 함수에서, 당신은 먼저 레이어렌더러에서 프레임을 쿼리할 것입니다.

00:16:02.000 --> 00:16:06.000
프레임 객체를 사용하면 타이밍 정보를 예측할 수 있습니다.

00:16:06.000 --> 00:16:11.000
그 타이밍 정보를 사용하여 업데이트 범위를 정하고 간격을 제출하세요.

00:16:11.000 --> 00:16:13.000
다음으로, 업데이트 섹션을 구현하세요.

00:16:13.000 --> 00:16:18.000
프레임의 시작 및 종료 업데이트를 호출하여 이 섹션을 정의하세요.

00:16:18.000 --> 00:16:23.000
내부에서, 당신은 장치 입력을 수집하고 프레임의 내용을 업데이트할 것입니다.

00:16:23.000 --> 00:16:29.000
업데이트가 완료되면, 제출을 시작하기 전에 최적의 입력 시간을 기다리세요.

00:16:29.000 --> 00:16:34.000
기다린 후, 제출 시작 및 종료를 호출하여 제출 섹션을 정의하십시오.

00:16:34.000 --> 00:16:38.000
이 섹션 내에서, 먼저 드로어블 객체를 쿼리하세요.

00:16:38.000 --> 00:16:47.000
CAMetalLayer와 마찬가지로, 드로어블 오브젝트에는 대상 텍스처와 렌더링 파이프라인을 설정하는 데 필요한 정보가 포함되어 있습니다.

00:16:47.000 --> 00:16:54.000
이제 드로어블이 있으므로, 컴포지터가 이 프레임을 렌더링하는 데 사용할 최종 타이밍 정보를 얻을 수 있습니다.

00:16:54.000 --> 00:16:57.000
최종 타이밍으로, 당신은 ar_pose를 쿼리할 수 있습니다.

00:16:57.000 --> 00:17:04.000
컴포지터가 프레임에서 재프로젝션을 수행하는 데 사용하기 때문에 드로어블에서 포즈를 설정하는 것이 중요합니다.

00:17:04.000 --> 00:17:10.000
여기서 나는 내 엔진 객체에서 get_ar_pose 함수를 호출하여 포즈를 취하고 있다.

00:17:10.000 --> 00:17:15.000
하지만 ARKit 세계 추적 API를 사용하여 이 기능의 내용을 구현해야 합니다.

00:17:15.000 --> 00:17:21.000
이 기능의 마지막 단계는 모든 GPU 작업을 인코딩하고 프레임을 제출하는 것입니다.

00:17:21.000 --> 00:17:26.000
Submit_frame 내부에서, drawable을 사용하여 평소와 같이 프레임의 내용을 렌더링하세요.

00:17:26.000 --> 00:17:32.000
이제 렌더링 루프가 프레임을 렌더링하고 있으니, 몰입형 경험을 인터랙티브하게 만들 때입니다.

00:17:32.000 --> 00:17:42.000
이 비디오는 Unity를 사용하는 RecRoom이 이미 ARKit 및 Compositor API를 활용하여 애플리케이션에 상호 작용을 추가하는 방법을 보여줍니다.

00:17:42.000 --> 00:17:46.000
이 상호 작용을 주도하는 두 가지 주요 입력 소스가 있다.

00:17:46.000 --> 00:17:51.000
ARKit의 HandTracking은 가상 손을 렌더링하기 위한 손 골격을 제공하고 있다.

00:17:51.000 --> 00:17:55.000
그리고 LayerRenderer의 핀치 이벤트는 사용자 상호 작용을 주도하고 있다.

00:17:55.000 --> 00:18:03.000
상호 작용하는 경험을 만들기 위해, 먼저 사용자 입력을 수집한 다음 장면의 내용에 적용할 것입니다.

00:18:03.000 --> 00:18:07.000
이 모든 작업은 프레임의 업데이트 섹션에서 이루어질 것이다.

00:18:07.000 --> 00:18:13.000
LayerRenderer와 ARKit HandTracking 공급자의 두 가지 주요 입력 소스가 있습니다.

00:18:13.000 --> 00:18:18.000
LayerRenderer를 사용하면 애플리케이션이 핀치 이벤트를 받을 때마다 업데이트를 받을 수 있습니다.

00:18:18.000 --> 00:18:23.000
이러한 업데이트는 공간적 사건의 형태로 노출된다.

00:18:23.000 --> 00:18:26.000
이 사건들은 세 가지 주요 속성을 포함한다.

00:18:26.000 --> 00:18:32.000
그 단계는 이벤트가 활성화되었는지, 끝났는지, 또는 취소되었는지 알려줄 것이다.

00:18:32.000 --> 00:18:38.000
선택 광선을 사용하면 이벤트가 시작되었을 때 주목을 받은 장면의 내용을 결정할 수 있습니다.

00:18:38.000 --> 00:18:41.000
그리고 마지막 이벤트 속성은 조작자 포즈이다.

00:18:41.000 --> 00:18:47.000
이것은 핀치의 포즈이며 이벤트 기간 동안 모든 프레임이 업데이트됩니다.

00:18:47.000 --> 00:18:54.000
HandTracking API에서 왼손과 오른손 모두의 골격을 얻을 수 있습니다.

00:18:54.000 --> 00:18:57.000
이제, 코드에 입력 지원을 추가할 때입니다.

00:18:57.000 --> 00:19:04.000
입력을 수집하기 전에, 애플리케이션이 가상 핸드를 렌더링하는지 아니면 패스스루 핸드를 사용하는지 결정할 것입니다.

00:19:04.000 --> 00:19:12.000
Passthrough 손을 보거나 숨길 수 있도록 upperLimbVisibility 장면 수정자를 ImmersiveSpace에 추가하세요.

00:19:12.000 --> 00:19:18.000
공간 이벤트에 액세스하려면, CompositorLayer 렌더링 핸들러를 정의한 곳으로 돌아가세요.

00:19:18.000 --> 00:19:25.000
여기서, 새로운 공간 이벤트가 있을 때마다 업데이트를 받으려면 layerRenderer에 블록을 등록하세요.

00:19:25.000 --> 00:19:32.000
엔진 코드를 C로 작성하는 경우, SwiftUI 공간 이벤트를 C 유형으로 매핑할 수 있습니다.

00:19:32.000 --> 00:19:37.000
C 코드 내에서, 당신은 이제 C 이벤트 컬렉션을 받을 수 있습니다.

00:19:37.000 --> 00:19:44.000
공간 이벤트 업데이트를 처리할 때 명심해야 할 한 가지는 업데이트가 메인 스레드에서 전달된다는 것입니다.

00:19:44.000 --> 00:19:51.000
이것은 엔진에서 이벤트를 읽고 쓸 때 동기화 메커니즘을 사용할 것이라는 것을 의미합니다.

00:19:51.000 --> 00:19:57.000
이제 이벤트가 엔진에 저장되므로, 수집 입력 기능을 구현할 때입니다.

00:19:57.000 --> 00:20:02.000
첫 번째 단계는 이 프레임의 현재 입력 상태를 저장할 객체를 만드는 것이다.

00:20:02.000 --> 00:20:07.000
이 입력 상태는 LayerRenderer에서 받은 이벤트를 저장합니다.

00:20:07.000 --> 00:20:11.000
안전한 방법으로 내부 저장소에 접근하고 있는지 확인하세요.

00:20:11.000 --> 00:20:19.000
손 골격의 경우, ARKit의 손 추적 공급자 API를 사용하여 최신 손 앵커를 얻을 수 있습니다.

00:20:19.000 --> 00:20:27.000
그리고 이제 애플리케이션에 입력 지원이 제공되었으므로, xrOS에서 완전히 몰입형 경험을 만들 수 있는 모든 도구를 사용할 수 있습니다.

00:20:27.000 --> 00:20:31.000
요약하자면, SwiftUI를 사용하면 애플리케이션을 정의할 수 있습니다.

00:20:31.000 --> 00:20:36.000
CompositorServices와 Metal을 사용하면 렌더링 루프를 설정하고 3D 콘텐츠를 표시할 수 있습니다.

00:20:36.000 --> 00:20:41.000
그리고 마지막으로, ARKit을 사용하면, 당신의 경험을 인터랙티브하게 만들 수 있을 것입니다.

00:20:41.000 --> 00:20:43.000
봐줘서 고마워!

00:20:43.000 --> 23:59:59.000
♪

