WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:16.000
Julian: Xin chào, chào mừng đến với "Có gì mới trong xử lý giọng nói." Tôi là Julian từ nhóm Core Audio.

00:00:16.000 --> 00:00:25.000
Các ứng dụng Voice over IP đã trở nên cần thiết hơn bao giờ hết, để giúp mọi người duy trì kết nối với đồng nghiệp, bạn bè và gia đình của họ.

00:00:25.000 --> 00:00:31.000
Chất lượng âm thanh trong trò chuyện thoại đóng một vai trò quan trọng trong việc mang lại trải nghiệm người dùng tuyệt vời.

00:00:31.000 --> 00:00:39.000
Điều quan trọng nhưng đầy thách thức là thực hiện xử lý âm thanh nghe có vẻ tuyệt vời trong mọi trường hợp.

00:00:39.000 --> 00:00:56.000
Đó là lý do tại sao Apple cung cấp API xử lý giọng nói, để mọi người luôn có thể tận hưởng trải nghiệm âm thanh tốt nhất có thể khi họ trò chuyện trong ứng dụng của bạn, bất kể họ đang ở trong môi trường âm thanh nào, họ đang sử dụng sản phẩm Apple nào và phụ kiện âm thanh nào đang được kết nối.

00:00:56.000 --> 00:01:03.000
API xử lý giọng nói của Apple được sử dụng rộng rãi bởi nhiều ứng dụng, bao gồm ứng dụng FaceTime và Điện thoại của riêng chúng tôi.

00:01:03.000 --> 00:01:14.000
Nó cung cấp khả năng xử lý tín hiệu âm thanh tốt nhất trong lớp, bao gồm khử tiếng vang, khử tiếng ồn, kiểm soát độ lợi tự động, trong số những thứ khác để tăng cường âm thanh trò chuyện bằng giọng nói.

00:01:14.000 --> 00:01:25.000
Hiệu suất của nó được điều chỉnh bởi các kỹ sư âm thanh cho từng mẫu sản phẩm của Apple kết hợp với từng loại thiết bị âm thanh, để tính đến các đặc tính âm thanh độc đáo của chúng.

00:01:25.000 --> 00:01:37.000
Chọn API xử lý giọng nói của Apple cũng cho phép người dùng toàn quyền kiểm soát cài đặt chế độ micrô cho ứng dụng của bạn, bao gồm Tiêu chuẩn, Cách ly giọng nói và Phổ rộng.

00:01:37.000 --> 00:01:43.000
Chúng tôi thực sự khuyên bạn nên sử dụng API xử lý giọng nói của Apple cho các ứng dụng Thoại qua IP của bạn.

00:01:43.000 --> 00:01:47.000
API xử lý giọng nói của Apple có sẵn trong hai tùy chọn.

00:01:47.000 --> 00:01:56.000
Tùy chọn đầu tiên là một đơn vị âm thanh I/O được gọi là AUVoiceIO, còn được gọi là AUVoiceProcessingIO.

00:01:56.000 --> 00:02:02.000
Tùy chọn này dành cho các ứng dụng cần tương tác trực tiếp với đơn vị âm thanh I/O.

00:02:02.000 --> 00:02:13.000
Tùy chọn thứ hai là AVAudioEngine, cụ thể hơn bằng cách bật chế độ "xử lý giọng nói" của AVAudioEngine.

00:02:13.000 --> 00:02:17.000
AVAudioEngine là API cấp cao hơn.

00:02:17.000 --> 00:02:23.000
Nó thường dễ sử dụng hơn và nó làm giảm lượng mã bạn phải viết khi làm việc với âm thanh.

00:02:23.000 --> 00:02:28.000
Cả hai tùy chọn đều cung cấp khả năng xử lý giọng nói giống nhau.

00:02:28.000 --> 00:02:30.000
Bây giờ, có gì mới?

00:02:30.000 --> 00:02:35.000
Lần đầu tiên chúng tôi làm cho các API xử lý giọng nói có sẵn trên tvOS.

00:02:35.000 --> 00:02:42.000
Để biết thêm chi tiết về điều đó, vui lòng xem phiên "Khám phá Camera liên tục cho tvOS".

00:02:42.000 --> 00:02:55.000
Chúng tôi cũng đang thêm một vài API mới vào AUVoiceIO và AVAudioEngine để cung cấp cho bạn nhiều quyền kiểm soát hơn đối với xử lý giọng nói và giúp bạn triển khai các tính năng mới.

00:02:55.000 --> 00:03:04.000
API đầu tiên là giúp bạn kiểm soát hành vi né tránh của các âm thanh khác-- và tôi sẽ giải thích điều đó có nghĩa là gì trong một phút.

00:03:04.000 --> 00:03:10.000
API thứ hai là giúp bạn triển khai tính năng phát hiện người nói chuyện bị tắt tiếng cho ứng dụng của mình.

00:03:10.000 --> 00:03:16.000
Trong phiên này, tôi sẽ tập trung vào các chi tiết của hai API mới này.

00:03:16.000 --> 00:03:27.000
API đầu tiên tôi muốn nói đến là "Nuống âm thanh khác." Trước khi chúng ta đi sâu vào API này, trước tiên hãy để tôi giải thích âm thanh khác là gì và tại sao việc né tránh lại quan trọng.

00:03:27.000 --> 00:03:33.000
Khi bạn sử dụng API xử lý giọng nói của Apple, hãy xem điều gì đang xảy ra với âm thanh phát lại.

00:03:33.000 --> 00:03:41.000
Ứng dụng của bạn đang cung cấp luồng âm thanh trò chuyện thoại được xử lý bằng xử lý giọng nói của Apple và phát đến thiết bị đầu ra.

00:03:41.000 --> 00:03:46.000
Tuy nhiên, có thể có các luồng âm thanh khác phát cùng một lúc.

00:03:46.000 --> 00:03:55.000
Ví dụ, ứng dụng của bạn có thể đang phát một luồng âm thanh khác không được hiển thị thông qua API xử lý giọng nói.

00:03:55.000 --> 00:04:00.000
Cũng có thể có các ứng dụng khác phát âm thanh cùng lúc với ứng dụng của bạn.

00:04:00.000 --> 00:04:13.000
Tất cả các luồng âm thanh khác với luồng âm thanh giọng nói từ ứng dụng của bạn được coi là "âm thanh khác" bởi bộ xử lý giọng nói của Apple và âm thanh giọng nói của bạn được trộn lẫn với âm thanh khác trước khi phát đến thiết bị đầu ra.

00:04:13.000 --> 00:04:19.000
Đối với các ứng dụng trò chuyện thoại, thông thường trọng tâm chính của âm thanh phát lại là âm thanh trò chuyện thoại.

00:04:19.000 --> 00:04:26.000
Đó là lý do tại sao chúng tôi tránh mức âm lượng của các âm thanh khác, để cải thiện tính dễ hiểu của âm thanh giọng nói.

00:04:26.000 --> 00:04:31.000
Trong quá khứ, chúng tôi đã áp dụng một lượng cố định cho các âm thanh khác.

00:04:31.000 --> 00:04:39.000
Điều này đã hoạt động tốt cho hầu hết các ứng dụng và nếu ứng dụng của bạn hài lòng với hành vi né tránh hiện tại, thì bạn không cần phải làm gì cả.

00:04:39.000 --> 00:04:48.000
Tuy nhiên, chúng tôi nhận ra rằng một số ứng dụng muốn có nhiều quyền kiểm soát hơn đối với hành vi né tránh và API này sẽ giúp bạn thực hiện điều đó.

00:04:48.000 --> 00:04:55.000
Hãy kiểm tra API này cho AUVoiceIO trước, và chúng ta sẽ đến AVAudioEngine sau.

00:04:55.000 --> 00:05:00.000
Đối với AUVoiceIO, đây là cấu trúc của cấu hình vịt âm thanh khác.

00:05:00.000 --> 00:05:15.000
Nó cung cấp sự kiểm soát hai khía cạnh độc lập của việc né tránh - phong cách né tránh; đó là, mEnableAdvancedDucking, và số lượng vịt, đó là mDuckingLevel.

00:05:15.000 --> 00:05:19.000
Đối với mEnableAdvancedDucking, theo mặc định, điều này bị vô hiệu hóa.

00:05:19.000 --> 00:05:29.000
Sau khi được bật, nó sẽ điều chỉnh mức độ né tránh động dựa trên sự hiện diện của hoạt động bằng giọng nói từ hai bên của những người tham gia trò chuyện.

00:05:29.000 --> 00:05:37.000
Nói cách khác, nó áp dụng nhiều hơn khi người dùng từ hai bên đang nói chuyện và nó làm giảm việc né tránh khi không bên nào đang nói.

00:05:37.000 --> 00:05:51.000
Điều này rất giống với việc né tránh trong FaceTime SharePlay, nơi âm lượng phát lại phương tiện cao khi không bên nào trong FaceTime đang nói chuyện, nhưng ngay khi ai đó bắt đầu nói chuyện, âm lượng phát lại phương tiện sẽ giảm.

00:05:51.000 --> 00:05:54.000
Tiếp theo, mDuckingLevel.

00:05:54.000 --> 00:06:01.000
Có bốn cấp độ điều khiển: mặc định (Mặc định), tối thiểu (tối thiểu), trung bình (Trung bình) và tối đa (Tối đa).

00:06:01.000 --> 00:06:09.000
Mức ducking mặc định (Mặc định) áp dụng cùng một lượng ducking mà chúng tôi đã áp dụng và đây sẽ tiếp tục là cài đặt mặc định của chúng tôi.

00:06:09.000 --> 00:06:13.000
Mức độ ducking tối thiểu (Min) giảm thiểu số lượng ducking mà chúng tôi áp dụng.

00:06:13.000 --> 00:06:21.000
Nói cách khác, đây là cài đặt để sử dụng nếu bạn muốn âm lượng âm thanh khác càng lớn càng tốt.

00:06:21.000 --> 00:06:27.000
Ngược lại, mức độ vịt tối đa (Tối đa) tối đa hóa số lượng vịt mà chúng tôi áp dụng.

00:06:27.000 --> 00:06:35.000
Nói chung, việc chọn mức độ né tránh cao hơn giúp cải thiện tính dễ hiểu của trò chuyện thoại.

00:06:35.000 --> 00:06:38.000
Hai điều khiển có thể được sử dụng độc lập.

00:06:38.000 --> 00:06:45.000
Khi được sử dụng kết hợp, nó mang lại cho bạn sự linh hoạt hoàn toàn trong việc kiểm soát hành vi né tránh.

00:06:45.000 --> 00:06:52.000
Bây giờ chúng tôi đã đề cập đến những gì cấu hình ducking làm, bạn có thể tạo một cấu hình phù hợp với ứng dụng của mình.

00:06:52.000 --> 00:07:01.000
Ví dụ, ở đây tôi sẽ kích hoạt vịt vịt nâng cao và chọn cấp độ vịt là tối thiểu.

00:07:01.000 --> 00:07:13.000
Sau đó, tôi sẽ đặt cấu hình ducking này thành AUVoiceIO, thông qua kAUVoiceIOProperty_ OtherAudioDuckingConfiguration.

00:07:13.000 --> 00:07:19.000
Đối với khách hàng AVAudioEngine, API trông rất giống nhau.

00:07:19.000 --> 00:07:31.000
Đây là định nghĩa cấu trúc của cấu hình ducking âm thanh khác, và đây là định nghĩa enum của mức độ ducking.

00:07:31.000 --> 00:07:47.000
Để sử dụng API này với AVAudioEngine, trước tiên bạn sẽ bật xử lý giọng nói trên nút đầu vào của công cụ sau đó thiết lập cấu hình ducking.

00:07:47.000 --> 00:07:52.000
Và cuối cùng, thiết lập cấu hình trên nút đầu vào.

00:07:52.000 --> 00:07:58.000
Tiếp theo, hãy nói về một API khác giúp bạn triển khai một tính năng rất hữu ích trong ứng dụng của mình.

00:07:58.000 --> 00:08:12.000
Bạn đã bao giờ ở trong tình huống trong một cuộc họp trực tuyến mà bạn nghĩ rằng bạn đang trò chuyện với đồng nghiệp hoặc bạn bè, nhưng một lúc sau, bạn nhận ra mình bị tắt tiếng và không ai thực sự nghe thấy những điểm tuyệt vời hoặc những câu chuyện hài hước của bạn?

00:08:12.000 --> 00:08:14.000
Vâng, thật khó xử.

00:08:14.000 --> 00:08:23.000
Nó khá hữu ích để cung cấp tính năng phát hiện người nói bị tắt tiếng trong ứng dụng của bạn, giống như những gì FaceTime đang làm ở đây.

00:08:23.000 --> 00:08:29.000
Đó là lý do tại sao chúng tôi đang cung cấp một API để bạn phát hiện sự hiện diện của một người nói chuyện bị tắt tiếng.

00:08:29.000 --> 00:08:38.000
Nó được giới thiệu lần đầu tiên trong iOS 15, và bây giờ chúng tôi đang cung cấp nó cho macOS 14 và tvOS 17.

00:08:38.000 --> 00:08:43.000
Đây là tổng quan cấp cao về cách sử dụng API này.

00:08:43.000 --> 00:08:53.000
Đầu tiên, bạn cần cung cấp một khối người nghe cho AUVoiceIO hoặc AVAudioEngine để nhận thông báo khi phát hiện người nói bị tắt tiếng.

00:08:53.000 --> 00:09:03.000
Khối người nghe bạn cung cấp được gọi bất cứ khi nào người nói bị tắt tiếng bắt đầu nói hoặc ngừng nói sau đó triển khai mã xử lý của bạn cho thông báo đó.

00:09:03.000 --> 00:09:12.000
Ví dụ, bạn có thể muốn nhắc người dùng tự bật tiếng nếu thông báo cho biết người dùng bắt đầu nói chuyện trong khi tắt tiếng.

00:09:12.000 --> 00:09:22.000
Cuối cùng nhưng không kém phần quan trọng, cần phải thực hiện tắt tiếng thông qua API tắt tiếng của AUVoiceIO hoặc AVAudioEngine.

00:09:22.000 --> 00:09:25.000
Hãy để tôi hướng dẫn bạn một số ví dụ mã với AUVoiceIO.

00:09:25.000 --> 00:09:28.000
Chúng ta sẽ đến ví dụ AVAudioEngine sau.

00:09:28.000 --> 00:09:34.000
Đầu tiên, chuẩn bị một khối người nghe xử lý thông báo.

00:09:34.000 --> 00:09:50.000
Khối có một tham số thuộc loại AUVoiceIOSpeechActivityEvent, có thể là một trong hai giá trị-- SpeechActivityHasStarted hoặc SpeechActivityHasEnded.

00:09:50.000 --> 00:09:57.000
Khối người nghe sẽ được gọi bất cứ khi nào sự kiện hoạt động lời nói thay đổi trong quá trình tắt tiếng.

00:09:57.000 --> 00:10:11.000
Bên trong khối, đây là nơi bạn thực hiện cách bạn muốn xử lý sự kiện này, Ví dụ: khi nhận được sự kiện SpeechActivityHasStarted, bạn có thể muốn nhắc người dùng tự bật tiếng.

00:10:11.000 --> 00:10:24.000
Khi bạn đã sẵn khối trình nghe này, hãy đăng ký khối với AUVoiceIO thông qua kAUVoiceIOProperty_MutedSpeechActivityEventListener.

00:10:24.000 --> 00:10:34.000
Khi người dùng tắt tiếng, hãy triển khai tắt tiếng thông qua API tắt tiếng kAUVoiceIOProperty_MuteOutput.

00:10:34.000 --> 00:10:44.000
Khối người nghe của bạn chỉ được gọi nếu A, người dùng bị tắt tiếng và B, khi trạng thái hoạt động lời nói thay đổi.

00:10:44.000 --> 00:10:53.000
Sự hiện diện liên tục hoặc thiếu hoạt động lời nói sẽ không gây ra thông báo dư thừa.

00:10:53.000 --> 00:10:57.000
Đối với khách hàng AVAudioEngine, việc triển khai rất giống nhau.

00:10:57.000 --> 00:11:06.000
Sau khi bạn bật xử lý giọng nói trên nút nhập của công cụ, hãy chuẩn bị một khối người nghe nơi bạn xử lý thông báo.

00:11:06.000 --> 00:11:13.000
Sau đó đăng ký khối người nghe bằng nút nhập.

00:11:13.000 --> 00:11:19.000
Khi người dùng tắt tiếng, hãy sử dụng API tắt tiếng xử lý giọng nói của AVAudioEngine để tắt tiếng.

00:11:19.000 --> 00:11:27.000
Bây giờ, chúng tôi đã đề cập đến việc triển khai tính năng phát hiện người nói bị tắt tiếng với AUVoiceIO và AVAudioEngine.

00:11:27.000 --> 00:11:37.000
Đối với những người bạn chưa sẵn sàng áp dụng API xử lý giọng nói của Apple, chúng tôi đang cung cấp một giải pháp thay thế để giúp bạn triển khai tính năng này.

00:11:37.000 --> 00:11:47.000
Giải pháp thay thế này chỉ có sẵn trên macOS thông qua CoreAudio HAL APIs, tức là API Lớp Trừu tượng Phần cứng.

00:11:47.000 --> 00:11:55.000
Có hai thuộc tính HAL mới để giúp bạn phát hiện hoạt động bằng giọng nói khi được sử dụng kết hợp.

00:11:55.000 --> 00:12:06.000
Đầu tiên, cho phép phát hiện hoạt động bằng giọng nói trên thiết bị đầu vào thông qua kAudioDevicePropertyVoiceActivityDetectionEnable.

00:12:06.000 --> 00:12:14.000
Sau đó đăng ký trình nghe thuộc tính HAL trên kAudioDevicePropertyVoiceActivityDetectionState.

00:12:14.000 --> 00:12:21.000
Trình nghe thuộc tính HAL này được gọi bất cứ khi nào có thay đổi trong trạng thái hoạt động bằng giọng nói.

00:12:21.000 --> 00:12:30.000
Khi ứng dụng của bạn được thông báo bởi người nghe tài sản, hãy truy vấn tài sản để có được giá trị hiện tại của nó.

00:12:30.000 --> 00:12:35.000
Bây giờ hãy để tôi hướng dẫn bạn điều này với một số ví dụ mã.

00:12:35.000 --> 00:12:46.000
Để cho phép phát hiện hoạt động bằng giọng nói trên thiết bị đầu vào, trước tiên hãy xây dựng địa chỉ thuộc tính HAL.

00:12:46.000 --> 00:12:52.000
Sau đó đặt thuộc tính vào thiết bị đầu vào để kích hoạt nó.

00:12:52.000 --> 00:13:05.000
Tiếp theo, để đăng ký trình nghe trên thuộc tính trạng thái phát hiện hoạt động bằng giọng nói, hãy xây dựng địa chỉ thuộc tính HAL, sau đó cung cấp trình nghe thuộc tính của bạn.

00:13:05.000 --> 00:13:12.000
Ở đây "listener_callback" là tên của hàm nghe của bạn.

00:13:12.000 --> 00:13:18.000
Đây là một ví dụ về cách triển khai trình nghe thuộc tính.

00:13:18.000 --> 00:13:22.000
Người nghe phù hợp với chữ ký chức năng này.

00:13:22.000 --> 00:13:37.000
Trong ví dụ này, chúng tôi giả định rằng trình nghe này chỉ được đăng ký cho một thuộc tính HAL, có nghĩa là khi nó được gọi, không có sự mơ hồ về thuộc tính HAL nào đã thay đổi.

00:13:37.000 --> 00:13:51.000
Nếu bạn đăng ký cùng một trình nghe để thông báo về nhiều thuộc tính HAL, thì trước tiên bạn phải xem qua mảng inAddresses để xem chính xác những gì đã thay đổi.

00:13:51.000 --> 00:14:06.000
Khi xử lý thông báo này, hãy truy vấn thuộc tính VoiceActivityDetectionState để lấy giá trị hiện tại của nó sau đó thực hiện logic của riêng bạn trong việc xử lý giá trị đó.

00:14:06.000 --> 00:14:13.000
Có một số chi tiết quan trọng về các API HAL phát hiện hoạt động bằng giọng nói này.

00:14:13.000 --> 00:14:24.000
Trước hết, nó phát hiện hoạt động của giọng nói từ đầu vào micrô bị khử tiếng vang nên nó lý tưởng cho các ứng dụng trò chuyện thoại.

00:14:24.000 --> 00:14:29.000
Thứ hai, phát hiện này hoạt động bất kể trạng thái tắt tiếng của quá trình.

00:14:29.000 --> 00:14:41.000
Để triển khai tính năng phát hiện người nói chuyện bị tắt tiếng với nó, tùy thuộc vào ứng dụng của bạn để triển khai logic bổ sung kết hợp trạng thái hoạt động bằng giọng nói với trạng thái tắt tiếng.

00:14:41.000 --> 00:14:48.000
Đối với các máy khách HAL API triển khai tắt tiếng, chúng tôi thực sự khuyên bạn nên sử dụng API tắt tiếng quy trình của HAL.

00:14:48.000 --> 00:14:58.000
Nó ngăn chặn đèn báo ghi âm trong thanh menu và mang lại cho người dùng sự tự tin rằng quyền riêng tư của họ được bảo vệ dưới chế độ tắt tiếng.

00:14:58.000 --> 00:15:01.000
Hãy tóm tắt lại những gì đã nói hôm nay.

00:15:01.000 --> 00:15:08.000
Chúng tôi đã thảo luận về API xử lý giọng nói của Apple và lý do tại sao chúng tôi đề xuất nó cho các ứng dụng thoại qua IP.

00:15:08.000 --> 00:15:20.000
Chúng tôi đã nói về việc né tránh các âm thanh khác và API để kiểm soát hành vi né tránh với các ví dụ mã về cách sử dụng nó với AUVoiceIO và AVAudioEngine.

00:15:20.000 --> 00:15:28.000
Chúng tôi cũng đã nói về cách triển khai phát hiện người nói bị tắt tiếng với các ví dụ mã của AUVoiceIO và AVAudioEngine.

00:15:28.000 --> 00:15:39.000
Và đối với những khách hàng chưa áp dụng API xử lý giọng nói của Apple, chúng tôi cũng đã chỉ ra một tùy chọn thay thế để thực hiện điều đó trên macOS với Core Audio HAL APIs.

00:15:39.000 --> 00:15:44.000
Chúng tôi đang mong chờ những ứng dụng tuyệt vời mà bạn sẽ xây dựng với các API xử lý giọng nói của Apple.

00:15:44.000 --> 00:15:45.000
Cảm ơn vì đã xem!

00:15:45.000 --> 23:59:59.000
♪ ♪

