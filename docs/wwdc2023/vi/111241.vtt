WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:13.000
Andrew: Xin chào, tôi là Andrew Rauh, một kỹ sư phần mềm về khung Vision.

00:00:13.000 --> 00:00:24.000
Hôm nay tôi sẽ nói về Tư thế cơ thể con người, sử dụng khung Depth in Vision và nâng mọi người lên khỏi hình ảnh bằng mặt nạ ví dụ.

00:00:24.000 --> 00:00:32.000
Phát hiện và hiểu con người luôn là trọng tâm của Tầm nhìn, và trong một vài năm, khung Tầm nhìn đã cung cấp Tư thế Cơ thể Con người ở dạng 2D.

00:00:32.000 --> 00:00:44.000
Để bồi dưỡng, Tư thế cơ thể con người trong 2D trả về một quan sát với tọa độ điểm ảnh chuẩn hóa của các điểm mốc được xác định trên bộ xương tương ứng với hình ảnh đầu vào.

00:00:44.000 --> 00:00:51.000
Nếu bạn muốn đi sâu vào chi tiết cụ thể hơn, vui lòng xem lại phiên "Khử kiến tư thế cơ thể và tay" nếu bạn chưa có.

00:00:51.000 --> 00:01:00.000
Vision đang mở rộng hỗ trợ chụp mọi người trong môi trường của họ thành 3D với một yêu cầu mới có tên VNDetectHumanBodyPose3DRequest.

00:01:00.000 --> 00:01:06.000
Yêu cầu này tạo ra một quan sát trả về một bộ xương 3D với 17 khớp.

00:01:06.000 --> 00:01:12.000
Các khớp có thể được truy cập bằng tên chung hoặc dưới dạng một bộ sưu tập bằng cách cung cấp tên nhóm chung.

00:01:12.000 --> 00:01:25.000
Không giống như các điểm được công nhận khác được trả về bởi Vision được chuẩn hóa thành gốc thấp hơn bên trái, vị trí của các khớp 3D được trả về bằng mét so với cảnh được chụp trong thế giới thực với nguồn gốc tại khớp gốc.

00:01:25.000 --> 00:01:31.000
Bản sửa đổi ban đầu này trả về một bộ xương cho người nổi bật nhất được phát hiện trong khung.

00:01:31.000 --> 00:01:41.000
Nếu bạn đang xây dựng một ứng dụng thể dục và chạy yêu cầu trên hình ảnh này của một lớp tập luyện trong phòng tập thể dục, quan sát sẽ tương ứng với người phụ nữ ở phía trước gần máy ảnh nhất.

00:01:41.000 --> 00:01:48.000
Để chứng minh tốt hơn cấu trúc của bộ xương 3D với một số bối cảnh, hãy chia nhỏ tư thế yoga này.

00:01:48.000 --> 00:01:56.000
Không có gì đáng ngạc nhiên, bộ xương 3D Human Body bắt đầu với một nhóm đầu chứa các điểm ở trung tâm và đỉnh đầu.

00:01:56.000 --> 00:02:06.000
Tiếp theo là nhóm thân, chứa khớp vai trái và phải, cột sống, khớp rễ, nằm ở trung tâm của hông và khớp hông.

00:02:06.000 --> 00:02:10.000
Hãy nhớ rằng một số khớp được trả về theo nhiều nhóm.

00:02:10.000 --> 00:02:17.000
Đối với cánh tay, có các nhóm cánh tay trái và phải, mỗi nhóm có cổ tay, vai và khuỷu tay.

00:02:17.000 --> 00:02:23.000
Trái và phải luôn liên quan đến con người, không phải bên trái hoặc bên phải của hình ảnh.

00:02:23.000 --> 00:02:33.000
Cuối cùng, bộ xương của chúng tôi chứa một nhóm chân trái và chân phải, mỗi nhóm có khớp hông, đầu gối và mắt cá chân tương ứng.

00:02:33.000 --> 00:02:41.000
Để sử dụng yêu cầu mới này, bạn tuân theo quy trình làm việc giống như các yêu cầu khác, vì vậy quy trình này sẽ quen thuộc với bạn nếu bạn đã sử dụng Vision trong mã của mình trước đây.

00:02:41.000 --> 00:02:53.000
Bạn sẽ bắt đầu bằng cách tạo một phiên bản của DetectHumanBodyPose3DRequest mới, sau đó khởi tạo trình xử lý yêu cầu hình ảnh với nội dung bạn muốn chạy tính năng phát hiện của mình.

00:02:53.000 --> 00:02:57.000
Để chạy yêu cầu của bạn, hãy chuyển phiên bản yêu cầu của bạn để thực hiện.

00:02:57.000 --> 00:03:05.000
Và nếu yêu cầu thành công, VNHumanBodyPose3DObservation sẽ được trả về mà không có lỗi.

00:03:05.000 --> 00:03:09.000
Tất cả các bức ảnh đều là đại diện 2D của con người trong thế giới 3D.

00:03:09.000 --> 00:03:16.000
Vision hiện cho phép bạn lấy vị trí 3D đó từ hình ảnh mà không cần ARKit hoặc ARSession.

00:03:16.000 --> 00:03:24.000
Đây là một lựa chọn mạnh mẽ, nhẹ nhàng để hiểu một chủ đề trong không gian 3D và mở ra một loạt các tính năng hoàn toàn mới trong ứng dụng.

00:03:24.000 --> 00:03:28.000
Tôi đã xây dựng một ứng dụng mẫu để giúp hiểu và hình dung điều này.

00:03:28.000 --> 00:03:34.000
Khi tôi mở nó ra, tôi có thể chọn bất kỳ hình ảnh nào từ thư viện ảnh của mình.

00:03:34.000 --> 00:03:42.000
Đồng nghiệp của tôi và tôi đã được truyền cảm hứng từ sự bình tĩnh của người hướng dẫn yoga trước đó, vì vậy chúng tôi đã nghỉ ngơi, đi ra ngoài và tự mình thử một vài tư thế.

00:03:42.000 --> 00:03:52.000
Bây giờ, tôi không linh hoạt như giáo viên đó, nhưng tôi đã làm khá tốt với tư thế này, và nó sẽ trông tuyệt vời ở dạng 3D.

00:03:52.000 --> 00:03:57.000
Hãy chạy yêu cầu và đưa tôi trở lại chiều không gian thứ ba.

00:03:57.000 --> 00:04:04.000
Yêu cầu đã thành công và một bộ xương 3D được căn chỉnh với vị trí của tôi trong hình ảnh đầu vào.

00:04:04.000 --> 00:04:13.000
Nếu tôi xoay cảnh, cánh tay của tôi mở rộng ra và chân trông chính xác so với hông dựa trên cách tôi đứng.

00:04:13.000 --> 00:04:20.000
Hình dạng kim tự tháp này đại diện cho vị trí của máy ảnh khi hình ảnh được chụp.

00:04:20.000 --> 00:04:25.000
Nếu tôi nhấn vào nút Chuyển đổi phối cảnh, chế độ xem bây giờ là từ vị trí của máy ảnh.

00:04:25.000 --> 00:04:34.000
Tôi sẽ hướng dẫn bạn thông qua mã và khái niệm bạn cần biết để tạo ra những trải nghiệm tuyệt vời bằng cách sử dụng 3D Human Body Pose trong ứng dụng của bạn.

00:04:34.000 --> 00:04:38.000
Xây dựng một ứng dụng bắt đầu bằng việc sử dụng các điểm được trả về trong quan sát.

00:04:38.000 --> 00:04:50.000
Có hai API chính để truy xuất chúng, recognizedPoint để truy cập vị trí của một khớp cụ thể hoặc recognizedPoints để truy cập một tập hợp các khớp với tên nhóm được chỉ định.

00:04:50.000 --> 00:04:56.000
Bên cạnh những phương pháp cốt lõi này, quan sát cung cấp một số thông tin hữu ích bổ sung.

00:04:56.000 --> 00:05:01.000
Đầu tiên, bodyHeight cho chiều cao ước tính của đối tượng của bạn tính bằng mét.

00:05:01.000 --> 00:05:09.000
Tùy thuộc vào siêu dữ liệu độ sâu có sẵn, chiều cao này sẽ là chiều cao đo chính xác hơn hoặc chiều cao tham chiếu là 1,8 mét.

00:05:09.000 --> 00:05:12.000
Tôi có nhiều điều để nói về Độ sâu và Tầm nhìn trong một phút.

00:05:12.000 --> 00:05:18.000
Bạn có thể xác định kỹ thuật được sử dụng để tính chiều cao với thuộc tính heightEstimation.

00:05:18.000 --> 00:05:23.000
Tiếp theo, vị trí máy ảnh có sẵn thông qua cameraOriginMatrix.

00:05:23.000 --> 00:05:33.000
Vì trong cuộc sống thực, máy ảnh có thể không đối diện chính xác với đối tượng của bạn, điều này rất hữu ích để hiểu được vị trí của máy ảnh so với người đó khi chụp khung hình.

00:05:33.000 --> 00:05:39.000
Quan sát cũng cung cấp một API để chiếu tọa độ chung trở lại 2D.

00:05:39.000 --> 00:05:45.000
Điều này rất hữu ích nếu bạn muốn phủ hoặc căn chỉnh các điểm trả về với hình ảnh đầu vào.

00:05:45.000 --> 00:05:56.000
Và cuối cùng, để hiểu cách một người di chuyển qua hai hình ảnh tương tự nhau, một API có sẵn để có được vị trí của một khớp nhất định so với máy ảnh.

00:05:56.000 --> 00:06:04.000
Trước khi tôi chỉ ra cách sử dụng các điểm 3D Human Body, tôi muốn giới thiệu các lớp hình học mới trong Vision mà nó kế thừa.

00:06:04.000 --> 00:06:11.000
VNPoint3D là lớp cơ sở xác định ma trận simd_float 4x4 để lưu trữ vị trí 3D.

00:06:11.000 --> 00:06:20.000
Biểu diễn này phù hợp với các khuôn khổ khác của Apple như ARKit và chứa tất cả thông tin dịch và xoay vòng có sẵn.

00:06:20.000 --> 00:06:27.000
Tiếp theo, có VNRecognizedPoint3D, kế thừa vị trí này nhưng cũng thêm một mã định danh.

00:06:27.000 --> 00:06:32.000
Cái này được sử dụng để lưu trữ thông tin tương ứng như một tên chung.

00:06:32.000 --> 00:06:40.000
Cuối cùng, trọng tâm của ngày hôm nay là VNHumanBodyRecognizedPoint3D, bổ sung một vị trí cục bộ và khớp mẹ.

00:06:40.000 --> 00:06:45.000
Hãy đi sâu vào một số chi tiết cụ thể hơn về cách làm việc với các thuộc tính của điểm.

00:06:45.000 --> 00:06:49.000
Sử dụng recognizedPoint API, tôi đã lấy lại vị trí cho cổ tay trái.

00:06:49.000 --> 00:06:58.000
Vị trí mô hình của khớp, hoặc thuộc tính vị trí của điểm, luôn liên quan đến khớp gốc của bộ xương ở trung tâm hông.

00:06:58.000 --> 00:07:04.000
Nếu chúng ta tập trung vào cột thứ ba trong ma trận vị trí, sẽ có các giá trị để dịch.

00:07:04.000 --> 00:07:12.000
Giá trị cho y cho cổ tay trái cao hơn hông của con số này 0,9 mét, có vẻ phù hợp với tư thế này.

00:07:12.000 --> 00:07:19.000
Tiếp theo, có thuộc tính localPosition của điểm trả về, là vị trí liên quan đến khớp mẹ.

00:07:19.000 --> 00:07:24.000
Vì vậy, trong trường hợp này, khuỷu tay trái sẽ là khớp mẹ của cổ tay trái.

00:07:24.000 --> 00:07:31.000
Cột cuối cùng ở đây hiển thị giá trị là -0,1 mét cho trục x, có vẻ cũng đúng.

00:07:31.000 --> 00:07:39.000
Các giá trị âm hoặc dương được xác định bởi điểm tham chiếu, và trong tư thế này, cổ tay nằm ở phía bên trái của khuỷu tay.

00:07:39.000 --> 00:07:43.000
localPosition rất hữu ích nếu ứng dụng của bạn chỉ hoạt động với một vùng trên cơ thể.

00:07:43.000 --> 00:07:48.000
Nó cũng đơn giản hóa việc xác định góc giữa khớp của con và khớp mẹ.

00:07:48.000 --> 00:07:52.000
Tôi sẽ chỉ ra cách tính góc này trong mã trong một giây.

00:07:52.000 --> 00:07:58.000
Khi làm việc với các điểm 3D được trả về, có một số khái niệm có thể hữu ích khi xây dựng ứng dụng của bạn.

00:07:58.000 --> 00:08:04.000
Đầu tiên, bạn thường cần xác định góc giữa khớp của con và khớp mẹ.

00:08:04.000 --> 00:08:12.000
Trong phương pháp calculateLocalAngleToParent, vị trí liên quan đến khớp mẹ được sử dụng để tìm góc đó.

00:08:12.000 --> 00:08:20.000
Xoay cho một nút bao gồm xoay đối với trục x, y và z, hoặc cao độ, ngáp và cuộn.

00:08:20.000 --> 00:08:32.000
Đối với cao độ, một vòng quay 90 độ được sử dụng để định vị hình học nút SceneKit từ hướng mặc định của nó hướng thẳng xuống đến một hình học phù hợp hơn cho bộ xương của chúng ta.

00:08:32.000 --> 00:08:39.000
Đối với ngáp, chúng ta sử dụng cosin cung của tọa độ z chia cho độ dài vectơ để có được góc thích hợp.

00:08:39.000 --> 00:08:46.000
Và đối với cuộn, phép đo góc thu được với tiếp tuyến cung của tọa độ y và x.

00:08:46.000 --> 00:08:53.000
Tiếp theo, ứng dụng của bạn có thể cần liên kết các vị trí 3D được trả về với hình ảnh gốc, như trong ứng dụng mẫu của tôi.

00:08:53.000 --> 00:09:01.000
Trong hình ảnh trực quan của mình, tôi sử dụng API điểm trong hình ảnh cho hai phép biến đổi đối với mặt phẳng hình ảnh của mình, tỷ lệ và bản dịch.

00:09:01.000 --> 00:09:07.000
Đầu tiên tôi cần chia tỷ lệ mặt phẳng hình ảnh của mình theo tỷ lệ thuận với các điểm trả về.

00:09:07.000 --> 00:09:21.000
Tôi lấy khoảng cách giữa hai khớp đã biết, như vai trung tâm và cột sống, cho cả 3D và 2D, liên hệ chúng theo tỷ lệ và chia tỷ lệ mặt phẳng hình ảnh của tôi theo số tiền này.

00:09:21.000 --> 00:09:31.000
Đối với thành phần dịch, tôi sử dụng pointInImage API để tìm nạp vị trí của khớp gốc trong hình ảnh 2D.

00:09:31.000 --> 00:09:45.000
Phương pháp này sử dụng vị trí đó để xác định sự dịch chuyển cho mặt phẳng hình ảnh cho trục x và y đồng thời chuyển đổi giữa gốc dưới bên trái của tọa độ VNPoint và gốc môi trường kết xuất ở trung tâm của hình ảnh.

00:09:45.000 --> 00:09:54.000
Cuối cùng, bạn có thể muốn xem cảnh từ góc nhìn của máy ảnh hoặc hiển thị một điểm tại vị trí của nó và bạn có thể truy xuất cảnh này từ cameraOriginMatrix.

00:09:54.000 --> 00:10:09.000
Hướng chính xác sẽ phụ thuộc vào môi trường kết xuất của bạn, nhưng đây là cách tôi định vị các nút của mình với thông tin chuyển đổi này bằng cách sử dụng biến đổi trục, liên quan đến hệ tọa độ cục bộ của nút này với phần còn lại của cảnh.

00:10:09.000 --> 00:10:21.000
Tôi cũng đã sử dụng thông tin xoay trong cameraOriginMatrix để xoay chính xác mặt phẳng hình ảnh của mình để đối mặt với máy ảnh bằng mã này bằng cách sử dụng biến đổi nghịch đảo.

00:10:21.000 --> 00:10:28.000
Vì chỉ cần thông tin xoay vòng ở đây, thông tin dịch thuật ở cột cuối cùng sẽ bị bỏ qua.

00:10:28.000 --> 00:10:34.000
Đặt tất cả các mảnh này lại với nhau cho phép cảnh được hiển thị trong ứng dụng mẫu của tôi.

00:10:34.000 --> 00:10:41.000
Bây giờ, tôi muốn dành vài phút để thảo luận về một số bổ sung thú vị liên quan đến Độ sâu trong Tầm nhìn.

00:10:41.000 --> 00:10:47.000
Khung tầm nhìn hiện chấp nhận Độ sâu làm đầu vào cùng với bộ đệm hình ảnh hoặc khung.

00:10:47.000 --> 00:10:57.000
VNImageRequestHandler đã thêm API khởi tạo cho cvPixelBuffer và cmSampleBuffer lấy một tham số mới cho AVDepthData.

00:10:57.000 --> 00:11:04.000
Ngoài ra, nếu tệp của bạn đã chứa dữ liệu Độ sâu, bạn có thể sử dụng các API hiện có mà không cần sửa đổi.

00:11:04.000 --> 00:11:08.000
Vision sẽ tự động tìm nạp Depth từ tệp cho bạn.

00:11:08.000 --> 00:11:16.000
Khi làm việc với Depth trong Apple SDKs, AVDepthData đóng vai trò là lớp vùng chứa để giao tiếp với tất cả siêu dữ liệu Depth.

00:11:16.000 --> 00:11:24.000
Siêu dữ liệu Độ sâu được chụp bởi cảm biến máy ảnh chứa bản đồ Độ sâu được biểu thị dưới dạng Định dạng Chênh lệch hoặc Độ sâu.

00:11:24.000 --> 00:11:29.000
Các định dạng này có thể hoán đổi cho nhau và có thể được chuyển đổi với nhau bằng cách sử dụng AVFoundation.

00:11:29.000 --> 00:11:39.000
Siêu dữ liệu độ sâu cũng chứa dữ liệu hiệu chuẩn máy ảnh, như nội tại, bên ngoài và biến dạng ống kính, cần thiết để tái tạo lại cảnh 3D.

00:11:39.000 --> 00:11:47.000
Nếu bạn cần tìm hiểu thêm chi tiết cụ thể, vui lòng xem lại phiên "Khám phá những tiến bộ trong chụp ảnh iOS" từ năm 2022.

00:11:47.000 --> 00:11:52.000
Độ sâu có thể đạt được thông qua các phiên chụp máy ảnh hoặc từ các tệp đã chụp trước đó.

00:11:52.000 --> 00:12:01.000
Hình ảnh được chụp bởi ứng dụng Máy ảnh, như hình ảnh Chân dung trong ảnh, luôn lưu trữ Độ sâu dưới dạng bản đồ chênh lệch với siêu dữ liệu hiệu chuẩn máy ảnh.

00:12:01.000 --> 00:12:09.000
Khi chụp Độ sâu trong phiên chụp trực tiếp, bạn có thêm lợi ích là chỉ định phiên sử dụng LiDAR nếu thiết bị hỗ trợ nó.

00:12:09.000 --> 00:12:14.000
LiDAR mạnh mẽ vì nó cho phép đo và đo chính xác cảnh.

00:12:14.000 --> 00:12:20.000
Vision cũng đang giới thiệu các API để tương tác với nhiều hơn một người trong một hình ảnh.

00:12:20.000 --> 00:12:26.000
Vision hiện đang cung cấp khả năng tách mọi người khỏi cảnh xung quanh với yêu cầu GeneratePersonSegmentation.

00:12:26.000 --> 00:12:31.000
Yêu cầu này trả về một mặt nạ duy nhất chứa tất cả mọi người trong khung.

00:12:31.000 --> 00:12:37.000
Vision hiện đang cho phép bạn chọn lọc hơn một chút với yêu cầu mặt nạ phiên bản người mới.

00:12:37.000 --> 00:12:43.000
API mới này xuất ra tối đa bốn mặt nạ cá nhân, mỗi mặt nạ có điểm tin cậy.

00:12:43.000 --> 00:12:48.000
Vì vậy, bây giờ bạn có thể chọn và nâng bạn bè của mình riêng biệt với một hình ảnh.

00:12:48.000 --> 00:12:58.000
Nếu bạn cần chọn và nâng các đối tượng khác ngoài con người, bạn có thể sử dụng API nâng chủ đề trong VisionKit hoặc yêu cầu mặt nạ phiên bản tiền cảnh trong khung Vision.

00:12:58.000 --> 00:13:04.000
Vui lòng xem phiên "Nâng đối tượng từ hình ảnh trong ứng dụng của bạn" để biết thêm thông tin.

00:13:04.000 --> 00:13:10.000
Đây là một số mã mẫu cho thấy cách chọn một ví dụ cụ thể của một người bạn muốn từ một hình ảnh.

00:13:10.000 --> 00:13:23.000
Hiện tại nó chỉ định để trả về tất cả các phiên bản, nhưng bạn có thể chọn phiên bản 1 hoặc 2, tùy thuộc vào người bạn bạn mà bạn muốn tập trung vào trong hình ảnh hoặc sử dụng phiên bản 0 để lấy nền.

00:13:23.000 --> 00:13:32.000
Yêu cầu mới này phân đoạn tối đa bốn người, vì vậy nếu có nhiều hơn bốn người trong hình ảnh của bạn, có một số điều kiện bổ sung để xử lý trong mã của bạn.

00:13:32.000 --> 00:13:38.000
Khi các cảnh chứa nhiều người, các quan sát được trả lại có thể bỏ lỡ mọi người hoặc kết hợp chúng.

00:13:38.000 --> 00:13:43.000
Thông thường, điều này xảy ra với những người có mặt trong nền.

00:13:43.000 --> 00:13:50.000
Nếu ứng dụng của bạn phải đối phó với những cảnh đông đúc, có những chiến lược bạn có thể sử dụng để xây dựng trải nghiệm tốt nhất có thể.

00:13:50.000 --> 00:14:04.000
API phát hiện khuôn mặt trong Vision có thể được sử dụng để đếm số lượng khuôn mặt trong hình ảnh và bạn có thể chọn bỏ qua hình ảnh với hơn bốn người hoặc sử dụng yêu cầu Phân đoạn người hiện có và làm việc với một mặt nạ cho mọi người.

00:14:04.000 --> 00:14:15.000
Tóm lại, Vision hiện cung cấp những cách mới mạnh mẽ để hiểu con người và môi trường của họ với sự hỗ trợ về chiều sâu, Tư thế cơ thể người 3D và mặt nạ người.

00:14:15.000 --> 00:14:17.000
Nhưng đó không phải là tất cả những gì Vision sẽ phát hành trong năm nay.

00:14:17.000 --> 00:14:25.000
Bạn có thể vượt xa con người và tạo ra những trải nghiệm tuyệt vời với hình ảnh của những người bạn lông lá trong phiên "Khử kiến tư thế động vật trong Tầm nhìn".

00:14:25.000 --> 00:14:31.000
Cảm ơn bạn, và tôi nóng lòng muốn xem bạn xây dựng những tính năng đáng kinh ngạc nào.

00:14:31.000 --> 23:59:59.000
♪ ♪

