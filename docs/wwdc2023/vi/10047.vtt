WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:14.000
Pulkit: Xin chào, tôi là Pulkit, và tôi là một kỹ sư trong nhóm Core ML.

00:00:14.000 --> 00:00:18.000
Tôi rất vui được chia sẻ một số cập nhật đã được thực hiện cho Core ML Tools.

00:00:18.000 --> 00:00:23.000
Những cập nhật này giúp bạn tối ưu hóa kích thước và hiệu suất của các mô hình học máy của mình.

00:00:23.000 --> 00:00:30.000
Với khả năng của các mô hình được cải thiện đáng kể, ngày càng có nhiều tính năng được thúc đẩy bởi máy học.

00:00:30.000 --> 00:00:34.000
Kết quả là, số lượng mô hình được triển khai trong một ứng dụng đang tăng lên.

00:00:34.000 --> 00:00:41.000
Cùng với đó, mỗi mô hình trong ứng dụng cũng ngày càng lớn hơn, gây áp lực tăng lên kích thước của ứng dụng.

00:00:41.000 --> 00:00:45.000
Vì vậy, điều quan trọng là phải kiểm tra kích thước mô hình.

00:00:45.000 --> 00:00:48.000
Có một số lợi ích của việc giảm kích thước mô hình.

00:00:48.000 --> 00:00:53.000
Bạn có thể vận chuyển nhiều mô hình hơn trong cùng một ngân sách bộ nhớ nếu mỗi mô hình nhỏ hơn.

00:00:53.000 --> 00:00:57.000
Nó cũng có thể cho phép bạn vận chuyển các mô hình lớn hơn, có khả năng hơn.

00:00:57.000 --> 00:01:00.000
Nó cũng có thể giúp làm cho mô hình chạy nhanh hơn.

00:01:00.000 --> 00:01:06.000
Điều này là do một mô hình nhỏ hơn có nghĩa là ít dữ liệu hơn để di chuyển giữa bộ nhớ và bộ xử lý.

00:01:06.000 --> 00:01:10.000
Vì vậy, có vẻ như giảm kích thước của mô hình là một ý tưởng tuyệt vời.

00:01:10.000 --> 00:01:12.000
Điều gì làm cho một mô hình lớn?

00:01:12.000 --> 00:01:15.000
Hãy để tôi xem qua một ví dụ để giúp bạn hiểu.

00:01:15.000 --> 00:01:19.000
ResNet50 là một mô hình phân loại hình ảnh phổ biến.

00:01:19.000 --> 00:01:23.000
Lớp đầu tiên của nó là một lớp tích chập với khoảng 9.000 tham số.

00:01:23.000 --> 00:01:28.000
Và nó có tổng cộng 53 lớp tích chập với các kích thước khác nhau.

00:01:28.000 --> 00:01:33.000
Cuối cùng, nó có một lớp tuyến tính với khoảng 2,1 triệu tham số.

00:01:33.000 --> 00:01:45.000
Tất cả điều này cộng thêm tới 25 triệu tham số Nếu tôi lưu mô hình bằng độ chính xác Float16, nó sử dụng 2 byte cho mỗi trọng lượng và tôi nhận được một mô hình có kích thước 50 megabyte.

00:01:45.000 --> 00:01:53.000
Một mô hình 50 megabyte là lớn, nhưng khi bạn sử dụng một số mô hình mới hơn như Stable Diffusion, bạn sẽ kết thúc với các mô hình thậm chí còn lớn hơn.

00:01:53.000 --> 00:01:57.000
Bây giờ, hãy nói về một số con đường để có được một mô hình nhỏ hơn.

00:01:57.000 --> 00:02:05.000
Một cách là thiết kế một kiến trúc mô hình hiệu quả hơn có thể đạt được hiệu suất tốt với trọng lượng ngày càng ít hơn.

00:02:05.000 --> 00:02:08.000
Một cách khác là nén trọng lượng của mô hình hiện tại của bạn.

00:02:08.000 --> 00:02:11.000
Con đường nén mô hình này là những gì tôi sẽ tập trung vào.

00:02:11.000 --> 00:02:15.000
Tôi sẽ bắt đầu bằng cách mô tả ba kỹ thuật hữu ích để nén mô hình.

00:02:15.000 --> 00:02:20.000
Tiếp theo, tôi sẽ trình bày hai quy trình làm việc tích hợp các kỹ thuật nén mô hình này.

00:02:20.000 --> 00:02:27.000
Sau đó tôi sẽ minh họa cách các Công cụ Core ML mới nhất giúp bạn áp dụng các kỹ thuật và quy trình làm việc này cho các mô hình của mình.

00:02:27.000 --> 00:02:32.000
Và cuối cùng, Srijan sẽ thảo luận về tác động của việc nén mô hình đối với hiệu suất thời gian chạy.

00:02:32.000 --> 00:02:35.000
Hãy bắt đầu với các kỹ thuật nén.

00:02:35.000 --> 00:02:38.000
Có một vài cách để nén trọng lượng mô hình.

00:02:38.000 --> 00:02:44.000
Cách đầu tiên là đóng gói chúng hiệu quả hơn bằng cách sử dụng biểu diễn ma trận thưa thớt.

00:02:44.000 --> 00:02:48.000
Điều này có thể đạt được bằng cách sử dụng một kỹ thuật gọi là cắt tỉa.

00:02:48.000 --> 00:02:52.000
Một cách khác là giảm độ chính xác được sử dụng để lưu trữ trọng lượng.

00:02:52.000 --> 00:02:57.000
Điều này có thể đạt được bằng cách lượng tử hóa hoặc bằng cách hóa nhạt.

00:02:57.000 --> 00:03:06.000
Cả hai chiến lược này đều mất dữ liệu và các mô hình nén thường kém chính xác hơn một chút so với các đối tác không nén của chúng.

00:03:06.000 --> 00:03:10.000
Bây giờ chúng ta hãy xem xét sâu hơn từng kỹ thuật này.

00:03:10.000 --> 00:03:15.000
Cắt tỉa trọng lượng giúp bạn đóng gói trọng lượng mô hình của mình một cách hiệu quả với một biểu diễn thưa thớt.

00:03:15.000 --> 00:03:20.000
Sparsifying hoặc cắt tỉa một ma trận trọng lượng có nghĩa là đặt một số giá trị trọng lượng thành 0.

00:03:20.000 --> 00:03:22.000
Tôi bắt đầu với một ma trận trọng lượng.

00:03:22.000 --> 00:03:27.000
Để cắt tỉa nó, tôi có thể đặt trọng lượng cường độ nhỏ nhất thành 0.

00:03:27.000 --> 00:03:32.000
Bây giờ, tôi chỉ cần lưu trữ các giá trị khác không.

00:03:32.000 --> 00:03:36.000
Cuối cùng tôi tiết kiệm được khoảng 2 byte dung lượng lưu trữ cho mỗi số không được giới thiệu.

00:03:36.000 --> 00:03:43.000
Tất nhiên, tôi cũng sẽ cần lưu trữ các vị trí của các số không, để tái tạo lại ma trận dày đặc sau này.

00:03:43.000 --> 00:03:47.000
Kích thước mô hình giảm tuyến tính với mức độ thưa thớt được giới thiệu.

00:03:47.000 --> 00:04:01.000
Mô hình thưa thớt 50% có nghĩa là 50% trọng lượng của nó bằng không và đối với mô hình ResNet50, nó có kích thước khoảng 28 megabyte, bằng khoảng một nửa kích thước Float16.

00:04:01.000 --> 00:04:08.000
Kỹ thuật nén trọng lượng thứ hai là lượng tử hóa, sử dụng độ chính xác 8 bit để lưu trữ trọng lượng.

00:04:08.000 --> 00:04:17.000
Để thực hiện lượng tử hóa, bạn lấy các giá trị trọng lượng và tỷ lệ, dịch chuyển và làm tròn chúng sao cho chúng nằm trong phạm vi INT8.

00:04:17.000 --> 00:04:26.000
Trong ví dụ này, tỷ lệ là 2,35, ánh xạ giá trị nhỏ nhất đến -127 và độ lệch là 0.

00:04:26.000 --> 00:04:33.000
Tùy thuộc vào mô hình, độ lệch khác 0 cũng có thể được sử dụng, điều này đôi khi giúp giảm lỗi lượng tử hóa.

00:04:33.000 --> 00:04:40.000
Thang đo và độ lệch sau đó có thể được sử dụng để khử lượng tử hóa trọng lượng để đưa chúng trở lại phạm vi ban đầu.

00:04:40.000 --> 00:04:47.000
Để giảm độ chính xác trọng lượng xuống dưới 8 bit, bạn có thể sử dụng một kỹ thuật gọi là phân cụm trọng lượng hoặc phân cụm.

00:04:47.000 --> 00:04:55.000
Trong kỹ thuật này, các trọng số có giá trị tương tự được nhóm lại với nhau và được biểu diễn bằng cách sử dụng giá trị của tâm cụm mà chúng thuộc về.

00:04:55.000 --> 00:04:58.000
Những tâm này được lưu trữ trong một bảng tra cứu.

00:04:58.000 --> 00:05:05.000
Và ma trận trọng lượng ban đầu được chuyển đổi thành một bảng chỉ mục, trong đó mỗi phần tử trỏ đến trung tâm cụm tương ứng.

00:05:05.000 --> 00:05:14.000
Trong ví dụ này, vì tôi có bốn cụm, tôi có thể biểu diễn từng trọng lượng bằng cách sử dụng 2 bit, đạt được độ nén gấp 8 lần trên Float16.

00:05:14.000 --> 00:05:23.000
Số lượng trung tâm cụm duy nhất có thể được sử dụng để biểu diễn trọng lượng bằng 2 với lũy thừa của n, trong đó n là độ chính xác được sử dụng để phân loại.

00:05:23.000 --> 00:05:27.000
Vì vậy, phân loại 4-bit có nghĩa là bạn có thể có 16 cụm.

00:05:27.000 --> 00:05:35.000
Trong khi lượng tử hóa làm giảm một nửa kích thước mô hình của bạn, palettization có thể giúp bạn làm cho nó nhỏ hơn tới 8 lần.

00:05:35.000 --> 00:05:40.000
Tóm lại, có ba kỹ thuật khác nhau để nén trọng lượng.

00:05:40.000 --> 00:05:44.000
Mỗi người trong số họ sử dụng một cách khác nhau để thể hiện trọng lượng.

00:05:44.000 --> 00:05:53.000
Chúng cung cấp các mức độ nén khác nhau, có thể được kiểm soát bởi các thông số tương ứng của chúng, như lượng thưa thớt để cắt tỉa và số bit để lặp lại.

00:05:53.000 --> 00:05:58.000
Bây giờ, tôi sẽ minh họa cách bạn có thể tích hợp các kỹ thuật này vào quy trình phát triển mô hình của mình.

00:05:58.000 --> 00:06:02.000
Trước tiên hãy bắt đầu với quy trình làm việc để chuyển đổi mô hình Core ML.

00:06:02.000 --> 00:06:11.000
Bạn có thể bắt đầu bằng cách đào tạo một mô hình với khung đào tạo python yêu thích của mình và sau đó sử dụng Công cụ Core ML để chuyển đổi mô hình đó thành Core ML.

00:06:11.000 --> 00:06:17.000
Quy trình làm việc này có thể được mở rộng thêm một bước nữa để trở thành quy trình nén sau đào tạo.

00:06:17.000 --> 00:06:25.000
Để làm điều đó, bạn thêm một bước nén hoạt động trên các trọng lượng mô hình đã được đào tạo và chuyển đổi để giảm kích thước tổng thể.

00:06:25.000 --> 00:06:28.000
Lưu ý rằng quy trình làm việc này có thể bắt đầu bất cứ lúc nào.

00:06:28.000 --> 00:06:36.000
Ví dụ, bạn có thể bắt đầu với một mô hình được đào tạo trước mà không cần dữ liệu đào tạo hoặc một mô hình Core ML đã được chuyển đổi.

00:06:36.000 --> 00:06:41.000
Khi áp dụng quy trình làm việc này, bạn sẽ có một tùy chọn để chọn lượng nén được áp dụng.

00:06:41.000 --> 00:06:48.000
Bạn càng áp dụng nhiều nén, mô hình kết quả của bạn sẽ càng nhỏ, nhưng như người ta có thể mong đợi, có một số sự đánh đổi.

00:06:48.000 --> 00:06:54.000
Cụ thể, bạn sẽ bắt đầu với một mô hình không nén đạt được độ chính xác nhất định.

00:06:54.000 --> 00:07:01.000
Khi bạn áp dụng một số nén, kích thước mô hình của bạn sẽ giảm, nhưng nó cũng có thể ảnh hưởng đến độ chính xác của bạn.

00:07:01.000 --> 00:07:09.000
Khi bạn áp dụng nén nhiều hơn, tác động này có thể trở nên nổi bật hơn và việc mất độ chính xác có thể trở nên không thể chấp nhận được.

00:07:09.000 --> 00:07:15.000
Xu hướng này và sự đánh đổi có thể chấp nhận được sẽ khác nhau đối với từng trường hợp sử dụng và nó phụ thuộc vào mô hình và tập dữ liệu.

00:07:15.000 --> 00:07:21.000
Để thấy sự đánh đổi này trong thực tế, hãy xem xét một mô hình phân đoạn các đối tượng trong một hình ảnh.

00:07:21.000 --> 00:07:26.000
Đối với hình ảnh của tôi, mô hình trả về xác suất của mỗi điểm ảnh thuộc về ghế sofa.

00:07:26.000 --> 00:07:29.000
Mô hình Float16 cơ bản phân đoạn đối tượng rất tốt.

00:07:29.000 --> 00:07:33.000
Đối với mô hình cắt tỉa 10%, đầu ra rất giống với mô hình cơ bản.

00:07:33.000 --> 00:07:40.000
Các hiện vật bắt đầu xuất hiện ở mức độ thưa thớt 30% và tăng lên với mức độ cao hơn.

00:07:40.000 --> 00:07:47.000
Khi tôi cắt tỉa tới 40%, mô hình bị phá vỡ hoàn toàn và bản đồ xác suất trở nên không thể nhận ra.

00:07:47.000 --> 00:07:53.000
Tương tự, lượng tử hóa 8 bit và định vị hóa 6 bit bảo toàn đầu ra của mô hình cơ sở.

00:07:53.000 --> 00:08:03.000
Ở độ nhợt nhạt 4 bit, bạn bắt đầu thấy một số hiện vật và ở độ nhợt sáng 2 bit, mô hình không phân đoạn hoàn toàn đối tượng.

00:08:03.000 --> 00:08:08.000
Để khắc phục sự suy giảm hiệu suất mô hình này ở tốc độ nén cao hơn, bạn có thể sử dụng một quy trình làm việc khác.

00:08:08.000 --> 00:08:11.000
Quy trình làm việc này được gọi là nén thời gian đào tạo.

00:08:11.000 --> 00:08:16.000
Ở đây, bạn tinh chỉnh mô hình của mình trên một số dữ liệu trong khi nén các trọng số.

00:08:16.000 --> 00:08:24.000
Nén được giới thiệu dần dần và theo cách khác biệt để cho phép trọng lượng điều chỉnh lại các ràng buộc mới áp đặt lên chúng.

00:08:24.000 --> 00:08:31.000
Khi mô hình của bạn đạt được độ chính xác thỏa đáng, bạn có thể chuyển đổi nó và nhận mô hình Core ML nén.

00:08:31.000 --> 00:08:39.000
Lưu ý rằng bạn có thể kết hợp nén thời gian đào tạo trong quy trình đào tạo mô hình hiện tại của mình hoặc bắt đầu với mô hình được đào tạo trước.

00:08:39.000 --> 00:08:50.000
Nén thời gian đào tạo cải thiện sự cân bằng giữa độ chính xác của mô hình và lượng nén, cho phép bạn duy trì cùng một hiệu suất mô hình với tốc độ nén cao hơn.

00:08:50.000 --> 00:08:53.000
Hãy xem lại cùng một mô hình phân đoạn hình ảnh.

00:08:53.000 --> 00:08:58.000
Để cắt tỉa thời gian đào tạo, sản lượng mô hình không thay đổi lên đến 40% thưa thớt.

00:08:58.000 --> 00:09:02.000
Đây là nơi độ chính xác sau đào tạo bị phá vỡ.

00:09:02.000 --> 00:09:10.000
Trên thực tế, bây giờ ngay cả ở mức thưa thớt 50% và 75%, mô hình đạt được bản đồ xác suất tương tự như mô hình cơ sở.

00:09:10.000 --> 00:09:16.000
Ở độ thưa thớt 90%, bạn bắt đầu quan sát thấy sự suy giảm đáng kể về độ chính xác của mô hình.

00:09:16.000 --> 00:09:24.000
Tương tự, lượng tử hóa thời gian đào tạo và palettization cũng duy trì đầu ra của mô hình cơ sở, thậm chí lên đến 2 bit nén trong trường hợp này.

00:09:24.000 --> 00:09:31.000
Tóm lại, bạn có thể áp dụng nén trọng lượng trong quá trình chuyển đổi mô hình hoặc trong quá trình đào tạo mô hình.

00:09:31.000 --> 00:09:36.000
Cái sau cung cấp sự đánh đổi chính xác tốt hơn với chi phí thời gian đào tạo dài hơn.

00:09:36.000 --> 00:09:44.000
Bởi vì quy trình làm việc thứ hai áp dụng nén trong quá trình đào tạo, chúng tôi cũng cần sử dụng các hoạt động khả vi để thực hiện các thuật toán nén.

00:09:44.000 --> 00:09:49.000
Bây giờ chúng ta hãy khám phá cách các quy trình nén này có thể được thực thi với Công cụ Core ML.

00:09:49.000 --> 00:09:59.000
Các API nén mô hình sau đào tạo đã có sẵn trong Core ML Tools 6 để cắt tỉa, làm nhợt hóa và lượng tử hóa theo mô-đun con nén utils.

00:09:59.000 --> 00:10:03.000
Tuy nhiên, không có API để nén thời gian đào tạo.

00:10:03.000 --> 00:10:10.000
Với Core ML Tools 7, các API mới đã được thêm vào để cung cấp khả năng nén thời gian đào tạo.

00:10:10.000 --> 00:10:17.000
Và chúng tôi đã hợp nhất các API cũ hơn và các API mới trong một mô-đun duy nhất được gọi là coremltools.optimize.

00:10:17.000 --> 00:10:28.000
Các API nén sau đào tạo đã được di chuyển trong coremltools.optimize.coreml và các API thời gian đào tạo mới có sẵn trong coremltools.optimize.torch.

00:10:28.000 --> 00:10:31.000
Cái sau hoạt động với các mô hình PyTorch.

00:10:31.000 --> 00:10:35.000
Trước tiên chúng ta hãy xem xét kỹ hơn các API sau đào tạo.

00:10:35.000 --> 00:10:40.000
Trong quy trình nén sau đào tạo, đầu vào là mô hình Core ML.

00:10:40.000 --> 00:10:49.000
Nó có thể được cập nhật bằng ba phương pháp có sẵn trong mô-đun optimize.coreml, áp dụng từng kỹ thuật trong ba kỹ thuật nén mà tôi đã mô tả.

00:10:49.000 --> 00:10:57.000
Để sử dụng các phương pháp này, bạn bắt đầu bằng cách tạo một đối tượng OptimizationConfig, mô tả cách bạn muốn nén mô hình.

00:10:57.000 --> 00:11:02.000
Ở đây, tôi đang cắt tỉa độ lớn với 75% độ thưa thớt mục tiêu.

00:11:02.000 --> 00:11:07.000
Khi cấu hình được xác định, bạn có thể sử dụng phương pháp prune_weights để cắt tỉa mô hình.

00:11:07.000 --> 00:11:10.000
Đó là một quy trình đơn giản, một bước để có được một mô hình nén.

00:11:10.000 --> 00:11:17.000
Bạn có thể sử dụng các API tương tự để định vị và định lượng các trọng số bằng cách sử dụng các cấu hình cụ thể cho các kỹ thuật đó.

00:11:17.000 --> 00:11:20.000
Hãy xem xét quy trình nén thời gian đào tạo ngay bây giờ.

00:11:20.000 --> 00:11:24.000
Trong trường hợp này, như tôi đã mô tả trước đó, bạn cần một mô hình và dữ liệu có thể đào tạo được.

00:11:24.000 --> 00:11:32.000
Cụ thể hơn, để nén mô hình bằng Công cụ Core ML, bạn bắt đầu với mô hình PyTorch, có thể với trọng lượng được đào tạo trước.

00:11:32.000 --> 00:11:41.000
Sau đó sử dụng một trong những API có sẵn trong mô-đun optimize.torch để cập nhật nó và nhận một mô hình PyTorch mới với các lớp nén được chèn vào đó.

00:11:41.000 --> 00:11:46.000
Và sau đó tinh chỉnh nó, sử dụng dữ liệu và mã đào tạo PyTorch gốc.

00:11:46.000 --> 00:11:51.000
Đây là bước mà trọng lượng sẽ được điều chỉnh để cho phép nén.

00:11:51.000 --> 00:11:56.000
Và bạn có thể thực hiện bước này trên MacBook của mình cục bộ, sử dụng phụ trợ MPS PyTorch.

00:11:56.000 --> 00:12:01.000
Khi mô hình được đào tạo để lấy lại độ chính xác, hãy chuyển đổi nó để có được mô hình Core ML.

00:12:01.000 --> 00:12:04.000
Hãy cùng khám phá điều này sâu hơn thông qua một ví dụ mã.

00:12:04.000 --> 00:12:09.000
Tôi đang bắt đầu với mã PyTorch cần thiết để tinh chỉnh mô hình mà tôi muốn nén.

00:12:09.000 --> 00:12:15.000
Bạn có thể dễ dàng tận dụng Công cụ Core ML để thêm thời gian cắt tỉa đào tạo bằng cách chỉ thêm một vài dòng mã.

00:12:15.000 --> 00:12:20.000
Đầu tiên bạn tạo một đối tượng MagnitudePrunerConfig mô tả cách bạn muốn cắt tỉa mô hình.

00:12:20.000 --> 00:12:24.000
Ở đây, tôi đang đặt mục tiêu thưa thớt là 75%.

00:12:24.000 --> 00:12:29.000
Bạn cũng có thể viết cấu hình trong tệp yaml và tải nó bằng phương thức from_yaml.

00:12:29.000 --> 00:12:34.000
Sau đó, bạn tạo một đối tượng pruner với mô hình bạn muốn nén và cấu hình bạn vừa tạo.

00:12:34.000 --> 00:12:38.000
Tiếp theo, bạn gọi chuẩn bị để chèn các lớp cắt tỉa vào mô hình.

00:12:38.000 --> 00:12:43.000
Trong khi tinh chỉnh mô hình, bạn gọi API bước để cập nhật trạng thái bên trong của pruner.

00:12:43.000 --> 00:12:48.000
Khi kết thúc khóa đào tạo, bạn gọi hoàn thiện để gấp mặt nạ cắt tỉa vào tạ.

00:12:48.000 --> 00:12:52.000
Mô hình này sau đó có thể được chuyển đổi sang Core ML bằng cách sử dụng các API chuyển đổi.

00:12:52.000 --> 00:12:57.000
Quy trình làm việc tương tự cũng có thể được sử dụng để lượng tử hóa và phân loại.

00:12:57.000 --> 00:13:05.000
Bây giờ, Srijan sẽ hướng dẫn bạn một bản demo cho thấy cách bạn có thể sử dụng Core ML Tools APIs để làm mờ mô hình phát hiện đối tượng.

00:13:05.000 --> 00:13:06.000
Srijan: Cảm ơn bạn, Pulkit.

00:13:06.000 --> 00:13:13.000
Tên tôi là Srijan, và tôi sẽ hướng dẫn bạn bản demo của API tối ưu hóa Công cụ Core ML.

00:13:13.000 --> 00:13:21.000
Tôi sẽ sử dụng mô hình SSD với xương sống ResNet18 để phát hiện mọi người trong hình ảnh.

00:13:21.000 --> 00:13:25.000
Trước tiên hãy nhập một số mô hình cơ bản và các tiện ích đào tạo.

00:13:25.000 --> 00:13:32.000
Tôi sẽ bắt đầu với việc lấy một phiên bản của mẫu SSD ResNet18 mà tôi vừa nói đến.

00:13:32.000 --> 00:13:40.000
Để đơn giản hóa mọi thứ, tôi sẽ chỉ gọi tiện ích get_ssd_model được viết sẵn cho điều đó.

00:13:40.000 --> 00:13:44.000
Bây giờ mô hình đã được tải, hãy đào tạo nó trong một vài kỷ nguyên.

00:13:44.000 --> 00:13:53.000
Vì nó là một mô hình phát hiện đối tượng, mục tiêu của việc đào tạo sẽ là giảm sự mất mát SSD của nhiệm vụ phát hiện.

00:13:53.000 --> 00:14:08.000
Để đồng nhất, tiện ích train_epoch đóng gói mã cần thiết để đào tạo mô hình cho một kỷ nguyên, như gọi chuyển tiếp qua các lô khác nhau, tính toán tổn thất và thực hiện giảm độ dốc.

00:14:08.000 --> 00:14:12.000
Trong quá trình đào tạo, sự mất mát SSD dường như đang giảm xuống.

00:14:12.000 --> 00:14:16.000
Bây giờ tôi sẽ chuyển đổi mô hình thành mô hình Core ML.

00:14:16.000 --> 00:14:24.000
Để làm điều đó, trước tiên tôi sẽ theo dõi mô hình và sau đó gọi coremltools.convert API.

00:14:24.000 --> 00:14:30.000
Hãy gọi một tiện ích đã nhập để kiểm tra kích thước của mô hình.

00:14:30.000 --> 00:14:34.000
Kích thước của mô hình là 23,6 megabyte.

00:14:34.000 --> 00:14:38.000
Bây giờ, tôi sẽ chạy dự đoán trên mô hình Core ML.

00:14:38.000 --> 00:14:45.000
Tôi đã chọn một hình ảnh của mình từ chuyến đi London của mình cũng như một hình ảnh khác để kiểm tra các phát hiện.

00:14:45.000 --> 00:15:05.000
Ngưỡng tin cậy để mô hình phát hiện một đối tượng được đặt thành 30% vì vậy nó sẽ chỉ vẽ các hộp mà nó tự tin ít nhất 30% về đối tượng có mặt.

00:15:05.000 --> 00:15:07.000
Phát hiện đó có vẻ đúng.

00:15:07.000 --> 00:15:12.000
Bây giờ tôi tò mò muốn xem liệu tôi có thể giảm kích thước của mô hình này không.

00:15:12.000 --> 00:15:16.000
Tôi sẽ thử tái tạo sau đào tạo trước.

00:15:16.000 --> 00:15:26.000
Đối với điều đó, tôi sẽ nhập một số lớp cấu hình và phương thức từ coremltools.optimize.coreml.

00:15:26.000 --> 00:15:30.000
Bây giờ tôi sẽ làm mờ trọng lượng của mô hình với 6 bit.

00:15:30.000 --> 00:15:38.000
Đối với điều đó, tôi sẽ tạo một đối tượng OpPalettizerConfig, chỉ định chế độ là kmeans và nbit là 6.

00:15:38.000 --> 00:15:45.000
Điều này sẽ chỉ định các tham số ở cấp độ op và tôi có thể làm mờ từng op khác nhau.

00:15:45.000 --> 00:15:51.000
Tuy nhiên, ngay bây giờ, tôi sẽ áp dụng cùng một chế độ 6-bit cho tất cả các hoạt động.

00:15:51.000 --> 00:16:00.000
Tôi sẽ làm điều đó bằng cách xác định OptimizationConfig và chuyển op_config này làm tham số toàn cầu cho nó.

00:16:00.000 --> 00:16:10.000
Cấu hình tối ưu hóa sau đó được chuyển đến phương thức palettize_weights cùng với mô hình được chuyển đổi để có được mô hình palettized.

00:16:10.000 --> 00:16:15.000
Hãy xem kích thước đã giảm xuống bây giờ là bao nhiêu.

00:16:15.000 --> 00:16:22.000
Kích thước của mô hình đã giảm xuống còn khoảng 9 megabyte, nhưng nó có ảnh hưởng đến hiệu suất trên các hình ảnh thử nghiệm không?

00:16:22.000 --> 00:16:24.000
Hãy cùng tìm hiểu.

00:16:24.000 --> 00:16:27.000
Chà, việc phát hiện vẫn hoạt động tốt.

00:16:27.000 --> 00:16:35.000
Tôi thực sự hào hứng khi đẩy vận may của mình để thử nhợt hóa sau đào tạo 2-bit ngay bây giờ.

00:16:35.000 --> 00:16:48.000
Làm điều đó đơn giản như chỉ cần thay đổi nbit từ 6 thành 2 trong OpPalettizerConfig và chạy lại API palettize_weights.

00:16:48.000 --> 00:16:57.000
Hãy sử dụng các tiện ích để xem kích thước và hiệu suất của mô hình Core ML này.

00:16:57.000 --> 00:17:03.000
Đúng như dự đoán, kích thước của mô hình đã giảm xuống còn khoảng 3 megabyte.

00:17:03.000 --> 00:17:11.000
Tuy nhiên, hiệu suất không tối ưu vì mô hình không thể phát hiện mọi người trong cả hai hình ảnh.

00:17:11.000 --> 00:17:21.000
Không có hộp nào hiển thị trong dự đoán, vì không có hộp nào được dự đoán bởi mô hình có xác suất tin cậy trên ngưỡng 30%.

00:17:21.000 --> 00:17:28.000
Hãy thử chế tạo thời gian đào tạo 2 bit để xem liệu điều đó có hoạt động tốt hơn không.

00:17:28.000 --> 00:17:38.000
Tôi sẽ bắt đầu bằng cách nhập DKMPalettizerConfig và DKMPalettizer từ coremltools.optimize.torch để làm điều đó.

00:17:38.000 --> 00:17:48.000
DKM là một thuật toán để tìm hiểu các cụm trọng lượng bằng cách thực hiện thao tác kmeans khả vi dựa trên sự chú ý trên chúng.

00:17:48.000 --> 00:17:52.000
Bây giờ là lúc để xác định cấu hình palettization.

00:17:52.000 --> 00:18:01.000
Chỉ cần chỉ định đơn giản n_bit là 2 trong cấu hình_to toàn cầu và tất cả các mô-đun được hỗ trợ sẽ được phân loại 2-bit.

00:18:01.000 --> 00:18:07.000
Và ở đây, tôi sẽ tạo một đối tượng palettizer từ mô hình và cấu hình.

00:18:07.000 --> 00:18:13.000
Gọi API chuẩn bị ngay bây giờ sẽ chèn các mô-đun thân thiện với palettization vào mô hình.

00:18:13.000 --> 00:18:17.000
Đã đến lúc tinh chỉnh mô hình cho một vài kỷ nguyên.

00:18:17.000 --> 00:18:29.000
Bây giờ mô hình đã được tinh chỉnh, tôi sẽ gọi API hoàn thiện sẽ khôi phục các trọng số được đánh màu dưới dạng trọng số của mô hình, do đó hoàn thành quy trình.

00:18:29.000 --> 00:18:32.000
Bước tiếp theo là kiểm tra kích thước của mô hình.

00:18:32.000 --> 00:18:38.000
Đối với điều đó, tôi sẽ chuyển đổi mô hình ngọn đuốc thành mô hình Core ML.

00:18:38.000 --> 00:18:42.000
Hãy bắt đầu bằng cách truy tìm mô hình bằng cách sử dụng torch.jit.trace.

00:18:42.000 --> 00:18:53.000
Bây giờ tôi sẽ gọi API chuyển đổi và lần này, tôi sẽ sử dụng một cờ bổ sung có tên PassPipeline và đặt giá trị của nó thành DEFAULT_PALETTIZATION.

00:18:53.000 --> 00:19:03.000
Điều này sẽ chỉ ra cho bộ chuyển đổi sử dụng biểu diễn palettized cho các trọng số được chuyển đổi.

00:19:03.000 --> 00:19:08.000
Hãy xem kích thước của mô hình và hiệu suất của nó trên các hình ảnh thử nghiệm.

00:19:08.000 --> 00:19:23.000
Tôi có thể thấy rằng mô hình tôi thời gian đào tạo được phân loại cũng khoảng 3 megabyte, giảm xuống mức nén gấp 8 lần, nhưng không giống như mô hình phân dạng sau đào tạo, mô hình này đang thực hiện phát hiện chính xác trên hình ảnh thử nghiệm.

00:19:23.000 --> 00:19:29.000
Vì đây là bản demo, tôi vừa kiểm tra hiệu suất mô hình trên hai hình ảnh mẫu.

00:19:29.000 --> 00:19:38.000
Trong một kịch bản trong thế giới thực, tôi sẽ sử dụng một số liệu như độ chính xác trung bình trung bình và đánh giá trên tập dữ liệu xác thực.

00:19:38.000 --> 00:19:40.000
Hãy tóm tắt lại.

00:19:40.000 --> 00:19:48.000
Tôi bắt đầu với một mô hình được đào tạo và chuyển đổi nó để có được một mô hình 23,6 megabyte với trọng lượng Float16.

00:19:48.000 --> 00:19:57.000
Sau đó, tôi đã sử dụng API palettize_weights để nhanh chóng có được một mô hình nhỏ hơn với trọng số 6-bit, điều này đã hoạt động tốt trên dữ liệu của tôi.

00:19:57.000 --> 00:20:04.000
Tuy nhiên, khi tôi đẩy nó xa hơn đến 2 bit, nó cho thấy hiệu suất giảm rõ ràng.

00:20:04.000 --> 00:20:15.000
Đăng bài này, tôi đã cập nhật mô hình ngọn đuốc với API optimize.torch và sử dụng thuật toán kmeans khả vi để tinh chỉnh cho một vài kỷ nguyên.

00:20:15.000 --> 00:20:22.000
Với điều đó, tôi đã có thể có được độ chính xác tốt với tùy chọn nén 2-bit.

00:20:22.000 --> 00:20:40.000
Mặc dù bản demo sử dụng kết hợp mô hình và thuật toán tối ưu hóa cụ thể, quy trình làm việc này sẽ khái quát hóa cho trường hợp sử dụng của bạn và sẽ giúp bạn tìm ra sự cân bằng giữa lượng nén bạn mong muốn và thời gian và dữ liệu cần thiết để đào tạo lại mô hình.

00:20:40.000 --> 00:20:44.000
Điều này đưa chúng ta đến chủ đề cuối cùng của chúng ta, hiệu suất.

00:20:44.000 --> 00:20:55.000
Tôi muốn đề cập ngắn gọn đến những cải tiến đã được thực hiện đối với thời gian chạy Core ML để thực thi các mô hình như vậy hiệu quả hơn khi được triển khai trong ứng dụng của bạn.

00:20:55.000 --> 00:21:02.000
Hãy xem xét một vài điểm khác biệt chính giữa thời gian chạy trong iOS 16 và iOS 17.

00:21:02.000 --> 00:21:14.000
Trong khi ở iOS 16, có sự hỗ trợ cho các mô hình nén chỉ có trọng lượng, trong iOS 17, các mô hình lượng tử hóa kích hoạt 8-bit cũng có thể được thực thi.

00:21:14.000 --> 00:21:31.000
Trong iOS 16, một mô hình nén trọng lượng chạy với tốc độ tương tự như mô hình tương ứng với trọng lượng float, trong khi trong iOS 17, thời gian chạy Core ML đã được cập nhật và bây giờ các mô hình nén chạy nhanh hơn trong một số tình huống nhất định.

00:21:31.000 --> 00:21:40.000
Các cải tiến thời gian chạy tương tự cũng có sẵn trong các phiên bản macOS, tvOS và watchOS mới hơn.

00:21:40.000 --> 00:21:43.000
Nhưng những cải tiến này đạt được như thế nào?

00:21:43.000 --> 00:22:03.000
Trong các mô hình, nơi chỉ có trọng số được nén, vì các kích hoạt có độ chính xác dấu phẩy động, trước khi một thao tác như tích chập hoặc nhân ma trận có thể xảy ra, các giá trị trọng số cần được giải nén để phù hợp với độ chính xác của đầu vào khác.

00:22:03.000 --> 00:22:10.000
Bước giải nén này diễn ra trước thời hạn trong thời gian chạy iOS 16.

00:22:10.000 --> 00:22:19.000
Do đó, trong trường hợp này, mô hình được chuyển đổi thành mô hình chính xác nổi hoàn toàn trong bộ nhớ trước khi thực thi.

00:22:19.000 --> 00:22:23.000
Do đó, không có thay đổi nào được quan sát thấy trong độ trễ suy luận.

00:22:23.000 --> 00:22:33.000
Tuy nhiên, trong iOS 17, trong một số tình huống nhất định, trọng số được giải nén kịp thời, ngay trước khi thao tác được thực hiện.

00:22:33.000 --> 00:22:44.000
Điều này có lợi thế là tải các trọng lượng bit nhỏ hơn từ bộ nhớ với chi phí giải nén trong mọi cuộc gọi suy luận.

00:22:44.000 --> 00:22:55.000
Đối với một số đơn vị tính toán nhất định, chẳng hạn như Neural Engine và một số loại mô hình nhất định bị ràng buộc với bộ nhớ, điều này có thể dẫn đến lợi ích suy luận.

00:22:55.000 --> 00:23:08.000
Để minh họa những lợi ích thời gian chạy này, tôi đã chọn và lập hồ sơ một vài mô hình và vẽ biểu đồ số lượng tương đối mà suy luận của chúng được tăng tốc so với biến thể Float16 của chúng.

00:23:08.000 --> 00:23:14.000
Đúng như dự đoán, số lượng tăng tốc phụ thuộc vào mô hình và phần cứng.

00:23:14.000 --> 00:23:21.000
Đây là phạm vi tăng tốc cho các mẫu xe cổ điển 4-bit trên iPhone 14 Pro Max.

00:23:21.000 --> 00:23:27.000
Những cải tiến thay đổi từ 5% đến 30%.

00:23:27.000 --> 00:23:39.000
Đối với các mô hình thưa thớt cũng vậy, có những cải tiến khác nhau dựa trên loại mô hình, với một số mô hình chạy nhanh hơn 75% so với các biến thể Float16 của chúng.

00:23:39.000 --> 00:23:46.000
Câu hỏi bây giờ được đặt ra: chiến lược để có được hiệu suất độ trễ tốt nhất là gì?

00:23:46.000 --> 00:23:56.000
Đó sẽ là bắt đầu với một mô hình float và sử dụng optimize.coreml APIs để khám phá các biểu diễn khác nhau của mô hình.

00:23:56.000 --> 00:24:02.000
Điều này sẽ nhanh chóng, vì nó không yêu cầu đào tạo lại mô hình.

00:24:02.000 --> 00:24:05.000
Sau đó, lập hồ sơ nó trên thiết bị mà bạn quan tâm.

00:24:05.000 --> 00:24:16.000
Đối với điều này, các báo cáo hiệu suất Core ML trong Xcode sẽ cung cấp cho bạn rất nhiều khả năng hiển thị về suy luận, bao gồm cả nơi các hoạt động chạy.

00:24:16.000 --> 00:24:21.000
Sau đó, danh sách rút gọn dựa trên cấu hình nào mang lại cho bạn lợi nhuận tốt nhất.

00:24:21.000 --> 00:24:36.000
Sau đó, bạn có thể tập trung vào việc đánh giá độ chính xác và cố gắng cải thiện, điều này có thể yêu cầu áp dụng một số nén thời gian đào tạo với ngọn đuốc và Công cụ Core ML trước khi hoàn thiện mô hình của bạn.

00:24:36.000 --> 00:24:50.000
Tóm lại, điều quan trọng là phải giảm kích thước của các mô hình và bây giờ bạn có thể làm điều đó dễ dàng hơn bao giờ hết với các API Công cụ Core ML mới và đạt được tốc độ suy luận và dấu chân bộ nhớ thấp hơn.

00:24:50.000 --> 00:24:56.000
Để kiểm tra thêm các tùy chọn và dữ liệu điểm chuẩn, hãy truy cập tài liệu của chúng tôi.

00:24:56.000 --> 00:25:08.000
Tôi cũng khuyên bạn nên điều chỉnh video "Cải thiện tích hợp Core ML với dự đoán không đồng bộ" nói về những cải tiến được thực hiện đối với khung Core ML mà tôi không đề cập trong các trang trình bày hôm nay.

00:25:08.000 --> 23:59:59.000
Cảm ơn bạn, và chúc bạn nén vui vẻ.

