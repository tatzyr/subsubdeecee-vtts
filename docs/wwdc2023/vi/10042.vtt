WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:15.000
Doug: Chào mừng mọi người. Tôi là Doug Davidson, và tôi ở đây để nói chuyện với bạn về Xử lý Ngôn ngữ Tự nhiên.

00:00:15.000 --> 00:00:19.000
Bây giờ, đã có một số phiên về Xử lý Ngôn ngữ Tự nhiên trong những năm qua.

00:00:19.000 --> 00:00:25.000
Những gì chúng ta sẽ nói về ngày hôm nay được xây dựng dựa trên tất cả những điều đó, với việc bổ sung một số chức năng mới thú vị.

00:00:25.000 --> 00:00:31.000
Đầu tiên tôi sẽ đưa ra một số thông tin cơ bản về các mô hình NLP và NLP.

00:00:31.000 --> 00:00:34.000
Sau đó tôi sẽ tóm tắt chức năng hiện có.

00:00:34.000 --> 00:00:37.000
Sau đó tôi sẽ nói về những gì mới trong năm nay.

00:00:37.000 --> 00:00:40.000
Tôi sẽ thảo luận về một số ứng dụng nâng cao.

00:00:40.000 --> 00:00:43.000
Và sau đó tôi sẽ gói nó lại.

00:00:43.000 --> 00:00:45.000
Hãy bắt đầu với một số thông tin cơ bản.

00:00:45.000 --> 00:00:50.000
Về mặt sơ đồ, các mô hình NLP thường có quy trình tương tự.

00:00:50.000 --> 00:01:03.000
Họ bắt đầu với dữ liệu văn bản, sau đó có một lớp đầu vào chuyển đổi nó thành biểu diễn tính năng số, trên đó một mô hình học máy có thể hoạt động và tạo ra một số đầu ra.

00:01:03.000 --> 00:01:12.000
Các ví dụ rõ ràng nhất về điều này từ những năm trước là các mô hình Tạo ML được hỗ trợ để phân loại văn bản và gắn thẻ từ.

00:01:12.000 --> 00:01:22.000
Sự phát triển của NLP như một lĩnh vực có thể được truy tìm khá chặt chẽ chỉ bằng sự phát triển của các phiên bản ngày càng tinh vi của các lớp đầu vào.

00:01:22.000 --> 00:01:26.000
Mười hoặc hai mươi năm trước, đây là những đặc điểm chính tả đơn giản.

00:01:26.000 --> 00:01:34.000
Sau đó khoảng một thập kỷ trước, mọi thứ chuyển sang sử dụng nhúng từ tĩnh, chẳng hạn như Word2Vec và GloVe.

00:01:34.000 --> 00:01:42.000
Sau đó là nhúng từ theo ngữ cảnh dựa trên các mô hình mạng thần kinh, chẳng hạn như CNN và LSTM.

00:01:42.000 --> 00:01:46.000
Và gần đây hơn, các mô hình ngôn ngữ dựa trên máy biến áp.

00:01:46.000 --> 00:01:49.000
Tôi nên nói một vài từ về việc nhúng là gì.

00:01:49.000 --> 00:02:03.000
Ở dạng đơn giản nhất, nó chỉ là một bản đồ từ các từ trong một ngôn ngữ đến các vectơ trong một số không gian vectơ trừu tượng, nhưng được đào tạo như một mô hình học máy sao cho các từ có ý nghĩa tương tự gần nhau trong không gian vectơ.

00:02:03.000 --> 00:02:07.000
Điều này cho phép nó kết hợp kiến thức ngôn ngữ.

00:02:07.000 --> 00:02:11.000
Nhúng tĩnh chỉ là một bản đồ đơn giản từ từ đến vectơ.

00:02:11.000 --> 00:02:15.000
Chuyển một từ, mô hình tra cứu nó trong một bảng và cung cấp một vectơ.

00:02:15.000 --> 00:02:20.000
Những thứ này được đào tạo sao cho các từ có ý nghĩa tương tự gần nhau trong không gian vectơ.

00:02:20.000 --> 00:02:24.000
Điều này khá hữu ích cho việc hiểu từng từ riêng lẻ.

00:02:24.000 --> 00:02:33.000
Các nhúng phức tạp hơn là động và theo ngữ cảnh sao cho mỗi từ trong câu được ánh xạ đến một vectơ khác nhau tùy thuộc vào việc sử dụng nó trong câu.

00:02:33.000 --> 00:02:43.000
Ví dụ, "thực phẩm" trong "căn ăn nhanh" có ý nghĩa khác với "thực phẩm" trong "thực phẩm để suy nghĩ", vì vậy chúng sẽ nhận được các vectơ nhúng khác nhau.

00:02:43.000 --> 00:02:49.000
Bây giờ, mục đích của việc nhúng mạnh mẽ làm lớp đầu vào là cho phép học chuyển giao.

00:02:49.000 --> 00:03:03.000
Việc nhúng được đào tạo trên một lượng lớn dữ liệu và gói gọn kiến thức chung về ngôn ngữ, có thể được chuyển sang nhiệm vụ cụ thể của bạn mà không yêu cầu một lượng lớn dữ liệu đào tạo dành riêng cho nhiệm vụ.

00:03:03.000 --> 00:03:08.000
Hiện tại, Create ML hỗ trợ nhúng loại này bằng cách sử dụng các mô hình ELMo.

00:03:08.000 --> 00:03:14.000
Các mô hình này dựa trên LSTM có đầu ra được kết hợp để tạo ra vectơ nhúng.

00:03:14.000 --> 00:03:20.000
Chúng có thể được sử dụng thông qua Create ML để phân loại đào tạo và gắn thẻ các mô hình.

00:03:20.000 --> 00:03:24.000
Bây giờ, hãy để tôi thảo luận về các mô hình đã được hỗ trợ cho đến nay.

00:03:24.000 --> 00:03:33.000
Những điều này đã được thảo luận rất chi tiết trong các phiên trước vào năm 2019 và 2020, vì vậy tôi sẽ chỉ mô tả ngắn gọn chúng ở đây.

00:03:33.000 --> 00:03:42.000
Ngôn ngữ tự nhiên hỗ trợ đào tạo mô hình bằng cách sử dụng Tạo ML thường tuân theo mô hình mà chúng tôi đã thấy cho các mô hình NLP.

00:03:42.000 --> 00:03:48.000
Điều này liên quan đến các mô hình cho hai nhiệm vụ khác nhau: phân loại văn bản và gắn thẻ từ.

00:03:48.000 --> 00:03:54.000
Trong phân loại văn bản, đầu ra mô tả văn bản đầu vào bằng cách sử dụng một trong một tập hợp các lớp.

00:03:54.000 --> 00:03:57.000
Ví dụ, nó có thể là một chủ đề hoặc một tình cảm.

00:03:57.000 --> 00:04:08.000
Và trong gắn thẻ từ, đầu ra đặt nhãn trên mỗi từ trong văn bản đầu vào, ví dụ, một phần của bài phát biểu hoặc nhãn vai trò.

00:04:08.000 --> 00:04:25.000
Và các mô hình Create ML được hỗ trợ thường theo dõi sự phát triển của trường NLP, bắt đầu với các mô hình tối đa và dựa trên CRF, sau đó thêm hỗ trợ cho nhúng từ tĩnh, và sau đó nhúng từ động cho các mô hình Tạo ML bằng cách sử dụng nhúng ELMo.

00:04:25.000 --> 00:04:35.000
Và bạn có thể xem chi tiết về điều này trong các phiên trước, "Những tiến bộ trong Khung ngôn ngữ tự nhiên" từ năm 2019 và "Làm cho ứng dụng thông minh hơn với ngôn ngữ tự nhiên" từ năm 2020.

00:04:35.000 --> 00:04:40.000
Bây giờ hãy để tôi chuyển sang những gì mới trong năm nay trong Ngôn ngữ Tự nhiên.

00:04:40.000 --> 00:04:45.000
Tôi rất vui khi nói rằng bây giờ chúng tôi cung cấp các nhúng theo ngữ cảnh dựa trên máy biến áp.

00:04:45.000 --> 00:04:48.000
Cụ thể, đây là những nhúng BERT.

00:04:48.000 --> 00:04:53.000
Điều đó chỉ là viết tắt của Biểu diễn Bộ mã hóa Hai chiều từ Transformers.

00:04:53.000 --> 00:05:00.000
Đây là những mô hình nhúng được đào tạo trên một lượng lớn văn bản bằng cách sử dụng phong cách đào tạo mô hình ngôn ngữ đeo mặt nạ.

00:05:00.000 --> 00:05:14.000
Điều này có nghĩa là mô hình được đưa ra một câu với một từ được che giấu và yêu cầu đề xuất từ đó, ví dụ, "thực phẩm" trong "thực phẩm để suy nghĩ" và được đào tạo để làm tốt hơn và tốt hơn trong việc này.

00:05:14.000 --> 00:05:31.000
Các máy biến áp trong trái tim của chúng dựa trên cái được gọi là cơ chế chú ý, cụ thể là sự tự chú ý nhiều đầu, cho phép mô hình tính đến các phần khác nhau của văn bản với trọng lượng khác nhau, theo nhiều cách khác nhau cùng một lúc.

00:05:31.000 --> 00:05:44.000
Cơ chế tự chú ý nhiều đầu được bao bọc với nhiều lớp khác, sau đó được lặp lại nhiều lần, điều này hoàn toàn cung cấp một mô hình mạnh mẽ và linh hoạt có thể tận dụng một lượng lớn dữ liệu văn bản.

00:05:44.000 --> 00:05:52.000
Trên thực tế, nó có thể được đào tạo trên nhiều ngôn ngữ cùng một lúc, dẫn đến một mô hình đa ngôn ngữ.

00:05:52.000 --> 00:05:54.000
Điều này có một số lợi thế.

00:05:54.000 --> 00:06:00.000
Nó cho phép hỗ trợ nhiều ngôn ngữ ngay lập tức và thậm chí nhiều ngôn ngữ cùng một lúc.

00:06:00.000 --> 00:06:10.000
Nhưng thậm chí còn hơn thế nữa, vì sự tương đồng giữa các ngôn ngữ, có một số sức mạnh tổng hợp như dữ liệu cho một ngôn ngữ giúp ích cho các ngôn ngữ khác.

00:06:10.000 --> 00:06:17.000
Vì vậy, chúng tôi đã ngay lập tức hỗ trợ 27 ngôn ngữ khác nhau trên nhiều họ ngôn ngữ khác nhau.

00:06:17.000 --> 00:06:25.000
Điều này được thực hiện với ba mô hình riêng biệt, mỗi mô hình dành cho các nhóm ngôn ngữ có chung hệ thống chữ viết liên quan.

00:06:25.000 --> 00:06:35.000
Vì vậy, có một mô hình cho các ngôn ngữ viết Latinh, một cho các ngôn ngữ sử dụng Cyrillic và một cho tiếng Trung, tiếng Nhật và tiếng Hàn.

00:06:35.000 --> 00:06:42.000
Các mô hình nhúng này phù hợp với khóa đào tạo Create ML mà chúng ta đã thảo luận trước đó, đóng vai trò là lớp mã hóa đầu vào.

00:06:42.000 --> 00:06:46.000
Đây là một mã hóa mạnh mẽ cho nhiều mô hình khác nhau.

00:06:46.000 --> 00:06:51.000
Ngoài ra, dữ liệu mà bạn sử dụng để đào tạo không nhất thiết phải bằng một ngôn ngữ duy nhất.

00:06:51.000 --> 00:06:54.000
Hãy để tôi chỉ cho bạn cách hoạt động của nó với một ví dụ.

00:06:54.000 --> 00:07:02.000
Giả sử bạn đang viết một ứng dụng nhắn tin và muốn hỗ trợ người dùng bằng cách tự động phân loại các tin nhắn mà họ nhận được.

00:07:02.000 --> 00:07:17.000
Giả sử bạn muốn chia chúng thành ba loại: tin nhắn cá nhân, chẳng hạn như bạn có thể nhận được từ bạn bè, tin nhắn kinh doanh, chẳng hạn như bạn có thể nhận được từ đồng nghiệp và tin nhắn thương mại, chẳng hạn như bạn có thể nhận được từ các doanh nghiệp mà bạn tương tác.

00:07:17.000 --> 00:07:22.000
Nhưng người dùng có thể nhận được tin nhắn bằng nhiều ngôn ngữ khác nhau và bạn muốn xử lý điều đó.

00:07:22.000 --> 00:07:29.000
Đối với ví dụ này, tôi đã tập hợp một số dữ liệu đào tạo bằng nhiều ngôn ngữ, tiếng Anh, tiếng Ý, tiếng Đức và tiếng Tây Ban Nha.

00:07:29.000 --> 00:07:35.000
Tôi đã sử dụng định dạng json, nhưng bạn cũng có thể sử dụng thư mục hoặc CSV.

00:07:35.000 --> 00:07:39.000
Để đào tạo mô hình của chúng tôi, chúng tôi vào ứng dụng Tạo ML và tạo một dự án.

00:07:39.000 --> 00:07:48.000
Sau đó chúng ta cần chọn dữ liệu đào tạo của mình.

00:07:48.000 --> 00:08:00.000
Tôi cũng đã chuẩn bị dữ liệu xác thực và dữ liệu thử nghiệm để đi cùng với nó.

00:08:00.000 --> 00:08:09.000
Sau đó, chúng ta cần chọn thuật toán của mình và chúng ta có một lựa chọn mới ở đây: nhúng BERT.

00:08:09.000 --> 00:08:13.000
Một khi chúng ta đã chọn những thứ đó, chúng ta có thể chọn kịch bản.

00:08:13.000 --> 00:08:17.000
Vì đây là những ngôn ngữ viết bằng tiếng Latinh, tôi sẽ để nó bằng tiếng Latinh.

00:08:17.000 --> 00:08:27.000
Nếu chúng tôi đang sử dụng một ngôn ngữ duy nhất, chúng tôi sẽ có tùy chọn chỉ định nó ở đây, nhưng đây là đa ngôn ngữ, vì vậy chúng tôi sẽ để nó ở chế độ tự động.

00:08:27.000 --> 00:08:36.000
Sau đó, tất cả những gì chúng ta cần làm là nhấn Train, và đào tạo mô hình sẽ bắt đầu.

00:08:36.000 --> 00:08:41.000
Phần tốn nhiều thời gian nhất của khóa đào tạo là áp dụng những nhúng mạnh mẽ này vào văn bản.

00:08:41.000 --> 00:08:49.000
Sau đó, mô hình đào tạo khá nhanh với độ chính xác cao.

00:08:49.000 --> 00:08:56.000
Tại thời điểm đó, chúng ta có thể thử nó trên một số tin nhắn ví dụ.

00:08:56.000 --> 00:09:08.000
Bằng Tiếng Anh... Hoặc Tiếng Tây Ban Nha.

00:09:08.000 --> 00:09:12.000
Và người mẫu khá tự tin rằng đây là những thông điệp thương mại.

00:09:12.000 --> 00:09:25.000
Như một ví dụ về sự phối hợp có thể xảy ra, mô hình này chưa được đào tạo về tiếng Pháp, nhưng nó vẫn có thể phân loại một số văn bản tiếng Pháp.

00:09:25.000 --> 00:09:31.000
Tuy nhiên, tôi khuyên bạn nên sử dụng dữ liệu đào tạo cho từng ngôn ngữ mà bạn quan tâm.

00:09:31.000 --> 00:09:42.000
Bây giờ, cho đến nay chúng tôi vừa làm việc với Create ML, nhưng cũng có thể làm việc với các nhúng này bằng cách sử dụng khung Ngôn ngữ Tự nhiên với một lớp mới gọi là NLContextualEmbedding.

00:09:42.000 --> 00:09:48.000
Điều này cho phép bạn xác định mô hình nhúng mà bạn muốn và tìm ra một số thuộc tính của nó.

00:09:48.000 --> 00:09:55.000
Bạn có thể tìm kiếm một mô hình nhúng theo nhiều cách khác nhau, ví dụ, theo ngôn ngữ hoặc theo kịch bản.

00:09:55.000 --> 00:10:01.000
Một khi bạn có một mô hình như vậy, bạn có thể nhận được các thuộc tính như kích thước của các vectơ.

00:10:01.000 --> 00:10:07.000
Ngoài ra, mỗi mô hình có một mã định danh, chỉ là một chuỗi xác định duy nhất mô hình.

00:10:07.000 --> 00:10:20.000
Ví dụ: khi bạn bắt đầu làm việc với một mô hình, bạn có thể định vị nó theo ngôn ngữ, nhưng sau này bạn sẽ muốn đảm bảo rằng bạn đang sử dụng cùng một mô hình và mã định danh sẽ cho phép bạn thực hiện việc này.

00:10:20.000 --> 00:10:29.000
Một điều cần lưu ý là, giống như nhiều tính năng Ngôn ngữ Tự nhiên khác, các mô hình nhúng này dựa vào các nội dung được tải xuống khi cần thiết.

00:10:29.000 --> 00:10:40.000
NLContextualEmbedding cung cấp một số API để cung cấp cho bạn quyền kiểm soát bổ sung đối với điều này, ví dụ, để yêu cầu tải xuống trước khi sử dụng.

00:10:40.000 --> 00:10:51.000
Bạn có thể hỏi liệu một mô hình nhúng nhất định hiện có tài sản có sẵn trên thiết bị hay không và nếu không đưa vào yêu cầu, điều này sẽ dẫn đến việc chúng được tải xuống.

00:10:51.000 --> 00:11:00.000
Bây giờ, một số bạn có thể nói, tôi có một số mô hình mà tôi không đào tạo bằng Create ML, nhưng thay vào đó tôi đào tạo bằng PyTorch hoặc TensorFlow.

00:11:00.000 --> 00:11:03.000
Tôi vẫn có thể sử dụng những nhúng BERT mới này chứ?

00:11:03.000 --> 00:11:05.000
Vâng, bạn có thể.

00:11:05.000 --> 00:11:14.000
Chúng tôi cung cấp các mô hình nhúng đa ngôn ngữ được đào tạo trước này có sẵn cho bạn, bạn có thể sử dụng làm lớp đầu vào cho bất kỳ mô hình nào bạn muốn đào tạo.

00:11:14.000 --> 00:11:17.000
Đây là cách nó sẽ hoạt động.

00:11:17.000 --> 00:11:24.000
Trên thiết bị macOS của bạn, bạn sẽ sử dụng NLContextualEmbedding để lấy các vectơ nhúng cho dữ liệu đào tạo của mình.

00:11:24.000 --> 00:11:36.000
Sau đó, bạn sẽ cung cấp những thứ này làm đầu vào cho khóa đào tạo của mình bằng cách sử dụng PyTorch hoặc TensorFlow và chuyển đổi kết quả thành mô hình Core ML bằng các công cụ Core ML.

00:11:36.000 --> 00:11:47.000
Sau đó, tại thời điểm suy luận trên thiết bị, bạn sẽ sử dụng NLContextualEmbedding để lấy các vectơ nhúng cho dữ liệu đầu vào của mình, chuyển chúng vào mô hình Core ML của bạn để có được đầu ra.

00:11:47.000 --> 00:11:58.000
Để hỗ trợ điều này, có các API NLContextualEmbedding bổ sung cho phép bạn tải một mô hình, áp dụng nó vào một đoạn văn bản và nhận các vectơ nhúng kết quả.

00:11:58.000 --> 00:12:06.000
Nếu bạn nhớ mã định danh mô hình từ trước đó, bạn có thể sử dụng nó để truy xuất cùng một mô hình mà bạn đã sử dụng để đào tạo.

00:12:06.000 --> 00:12:14.000
Sau đó, bạn có thể áp dụng mô hình cho một đoạn văn bản, tạo ra một đối tượng NLContextualEmbeddingResult.

00:12:14.000 --> 00:12:20.000
Khi bạn có đối tượng này, bạn có thể sử dụng nó để lặp lại các vectơ nhúng.

00:12:20.000 --> 00:12:26.000
Bây giờ, để cung cấp cho bạn một hương vị về những gì có thể với điều này, chúng tôi đã chuẩn bị một mô hình ví dụ đơn giản.

00:12:26.000 --> 00:12:46.000
Chúng tôi bắt đầu với một mô hình khuếch tán ổn định bằng tiếng Anh hiện có, sau đó sử dụng một số dữ liệu đa ngôn ngữ để tinh chỉnh nó để sử dụng các nhúng BERT mới làm lớp đầu vào, lấy chúng làm cố định và cũng đào tạo một lớp chiếu tuyến tính đơn giản để chuyển đổi chiều.

00:12:46.000 --> 00:12:52.000
Kết quả sau đó là một mô hình Khuếch tán Ổn định lấy đầu vào đa ngôn ngữ.

00:12:52.000 --> 00:12:55.000
Đây là một số ví dụ về đầu ra từ mô hình.

00:12:55.000 --> 00:13:06.000
Nếu tôi đi qua một số văn bản tiếng Anh, ví dụ, "Một con đường xuyên qua một khu vườn đầy hoa màu hồng", mô hình sẽ dẫn chúng ta xuống một con đường vào một khu vườn đầy hoa màu hồng.

00:13:06.000 --> 00:13:17.000
Ngoài ra, nếu tôi dịch cùng một câu sang tiếng Pháp, tiếng Tây Ban Nha, tiếng Ý và tiếng Đức, mô hình sẽ tạo ra hình ảnh của những con đường và khu vườn đầy hoa màu hồng cho mỗi người.

00:13:17.000 --> 00:13:20.000
Hãy để tôi lấy một ví dụ phức tạp hơn một chút.

00:13:20.000 --> 00:13:31.000
"Một con đường trước cây cối và núi non dưới bầu trời nhiều mây." Đây là một số đầu ra từ mô hình, với đường, cây cối, núi và mây.

00:13:31.000 --> 00:13:45.000
Nhưng tương tự như vậy, tôi có thể dịch cùng một câu sang tiếng Pháp, tiếng Tây Ban Nha, tiếng Ý và tiếng Đức, hoặc bất kỳ ngôn ngữ nào khác, và đối với mỗi ngôn ngữ có được hình ảnh đường, cây cối, núi và mây.

00:13:45.000 --> 00:13:49.000
Bây giờ hãy để tôi tóm tắt các bài học từ phiên này.

00:13:49.000 --> 00:14:01.000
Bạn có thể sử dụng Create ML để dễ dàng đào tạo các mô hình cho các nhiệm vụ phân loại văn bản hoặc gắn thẻ từ và các mô hình nhúng BERT đa ngôn ngữ mới cung cấp một lớp mã hóa đầu vào mạnh mẽ cho mục đích này.

00:14:01.000 --> 00:14:05.000
Những mô hình này có thể là một ngôn ngữ hoặc đa ngôn ngữ.

00:14:05.000 --> 00:14:13.000
Bạn cũng có thể sử dụng nhúng BERT làm lớp đầu vào cho bất kỳ mô hình nào bạn muốn đào tạo với PyTorch hoặc TensorFlow.

00:14:13.000 --> 00:14:14.000
Cảm ơn bạn.

00:14:14.000 --> 00:14:17.000
Bây giờ hãy ra ngoài và bắt đầu đào tạo một số người mẫu.

00:14:17.000 --> 23:59:59.000
♪ ♪

