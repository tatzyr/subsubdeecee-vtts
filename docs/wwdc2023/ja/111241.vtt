WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:13.000
アンドリュー：こんにちは、ビジョンフレームワークのソフトウェアエンジニアのアンドリュー・ラウです。

00:00:13.000 --> 00:00:24.000
今日は、人体のポーズ、ビジョンフレームワークの深さの使用、インスタンスマスクで画像から人々を持ち上げることについて話します。

00:00:24.000 --> 00:00:32.000
人々の検出と理解は、常にビジョンの焦点であり、数年間、ビジョンフレームワークは2Dで人体ポーズを提供してきました。

00:00:32.000 --> 00:00:44.000
復習として、2Dの人体ポーズは、入力画像に対応するスケルトンに定義されたランドマークポイントの正規化されたピクセル座標で観察を返します。

00:00:44.000 --> 00:00:51.000
より詳細に飛び込みたい場合は、まだ見ていない場合は、「体と手のポーズを検出する」セッションを確認してください。

00:00:51.000 --> 00:01:00.000
ビジョンは、VNDetectHumanBodyPose3DRequestという名前の新しいリクエストで、環境内の人々を3Dにキャプチャするためのサポートを拡大しています。

00:01:00.000 --> 00:01:06.000
この要求は、17の関節を持つ3Dスケルトンを返す観測を生成します。

00:01:06.000 --> 00:01:12.000
ジョイントは、ジョイント名で、またはジョイントグループ名を提供することでコレクションとしてアクセスできます。

00:01:12.000 --> 00:01:25.000
左下の原点に正規化されたビジョンによって返される他の認識されたポイントとは異なり、3D関節の位置は、根関節の原点を持つ現実世界でキャプチャされたシーンに対してメートル単位で返されます。

00:01:25.000 --> 00:01:31.000
この最初のリビジョンは、フレーム内で検出された最も著名な人物の1つのスケルトンを返します。

00:01:31.000 --> 00:01:41.000
フィットネスアプリを構築していて、ジムでのワークアウトクラスのこの画像でリクエストを実行した場合、観察はカメラに最も近い正面の女性に対応します。

00:01:41.000 --> 00:01:48.000
いくつかの文脈で3Dスケルトンの構造をよりよく実証するために、このヨガのポーズを分解しましょう。

00:01:48.000 --> 00:01:56.000
当然のことながら、3D人体の骨格は、頭の中央と上部にポイントを含むヘッドグループから始まります。

00:01:56.000 --> 00:02:06.000
次に、左右の肩関節、背骨、股関節の中央にある根関節、股関節を含む胴体グループがあります。

00:02:06.000 --> 00:02:10.000
一部の関節は複数のグループで返されることを覚えておいてください。

00:02:10.000 --> 00:02:17.000
腕には、左右の腕のグループがあり、それぞれに手首、肩、肘があります。

00:02:17.000 --> 00:02:23.000
左右は常に人に関連しており、画像の左側や右側ではありません。

00:02:23.000 --> 00:02:33.000
最後に、私たちのスケルトンには左右の脚のグループが含まれており、それぞれに対応する股関節、膝、足首の関節があります。

00:02:33.000 --> 00:02:41.000
この新しいリクエストを使用するには、他のリクエストと同じワークフローに従うため、以前にコードでVisionを使用したことがある場合は、このフローに馴染みがあるはずです。

00:02:41.000 --> 00:02:53.000
まず、新しいDetectHumanBodyPose3DRequestのインスタンスを作成し、検出を実行するアセットで画像要求ハンドラを初期化します。

00:02:53.000 --> 00:02:57.000
リクエストを実行するには、リクエストインスタンスをperformに渡します。

00:02:57.000 --> 00:03:05.000
また、リクエストが成功すると、VNHumanBodyPose3DObservationがエラーなく返されます。

00:03:05.000 --> 00:03:09.000
すべての写真は、3D世界の人々の2D表現です。

00:03:09.000 --> 00:03:16.000
ビジョンにより、ARKitやARSessionなしで画像からその3D位置を取得できるようになりました。

00:03:16.000 --> 00:03:24.000
これは、3D空間で主題を理解するための強力で軽量なオプションであり、アプリのまったく新しい機能のロックを解除します。

00:03:24.000 --> 00:03:28.000
これを理解し、視覚化するのに役立つサンプルアプリを構築しました。

00:03:28.000 --> 00:03:34.000
開くと、フォトライブラリから任意の画像を選択できます。

00:03:34.000 --> 00:03:42.000
同僚と私は以前のヨガインストラクターの落ち着きに触発されたので、休憩を取り、外に出て、自分でいくつかのポーズを試しました。

00:03:42.000 --> 00:03:52.000
今、私はその先生ほど柔軟ではありませんが、私はこのポーズでかなり良い仕事をしました、そしてそれは3Dで素晴らしく見えるはずです。

00:03:52.000 --> 00:03:57.000
リクエストを実行して、私を3次元に戻しましょう。

00:03:57.000 --> 00:04:04.000
リクエストは成功し、3Dスケルトンは入力画像のどこにいるかと一致しています。

00:04:04.000 --> 00:04:13.000
シーンを回転させると、私の腕は伸び、足は私がどのように立っていたかに基づいて腰に比べて正しく見えます。

00:04:13.000 --> 00:04:20.000
このピラミッド形状は、画像がキャプチャされたときにカメラが配置された場所を表しています。

00:04:20.000 --> 00:04:25.000
パースペクティブの切り替えボタンをタップすると、カメラの位置からのビューになります。

00:04:25.000 --> 00:04:34.000
アプリで3D Human Body Poseを使用して素晴らしい体験を生み出すために必要なコードとコンセプトをご案内します。

00:04:34.000 --> 00:04:38.000
アプリの構築は、オブザベーションで返されたポイントを使用することから始まります。

00:04:38.000 --> 00:04:50.000
それらを取得するための2つの主要なAPIがあります。特定のジョイントの位置にアクセスするための認識されたポイントと、指定されたグループ名を持つジョイントのコレクションにアクセスするための認識されたポイントです。

00:04:50.000 --> 00:04:56.000
これらのコア方法に加えて、観察はいくつかの追加の有用な情報を提供します。

00:04:56.000 --> 00:05:01.000
まず、bodyHeightは被写体の推定身長をメートル単位で与えます。

00:05:01.000 --> 00:05:09.000
利用可能な深さのメタデータに応じて、この高さはより正確に測定された高さまたは1.8メートルの基準高さのいずれかになります。

00:05:09.000 --> 00:05:12.000
深さとビジョンについて1分で言いたいことがたくさんあります。

00:05:12.000 --> 00:05:18.000
heightEstimationプロパティを使用して、高さを計算するために使用されるテクニックを決定できます。

00:05:18.000 --> 00:05:23.000
次に、カメラの位置はcameraOriginMatrixで利用できます。

00:05:23.000 --> 00:05:33.000
実生活では、カメラは被写体に正確に直面していない可能性があるため、これは、フレームがキャプチャされたときにカメラが人に関連していた場所を理解するのに役立ちます。

00:05:33.000 --> 00:05:39.000
この観測では、共同座標を2Dに戻すためのAPIも提供しています。

00:05:39.000 --> 00:05:45.000
これは、返されたポイントを入力画像にオーバーレイまたは整列したい場合に役立ちます。

00:05:45.000 --> 00:05:56.000
そして最後に、人が2つの同様の画像を横切ってどのように移動したかを理解するために、カメラに対する特定の関節の位置を取得するためのAPIが利用可能です。

00:05:56.000 --> 00:06:04.000
3D人体ポイントの使い方を示す前に、それが継承するビジョンの新しい幾何学クラスを紹介したいと思います。

00:06:04.000 --> 00:06:11.000
VNPoint3Dは、3D位置を格納するためのsimd_float 4x4行列を定義する基本クラスです。

00:06:11.000 --> 00:06:20.000
この表現は、ARKitのような他のAppleフレームワークと一致しており、利用可能なすべてのローテーションと翻訳情報が含まれています。

00:06:20.000 --> 00:06:27.000
次に、この位置を継承するだけでなく、識別子も追加するVNRecognizedPoint3Dがあります。

00:06:27.000 --> 00:06:32.000
これは、共同名などの対応する情報を格納するために使用されます。

00:06:32.000 --> 00:06:40.000
最後に、今日の焦点は、ローカルポジションと親ジョイントを追加するVNHumanBodyRecognizedPoint3Dです。

00:06:40.000 --> 00:06:45.000
ポイントのプロパティを操作する方法について、もう少し詳しく見てみましょう。

00:06:45.000 --> 00:06:49.000
recognizedPoint APIを使用して、左手首の位置を取得しました。

00:06:49.000 --> 00:06:58.000
関節のモデル位置、またはポイントの位置特性は、常に股関節の中心にある骨格の根関節に相対的です。

00:06:58.000 --> 00:07:04.000
位置行列の3番目の列に焦点を合わせると、翻訳の値があります。

00:07:04.000 --> 00:07:12.000
左手首のyの値は、この図の腰から0.9メートル上にあり、このポーズに適しているようです。

00:07:12.000 --> 00:07:19.000
次に、返されたポイントのlocalPositionプロパティがあります。これは、親ジョイントに対する相対的な位置です。

00:07:19.000 --> 00:07:24.000
したがって、この場合、左肘は左手首の親関節になります。

00:07:24.000 --> 00:07:31.000
ここの最後の列は、x軸の-0.1メートルの値を示していますが、これは正しいようです。

00:07:31.000 --> 00:07:39.000
負の値または正の値は基準点によって決定され、このポーズでは、手首は肘の左側にあります。

00:07:39.000 --> 00:07:43.000
localPositionは、アプリが体の1つの領域でのみ動作している場合に便利です。

00:07:43.000 --> 00:07:48.000
また、子関節と親関節の間の角度の決定も簡素化されます。

00:07:48.000 --> 00:07:52.000
この角度をコードで計算する方法を1秒で示します。

00:07:52.000 --> 00:07:58.000
返された3Dポイントを扱う場合、アプリを構築する際に役立つ概念がいくつかあります。

00:07:58.000 --> 00:08:04.000
まず、多くの場合、子供と親の関節の間の角度を決定する必要があります。

00:08:04.000 --> 00:08:12.000
calculateLocalAngleToParentメソッドでは、親関節に対する相対的な位置を使用してその角度を見つけます。

00:08:12.000 --> 00:08:20.000
ノードの回転は、x、y、z軸、またはピッチ、ヨー、ロールに対する回転で構成されています。

00:08:20.000 --> 00:08:32.000
ピッチの場合、90度の回転を使用して、SceneKitノードジオメトリをデフォルトの向きからまっすぐ下向きにして、スケルトンに適したものを配置します。

00:08:32.000 --> 00:08:39.000
ヨーには、適切な角度を得るために、z座標のアークコサインをベクトルの長さで割った値を使用します。

00:08:39.000 --> 00:08:46.000
そして、ロールの場合、角度測定はy座標とx座標のアークタンジェントで得られます。

00:08:46.000 --> 00:08:53.000
次に、あなたのアプリは、私のサンプルアプリのように、返された3D位置を元の画像に関連付ける必要があるかもしれません。

00:08:53.000 --> 00:09:01.000
ビジュアライゼーションでは、ポイントインイメージAPIを使用して、イメージプレーン、スケールと翻訳の2つの変換に使用します。

00:09:01.000 --> 00:09:07.000
まず、画像平面を戻り値に比例して拡大縮小する必要があります。

00:09:07.000 --> 00:09:21.000
私は、3Dと2Dの両方で、中央の肩と背骨のような2つの既知の関節の間の距離を取得し、それらを比例して関連付け、この量で画像面を拡大します。

00:09:21.000 --> 00:09:31.000
翻訳コンポーネントでは、pointInImage APIを使用して、2D画像内のルートジョイントの位置を取得します。

00:09:31.000 --> 00:09:45.000
この方法は、その場所を使用して、x軸とy軸の画像平面のシフトを決定し、VNPoint座標の左下原点と画像の中央にあるレンダリング環境原点の間で変換します。

00:09:45.000 --> 00:09:54.000
最後に、カメラの視点からシーンを見たり、その場所でポイントをレンダリングしたりして、cameraOriginMatrixからこれを取得することができます。

00:09:54.000 --> 00:10:09.000
正しい向きはレンダリング環境によって異なりますが、これは、このノードのローカル座標系をシーンの残りの部分に関連付けるピボット変換を使用して、この変換情報を使用してノードを配置した方法です。

00:10:09.000 --> 00:10:21.000
また、cameraOriginMatrixの回転情報を使用して、逆変換を使用してこのコードでカメラに直面するように画像平面を正しく回転させました。

00:10:21.000 --> 00:10:28.000
ここではローテーション情報のみが必要なため、最後の列の翻訳情報は無視されます。

00:10:28.000 --> 00:10:34.000
これらすべてのピースをまとめると、私のサンプルアプリに表示されるシーンが可能になりました。

00:10:34.000 --> 00:10:41.000
さて、私は数分かけて、ビジョンの深さを含むいくつかのエキサイティングな追加について話し合いたいと思います。

00:10:41.000 --> 00:10:47.000
ビジョンフレームワークは、画像またはフレームバッファと一緒に入力として深度を受け入れるようになりました。

00:10:47.000 --> 00:10:57.000
VNImageRequestHandlerは、AVDepthDataの新しいパラメータを取るcvPixelBufferとcmSampleBufferの初期化APIを追加しました。

00:10:57.000 --> 00:11:04.000
さらに、ファイルにすでに深度データが含まれている場合は、変更せずに既存のAPIを使用できます。

00:11:04.000 --> 00:11:08.000
ビジョンは自動的にファイルから深度を取得します。

00:11:08.000 --> 00:11:16.000
Apple SDKでDepthを操作する場合、AVDepthDataはすべてのDepthメタデータとインタフェースするためのコンテナクラスとして機能します。

00:11:16.000 --> 00:11:24.000
カメラセンサーによってキャプチャされた深度メタデータには、格差または深度形式として表される深度マップが含まれています。

00:11:24.000 --> 00:11:29.000
これらのフォーマットは交換可能で、AVFoundationを使用して互いに変換できます。

00:11:29.000 --> 00:11:39.000
深度メタデータには、3Dシーンを再構築するために必要な本質的、外因性、レンズの歪みなどのカメラキャリブレーションデータも含まれています。

00:11:39.000 --> 00:11:47.000
詳細を知る必要がある場合は、2022年からの「iOSカメラキャプチャの進歩を発見する」セッションを確認してください。

00:11:47.000 --> 00:11:52.000
深さは、カメラキャプチャセッションまたは以前にキャプチャしたファイルから取得できます。

00:11:52.000 --> 00:12:01.000
カメラアプリでキャプチャされた画像は、写真のポートレート画像と同様に、常にカメラのキャリブレーションメタデータと格差マップとして深度を保存します。

00:12:01.000 --> 00:12:09.000
ライブキャプチャセッションで深度をキャプチャする場合、デバイスがLiDARをサポートしている場合は、LiDARを使用するセッションを指定するという利点があります。

00:12:09.000 --> 00:12:14.000
LiDARは、シーンの正確なスケールと測定を可能にするため、強力です。

00:12:14.000 --> 00:12:20.000
ビジョンはまた、画像内の複数の人と対話するためのAPIを導入しています。

00:12:20.000 --> 00:12:26.000
ビジョンは現在、GeneratePersonSegmentationリクエストで周囲のシーンから人々を分離する機能を提供しています。

00:12:26.000 --> 00:12:31.000
このリクエストは、フレーム内のすべての人を含む単一のマスクを返します。

00:12:31.000 --> 00:12:37.000
ビジョンにより、新しい人インスタンスマスクリクエストでもう少し選択できるようになりました。

00:12:37.000 --> 00:12:43.000
この新しいAPIは、最大4つの個人マスクを出力し、それぞれに信頼スコアがあります。

00:12:43.000 --> 00:12:48.000
だから今、あなたは画像とは別にあなたの友人を選択して持ち上げることができます。

00:12:48.000 --> 00:12:58.000
人以外の被写体を選択して持ち上げる必要がある場合は、VisionKitの被写体リフティングAPIまたはVisionフレームワークのフォアグラウンドインスタンスマスクリクエストを使用できます。

00:12:58.000 --> 00:13:04.000
詳細については、「アプリ内の画像から被写体を持ち上げる」セッションをチェックしてください。

00:13:04.000 --> 00:13:10.000
以下は、画像から必要な人の特定のインスタンスを選択する方法を示すサンプルコードです。

00:13:10.000 --> 00:13:23.000
現在、すべてのインスタンスを返すように指定されていますが、画像内のどの友人に焦点を合わせるかに応じてインスタンス1または2を選択するか、インスタンス0を使用して背景を取得できます。

00:13:23.000 --> 00:13:32.000
この新しいリクエストは最大4人までセグメント化されるため、画像に4人以上いる場合は、コードで処理する追加の条件がいくつかあります。

00:13:32.000 --> 00:13:38.000
シーンに多くの人々が含まれている場合、返された観察は人々を見逃したり、それらを組み合わせたりする可能性があります。

00:13:38.000 --> 00:13:43.000
通常、これはバックグラウンドにいる人々で発生します。

00:13:43.000 --> 00:13:50.000
アプリが混雑したシーンに対処しなければならない場合は、可能な限り最高の体験を構築するために使用できる戦略があります。

00:13:50.000 --> 00:14:04.000
ビジョンの顔検出APIを使用して、画像の顔の数を数えることができ、4人以上の画像をスキップするか、既存の人物セグメンテーションリクエストを使用して、全員に1つのマスクで作業するかを選択できます。

00:14:04.000 --> 00:14:15.000
要約すると、ビジョンは現在、深さ、3D人体ポーズ、および人のインスタンスマスクをサポートして、人々とその環境を理解するための強力な新しい方法を提供しています。

00:14:15.000 --> 00:14:17.000
しかし、ビジョンが今年リリースするのはそれだけではありません。

00:14:17.000 --> 00:14:25.000
「ビジョンで動物のポーズを検出する」セッションでは、人を超えて、毛むくじゃらの友人の画像で素晴らしい体験を生み出すことができます。

00:14:25.000 --> 00:14:31.000
ありがとう、そしてあなたがどんな素晴らしい機能を構築するかを見るのが待ちきれません。

00:14:31.000 --> 23:59:59.000
♪ ♪

