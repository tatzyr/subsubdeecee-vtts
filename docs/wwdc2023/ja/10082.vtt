WEBVTT

00:00:00.000 --> 00:00:02.000
♪まろやかなインストゥルメンタルヒップホップ♪

00:00:02.000 --> 00:00:10.000
♪

00:00:10.000 --> 00:00:12.000
ライアン・テイラー:こんにちは!私の名前はライアンです。

00:00:12.000 --> 00:00:13.000
コナー・ブルックス:そして、私はコナーです。

00:00:13.000 --> 00:00:18.000
ライアン：このセッションでは、空間コンピューティング用のARKitを紹介します。

00:00:18.000 --> 00:00:25.000
この新しいプラットフォームで果たす重要な役割と、それを活用して次世代のアプリを構築する方法について説明します。

00:00:25.000 --> 00:00:32.000
ARKitは、洗練されたコンピュータビジョンアルゴリズムを使用して、あなたの周りの世界とあなたの動きの理解を構築します。

00:00:32.000 --> 00:00:42.000
開発者が手のひらで使用できる素晴らしい拡張現実体験を作成する方法として、この技術をiOS 11に初めて導入しました。

00:00:42.000 --> 00:00:50.000
このプラットフォームでは、ARKitは本格的なシステムサービスに成熟し、新しいリアルタイム基盤でゼロから再構築されました。

00:00:50.000 --> 00:01:00.000
ARKitはオペレーティングシステム全体のファブリックに深く織り込まれており、ウィンドウとのやり取りから没入型ゲームプレイまで、あらゆるものに電力を供給しています。

00:01:00.000 --> 00:01:04.000
この旅の一環として、私たちはAPIを完全にオーバーホールしました。

00:01:04.000 --> 00:01:12.000
新しいデザインは、iOSで学んだすべてのことと、空間コンピューティングのユニークなニーズの結果であり、あなたはそれを気に入ると思います。

00:01:12.000 --> 00:01:21.000
ARKitは、仮想コンテンツをテーブルに置くなど、素晴らしいことをするために組み合わせることができるさまざまな強力な機能を提供します。

00:01:21.000 --> 00:01:27.000
まるで本当にそこにあるかのように、手を差し伸べてコンテンツに触れ、コンテンツが現実世界と相互作用するのを見ることができます。

00:01:27.000 --> 00:01:30.000
それは本当に魔法のような経験です。

00:01:30.000 --> 00:01:38.000
この新しいプラットフォームでARKitを使用して何が達成できるかを垣間見たので、私たちの議題をご案内しましょう。

00:01:38.000 --> 00:01:43.000
APIを構成する基本的な概念とビルディングブロックの概要から始めます。

00:01:43.000 --> 00:01:50.000
次に、現実世界に関連して仮想コンテンツを配置するために不可欠な世界追跡に飛び込みます。

00:01:50.000 --> 00:01:57.000
次に、周囲に関する有用な情報を提供するシーン理解機能を探ります。

00:01:57.000 --> 00:02:10.000
その後、最新の機能であるハンドトラッキングを紹介します。これは、手に対して仮想コンテンツを配置したり、他のタイプのオーダーメイドのインタラクションを構築したりするために活用できるエキサイティングな新しい追加です。

00:02:10.000 --> 00:02:20.000
そして最後に、私たちは一周し、少し前にお見せしたビデオのコードを調べることによって、これらの機能のいくつかの実用的なアプリケーションを見ていきます。

00:02:20.000 --> 00:02:22.000
よし、始めよう！

00:02:22.000 --> 00:02:31.000
私たちの新しいAPIは、モダンスウィフトとクラシックCの2つの爽快なフレーバーで細心の注意を払って作られています。

00:02:31.000 --> 00:02:35.000
すべてのARKit機能は現在、アラカルトで提供されています。

00:02:35.000 --> 00:02:43.000
私たちは、開発者が可能な限り柔軟性を持ち、エクスペリエンスを構築するために必要なものを簡単に選択できるようにしたかったのです。

00:02:43.000 --> 00:02:47.000
ARKitデータへのアクセスは、プライバシー第一のアプローチで設計されています。

00:02:47.000 --> 00:02:54.000
私たちは、開発者のシンプルさを維持しながら、人々のプライバシーを保護するためのセーフガードを導入しています。

00:02:54.000 --> 00:03:03.000
APIは、セッション、データプロバイダー、アンカーの3つの基本的な構成要素で構成されています。

00:03:03.000 --> 00:03:08.000
アンカーから始めて、セッションに戻りましょう。

00:03:08.000 --> 00:03:12.000
アンカーは、現実世界での位置と向きを表します。

00:03:12.000 --> 00:03:17.000
すべてのアンカーには、一意の識別子と変換が含まれています。

00:03:17.000 --> 00:03:20.000
一部のタイプのアンカーも追跡可能です。

00:03:20.000 --> 00:03:28.000
追跡可能なアンカーが追跡されていない場合は、アンカーした仮想コンテンツを非表示にする必要があります。

00:03:28.000 --> 00:03:32.000
データプロバイダーは、個々のARKit機能を表します。

00:03:32.000 --> 00:03:37.000
データプロバイダーを使用すると、アンカーの変更などのデータの更新をポーリングまたは観察できます。

00:03:37.000 --> 00:03:42.000
さまざまな種類のデータプロバイダーは、さまざまな種類のデータを提供します。

00:03:42.000 --> 00:03:48.000
セッションは、特定の体験のために一緒に使用したいARKit機能の組み合わせを表します。

00:03:48.000 --> 00:03:53.000
一連のデータプロバイダーを提供することで、セッションを実行します。

00:03:53.000 --> 00:03:58.000
セッションが実行されると、データプロバイダーはデータの受信を開始します。

00:03:58.000 --> 00:04:03.000
更新は、データの種類に応じて、非同期かつ異なる周波数で到着します。

00:04:03.000 --> 00:04:08.000
では、プライバシーとアプリがARKitデータにアクセスする方法について話しましょう。

00:04:08.000 --> 00:04:11.000
プライバシーは基本的人権です。

00:04:11.000 --> 00:04:14.000
それはまた、私たちのコアバリューの1つです。

00:04:14.000 --> 00:04:18.000
ARKitのアーキテクチャとAPIは、人々のプライバシーを保護するために思慮深く設計されています。

00:04:18.000 --> 00:04:25.000
ARKitがあなたの周りの世界の理解を構築するために、このデバイスには多くのカメラやその他のタイプのセンサーがあります。

00:04:25.000 --> 00:04:30.000
カメラフレームなどのこれらのセンサーからのデータは、クライアントスペースに送信されることはありません。

00:04:30.000 --> 00:04:36.000
代わりに、センサーデータは、アルゴリズムによる安全な処理のためにARKitのデーモンに送信されます。

00:04:36.000 --> 00:04:45.000
これらのアルゴリズムによって生成される結果のデータは、アプリなどのデータを要求しているクライアントに転送される前に慎重にキュレーションされます。

00:04:45.000 --> 00:04:49.000
ARKitデータにアクセスするための前提条件がいくつかあります。

00:04:49.000 --> 00:04:52.000
まず、アプリはフルスペースを入力する必要があります。

00:04:52.000 --> 00:04:57.000
ARKitは、共有スペースにあるアプリにデータを送信しません。

00:04:57.000 --> 00:05:01.000
第二に、一部のタイプのARKitデータにはアクセス許可が必要です。

00:05:01.000 --> 00:05:07.000
その人が許可を与えない場合、私たちはその種類のデータをあなたのアプリに送信しません。

00:05:07.000 --> 00:05:15.000
これを容易にするために、ARKitは許可を処理するための便利な承認APIを提供します。

00:05:15.000 --> 00:05:21.000
セッションを使用して、アクセスしたいデータの種類の承認をリクエストできます。

00:05:21.000 --> 00:05:29.000
これを行わないと、必要に応じて、セッションを実行するときにARKitが自動的にその人に許可を求めます。

00:05:29.000 --> 00:05:32.000
ここでは、ハンドトラッキングデータへのアクセスを要求しています。

00:05:32.000 --> 00:05:39.000
必要なすべての承認タイプを1つのリクエストに一括処理できます。

00:05:39.000 --> 00:05:46.000
承認結果が得られたら、それらを反復し、各承認タイプのステータスを確認します。

00:05:46.000 --> 00:05:51.000
その人が許可を与えた場合、ステータスは許可されます。

00:05:51.000 --> 00:05:59.000
その人がアクセスを拒否したデータを提供するデータプロバイダーとセッションを実行しようとすると、セッションが失敗します。

00:05:59.000 --> 00:06:06.000
では、ワールドトラッキングから始めて、ARKitがこのプラットフォームでサポートしている各機能を詳しく見てみましょう。

00:06:06.000 --> 00:06:10.000
ワールドトラッキングを使用すると、現実世界で仮想コンテンツを固定できます。

00:06:10.000 --> 00:06:19.000
ARKitは、デバイスの動きを6つの自由度で追跡し、各アンカーを更新して、周囲に対して同じ場所にとどまるようにします。

00:06:19.000 --> 00:06:26.000
ワールドトラッキングが使用するDataProviderのタイプはWorldTrackingProviderと呼ばれ、いくつかの重要な機能を提供します。

00:06:26.000 --> 00:06:34.000
これにより、WorldAnchorsを追加できます。これにより、デバイスが移動するにつれて、ARKitが更新され、人々の周囲に対して固定されたままになります。

00:06:34.000 --> 00:06:38.000
WorldAnchorsは、仮想コンテンツの配置に不可欠なツールです。

00:06:38.000 --> 00:06:43.000
追加したWorldAnchorsは、アプリの起動と再起動で自動的に保持されます。

00:06:43.000 --> 00:06:52.000
この動作があなたが構築している経験にとって望ましくない場合は、あなたがそれらを終えたときに単にアンカーを取り外すことができ、それらはもはや持続しません。

00:06:52.000 --> 00:06:57.000
永続性が利用できない場合があることに注意することが重要です。

00:06:57.000 --> 00:07:06.000
また、WorldTrackingProviderを使用して、アプリのオリジンに対するデバイスのポーズを取得することもできます。これは、Metalを使用して独自のレンダリングを行う場合に必要です。

00:07:06.000 --> 00:07:11.000
まず、WorldAnchorとは何か、なぜ使用したいのかを詳しく見てみましょう。

00:07:11.000 --> 00:07:22.000
WorldAnchorは、アプリの原点に対してアンカーを配置したい位置と向きである変換を取る初期化子を備えたTrackableAnchorです。

00:07:22.000 --> 00:07:29.000
アンカーされていない仮想コンテンツとアンカーされているコンテンツの違いを視覚化するのに役立つ例を用意しました。

00:07:29.000 --> 00:07:32.000
ここには2つのキューブがあります。

00:07:32.000 --> 00:07:40.000
左側の青い立方体はWorldAnchorによって更新されていませんが、右側の赤い立方体はWorldAnchorによって更新されています。

00:07:40.000 --> 00:07:45.000
両方のキューブは、アプリが起動されたときにアプリの原点を基準にして配置されました。

00:07:45.000 --> 00:07:50.000
デバイスが動き回ると、両方のキューブは配置された場所に残ります。

00:07:50.000 --> 00:07:54.000
クラウンを長押しして、アプリを更新できます。

00:07:54.000 --> 00:07:59.000
最近の更新が発生すると、アプリのオリジンが現在の場所に移動されます。

00:07:59.000 --> 00:08:11.000
固定されていない青い立方体は、アプリの原点との相対的な配置を維持するために再配置されることに注意してください。一方、固定されている赤い立方体は、現実世界に対して固定されたままです。

00:08:11.000 --> 00:08:14.000
WorldAnchorの永続性がどのように機能するかを見てみましょう。

00:08:14.000 --> 00:08:19.000
デバイスが動き回ると、ARKitは周囲の地図を作成します。

00:08:19.000 --> 00:08:25.000
WorldAnchorsを追加すると、マップに挿入し、自動的に保持します。

00:08:25.000 --> 00:08:29.000
WorldAnchorの識別子と変換のみが保持されます。

00:08:29.000 --> 00:08:33.000
仮想コンテンツなどの他のデータは含まれていません。

00:08:33.000 --> 00:08:41.000
WorldAnchor識別子を関連付ける仮想コンテンツへのマッピングを維持するのはあなた次第です。

00:08:41.000 --> 00:08:53.000
マップは場所に基づいているため、デバイスを新しい場所（たとえば、自宅からオフィスまで）に持ち込むと、自宅の地図がアンロードされ、別のマップがオフィス用にローカライズされます。

00:08:53.000 --> 00:08:59.000
この新しい場所で追加したアンカーは、そのマップに入ります。

00:08:59.000 --> 00:09:09.000
一日の終わりにオフィスを出て家に帰ると、ARKitがオフィスで構築していた地図と、そこに置いたアンカーがアンロードされます。

00:09:09.000 --> 00:09:14.000
しかし、もう一度、私たちはあなたのアンカーと一緒に自動的にマップを永続化しています。

00:09:14.000 --> 00:09:26.000
帰国後、ARKitは場所が変更されたことを認識し、この場所の既存の地図をチェックして再ローカライズのプロセスを開始します。

00:09:26.000 --> 00:09:35.000
見つけたら、それをローカライズし、以前に自宅で追加したすべてのアンカーが再び追跡されます。

00:09:35.000 --> 00:09:38.000
デバイスのポーズに移りましょう。

00:09:38.000 --> 00:09:45.000
WorldAnchorsの追加と削除に加えて、WorldTrackingProviderを使用してデバイスのポーズを取得することもできます。

00:09:45.000 --> 00:09:50.000
ポーズは、アプリの原点に対するデバイスの位置と向きです。

00:09:50.000 --> 00:09:58.000
完全に没入型体験でMetalとCompositorServicesで独自のレンダリングを行う場合は、ポーズのクエリが必要です。

00:09:58.000 --> 00:10:00.000
このクエリは比較的高価です。

00:10:00.000 --> 00:10:07.000
コンテンツの配置など、他のタイプのアプリロジックに対してデバイスのポーズを照会するときは注意してください。

00:10:07.000 --> 00:10:16.000
ARKitからCompositorServicesにデバイスのポーズを提供する方法を示すために、簡略化されたレンダリング例を簡単に見てみましょう。

00:10:16.000 --> 00:10:23.000
セッション、ワールドトラッキングプロバイダー、最新のポーズを開催するレンダラー構造体があります。

00:10:23.000 --> 00:10:28.000
レンダラーの初期化時には、セッションを作成することから始めます。

00:10:28.000 --> 00:10:36.000
次に、世界追跡プロバイダーを作成し、各フレームをレンダリングするときにデバイスのポーズを照会するために使用します。

00:10:36.000 --> 00:10:41.000
これで、必要なデータプロバイダーとのセッションを実行できます。

00:10:41.000 --> 00:10:45.000
この場合、ワールドトラッキングプロバイダーのみを使用しています。

00:10:45.000 --> 00:10:49.000
また、レンダリング関数での割り当てを避けるためにポーズを作成します。

00:10:49.000 --> 00:10:55.000
フレームレートで呼び出すレンダリング関数にジャンプしてみましょう。

00:10:55.000 --> 00:11:00.000
CompositorServicesのdrawableを使用して、ターゲットレンダリング時間を取得します。

00:11:00.000 --> 00:11:06.000
次に、ターゲットレンダリング時間を使用して、デバイスのポーズを照会します。

00:11:06.000 --> 00:11:11.000
成功すれば、アプリの原点に対するポーズの変換を抽出できます。

00:11:11.000 --> 00:11:15.000
これは、コンテンツのレンダリングに使用する変換です。

00:11:15.000 --> 00:11:25.000
最後に、合成用のフレームを提出する前に、ドローアブルにポーズを設定して、コンポジターがフレームのコンテンツをレンダリングするために使用したポーズを知るようにします。

00:11:25.000 --> 00:11:32.000
独自のレンダリングの詳細については、Metalを使用して没入型アプリを作成するための専用セッションを参照してください。

00:11:32.000 --> 00:11:39.000
さらに、空間コンピューティングのパフォーマンスに関する考慮事項に関する素晴らしいセッションがあり、チェックすることをお勧めします。

00:11:39.000 --> 00:11:43.000
次に、シーンの理解を見てみましょう。

00:11:43.000 --> 00:11:48.000
シーンの理解は、さまざまな方法で周囲について知らせる機能のカテゴリです。

00:11:48.000 --> 00:11:51.000
平面検出から始めましょう。

00:11:51.000 --> 00:11:58.000
平面検出は、ARKitが現実世界で検出する水平および垂直表面のアンカーを提供します。

00:11:58.000 --> 00:12:03.000
平面検出が使用するDataProviderのタイプは、PlaneDetectionProviderと呼ばれます。

00:12:03.000 --> 00:12:09.000
飛行機が周囲で検出されると、PlaneAnchorsの形で提供されます。

00:12:09.000 --> 00:12:16.000
PlaneAnchorsは、テーブルの上に仮想オブジェクトを配置するなど、コンテンツの配置を容易にするために使用できます。

00:12:16.000 --> 00:12:24.000
さらに、床や壁などの基本的な平坦なジオメトリで十分な物理シミュレーションに平面を使用できます。

00:12:24.000 --> 00:12:35.000
各PlaneAnchorには、水平または垂直のアライメント、平面のジオメトリ、および意味的分類が含まれます。

00:12:35.000 --> 00:12:40.000
平面は、床やテーブルなど、さまざまな種類の表面に分類できます。

00:12:40.000 --> 00:12:51.000
特定の表面を特定できない場合、提供された分類は、状況に応じて、不明、未定、または利用できないとマークされます。

00:12:51.000 --> 00:12:55.000
では、シーンジオメトリに移りましょう。

00:12:55.000 --> 00:13:02.000
シーンジオメトリは、現実世界の形状を推定する多角形メッシュを含むアンカーを提供します。

00:13:02.000 --> 00:13:08.000
シーンジオメトリが使用するDataProviderのタイプは、SceneReconstructionProviderと呼ばれます。

00:13:08.000 --> 00:13:18.000
ARKitがあなたの周りの世界をスキャンすると、私たちはあなたの周囲を細分化されたメッシュとして再構築し、MeshAnchorsの形で提供されます。

00:13:18.000 --> 00:13:23.000
PlaneAnchorsと同様に、MeshAnchorsはコンテンツの配置を容易にするために使用できます。

00:13:23.000 --> 00:13:33.000
また、シンプルで平らな表面だけでなく、オブジェクトと対話するために仮想コンテンツが必要な場合に、より忠実度の高い物理シミュレーションを実現することもできます。

00:13:33.000 --> 00:13:37.000
各メッシュアンカーには、メッシュのジオメトリが含まれています。

00:13:37.000 --> 00:13:47.000
このジオメトリには、面ごとの頂点、法線、面、およびセマンティック分類が含まれています。

00:13:47.000 --> 00:13:51.000
メッシュ面は、さまざまな種類のオブジェクトに分類できます。

00:13:51.000 --> 00:13:58.000
特定のオブジェクトを識別できない場合、提供された分類はなしになります。

00:13:58.000 --> 00:14:02.000
最後に、画像追跡を見てみましょう。 

00:14:02.000 --> 00:14:07.000
画像追跡を使用すると、現実世界で2D画像を検出できます。

00:14:07.000 --> 00:14:13.000
画像追跡が使用するDataProviderのタイプは、ImageTrackingProviderと呼ばれます。

00:14:13.000 --> 00:14:18.000
検出したいReferenceImagesのセットでImageTrackingProviderを設定します。

00:14:18.000 --> 00:14:22.000
これらのReferenceImagesは、いくつかの異なる方法で作成できます。

00:14:22.000 --> 00:14:28.000
1つのオプションは、プロジェクトのアセットカタログのARリソースグループからそれらをロードすることです。

00:14:28.000 --> 00:14:36.000
または、CVPixelBufferまたはCGImageを提供することで、ReferenceImageを自分で初期化することもできます。

00:14:36.000 --> 00:14:41.000
画像が検出されると、ARKitはImageAnchorを提供します。

00:14:41.000 --> 00:14:46.000
ImageAnchorsは、既知の静的に配置された画像にコンテンツを配置するために使用できます。

00:14:46.000 --> 00:14:52.000
たとえば、映画のポスターの横に映画に関する情報を表示できます。

00:14:52.000 --> 00:15:05.000
ImageAnchorsは、検出された画像のサイズが指定した物理サイズとアンカーが対応するReferenceImageとどのように比較されるかを示す推定スケールファクターを含むTrackableAnchorsです。

00:15:05.000 --> 00:15:12.000
さて、私たちの新機能、ハンドトラッキングについてお話しし、例を順を追って説明します、これがコナーです。

00:15:12.000 --> 00:15:16.000
コナー:こんにちは。ARKitに新しく追加されたハンドトラッキングを見てみましょう。

00:15:16.000 --> 00:15:21.000
ハンドトラッキングは、各手の骨格データを含むアンカーを提供します。

00:15:21.000 --> 00:15:26.000
ハンドトラッキングが使用するDataProviderのタイプは、HandTrackingProviderと呼ばれます。

00:15:26.000 --> 00:15:30.000
あなたの手が検出されると、ハンドアンカーの形で提供されます。 ハンドアンカーの形で提供されます。

00:15:30.000 --> 00:15:33.000
ハンドアンカーは追跡可能なアンカーです。

00:15:33.000 --> 00:15:37.000
ハンドアンカーには、骨格とキラリティが含まれています。

00:15:37.000 --> 00:15:41.000
キラリティは、これが左手か右手かを教えてくれます。

00:15:41.000 --> 00:15:47.000
HandAnchorの変換は、アプリのオリジンに対する手首の変換です。

00:15:47.000 --> 00:15:51.000
スケルトンは関節で構成されており、名前で照会できます。

00:15:51.000 --> 00:16:07.000
関節には、親関節、その名前、親関節に相対するlocalTransform、根関節に相対するrootTransform、そして最後に、各関節には、この関節が追跡されているかどうかを示すブールが含まれています。

00:16:07.000 --> 00:16:11.000
ここでは、手の骨格で利用可能なすべての関節を列挙します。

00:16:11.000 --> 00:16:14.000
ジョイントの階層のサブセットを見てみましょう。

00:16:14.000 --> 00:16:17.000
手首は手の根関節です。

00:16:17.000 --> 00:16:25.000
各指について、最初の関節は手首に親が付けられています。例えば、1は0に親が付けられます。

00:16:25.000 --> 00:16:33.000
その後の指の関節は、前の関節に育てられます。例えば、2は1に育てられます。

00:16:33.000 --> 00:16:38.000
HandAnchorsは、あなたの手に関連してコンテンツを配置したり、カスタムジェスチャーを検出したりするために使用できます。

00:16:38.000 --> 00:16:46.000
HandAnchorsを受け取るには2つのオプションがあります。更新をポーリングするか、利用可能なときにアンカーを非同期に受信することができます。

00:16:46.000 --> 00:16:54.000
後でSwiftの例で非同期更新を見ていきますので、先ほどのレンダラーにハンドアンカーポーリングを追加しましょう。

00:16:54.000 --> 00:16:56.000
これが更新された構造体の定義です。

00:16:56.000 --> 00:17:01.000
左右のアンカーとともに、ハンドトラッキングプロバイダーを追加しました。

00:17:01.000 --> 00:17:12.000
更新されたinit関数では、新しいハンドトラッキングプロバイダーを作成し、実行するプロバイダーのリストに追加します。次に、ポーリング時に必要な左右のアンカーを作成します。

00:17:12.000 --> 00:17:17.000
レンダーループでの割り当てを避けるために、これらを事前に作成することに注意してください。

00:17:17.000 --> 00:17:23.000
構造体を更新して初期化すると、レンダリング関数でget_latest_anchorsを呼び出すことができます。

00:17:23.000 --> 00:17:27.000
プロバイダーと事前に割り当てられたハンドアンカーを渡します。

00:17:27.000 --> 00:17:32.000
私たちのアンカーには、利用可能な最新のデータが入力されます。

00:17:32.000 --> 00:17:36.000
最新のアンカーが入力されたので、私たちの経験で彼らのデータを使用できるようになりました。

00:17:36.000 --> 00:17:38.000
とてもかっこいい。

00:17:38.000 --> 00:17:41.000
では、先ほどお見せした例を再検討する時間です。

00:17:41.000 --> 00:17:45.000
ARKitとRealityKitの機能を組み合わせて、この体験を構築しました。

00:17:45.000 --> 00:17:52.000
シーンジオメトリは物理学やジェスチャーのコライダーとして使用され、ハンドトラッキングはキューブエンティティと直接対話するために使用されました。

00:17:52.000 --> 00:17:55.000
この例をどのように構築したかを見てみましょう。

00:17:55.000 --> 00:17:59.000
まず、アプリの構造とビューモデルを確認します。

00:17:59.000 --> 00:18:02.000
次に、ARKitセッションを初期化します。

00:18:02.000 --> 00:18:07.000
次に、指先用のコライダーを追加し、シーンの再構築からコライダーを追加します。

00:18:07.000 --> 00:18:10.000
最後に、ジェスチャーでキューブを追加する方法を見ていきます。

00:18:10.000 --> 00:18:13.000
すぐに飛び込みましょう。

00:18:13.000 --> 00:18:17.000
これが私たちのアプリ、TimeForCubeです。

00:18:17.000 --> 00:18:21.000
比較的標準的なSwiftUIアプリとシーン設定があります。

00:18:21.000 --> 00:18:24.000
私たちのシーンの中で、私たちはImmersiveSpaceを宣言します。

00:18:24.000 --> 00:18:29.000
ARKitデータにアクセスするには、フルスペースに移動する必要があるため、IimmersiveSpaceが必要です。

00:18:29.000 --> 00:18:35.000
ImmersiveSpace内では、ビューモデルのコンテンツを表示するRealityViewを定義します。

00:18:35.000 --> 00:18:38.000
ビューモデルは、私たちのアプリのロジックのほとんどが住む場所です。

00:18:38.000 --> 00:18:41.000
ざっと見てみましょう。

00:18:41.000 --> 00:18:55.000
ビューモデルは、ARKitセッション、使用するデータプロバイダー、作成する他のすべてのエンティティを含むコンテンツエンティティ、およびシーンマップとハンドコライダーマップの両方を保持します。

00:18:55.000 --> 00:18:59.000
ビューモデルは、アプリから呼び出すさまざまな機能も提供します。

00:18:59.000 --> 00:19:02.000
アプリからこれらのそれぞれをコンテキストで確認します。

00:19:02.000 --> 00:19:09.000
最初に呼び出す関数は、contentEntityを設定するためのRealityViewのmakeクロージャ内です。

00:19:09.000 --> 00:19:17.000
ビューモデルがビューのコンテンツにエンティティを追加できるように、このエンティティをRealityViewのコンテンツに追加します。

00:19:17.000 --> 00:19:24.000
setupContentEntityは、マップ内のすべてのフィンガーエンティティをcontentEntityの子として追加し、それを返します。

00:19:24.000 --> 00:19:25.000
いいね！

00:19:25.000 --> 00:19:28.000
セッションの初期化に移りましょう。

00:19:28.000 --> 00:19:31.000
セッションの初期化は、3つのタスクのいずれかで実行されます。

00:19:31.000 --> 00:19:34.000
最初のタスクはrunSession関数を呼び出します。

00:19:34.000 --> 00:19:39.000
この機能は、2つのプロバイダーとのセッションを実行するだけです。

00:19:39.000 --> 00:19:42.000
セッションを実行すると、アンカーの更新の受信を開始できます。

00:19:42.000 --> 00:19:47.000
キューブと対話するために使用する指先コライダーを作成して更新しましょう。

00:19:47.000 --> 00:19:51.000
これは、手の更新を処理するためのタスクです。

00:19:51.000 --> 00:19:56.000
その機能は、プロバイダーのアンカー更新の非同期シーケンスを反復します。

00:19:56.000 --> 00:20:06.000
ハンドアンカーが追跡されていることを確認し、人差し指の関節を取得し、関節自体も追跡されていることを確認します。

00:20:06.000 --> 00:20:12.000
次に、アプリのオリジンに対する人差し指の先端の変換を計算します。

00:20:12.000 --> 00:20:19.000
最後に、どのフィンガーエンティティを更新するかを調べて、その変換を設定します。

00:20:19.000 --> 00:20:21.000
フィンガーエンティティマップを再検討しましょう。

00:20:21.000 --> 00:20:31.000
ModelEntityへの拡張を介してハンドごとにエンティティを作成します。この拡張は、衝突形状の5mmの球体を作成します。

00:20:31.000 --> 00:20:37.000
運動物理学のボディコンポーネントを追加し、不透明度コンポーネントを追加してこのエンティティを非表示にします。

00:20:37.000 --> 00:20:43.000
ユースケースのためにこれらを非表示にしますが、すべてが期待どおりに機能していることを確認するために、指先エンティティを視覚化すると良いでしょう。

00:20:43.000 --> 00:20:48.000
一時的に不透明度を1に設定し、エンティティが正しい場所にあることを確認しましょう。

00:20:48.000 --> 00:20:49.000
すごい！

00:20:49.000 --> 00:20:51.000
指先があるところに球体が見えます!

00:20:51.000 --> 00:20:54.000
注意してください、私たちの手は球体を部分的に覆っています。

00:20:54.000 --> 00:21:01.000
これはハンドオクルージョンと呼ばれ、人が仮想コンテンツの上に自分の手を見ることができるシステム機能です。

00:21:01.000 --> 00:21:11.000
これはデフォルトで有効になっていますが、球体をもう少し明確に見たい場合は、シーンでupperLimbVisibilityセッターを使用してハンドオクルージョンの可視性を設定できます。

00:21:11.000 --> 00:21:18.000
手足の視認性を非表示に設定すると、手がどこにあるかに関係なく、球体全体を見ることができます。

00:21:18.000 --> 00:21:23.000
この例では、上肢の可視性をデフォルト値として残し、不透明度をゼロに戻します。

00:21:23.000 --> 00:21:29.000
きちんとした！では、シーンコライダーを追加しましょう。これらを物理学やジェスチャーターゲットとして使用します。

00:21:29.000 --> 00:21:32.000
これが私たちのモデルの関数を呼び出すタスクです。

00:21:32.000 --> 00:21:44.000
プロバイダーのアンカー更新の非同期シーケンスを反復し、MeshAnchorからShapeResourceを生成してから、アンカー更新のイベントをオンにします。

00:21:44.000 --> 00:21:56.000
アンカーを追加する場合は、新しいエンティティを作成し、その変換を設定し、衝突と物理ボディコンポーネントを追加し、入力ターゲットコンポーネントを追加して、このコライダーがジェスチャーのターゲットになるようにします。

00:21:56.000 --> 00:22:02.000
最後に、新しいエンティティをマップに追加し、コンテンツエンティティの子として追加します。

00:22:02.000 --> 00:22:09.000
エンティティを更新するには、マップから取得し、その変換と衝突コンポーネントの形状を更新します。

00:22:09.000 --> 00:22:15.000
削除するには、対応するエンティティを親とマップから削除します。

00:22:15.000 --> 00:22:19.000
ハンドコライダーとシーンコライダーができ、ジェスチャーを使ってキューブを追加できます。

00:22:19.000 --> 00:22:27.000
任意のエンティティをターゲットにしたSpatialTapGestureを追加します。これにより、誰かがRealityViewのコンテンツ内のエンティティをタップしたかどうかを知らせます。

00:22:27.000 --> 00:22:33.000
そのタップが終了すると、グローバル座標からシーン座標に変換する3Dロケーションを受け取ります。

00:22:33.000 --> 00:22:35.000
この場所を視覚化しましょう。

00:22:35.000 --> 00:22:40.000
タップの位置に球体を追加した場合、次のことがわかります。

00:22:40.000 --> 00:22:44.000
次に、ビューモデルに、この場所に対して相対的に立方体を追加するように指示します。

00:22:44.000 --> 00:22:51.000
キューブを追加するには、まずタップ位置から20センチ上の配置位置を計算します。

00:22:51.000 --> 00:22:55.000
次に、キューブを作成し、その位置を計算された配置位置に設定します。

00:22:55.000 --> 00:23:01.000
InputTargetComponentを追加します。これにより、エンティティが応答するジェスチャーの種類を設定できます。

00:23:01.000 --> 00:23:09.000
私たちのユースケースでは、フィンガーチップコライダーが直接的な相互作用を提供するため、これらのキューブの間接的な入力タイプのみを許可します。

00:23:09.000 --> 00:23:14.000
物理学の相互作用を少し良くするために、カスタムパラメータを持つPhysicsBodyComponentを追加します。

00:23:14.000 --> 00:23:20.000
最後に、キューブをコンテンツエンティティに追加します。つまり、ついにキューブの時間です。

00:23:20.000 --> 00:23:24.000
最後に、私たちの例を端から端まで見てみましょう。

00:23:24.000 --> 00:23:29.000
シーンコライダーやキューブをタップするたびに、タップ位置の上に新しいキューブが追加されます。

00:23:29.000 --> 00:23:36.000
物理システムは、キューブをシーンコライダーに落下させ、ハンドコライダーはキューブと対話することを可能にします。

00:23:36.000 --> 00:23:42.000
RealityKitの詳細については、空間コンピューティングにRealityKitを使用することに関する入門セッションをご覧ください。

00:23:42.000 --> 00:23:51.000
また、このプラットフォームに持ち込むことに興味があるiOSの既存のARKitエクスペリエンスがすでにある場合は、このトピックに関する専用セッションを必ずご覧ください。

00:23:51.000 --> 00:23:56.000
私たちのチーム全体が、あなたがARKitの新しいバージョンを手に入れることに非常に興奮しています。

00:23:56.000 --> 00:24:01.000
このエキサイティングな新しいプラットフォームのために作成する画期的なアプリをすべて見るのが待ちきれません。

00:24:01.000 --> 00:24:03.000
ライアン：見てくれてありがとう！

00:24:03.000 --> 23:59:59.000
♪

