WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:15.000
ダグ:皆さん、ようこそ。私はダグ・デビッドソンです。自然言語処理についてお話しします。

00:00:15.000 --> 00:00:19.000
現在、過去数年間、自然言語処理に関する多くのセッションがありました。

00:00:19.000 --> 00:00:25.000
今日私たちが話すことは、いくつかのエキサイティングな新機能を追加して、そのすべてに基づいています。

00:00:25.000 --> 00:00:31.000
まず、NLPとNLPモデルの背景をいくつかお伝えします。

00:00:31.000 --> 00:00:34.000
次に、既存の機能をまとめます。

00:00:34.000 --> 00:00:37.000
では、今年の新機能について話します。

00:00:37.000 --> 00:00:40.000
高度なアプリケーションについて話し合います。

00:00:40.000 --> 00:00:43.000
そして、私はそれを包みます。

00:00:43.000 --> 00:00:45.000
いくつかの背景から始めましょう。

00:00:45.000 --> 00:00:50.000
模式的には、NLPモデルは一般的に同様の流れを持っています。

00:00:50.000 --> 00:01:03.000
彼らはテキストデータから始まり、それを数値特徴表現に変換する入力レイヤーを持ち、その上で機械学習モデルが行動し、いくつかの出力を生成します。

00:01:03.000 --> 00:01:12.000
過去数年間の最も明白な例は、テキストの分類と単語のタグ付けでサポートされているCreate MLモデルです。

00:01:12.000 --> 00:01:22.000
フィールドとしてのNLPの開発は、入力層のますます洗練されたバージョンの開発だけで、かなり密接に追跡できます。

00:01:22.000 --> 00:01:26.000
10年か20年前、これらは単純な正書法の特徴でした。

00:01:26.000 --> 00:01:34.000
それから約10年前、Word2VecやGloVeなどの静的な単語埋め込みの使用に移行しました。

00:01:34.000 --> 00:01:42.000
次に、CNNやLSTMなどのニューラルネットワークモデルに基づくコンテキストワード埋め込み。

00:01:42.000 --> 00:01:46.000
そして最近では、トランスフォーマーベースの言語モデル。

00:01:46.000 --> 00:01:49.000
埋め込みとは何かについて一言言うべきです。

00:01:49.000 --> 00:02:03.000
最も単純な形式では、言語内の単語から抽象的なベクトル空間のベクトルへの単なるマップですが、同様の意味を持つ単語がベクトル空間で互いに近いように機械学習モデルとして訓練されています。

00:02:03.000 --> 00:02:07.000
これにより、言語知識を組み込むことができます。

00:02:07.000 --> 00:02:11.000
静的埋め込みは、単語からベクトルへの単純なマップにすぎません。

00:02:11.000 --> 00:02:15.000
単語を渡すと、モデルはそれをテーブルでルックアップし、ベクトルを提供します。

00:02:15.000 --> 00:02:20.000
これらは、似たような意味を持つ単語がベクトル空間で互いに近いように訓練されています。

00:02:20.000 --> 00:02:24.000
これは個々の単語を理解するのに非常に役立ちます。

00:02:24.000 --> 00:02:33.000
より洗練された埋め込みは、文内の各単語が文での使用に応じて異なるベクトルにマッピングされるように、動的で文脈的です。

00:02:33.000 --> 00:02:43.000
たとえば、「ファーストフードジョイント」の「食べ物」は、「思考のための食べ物」の「食べ物」とは異なる意味を持つため、異なる埋め込みベクトルを取得します。

00:02:43.000 --> 00:02:49.000
さて、入力レイヤーとして強力な埋め込みを持つことのポイントは、転送学習を可能にすることです。

00:02:49.000 --> 00:03:03.000
埋め込みは大量のデータで訓練され、言語の一般的な知識をカプセル化し、大量のタスク固有のトレーニングデータを必要とせずに特定のタスクに転送できます。

00:03:03.000 --> 00:03:08.000
現在、Create MLはELMoモデルを使用してこの種の埋め込みをサポートしています。

00:03:08.000 --> 00:03:14.000
これらのモデルは、出力を組み合わせて埋め込みベクトルを生成するLSTMに基づいています。

00:03:14.000 --> 00:03:20.000
これらは、Create MLを介してトレーニング分類とタグ付けモデルに使用できます。

00:03:20.000 --> 00:03:24.000
さて、これまでにサポートされてきたモデルについて説明しましょう。

00:03:24.000 --> 00:03:33.000
これらは2019年と2020年の以前のセッションで非常に詳細に議論されたので、ここで簡単に説明します。

00:03:33.000 --> 00:03:42.000
自然言語は、一般的にNLPモデルで見てきたパターンに従うCreate MLを使用したモデルトレーニングをサポートしています。

00:03:42.000 --> 00:03:48.000
これには、テキスト分類と単語タグ付けの2つの異なるタスクのモデルが含まれます。

00:03:48.000 --> 00:03:54.000
テキスト分類では、出力は一連のクラスの1つを使用して入力テキストを記述します。

00:03:54.000 --> 00:03:57.000
例えば、それはトピックや感情かもしれません。

00:03:57.000 --> 00:04:08.000
また、単語のタグ付けでは、入力テキストの各単語にラベルを付けます。たとえば、スピーチの一部やロールラベルです。

00:04:08.000 --> 00:04:25.000
また、サポートされているCreate MLモデルは、一般的にNLPフィールドの進化に従い、maxentおよびCRFベースのモデルから始まり、静的単語埋め込みのサポートを追加し、ELMo埋め込みを使用してCreate MLモデルの動的単語埋め込みを追加しました。

00:04:25.000 --> 00:04:35.000
また、以前のセッション、2019年の「自然言語フレームワークの進歩」と2020年の「自然言語でアプリをよりスマートにする」で、この詳細を見ることができます。

00:04:35.000 --> 00:04:40.000
では、今年の自然言語の新機能に目を向けてみましょう。

00:04:40.000 --> 00:04:45.000
私たちは今、トランスフォーマーベースのコンテキスト埋め込みを提供していることを嬉しく思います。

00:04:45.000 --> 00:04:48.000
具体的には、これらはBERT埋め込みです。

00:04:48.000 --> 00:04:53.000
これは、トランスフォーマーからの双方向エンコーダ表現の略です。

00:04:53.000 --> 00:05:00.000
これらは、マスクされた言語モデルスタイルのトレーニングを使用して、大量のテキストでトレーニングされた埋め込みモデルです。

00:05:00.000 --> 00:05:14.000
これは、モデルに1つの単語がマスクされた文を与えられ、例えば「思考のための食べ物」の「食べ物」という単語を提案するように求められ、これをより良くするために訓練されることを意味します。

00:05:14.000 --> 00:05:31.000
彼らの中心にあるトランスフォーマーは、注意メカニズムと呼ばれるもの、特にマルチヘッドの自己注意に基づいており、モデルは一度に複数の異なる方法で、異なる重みを持つテキストの異なる部分を考慮することができます。

00:05:31.000 --> 00:05:44.000
多頭の自己注意メカニズムは、他の複数のレイヤーで包まれ、数回繰り返され、大量のテキストデータを活用できる強力で柔軟なモデルを提供します。

00:05:44.000 --> 00:05:52.000
実際、一度に複数の言語で訓練することができ、多言語モデルにつながります。

00:05:52.000 --> 00:05:54.000
これにはいくつかの利点があります。

00:05:54.000 --> 00:06:00.000
これにより、多くの言語をすぐにサポートし、一度に複数の言語をサポートすることさえできます。

00:06:00.000 --> 00:06:10.000
しかし、それ以上に、言語間の類似性のために、ある言語のデータが他の言語に役立つような相乗効果があります。

00:06:10.000 --> 00:06:17.000
そのため、さまざまな言語ファミリーで27の異なる言語をすぐにサポートしました。

00:06:17.000 --> 00:06:25.000
これは、関連するライティングシステムを共有する言語のグループのためにそれぞれ1つずつ、3つの別々のモデルで行われます。

00:06:25.000 --> 00:06:35.000
したがって、ラテン文字言語のモデルが1つ、キリル文字を使用する言語のモデルが1つ、中国語、日本語、韓国語のモデルが1つあります。

00:06:35.000 --> 00:06:42.000
これらの埋め込みモデルは、先ほど説明したCreate MLトレーニングにぴったりフィットし、入力エンコーディングレイヤーとして機能します。

00:06:42.000 --> 00:06:46.000
これは、多くの異なるモデルのための強力なエンコーディングです。

00:06:46.000 --> 00:06:51.000
さらに、トレーニングに使用するデータはすべて1つの言語である必要はありません。

00:06:51.000 --> 00:06:54.000
これがどのように機能するかを例で示しましょう。

00:06:54.000 --> 00:07:02.000
メッセージングアプリを書いていて、受信したメッセージを自動的に分類してユーザーを支援したいとします。

00:07:02.000 --> 00:07:17.000
それらを3つのカテゴリに分けたいとします。友人から受け取る可能性のある個人的なメッセージ、同僚から受け取る可能性のあるビジネスメッセージ、およびやり取りする企業から受け取る可能性のある商業メッセージです。

00:07:17.000 --> 00:07:22.000
しかし、ユーザーは多くの異なる言語でメッセージを受け取る可能性があり、あなたはそれを処理したい。

00:07:22.000 --> 00:07:29.000
この例では、英語、イタリア語、ドイツ語、スペイン語の複数の言語でトレーニングデータをまとめました。

00:07:29.000 --> 00:07:35.000
json形式を使用しましたが、ディレクトリやCSVを使用することもできます。

00:07:35.000 --> 00:07:39.000
モデルを訓練するために、Create MLアプリに入り、プロジェクトを作成します。

00:07:39.000 --> 00:07:48.000
次に、トレーニングデータを選択する必要があります。

00:07:48.000 --> 00:08:00.000
また、それに合わせて検証データとテストデータも準備しました。

00:08:00.000 --> 00:08:09.000
次に、アルゴリズムを選択する必要があり、ここに新しい選択肢があります。BERT埋め込みです。

00:08:09.000 --> 00:08:13.000
それらを選択したら、スクリプトを選択できます。

00:08:13.000 --> 00:08:17.000
これらはラテン文字の言語なので、ラテン語に残しておきます。

00:08:17.000 --> 00:08:27.000
単一の言語を使用している場合は、ここで指定するオプションがありますが、これは多言語なので、自動のままにします。

00:08:27.000 --> 00:08:36.000
その後、トレーニングを押すだけで、モデルトレーニングが開始されます。

00:08:36.000 --> 00:08:41.000
トレーニングの最も時間のかかる部分は、これらの強力な埋め込みをテキストに適用することです。

00:08:41.000 --> 00:08:49.000
その後、モデルは高い精度でかなり迅速に訓練されます。

00:08:49.000 --> 00:08:56.000
その時点で、いくつかのメッセージの例で試してみることができます。

00:08:56.000 --> 00:09:08.000
英語...またはスペイン語で。

00:09:08.000 --> 00:09:12.000
そして、モデルはこれらが商業的なメッセージであるとかなり確信しています。

00:09:12.000 --> 00:09:25.000
可能な相乗効果の例として、このモデルはフランス語で訓練されていませんが、フランス語のテキストも分類できます。

00:09:25.000 --> 00:09:31.000
ただし、興味のある言語ごとにトレーニングデータを使用することをお勧めします。

00:09:31.000 --> 00:09:42.000
これまでのところ、Create MLで作業しているばかりですが、NLContextualEmbeddingと呼ばれる新しいクラスで自然言語フレームワークを使用してこれらの埋め込みを操作することも可能です。

00:09:42.000 --> 00:09:48.000
これにより、必要な埋め込みモデルを識別し、そのプロパティのいくつかを見つけることができます。

00:09:48.000 --> 00:09:55.000
埋め込みモデルは、言語やスクリプトなど、さまざまな方法で検索できます。

00:09:55.000 --> 00:10:01.000
そのようなモデルを取得したら、ベクトルの次元などのプロパティを取得できます。

00:10:01.000 --> 00:10:07.000
また、各モデルには識別子があり、これはモデルを一意に識別する文字列にすぎません。

00:10:07.000 --> 00:10:20.000
たとえば、モデルで作業を開始すると、言語によってそれを見つけるかもしれませんが、後でまったく同じモデルを使用していることを確認し、識別子でこれを行うことができます。

00:10:20.000 --> 00:10:29.000
覚えておくべきことの1つは、他の多くの自然言語機能と同様に、これらの埋め込みモデルは必要に応じてダウンロードされるアセットに依存しているということです。

00:10:29.000 --> 00:10:40.000
NLContextualEmbeddingは、使用前にダウンロードを要求するなど、これに対する追加の制御を与えるためにいくつかのAPIを提供します。

00:10:40.000 --> 00:10:51.000
特定の埋め込みモデルに現在デバイスで利用可能なアセットがあるかどうかを尋ねることができ、リクエストを入れないと、ダウンロードされます。

00:10:51.000 --> 00:11:00.000
さて、あなた方の何人かは、私はCreate MLを使用してトレーニングしないいくつかのモデルを持っていますが、代わりにPyTorchまたはTensorFlowを使用してトレーニングしていると言っているかもしれません。

00:11:00.000 --> 00:11:03.000
これらの新しいBERT埋め込みを引き続き使用できますか?

00:11:03.000 --> 00:11:05.000
はい、できます。

00:11:05.000 --> 00:11:14.000
私たちは、あなたが訓練したいほぼすべてのモデルの入力レイヤーとして使用できる、利用可能なこれらの事前に訓練された多言語埋め込みモデルを提供します。

00:11:14.000 --> 00:11:17.000
これがその仕組みです。

00:11:17.000 --> 00:11:24.000
macOSデバイスでは、NLContextualEmbeddingを使用して、トレーニングデータの埋め込みベクトルを取得します。

00:11:24.000 --> 00:11:36.000
次に、これらを入力としてPyTorchまたはTensorFlowを使用してトレーニングにフィードし、Core MLツールを使用して結果をCore MLモデルに変換します。

00:11:36.000 --> 00:11:47.000
次に、デバイス上の推論時に、NLContextualEmbeddingを使用して入力データの埋め込みベクトルを取得し、それらをCore MLモデルに渡して出力を取得します。

00:11:47.000 --> 00:11:58.000
これをサポートするために、モデルをロードし、テキストに適用し、結果の埋め込みベクトルを取得できる追加のNLContextualEmbedding APIがあります。

00:11:58.000 --> 00:12:06.000
以前のモデル識別子を覚えている場合は、それを使用して、トレーニングに使用したのと同じモデルを取得できます。

00:12:06.000 --> 00:12:14.000
その後、モデルをテキストに適用して、NLContextualEmbeddingResultオブジェクトを与えることができます。

00:12:14.000 --> 00:12:20.000
このオブジェクトを取得したら、それを使用して埋め込みベクトルを反復できます。

00:12:20.000 --> 00:12:26.000
さて、これで何が可能かを味わうために、簡単なサンプルモデルを用意しました。

00:12:26.000 --> 00:12:46.000
既存の英語の安定した拡散モデルから始めて、いくつかの多言語データを使用して微調整し、新しいBERT埋め込みを入力レイヤーとして使用し、それらを固定として取り、寸法を変換するための単純な線形投影レイヤーをトレーニングしました。

00:12:46.000 --> 00:12:52.000
結果は、多言語入力を取る安定した拡散モデルです。

00:12:52.000 --> 00:12:55.000
モデルからの出力の例をいくつか紹介します。

00:12:55.000 --> 00:13:06.000
「ピンクの花でいっぱいの庭を通る道」など、英語のテキストを通り過ぎると、モデルは私たちをピンクの花でいっぱいの庭に導きます。

00:13:06.000 --> 00:13:17.000
しかし、また、同じ文章をフランス語、スペイン語、イタリア語、ドイツ語に翻訳すると、モデルはそれぞれにピンクの花でいっぱいの小道や庭の画像を生成します。

00:13:17.000 --> 00:13:20.000
もう少し複雑な例を挙げてみましょう。

00:13:20.000 --> 00:13:31.000
「曇り空の下の木々や山の前の道路」これは、道路、木、山、雲を含むモデルからの出力です。

00:13:31.000 --> 00:13:45.000
しかし、同様に、私は同じ文をフランス語、スペイン語、イタリア語、ドイツ語、または他の多くの言語のいずれかに翻訳することができ、それぞれについて道路、木、山、雲のイメージを得ることができます。

00:13:45.000 --> 00:13:49.000
では、このセッションの教訓をまとめましょう。

00:13:49.000 --> 00:14:01.000
Create MLを使用すると、テキスト分類や単語タグ付けタスクのモデルを簡単にトレーニングでき、新しい多言語BERT埋め込みモデルは、この目的のために強力な入力エンコーディングレイヤーを提供します。

00:14:01.000 --> 00:14:05.000
これらのモデルは、単一言語または多言語にすることができます。

00:14:05.000 --> 00:14:13.000
また、BERT埋め込みを、PyTorchまたはTensorFlowでトレーニングするモデルの入力レイヤーとして使用することもできます。

00:14:13.000 --> 00:14:14.000
ありがとうございます。

00:14:14.000 --> 00:14:17.000
さあ、外に出て、いくつかのモデルのトレーニングを始めましょう。

00:14:17.000 --> 23:59:59.000
♪ ♪

