WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:15.000
グラント:こんにちは、私の名前はグラントです。私はアクセシビリティチームのエンジニアです。

00:00:15.000 --> 00:00:23.000
多くの人がAppleプラットフォームで音声合成を使用しており、シンセサイザーの声に頼っている人もいます。

00:00:23.000 --> 00:00:26.000
これらの声は、彼らのデバイスへの窓です。

00:00:26.000 --> 00:00:31.000
したがって、彼らが選ぶ声はしばしば非常に個人的な選択です。

00:00:31.000 --> 00:00:37.000
iOSで音声合成を使用している人は、すでに多くの異なる声から選択できます。

00:00:37.000 --> 00:00:41.000
さらに多くのものを提供する方法を見てみましょう。 

00:00:41.000 --> 00:00:55.000
まず、音声合成マークアップ言語とは何か、カスタムボイスに没入型音声出力をもたらす方法、および音声プロバイダーがそれを採用すべき理由について説明します。

00:00:55.000 --> 00:01:05.000
次に、デバイス全体にシンセサイザーと音声体験をもたらすために、音声合成プロバイダーを実装する方法を説明します。

00:01:05.000 --> 00:01:09.000
そして最後に、パーソナルボイスに飛び込みます。

00:01:09.000 --> 00:01:11.000
これは新機能です。

00:01:11.000 --> 00:01:18.000
今、人々は自分の声を録音し、それらの録音から合成された声を生成することができます。

00:01:18.000 --> 00:01:24.000
だから今、あなたはユーザー自身の個人的な声でスピーチを合成することができます。

00:01:24.000 --> 00:01:29.000
SSMLを見ることから始めましょう。

00:01:29.000 --> 00:01:35.000
SSMLは、音声テキストを表すためのW3C標準です。

00:01:35.000 --> 00:01:43.000
SSMLスピーチは、さまざまなタグと属性を持つXML形式を使用して宣言的に表現されます。

00:01:43.000 --> 00:01:49.000
これらのタグを使用して、レートやピッチなどの音声プロパティを制御できます。

00:01:49.000 --> 00:01:53.000
SSMLはファーストパーティシンセサイザーで使用されます。

00:01:53.000 --> 00:02:00.000
これにはWebKitのWebSpeechが含まれており、音声シンセサイザーの標準入力です。

00:02:00.000 --> 00:02:04.000
SSMLの使い方を見てみましょう。 見てみましょう。

00:02:04.000 --> 00:02:07.000
一時停止があるこの例のフレーズを見てみましょう。

00:02:07.000 --> 00:02:11.000
この一時停止はSSMLで表すことができます。

00:02:11.000 --> 00:02:21.000
「こんにちは」の文字列から始めて、SSMLブレークタグを使用して1秒の一時停止を追加し、「はじめまして！」をスピードアップして終了します。

00:02:21.000 --> 00:02:29.000
これを行うには、SSMLプロソディタグを追加し、レート属性を200%に設定します。

00:02:29.000 --> 00:02:36.000
今、私たちはこのSSMLを取り、話すためのAVSpeechUtteranceを作成することができます。

00:02:36.000 --> 00:02:43.000
次に、独自のスピーチシンセサイザーボイスを実装する方法を見てみましょう。

00:02:43.000 --> 00:02:45.000
では、スピーチシンセサイザーとは何ですか?

00:02:45.000 --> 00:02:58.000
音声シンセサイザーは、SSMLの形式で目的の音声プロパティに関するテキストと情報を受け取り、そのテキストの音声表現を提供します。

00:02:58.000 --> 00:03:06.000
素晴らしい新しい声を持つシンセサイザーを持っていて、それをiOS、macOS、iPadOSに持ち込みたいとします。

00:03:06.000 --> 00:03:19.000
音声合成プロバイダーを使用すると、独自の音声シンセサイザーと音声を当社のプラットフォームに実装して、システム音声を超えてユーザーにさらにパーソナライズできます。

00:03:19.000 --> 00:03:21.000
これがどのように機能するか見てみましょう。

00:03:21.000 --> 00:03:30.000
音声合成プロバイダーのオーディオユニット拡張機能は、ホストアプリに埋め込まれ、SSMLの形で音声要求を受け取ります。

00:03:30.000 --> 00:03:42.000
拡張機能は、SSML入力のオーディオをレンダリングし、オプションでそれらのオーディオバッファ内の単語が発生する場所を示すマーカーを返す責任があります。

00:03:42.000 --> 00:03:46.000
その後、システムはその音声要求のすべての再生を管理します。

00:03:46.000 --> 00:03:55.000
オーディオセッション管理を処理する必要はありません。音声合成プロバイダーフレームワークによって内部的に管理されます。

00:03:55.000 --> 00:04:02.000
シンセサイザーが何であるかを理解したので、スピーチシンセサイザー拡張機能の構築を開始できます。

00:04:02.000 --> 00:04:20.000
Xcodeで新しいAudio Unit Extensionアプリプロジェクトを作成し、「Speech Synthesizer」Audio Unit Typeを選択し、シンセサイザーに4文字のサブタイプ識別子と、製造元として4文字の識別子を提供しましょう。

00:04:20.000 --> 00:04:27.000
オーディオユニット拡張機能は、音声シンセサイザー拡張機能が構築されたコアアーキテクチャです。

00:04:27.000 --> 00:04:33.000
シンセサイザーは、ホストアプリプロセスではなく、拡張プロセスで実行できます。

00:04:33.000 --> 00:04:42.000
私たちのアプリは、拡張機能が音声を合成する音声を購入して選択するためのシンプルなインターフェースを提供します。

00:04:42.000 --> 00:04:48.000
まず、購入可能な声を示すリストビューを作成します。

00:04:48.000 --> 00:04:52.000
各音声セルには、音声名と購入ボタンが表示されます。

00:04:52.000 --> 00:04:56.000
次に、いくつかの声をリストに入力します。

00:04:56.000 --> 00:05:04.000
ここでは、WWDCVoiceは音声名と識別子を保持するシンプルな構造体です。

00:05:04.000 --> 00:05:13.000
また、購入した音声を追跡するための状態変数と、それらを表示するための新しいセクションも必要です。

00:05:13.000 --> 00:05:17.000
次に、音声を購入する機能を作成しましょう。

00:05:17.000 --> 00:05:23.000
ここでは、新しく購入した音声をリストに追加し、それに応じてUIを更新することができます。

00:05:23.000 --> 00:05:29.000
AVSpeechSynthesisProviderVoiceメソッドupdateSpeechVoicesに注意してください。

00:05:29.000 --> 00:05:39.000
このようにして、アプリは、シンセサイザーで利用可能なボイスのセットが変更され、システムのボイスリストを再構築する必要があることを通知することができます。

00:05:39.000 --> 00:05:46.000
この例では、音声のアプリ内購入を完了した後にこの電話をかけることができます。

00:05:46.000 --> 00:05:53.000
また、スピーチシンセサイザー拡張機能で利用可能な音声を監視する方法も必要です。

00:05:53.000 --> 00:05:59.000
これは、アプリグループを通じて共有されるUserDefaultsのインスタンスを作成することで実行できます。

00:05:59.000 --> 00:06:05.000
アプリグループでは、ホストアプリと拡張機能の間でこの音声リストを共有できます。

00:06:05.000 --> 00:06:11.000
アプリグループを作成するときに提供したスイート名を明示的に指定しています。

00:06:11.000 --> 00:06:16.000
これにより、ホストアプリと拡張機能が同じドメインから確実に読み込まれます。

00:06:16.000 --> 00:06:25.000
購入機能を振り返ってみると、新しい音声が購入されたときにユーザーのデフォルトを更新する方法を実装しました。

00:06:25.000 --> 00:06:31.000
AVSpeechSynthesizerには、利用可能なシステム音声の変更をリッスンするための新しいAPIもあります。

00:06:31.000 --> 00:06:38.000
システムボイスのセットは、ユーザーがボイスを削除するか、新しいボイスをダウンロードすると変更される可能性があります。

00:06:38.000 --> 00:06:47.000
availableVoicesDidChangeNotificationを購読して、これらの変更に基づいてボイスのリストを更新できます。

00:06:47.000 --> 00:06:54.000
ホストアプリが完成したので、4つの主要コンポーネントで構成されるオーディオユニットを埋めましょう。

00:06:54.000 --> 00:07:01.000
最初に追加する必要があるのは、シンセサイザーが提供する声をシステムに知らせる方法です。

00:07:01.000 --> 00:07:13.000
これは、speechVoicesゲッターをオーバーライドして、先ほど指定したアプリグループUserDefaultsドメインから音声と読み取りのリストを提供することによって達成されます。

00:07:13.000 --> 00:07:21.000
音声リストの各項目について、米国英語AVSpeechSynthesisProviderVoiceを構築します。

00:07:21.000 --> 00:07:27.000
次に、システムがシンセサイザーにどのテキストを合成するかを伝える方法が必要です。

00:07:27.000 --> 00:07:36.000
synthesizeSpeechRequestメソッドは、システムがテキストの合成を開始する必要があることを拡張機能に通知したいときに呼び出されます。

00:07:36.000 --> 00:07:46.000
このメソッドの引数は、SSMLを含むAVSpeechSynthesisProviderRequestのインスタンスであり、どの音声で話すかです。

00:07:46.000 --> 00:07:51.000
次に、スピーチエンジンの実装で作成したヘルパーメソッドを呼び出します。

00:07:51.000 --> 00:08:01.000
この例では、getAudioBufferメソッドは、リクエストで指定された音声とSSML入力に基づいてオーディオデータを生成します。

00:08:01.000 --> 00:08:13.000
また、レンダリングブロックが呼び出されたときにレンダリングしたフレーム数を追跡し、バッファからフレームをコピーするために、framePositionと呼ばれるインスタンス変数を0に設定します。

00:08:13.000 --> 00:08:21.000
システムには、オーディオの合成を停止し、現在の音声要求を破棄するためにシンセサイザーに信号を送る方法も必要です。

00:08:21.000 --> 00:08:27.000
これは、現在のバッファを単に破棄するcancelSpeechRequestで達成されます。

00:08:27.000 --> 00:08:31.000
最後に、レンダリングブロックを実装する必要があります。

00:08:31.000 --> 00:08:35.000
レンダリングブロックは、目的のframeCountでシステムによって呼び出されます。

00:08:35.000 --> 00:08:42.000
その後、オーディオユニットは、要求されたフレーム数をoutputAudioBufferに入力する責任があります。

00:08:42.000 --> 00:08:53.000
次に、ターゲットバッファと、synthesizeSpeechRequest呼び出し中に以前に生成および保存したバッファへの参照を設定します。

00:08:53.000 --> 00:08:57.000
次に、フレームをターゲットバッファにコピーします。

00:08:57.000 --> 00:09:14.000
そして最後に、オーディオユニットが現在の音声要求のすべてのバッファを使い果たしたら、actionFlags引数をofflineUnitRenderAction_Completeに設定して、レンダリングが完了し、レンダリングするオーディオバッファがないことをシステムに通知する必要があります。

00:09:14.000 --> 00:09:16.000
実際に見てみましょう! 

00:09:16.000 --> 00:09:19.000
これは私のスピーチシンセサイザーアプリです。

00:09:19.000 --> 00:09:27.000
音声を購入し、新しい音声と音声エンジンを使用して音声を合成できるビューに移動します。

00:09:27.000 --> 00:09:34.000
まず、シンセサイザーに「こんにちは」と入力します。

00:09:34.000 --> 00:09:36.000
合成された声:こんにちは。

00:09:36.000 --> 00:09:41.000
グラント：それから私は「さようなら」と入力します。

00:09:41.000 --> 00:09:43.000
合成された声:さようなら。

00:09:43.000 --> 00:09:54.000
グラント:私たちは今、合成プロバイダーを実装し、VoiceOverから独自のアプリまで、システム全体で使用できる音声を提供するホスティングアプリを作成しました!

00:09:54.000 --> 00:10:01.000
これらのAPIを使用して作成する新しい音声とテキスト読み上げ体験を見るのが待ちきれません。

00:10:01.000 --> 00:10:06.000
先に進んで、パーソナルボイスと呼ばれる新機能について話しましょう。

00:10:06.000 --> 00:10:14.000
人々は今、自分のデバイスの力を使って、iOSとmacOSで自分の声を録音し、再作成することができます。

00:10:14.000 --> 00:10:19.000
パーソナルボイスは、サーバー上ではなくデバイス上で生成されます。

00:10:19.000 --> 00:10:26.000
この音声は、残りのシステム音声の中に表示され、ライブスピーチと呼ばれる新機能で使用できます。

00:10:26.000 --> 00:10:38.000
ライブスピーチは、iOS、iPadOS、macOS、watchOSで話すタイプ機能で、その場で自分の声でスピーチを合成することができます。

00:10:38.000 --> 00:10:46.000
パーソナルボイスの新しいリクエスト承認APIを使用して、これらの音声で音声を合成するためのアクセスをリクエストできます。

00:10:46.000 --> 00:10:55.000
パーソナルボイスの使用は敏感であり、主に増強または代替コミュニケーションアプリに使用される必要があることを覚えておいてください。

00:10:55.000 --> 00:11:00.000
パーソナルボイスを使うために作ったAACアプリをチェックアウトしましょう。

00:11:00.000 --> 00:11:11.000
私のアプリには、WWDCで言っている一般的なフレーズを話す2つのボタンと、パーソナルボイスを使用するためのアクセスを要求するボタンがあります。

00:11:11.000 --> 00:11:20.000
承認は、AVSpeechSynthesizerのrequestPersonalVoiceAuthorizationという新しいAPIで要求できます。

00:11:20.000 --> 00:11:34.000
承認されると、Personal VoicesはAVSpeechSynthesisVoice API speechVoicesのシステムボイスと一緒に表示され、isPersonalVoiceと呼ばれる新しいvoiceTraitで示されます。

00:11:34.000 --> 00:11:40.000
パーソナルボイスにアクセスできたので、それを使って話すことができます。

00:11:40.000 --> 00:11:43.000
動作中のパーソナルボイスのデモをチェックしてみましょう。

00:11:43.000 --> 00:11:54.000
まず、「Personal Voiceを使用」ボタンをタップして承認をリクエストし、承認したら、シンボルをタップして自分の声を聞くことができます。

00:11:54.000 --> 00:11:58.000
個人的な声:こんにちは、私の名前はグラントです。WWDC23へようこそ。

00:11:58.000 --> 00:12:00.000
グラント：それは素晴らしいことではありませんか？

00:12:00.000 --> 00:12:04.000
そして今、あなたはこれらの声をあなたのアプリでも使うことができます。

00:12:04.000 --> 00:12:13.000
SSMLについて議論したので、それを使用して音声入力を標準化し、アプリで豊富な音声体験を構築する必要があります。

00:12:13.000 --> 00:12:24.000
また、スピーチシンセサイザーをAppleプラットフォームに実装する方法も説明しましたので、人々がシステム全体で使用できる素晴らしい新しいスピーチボイスを提供できるようになりました。

00:12:24.000 --> 00:12:35.000
そして最後に、パーソナルボイスを使用すると、特に自分の声を失う危険性がある人々のために、アプリの合成にさらに多くの個人的なタッチをもたらすことができます。

00:12:35.000 --> 00:12:40.000
私たちは、あなたがこれらのAPIを使用してどのような経験を作成するかを見て非常に興奮しています。

00:12:40.000 --> 23:59:59.000
見てくれてありがとう。

