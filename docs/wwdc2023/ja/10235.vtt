WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:16.000
ジュリアン:こんにちは、「音声処理の新機能」へようこそ。私はコアオーディオチームのジュリアンです。

00:00:16.000 --> 00:00:25.000
Voice over IPアプリケーションは、人々が同僚、友人、家族とのつながりを維持するために、これまで以上に不可欠になっています。

00:00:25.000 --> 00:00:31.000
ボイスチャットのオーディオ品質は、優れたユーザーエクスペリエンスを提供する上で重要な役割を果たします。

00:00:31.000 --> 00:00:39.000
あらゆる状況下で素晴らしく聞こえるオーディオ処理を実装することは重要ですが、困難です。

00:00:39.000 --> 00:00:56.000
そのため、Appleは音声処理APIを提供しているため、どのような音響環境にいるか、どのApple製品を使用しているか、どのオーディオアクセサリに接続されているかに関係なく、アプリでチャットするとき、誰もが常に可能な限り最高のオーディオ体験を楽しむことができます。

00:00:56.000 --> 00:01:03.000
Appleの音声処理APIは、独自のFaceTimeや電話アプリなど、多くのアプリで広く使用されています。

00:01:03.000 --> 00:01:14.000
ボイスチャットオーディオを強化するために、エコーキャンセル、ノイズ抑制、自動ゲイン制御など、クラス最高のオーディオ信号処理を提供します。

00:01:14.000 --> 00:01:25.000
その性能は、独自の音響特性を考慮して、各タイプのオーディオデバイスと組み合わせて、各Apple製品モデルの音響エンジニアによって調整されています。

00:01:25.000 --> 00:01:37.000
Appleの音声処理APIを選択すると、ユーザーはStandard、Voice Isolation、Wide Spectrumなど、アプリのマイクモード設定を完全に制御できます。

00:01:37.000 --> 00:01:43.000
Voice over IPアプリケーションには、Appleの音声処理APIを使用することを強くお勧めします。

00:01:43.000 --> 00:01:47.000
Appleの音声処理APIには2つのオプションがあります。

00:01:47.000 --> 00:01:56.000
最初のオプションは、AUVoiceProcessingIOとも呼ばれるAUVoiceIOと呼ばれるI/Oオーディオユニットです。

00:01:56.000 --> 00:02:02.000
このオプションは、I/Oオーディオユニットと直接対話する必要があるアプリ向けです。

00:02:02.000 --> 00:02:13.000
2番目のオプションはAVAudioEngineで、より具体的にはAVAudioEngineの「音声処理」モードを有効にします。

00:02:13.000 --> 00:02:17.000
AVAudioEngineはより高いレベルのAPIです。

00:02:17.000 --> 00:02:23.000
一般的に使いやすく、オーディオで作業するときに書かなければならないコードの量を減らします。

00:02:23.000 --> 00:02:28.000
どちらのオプションも同じ音声処理機能を提供します。

00:02:28.000 --> 00:02:30.000
さて、何が新しいのですか?

00:02:30.000 --> 00:02:35.000
音声処理APIをtvOSで初めて利用できるようにしています。

00:02:35.000 --> 00:02:42.000
詳細については、セッション「Discover Continuity Camera for tvOS」をご覧ください。

00:02:42.000 --> 00:02:55.000
また、AUVoiceIOとAVAudioEngineにいくつかの新しいAPIを追加して、音声処理をより詳細に制御し、新機能を実装するのに役立ちます。

00:02:55.000 --> 00:03:04.000
最初のAPIは、他のオーディオのダッキング動作を制御するのに役立ちます。それが何を意味するのかをすぐに説明します。

00:03:04.000 --> 00:03:10.000
2番目のAPIは、アプリにミュートされたトーカー検出機能を実装するのに役立ちます。

00:03:10.000 --> 00:03:16.000
このセッションでは、これら2つの新しいAPIの詳細に焦点を当てます。

00:03:16.000 --> 00:03:27.000
私が話したい最初のAPIは「その他のオーディオダッキング」です。このAPIに飛び込む前に、まず他のオーディオとは何か、なぜダッキングが重要なのかを説明しましょう。

00:03:27.000 --> 00:03:33.000
Appleの音声処理APIを使用する場合は、再生オーディオに何が起こっているのかを見てみましょう。

00:03:33.000 --> 00:03:41.000
あなたのアプリは、Appleの音声処理で処理され、出力デバイスで再生されるボイスチャットオーディオストリームを提供しています。

00:03:41.000 --> 00:03:46.000
ただし、他のオーディオストリームが同時に再生される可能性があります。

00:03:46.000 --> 00:03:55.000
たとえば、アプリは音声処理APIを介してレンダリングされていない別のオーディオストリームを再生している可能性があります。

00:03:55.000 --> 00:04:00.000
また、アプリと同時にオーディオを再生する他のアプリがあるかもしれません。

00:04:00.000 --> 00:04:13.000
アプリからの音声オーディオストリーム以外のすべてのオーディオストリームは、Appleの音声処理によって「他のオーディオ」と見なされ、音声オーディオは出力デバイスに再生される前に他のオーディオと混在します。

00:04:13.000 --> 00:04:19.000
ボイスチャットアプリの場合、通常、再生オーディオの主な焦点はボイスチャットオーディオです。

00:04:19.000 --> 00:04:26.000
そのため、音声オーディオの明瞭度を向上させるために、他のオーディオの音量レベルを回避します。

00:04:26.000 --> 00:04:31.000
過去には、他のオーディオに一定量のダッキングを適用しました。

00:04:31.000 --> 00:04:39.000
これはほとんどのアプリでうまく機能しており、アプリが現在のダッキング動作に満足している場合は、何もする必要はありません。

00:04:39.000 --> 00:04:48.000
しかし、一部のアプリはダッキング動作をより詳細に制御したいと思っていることを認識しており、このAPIはそれを達成するのに役立ちます。

00:04:48.000 --> 00:04:55.000
最初にAUVoiceIOのこのAPIを調べて、後でAVAudioEngineにたどり着きます。

00:04:55.000 --> 00:05:00.000
AUVoiceIOの場合、これは他のオーディオダッキング構成の構造です。

00:05:00.000 --> 00:05:15.000
ダッキングの2つの独立した側面、つまりダッキングのスタイル、つまりmEnableAdvancedDuckingとダッキングの量、つまりmDuckingLevelのコントロールを提供します。

00:05:15.000 --> 00:05:19.000
mEnableAdvancedDuckingの場合、デフォルトでは、これは無効になっています。

00:05:19.000 --> 00:05:29.000
有効にすると、チャット参加者の両側からの音声アクティビティの存在に基づいて、ダッキングレベルを動的に調整します。

00:05:29.000 --> 00:05:37.000
言い換えれば、どちらかの側からユーザーが話しているときにより多くのダッキングを適用し、どちらの側も話していないときにダッキングを減らします。

00:05:37.000 --> 00:05:51.000
これは、FaceTime SharePlayのダッキングと非常によく似ています。FaceTimeのどちらの当事者も話していないときにメディア再生音量が高くなるが、誰かが話し始めるとすぐに、メディア再生音量が減少します。

00:05:51.000 --> 00:05:54.000
次に、mDuckingLevel。

00:05:54.000 --> 00:06:01.000
コントロールには、デフォルト(デフォルト)、最小(最小)、中(中)、最大(最大)の4つのレベルがあります。

00:06:01.000 --> 00:06:09.000
デフォルト(デフォルト)ダッキングレベルは、私たちが適用してきたのと同じ量のダッキングを適用し、これは引き続きデフォルト設定になります。

00:06:09.000 --> 00:06:13.000
最小(最小)ダッキングレベルは、私たちが適用するダッキングの量を最小限に抑えます。

00:06:13.000 --> 00:06:21.000
言い換えれば、これは他のオーディオボリュームをできるだけ大きくしたい場合に使用する設定です。

00:06:21.000 --> 00:06:27.000
逆に、最大(最大)ダッキングレベルは、私たちが適用するダッキングの量を最大化します。

00:06:27.000 --> 00:06:35.000
一般的に言えば、より高いダッキングレベルを選択することは、ボイスチャットの明瞭度を向上させるのに役立ちます。

00:06:35.000 --> 00:06:38.000
2つのコントロールは独立して使用できます。

00:06:38.000 --> 00:06:45.000
組み合わせて使用すると、ダッキング動作を制御する完全な柔軟性が得られます。

00:06:45.000 --> 00:06:52.000
ダッキング設定が何をするかを説明したので、アプリに適したものを作成できます。

00:06:52.000 --> 00:07:01.000
たとえば、ここでは高度なダッキングを有効にし、ダッキングレベルを最小値に選択します。

00:07:01.000 --> 00:07:13.000
次に、kAUVoiceIOProperty_ OtherAudioDuckingConfigurationを介して、このダッキング設定をAUVoiceIOに設定します。

00:07:13.000 --> 00:07:19.000
AVAudioEngineクライアントの場合、APIは非常によく似ています。

00:07:19.000 --> 00:07:31.000
これは他のオーディオダッキング構成の構造体定義であり、これはダッキングレベルの列挙型定義です。

00:07:31.000 --> 00:07:47.000
AVAudioEngineでこのAPIを使用するには、まずエンジンの入力ノードで音声処理を有効にしてから、ダッキング設定を設定します。

00:07:47.000 --> 00:07:52.000
そして最後に、入力ノードに設定を設定します。

00:07:52.000 --> 00:07:58.000
次に、アプリに非常に便利な機能を実装するのに役立つ別のAPIについて話しましょう。

00:07:58.000 --> 00:08:12.000
オンライン会議で、同僚や友人とチャットしていると思っていたが、すぐにミュートされていて、誰もあなたの素晴らしいポイントや面白い話を実際に聞いていないことに気づいたことがありますか?

00:08:12.000 --> 00:08:14.000
はい、気まずい。

00:08:14.000 --> 00:08:23.000
FaceTimeがここで何をしているかなど、アプリでミュートされたトーカー検出機能を提供することは非常に便利です。

00:08:23.000 --> 00:08:29.000
そのため、ミュートされたトーカーの存在を検出するためのAPIを提供しています。

00:08:29.000 --> 00:08:38.000
iOS 15で最初に導入され、現在はmacOS 14とtvOS 17で利用可能にしています。

00:08:38.000 --> 00:08:43.000
これは、このAPIの使用方法の概要です。

00:08:43.000 --> 00:08:53.000
まず、ミュートされたトーカーが検出されたときに通知を受け取るには、AUVoiceIOまたはAVAudioEngineにリスナーブロックを提供する必要があります。

00:08:53.000 --> 00:09:03.000
提供するリスナーブロックは、ミュートされたトーカーが話し始めるか、話すのをやめるたびに呼び出され、そのような通知の処理コードを実装します。

00:09:03.000 --> 00:09:12.000
たとえば、通知がユーザーがミュート中に会話を開始したことを示した場合、ユーザーにミュートを解除するように促したい場合があります。

00:09:12.000 --> 00:09:22.000
最後になりましたが、AUVoiceIOまたはAVAudioEngineのミュートAPIを介してミュートを実装する必要があります。

00:09:22.000 --> 00:09:25.000
AUVoiceIOでいくつかのコード例をご案内します。

00:09:25.000 --> 00:09:28.000
AVAudioEngineの例は後で説明します。

00:09:28.000 --> 00:09:34.000
まず、通知を処理するリスナーブロックを準備します。

00:09:34.000 --> 00:09:50.000
このブロックにはAUVoiceIOSpeechActivityEventタイプのパラメータがあり、SpeechActivityHasStartedまたはSpeechActivityHasEndedの2つの値のいずれかになります。

00:09:50.000 --> 00:09:57.000
リスナーブロックは、ミュート中に音声アクティビティイベントが変更されるたびに呼び出されます。

00:09:57.000 --> 00:10:11.000
ブロック内では、このイベントの処理方法を実装する場所です。たとえば、SpeechActivityHasStartedイベントを受信すると、ユーザーにミュートを解除するように促すことをお勧めします。

00:10:11.000 --> 00:10:24.000
このリスナーブロックの準備ができたら、kAUVoiceIOProperty_MutedSpeechActivityEventListenerを介してAUVoiceIOにブロックを登録します。

00:10:24.000 --> 00:10:34.000
ユーザーがミュートするときは、ミュートAPI kAUVoiceIOProperty_MuteOutputを介してミュートを実装します。

00:10:34.000 --> 00:10:44.000
リスナーブロックは、A、ユーザーがミュート、およびBが音声アクティビティ状態が変更されたときにのみ呼び出されます。

00:10:44.000 --> 00:10:53.000
音声活動の継続的な存在または欠如は、冗長な通知を引き起こすことはありません。

00:10:53.000 --> 00:10:57.000
AVAudioEngineクライアントの場合、実装は非常によく似ています。

00:10:57.000 --> 00:11:06.000
エンジンの入力ノードで音声処理を有効にした後、通知を処理するリスナーブロックを準備します。

00:11:06.000 --> 00:11:13.000
次に、入力ノードにリスナーブロックを登録します。

00:11:13.000 --> 00:11:19.000
ユーザーがミュートするときは、AVAudioEngineの音声処理ミュートAPIを使用してミュートします。

00:11:19.000 --> 00:11:27.000
さて、AUVoiceIOとAVAudioEngineでミュートされたトーカー検出機能の実装について説明しました。

00:11:27.000 --> 00:11:37.000
Appleの音声処理APIをまだ採用する準備ができていない方には、この機能を実装するのに役立つ代替手段を提供しています。

00:11:37.000 --> 00:11:47.000
この代替案は、CoreAudio HAL API、つまりハードウェア抽象化レイヤーAPIを介してmacOSでのみ利用可能です。

00:11:47.000 --> 00:11:55.000
組み合わせて使用すると音声アクティビティを検出するのに役立つ2つの新しいHALプロパティがあります。

00:11:55.000 --> 00:12:06.000
まず、kAudioDevicePropertyVoiceActivityDetectionEnableを介して入力デバイスで音声アクティビティ検出を有効にします。

00:12:06.000 --> 00:12:14.000
次に、kAudioDevicePropertyVoiceActivityDetectionStateにHALプロパティリスナーを登録します。

00:12:14.000 --> 00:12:21.000
このHALプロパティリスナーは、音声アクティビティ状態が変更されるたびに呼び出されます。

00:12:21.000 --> 00:12:30.000
アプリがプロパティリスナーから通知されたら、プロパティを照会して現在の値を取得します。

00:12:30.000 --> 00:12:35.000
さて、いくつかのコード例でこれを説明しましょう。

00:12:35.000 --> 00:12:46.000
入力デバイスで音声アクティビティ検出を有効にするには、まずHALプロパティアドレスを構築します。

00:12:46.000 --> 00:12:52.000
次に、入力デバイスにプロパティを設定して有効にします。

00:12:52.000 --> 00:13:05.000
次に、音声アクティビティ検出状態プロパティにリスナーを登録するには、HALプロパティアドレスを構築し、プロパティリスナーを指定します。

00:13:05.000 --> 00:13:12.000
ここで「listener_callback」はリスナー関数の名前です。

00:13:12.000 --> 00:13:18.000
これは、プロパティリスナーを実装する方法の例です。

00:13:18.000 --> 00:13:22.000
リスナーはこの関数のシグネチャに準拠しています。

00:13:22.000 --> 00:13:37.000
この例では、このリスナーが1つのHALプロパティにのみ登録されていることを前提としています。つまり、呼び出されたときに、どのHALプロパティが変更されたかに曖昧さはありません。

00:13:37.000 --> 00:13:51.000
複数のHALプロパティの通知に同じリスナーを登録する場合は、まずinAddressesの配列を調べて、何が正確に変更されたかを確認する必要があります。

00:13:51.000 --> 00:14:06.000
この通知を処理する際に、VoiceActivityDetectionStateプロパティを照会して現在の値を取得し、その値を処理する際に独自のロジックを実装します。

00:14:06.000 --> 00:14:13.000
これらの音声アクティビティ検出HAL APIに関する重要な詳細がいくつかあります。

00:14:13.000 --> 00:14:24.000
まず第一に、エコーキャンセルマイク入力から音声アクティビティを検出しているため、ボイスチャットアプリケーションに最適です。

00:14:24.000 --> 00:14:29.000
第二に、この検出はプロセスのミュート状態に関係なく機能します。

00:14:29.000 --> 00:14:41.000
ミュートされたトーカー検出機能を実装するには、音声アクティビティ状態とミュート状態を組み合わせた追加のロジックを実装するのはアプリ次第です。

00:14:41.000 --> 00:14:48.000
HAL APIクライアントがミュートを実装するには、HALのプロセスミュートAPIを使用することを強くお勧めします。

00:14:48.000 --> 00:14:58.000
メニューバーの録音インジケータライトを抑制し、プライバシーがミュートで保護されているという自信をユーザーに提供します。

00:14:58.000 --> 00:15:01.000
今日話したことをまとめましょう。 

00:15:01.000 --> 00:15:08.000
Appleの音声処理APIと、音声オーバーIPアプリケーションに推奨する理由について議論しました。

00:15:08.000 --> 00:15:20.000
私たちは、他のオーディオのダッキングと、AUVoiceIOとAVAudioEngineでそれを使用する方法のコード例でダッキング動作を制御するAPIについて話しました。

00:15:20.000 --> 00:15:28.000
また、AUVoiceIOとAVAudioEngineのコード例を使用して、ミュートされたトーカー検出を実装する方法についても説明しました。

00:15:28.000 --> 00:15:39.000
また、Appleの音声処理APIを採用していないクライアントのために、Core Audio HAL APIを使用してmacOSでそれを行うための代替オプションも示しました。

00:15:39.000 --> 00:15:44.000
Appleの音声処理APIで構築する素晴らしいアプリを楽しみにしています。

00:15:44.000 --> 00:15:45.000
見てくれてありがとう!

00:15:45.000 --> 23:59:59.000
♪ ♪

