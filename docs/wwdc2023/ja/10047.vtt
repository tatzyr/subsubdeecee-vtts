WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:14.000
Pulkit：こんにちは、私はPulkitで、Core MLチームのエンジニアです。

00:00:14.000 --> 00:00:18.000
Core ML Toolsに行われたいくつかのアップデートを共有できることを嬉しく思います。

00:00:18.000 --> 00:00:23.000
これらのアップデートは、機械学習モデルのサイズとパフォーマンスを最適化するのに役立ちます。

00:00:23.000 --> 00:00:30.000
モデルの機能が大幅に向上するにつれて、ますます多くの機能が機械学習によって推進されています。

00:00:30.000 --> 00:00:34.000
その結果、1つのアプリに展開されるモデルの数が増えています。

00:00:34.000 --> 00:00:41.000
それに伴い、アプリ内の各モデルも大きくなり、アプリのサイズに上向きの圧力をかけています。

00:00:41.000 --> 00:00:45.000
したがって、モデルのサイズを抑えることが重要です。

00:00:45.000 --> 00:00:48.000
モデルサイズを小さくすることにはいくつかの利点があります。

00:00:48.000 --> 00:00:53.000
各モデルが小さい場合は、同じメモリ予算でより多くのモデルを出荷できます。

00:00:53.000 --> 00:00:57.000
また、より大きく、より有能なモデルを出荷することもできます。

00:00:57.000 --> 00:01:00.000
また、モデルをより速く実行するのにも役立ちます。

00:01:00.000 --> 00:01:06.000
これは、モデルが小さいと、メモリとプロセッサの間を移動するデータが少なくなるためです。

00:01:06.000 --> 00:01:10.000
だから、モデルのサイズを小さくするのは素晴らしいアイデアのようです。

00:01:10.000 --> 00:01:12.000
モデルを大きくするものは何ですか?

00:01:12.000 --> 00:01:15.000
あなたが理解するのを助けるために例を説明させてください。

00:01:15.000 --> 00:01:19.000
ResNet50は人気のある画像分類モデルです。

00:01:19.000 --> 00:01:23.000
その最初の層は、約9,000のパラメータを持つ畳み込み層です。

00:01:23.000 --> 00:01:28.000
そして、さまざまなサイズの合計53の畳み込み層があります。

00:01:28.000 --> 00:01:33.000
最後に、それは約210万のパラメータを持つ線形層を持っています。

00:01:33.000 --> 00:01:45.000
これはすべて最大2500万のパラメータを追加します。Float16精度を使用してモデルを保存すると、重量あたり2バイトを使用し、サイズ50メガバイトのモデルが得られます。

00:01:45.000 --> 00:01:53.000
50メガバイトのモデルは大きいですが、安定した拡散のような新しいモデルにたどり着くと、さらに大きなモデルになります。

00:01:53.000 --> 00:01:57.000
さて、より小さなモデルを得るためのいくつかの道について話しましょう。

00:01:57.000 --> 00:02:05.000
1つの方法は、より少ない、より小さな重みで優れたパフォーマンスを達成できる、より効率的なモデルアーキテクチャを設計することです。

00:02:05.000 --> 00:02:08.000
もう1つの方法は、既存のモデルの重みを圧縮することです。

00:02:08.000 --> 00:02:11.000
このモデル圧縮のパスは、私が焦点を当てるものです。

00:02:11.000 --> 00:02:15.000
モデルの圧縮のための3つの有用なテクニックを説明することから始めます。

00:02:15.000 --> 00:02:20.000
次に、これらのモデル圧縮技術を統合した2つのワークフローを実演します。

00:02:20.000 --> 00:02:27.000
次に、最新のCore MLツールが、これらのテクニックとワークフローをモデルに適用するのにどのように役立つかを説明します。

00:02:27.000 --> 00:02:32.000
そして最後に、Srijanはモデル圧縮がランタイムパフォーマンスに与える影響について議論します。

00:02:32.000 --> 00:02:35.000
圧縮技術から始めましょう。

00:02:35.000 --> 00:02:38.000
モデルの重みを圧縮する方法はいくつかあります。

00:02:38.000 --> 00:02:44.000
最初の方法は、スパース行列表現を使用して、それらをより効率的にパックすることです。

00:02:44.000 --> 00:02:48.000
これは、剪定と呼ばれる技術を使用することで達成できます。

00:02:48.000 --> 00:02:52.000
もう1つの方法は、重みを保管するために使用される精度を下げることです。

00:02:52.000 --> 00:02:57.000
これは、量子化またはパレット化のいずれかによって達成することができます。

00:02:57.000 --> 00:03:06.000
これらの戦略は両方とも損失があり、圧縮されたモデルは通常、非圧縮モデルと比較して精度がわずかに低い。

00:03:06.000 --> 00:03:10.000
それでは、これらのテクニックのそれぞれを詳しく見てみましょう。

00:03:10.000 --> 00:03:15.000
重量剪定は、まばらな表現でモデルの重みを効率的に梱包するのに役立ちます。

00:03:15.000 --> 00:03:20.000
重み行列をスパース化または剪定することは、重み値の一部を0に設定することを意味します。

00:03:20.000 --> 00:03:22.000
私はウェイトマトリックスから始めます。

00:03:22.000 --> 00:03:27.000
それを剪定するには、最小のマグニチュードウェイトを0に設定できます。

00:03:27.000 --> 00:03:32.000
今、私はゼロ以外の値を格納するだけです。

00:03:32.000 --> 00:03:36.000
導入されたゼロごとに約2バイトのストレージを節約することになります。

00:03:36.000 --> 00:03:43.000
もちろん、後で濃密な行列を再構築するために、ゼロの位置も保存する必要があります。

00:03:43.000 --> 00:03:47.000
モデルサイズは、導入されたスパースのレベルとともに直線的に低下します。

00:03:47.000 --> 00:04:01.000
50%スパースモデルは、その重量の50%がゼロであることを意味し、ResNet50モデルの場合、サイズは約28メガバイトで、これはFloat16サイズの約半分です。

00:04:01.000 --> 00:04:08.000
2番目の重み圧縮技術は量子化で、8ビットの精度を使用して重みを格納します。

00:04:08.000 --> 00:04:17.000
量子化を実行するには、重み値とスケールを取り、シフトし、INT8の範囲になるように丸めます。

00:04:17.000 --> 00:04:26.000
この例では、スケールは2.35で、最小値を-127にマッピングし、バイアスは0です。

00:04:26.000 --> 00:04:33.000
モデルによっては、非ゼロバイアスも使用でき、量子化誤差を減らすのに役立つ場合があります。

00:04:33.000 --> 00:04:40.000
スケールとバイアスは、後で重みを非量子化して元の範囲に戻すことができます。

00:04:40.000 --> 00:04:47.000
重量精度を8ビット以下に下げるには、重量クラスタリングまたはパレット化と呼ばれる技術を使用できます。

00:04:47.000 --> 00:04:55.000
この手法では、同様の値を持つ重みがグループ化され、それらが属するクラスター重心の値を使用して表されます。

00:04:55.000 --> 00:04:58.000
これらの重心はルックアップテーブルに保管されています。

00:04:58.000 --> 00:05:05.000
そして、元の重み行列はインデックステーブルに変換され、各要素は対応するクラスター中心を指します。

00:05:05.000 --> 00:05:14.000
この例では、私は4つのクラスターを持っているので、2ビットを使用して各重みを表すことができ、Float16で8倍の圧縮を実現します。

00:05:14.000 --> 00:05:23.000
重みを表すために使用できる一意のクラスターセンターの数は、nの累乗に対して2に等しく、nはパレット化に使用される精度です。

00:05:23.000 --> 00:05:27.000
したがって、4ビットのパレット化は、16のクラスターを持つことができることを意味します。

00:05:27.000 --> 00:05:35.000
クオンタイゼーションはモデルサイズを半分に減らしますが、パレット化は最大8倍に小さくするのに役立ちます。

00:05:35.000 --> 00:05:40.000
要約すると、重量圧縮には3つの異なるテクニックがあります。

00:05:40.000 --> 00:05:44.000
それぞれが異なる方法で重みを表現しています。

00:05:44.000 --> 00:05:53.000
それらは、剪定のためのスパースの量やパレット化のためのビット数など、それぞれのパラメータによって制御できるさまざまなレベルの圧縮を提供します。

00:05:53.000 --> 00:05:58.000
次に、これらのテクニックをモデル開発ワークフローに統合する方法を説明します。

00:05:58.000 --> 00:06:02.000
まず、Core MLモデル変換のワークフローから始めましょう。

00:06:02.000 --> 00:06:11.000
お気に入りのPythonトレーニングフレームワークでモデルをトレーニングすることから始めて、Core ML Toolsを使用してそのモデルをCore MLに変換できます。

00:06:11.000 --> 00:06:17.000
このワークフローはさらに一歩拡張して、トレーニング後の圧縮ワークフローになることができます。

00:06:17.000 --> 00:06:25.000
これを行うには、全体的なサイズを小さくするために、すでに訓練され変換されたモデルウェイトで動作する圧縮ステップを追加します。

00:06:25.000 --> 00:06:28.000
このワークフローはいつでも開始できることに注意してください。

00:06:28.000 --> 00:06:36.000
たとえば、トレーニングデータやすでに変換されたCore MLモデルを必要としない、事前にトレーニングされたモデルから始めることができます。

00:06:36.000 --> 00:06:41.000
このワークフローを適用すると、適用される圧縮の量を選択するオプションがあります。

00:06:41.000 --> 00:06:48.000
圧縮を適用すればするほど、結果のモデルは小さくなりますが、予想通り、いくつかのトレードオフがあります。

00:06:48.000 --> 00:06:54.000
具体的には、一定の精度を達成する非圧縮モデルから始めます。

00:06:54.000 --> 00:07:01.000
圧縮を適用すると、モデルのサイズが小さくなりますが、精度にも影響する可能性があります。

00:07:01.000 --> 00:07:09.000
より多くの圧縮を適用すると、この影響がより顕著になり、精度の低下が受け入れられなくなる可能性があります。

00:07:09.000 --> 00:07:15.000
この傾向と許容可能なトレードオフは、ユースケースごとに異なり、モデルとデータセットに依存します。

00:07:15.000 --> 00:07:21.000
このトレードオフを実際に見るために、画像内のオブジェクトをセグメント化するモデルを見てみましょう。

00:07:21.000 --> 00:07:26.000
私の画像では、モデルはソファに属する各ピクセルの確率を返します。

00:07:26.000 --> 00:07:29.000
ベースラインFloat16モデルは、オブジェクトを非常にうまくセグメント化します。

00:07:29.000 --> 00:07:33.000
10%の剪定されたモデルの場合、出力は基本モデルと非常によく似ています。

00:07:33.000 --> 00:07:40.000
アーティファクトは30%のスパーシティで現れ始め、より高いレベルで増加します。

00:07:40.000 --> 00:07:47.000
40%の剪定になると、モデルは完全に故障し、確率マップが認識できなくなります。

00:07:47.000 --> 00:07:53.000
同様に、8ビットの量子化と6ビットのパレット化は、基本モデルの出力を保持します。

00:07:53.000 --> 00:08:03.000
4ビットのパレット化では、いくつかのアーティファクトが見え始め、2ビットのパレット化では、モデルはオブジェクトを完全にセグメント化できません。

00:08:03.000 --> 00:08:08.000
より高い圧縮率でモデルパフォーマンスの低下を克服するには、別のワークフローを使用できます。

00:08:08.000 --> 00:08:11.000
このワークフローはトレーニング時間圧縮と呼ばれます。

00:08:11.000 --> 00:08:16.000
ここでは、重みを圧縮しながら、いくつかのデータでモデルを微調整します。

00:08:16.000 --> 00:08:24.000
圧縮は、重みが課せられた新しい制約に再調整できるように、徐々に微分可能な方法で導入されます。

00:08:24.000 --> 00:08:31.000
モデルが満足のいく精度を達成したら、それを変換して圧縮されたCore MLモデルを取得できます。

00:08:31.000 --> 00:08:39.000
既存のモデルトレーニングワークフローにトレーニング時間の圧縮を組み込むか、事前にトレーニングされたモデルから始めることができることに注意してください。

00:08:39.000 --> 00:08:50.000
トレーニング時間の圧縮により、モデルの精度と圧縮量の間のトレードオフが改善され、より高い圧縮率で同じモデルのパフォーマンスを維持できます。

00:08:50.000 --> 00:08:53.000
同じ画像セグメンテーションモデルをもう一度見てみましょう。

00:08:53.000 --> 00:08:58.000
トレーニング時間の剪定のために、モデルの出力は40%のスパースまで変更されていません。

00:08:58.000 --> 00:09:02.000
これは、トレーニング後の精度が崩壊したところです。

00:09:02.000 --> 00:09:10.000
実際、今では50%と75%のスパースでも、このモデルは基本モデルと同様の確率マップを達成しています。

00:09:10.000 --> 00:09:16.000
モデルの精度の大幅な低下を観察し始めるのは90%のスパースです。

00:09:16.000 --> 00:09:24.000
同様に、トレーニング時間の量子化とパレット化は、この場合最大2ビットの圧縮であっても、ベースラインモデルの出力も保持します。

00:09:24.000 --> 00:09:31.000
要約すると、モデル変換中またはモデルトレーニング中に重量圧縮を適用できます。

00:09:31.000 --> 00:09:36.000
後者は、より長いトレーニング時間を犠牲にして、より良い精度のトレードオフを提供します。

00:09:36.000 --> 00:09:44.000
2番目のワークフローはトレーニング中に圧縮を適用するため、圧縮アルゴリズムを実装するために微分可能な操作を使用する必要があります。

00:09:44.000 --> 00:09:49.000
それでは、これらの圧縮ワークフローをCore ML Toolsでどのように実行できるかを探りましょう。

00:09:49.000 --> 00:09:59.000
トレーニング後のモデル圧縮APIは、圧縮utilsサブモジュールの下での剪定、パレット化、および量子化のためにCore ML Tools 6で利用可能です。

00:09:59.000 --> 00:10:03.000
しかし、トレーニング時間圧縮用のAPIはありませんでした。

00:10:03.000 --> 00:10:10.000
Core ML Tools 7では、トレーニング時間の圧縮機能を提供するために、新しいAPIが追加されました。

00:10:10.000 --> 00:10:17.000
そして、古いAPIと新しいAPIをcoremltools.optimizeと呼ばれる単一のモジュールに統合しました。

00:10:17.000 --> 00:10:28.000
トレーニング後の圧縮APIはcoremltools.optimize.coremlで移行され、新しいトレーニング時間APIはcoremltools.optimize.torchで利用できます。

00:10:28.000 --> 00:10:31.000
後者はPyTorchモデルで動作します。

00:10:31.000 --> 00:10:35.000
まず、トレーニング後のAPIを詳しく見てみましょう。

00:10:35.000 --> 00:10:40.000
トレーニング後の圧縮ワークフローでは、入力はCore MLモデルです。

00:10:40.000 --> 00:10:49.000
これは、私が説明した3つの圧縮技術のそれぞれを適用するOptimize.coremlモジュールで利用可能な3つの方法によって更新することができます。

00:10:49.000 --> 00:10:57.000
これらのメソッドを使用するには、まずOptimizationConfigオブジェクトを作成し、モデルを圧縮する方法を説明します。

00:10:57.000 --> 00:11:02.000
ここでは、75%のターゲットスパースでマグニチュードプルーニングをしています。

00:11:02.000 --> 00:11:07.000
設定が定義されたら、prune_weightsメソッドを使用してモデルをプルーニングできます。

00:11:07.000 --> 00:11:10.000
圧縮されたモデルを取得するためのシンプルでワンステップのプロセスです。

00:11:10.000 --> 00:11:17.000
これらのテクニックに固有の設定を使用して、重みをパレット化および量子化するために同様のAPIを使用できます。

00:11:17.000 --> 00:11:20.000
トレーニング時間の圧縮ワークフローを考えてみましょう。

00:11:20.000 --> 00:11:24.000
この場合、先に説明したように、トレーニング可能なモデルとデータが必要です。

00:11:24.000 --> 00:11:32.000
具体的には、Core ML Toolsでモデルを圧縮するには、おそらく事前にトレーニングされた重みを持つPyTorchモデルから始めます。

00:11:32.000 --> 00:11:41.000
次に、Optimize.torchモジュールで利用可能なAPIの1つを使用して更新し、圧縮レイヤーが挿入された新しいPyTorchモデルを取得します。

00:11:41.000 --> 00:11:46.000
そして、データと元のPyTorchトレーニングコードを使用して、それを微調整します。

00:11:46.000 --> 00:11:51.000
これは、圧縮を可能にするために重みが調整されるステップです。

00:11:51.000 --> 00:11:56.000
そして、MPS PyTorchバックエンドを使用して、ローカルでMacBookでこのステップを実行できます。

00:11:56.000 --> 00:12:01.000
モデルが精度を取り戻すように訓練されたら、それを変換してCore MLモデルを取得します。

00:12:01.000 --> 00:12:04.000
コード例を通して、これをさらに詳しく見てみましょう。

00:12:04.000 --> 00:12:09.000
圧縮したいモデルを微調整するために必要なPyTorchコードから始めます。

00:12:09.000 --> 00:12:15.000
わずか数行のコードを追加することで、Core ML Toolsを簡単に活用して、トレーニング時間の剪定を追加できます。

00:12:15.000 --> 00:12:20.000
まず、モデルをプルーニングする方法を説明するMagnitudePrunerConfigオブジェクトを作成します。

00:12:20.000 --> 00:12:24.000
ここでは、目標のスパーシティを75%に設定しています。

00:12:24.000 --> 00:12:29.000
設定をyamlファイルに書き込み、from_yamlメソッドを使用してロードすることもできます。

00:12:29.000 --> 00:12:34.000
次に、圧縮するモデルと作成した設定でプルーナーオブジェクトを作成します。

00:12:34.000 --> 00:12:38.000
次に、準備を呼び出すと、モデルに剪定レイヤーを挿入します。

00:12:38.000 --> 00:12:43.000
モデルを微調整しながら、ステップAPIを呼び出すと、プルーナーの内部状態が更新されます。

00:12:43.000 --> 00:12:48.000
トレーニングの最後に、剪定マスクを重みに折りたたむためにファイナライズを呼び出します。

00:12:48.000 --> 00:12:52.000
このモデルは、変換APIを使用してCore MLに変換できます。

00:12:52.000 --> 00:12:57.000
同じワークフローを量子化とパレット化にも使用できます。

00:12:57.000 --> 00:13:05.000
さて、Srijanは、Core ML Tools APIを使用してオブジェクト検出モデルをパレット化する方法を示すデモを紹介します。

00:13:05.000 --> 00:13:06.000
スリジャン:ありがとう、プルキット。

00:13:06.000 --> 00:13:13.000
私の名前はSrijanです。Core ML Tools optimize APIのデモを案内します。

00:13:13.000 --> 00:13:21.000
画像内の人を検出するために、ResNet18バックボーンを備えたSSDモデルを使用します。

00:13:21.000 --> 00:13:25.000
まず、いくつかの基本的なモデルとトレーニングユーティリティをインポートしましょう。

00:13:25.000 --> 00:13:32.000
先ほど話したSSD ResNet18モデルのインスタンスを入手することから始めます。

00:13:32.000 --> 00:13:40.000
物事を簡素化するために、事前に書かれたget_ssd_modelユーティリティを呼び出すだけです。

00:13:40.000 --> 00:13:44.000
モデルがロードされたので、いくつかのエポックのためにそれを訓練しましょう。

00:13:44.000 --> 00:13:53.000
物体検出モデルであるため、トレーニングの目標は、検出タスクのSSDの損失を減らすことです。

00:13:53.000 --> 00:14:08.000
簡潔さのために、train_epochユーティリティは、異なるバッチを介してフォワードの呼び出し、損失の計算、勾配降下の実行など、エポックのモデルを訓練するために必要なコードをカプセル化します。

00:14:08.000 --> 00:14:12.000
トレーニング中、SSDの損失は減少しているようです。

00:14:12.000 --> 00:14:16.000
今からモデルをCore MLモデルに変換します。

00:14:16.000 --> 00:14:24.000
これを行うには、まずモデルをトレースしてから、coremltools.convert APIを呼び出します。

00:14:24.000 --> 00:14:30.000
インポートされたユーティリティを呼び出して、モデルのサイズを確認しましょう。

00:14:30.000 --> 00:14:34.000
モデルのサイズは23.6メガバイトです。

00:14:34.000 --> 00:14:38.000
次に、Core MLモデルで予測を実行します。

00:14:38.000 --> 00:14:45.000
私はロンドン旅行から自分の画像と、検出をテストするために別の画像を選択しました。

00:14:45.000 --> 00:15:05.000
モデルがオブジェクトを検出するための信頼しきい値は30%に設定されているため、オブジェクトが存在することを少なくとも30%確信しているボックスのみをプロットします。

00:15:05.000 --> 00:15:07.000
その検出は的確なようです。

00:15:07.000 --> 00:15:12.000
私は今、このモデルのサイズを小さくできるかどうか興味があります。

00:15:12.000 --> 00:15:16.000
私は最初にトレーニング後のパレット化を試してみるつもりです。

00:15:16.000 --> 00:15:26.000
そのために、coremltools.optimize.coremlからいくつかの設定クラスとメソッドをインポートします。

00:15:26.000 --> 00:15:30.000
私は今、モデルの重みを6ビットでパレット化するつもりです。

00:15:30.000 --> 00:15:38.000
そのために、OpPalettizerConfigオブジェクトを作成し、モードをkmeans、nbitsを6として指定します。

00:15:38.000 --> 00:15:45.000
これにより、opレベルでパラメータが指定され、各opを異なる方法でパラメータ化できます。

00:15:45.000 --> 00:15:51.000
しかし、今、私はすべての操作に同じ6ビットモードを適用するつもりです。

00:15:51.000 --> 00:16:00.000
OptimizationConfigを定義して、このop_configをグローバルパラメータとして渡します。

00:16:00.000 --> 00:16:10.000
その後、最適化設定は、変換されたモデルとともに palettize_weights メソッドに渡され、パレット化されたモデルを取得します。

00:16:10.000 --> 00:16:15.000
サイズが今までに減ったものを見てみましょう。

00:16:15.000 --> 00:16:22.000
モデルのサイズは約9メガバイトに減少しましたが、テスト画像のパフォーマンスに影響しましたか？

00:16:22.000 --> 00:16:24.000
調べてみましょう。

00:16:24.000 --> 00:16:27.000
うわー、検出はまだうまく機能します。

00:16:27.000 --> 00:16:35.000
私は今、2ビットのトレーニング後のパレット化を試すことに私の運をプッシュすることに本当に興奮しています。

00:16:35.000 --> 00:16:48.000
これを行うには、OpPalettizerConfigでnbitsを6から2に変更し、palettize_weights APIを再度実行するだけです。

00:16:48.000 --> 00:16:57.000
ユーティリティを使用して、このCore MLモデルのサイズとパフォーマンスを見てみましょう。

00:16:57.000 --> 00:17:03.000
予想通り、モデルのサイズは縮小し、約3メガバイトに減少しました。

00:17:03.000 --> 00:17:11.000
しかし、モデルは両方の画像で人を検出できないため、パフォーマンスは最適ではありません。

00:17:11.000 --> 00:17:21.000
モデルによって予測されたボックスのいずれも、30%のしきい値を超える信頼確率がないため、予測にはボックスは表示されません。

00:17:21.000 --> 00:17:28.000
2ビットのトレーニング時間のパレット化を試して、それがより良いパフォーマンスを発揮するかどうかを見てみましょう。

00:17:28.000 --> 00:17:38.000
これを行うには、coremltools.optimize.torchからDKMPalettizerConfigとDKMPalettizerをインポートすることから始めます。

00:17:38.000 --> 00:17:48.000
DKMは、アテンションベースの微分可能なkmeans操作を実行することによって、重みクラスターを学習するアルゴリズムです。

00:17:48.000 --> 00:17:52.000
さあ、パレット化設定を定義する時が来ました。

00:17:52.000 --> 00:18:01.000
Global_configでn_bitsを2として指定するだけで、サポートされているすべてのモジュールが2ビット化されます。

00:18:01.000 --> 00:18:07.000
そしてここで、モデルと設定からパレットイザーオブジェクトを作成します。

00:18:07.000 --> 00:18:13.000
今すぐ準備APIを呼び出すと、パレット化に優しいモジュールがモデルに挿入されます。

00:18:13.000 --> 00:18:17.000
いくつかのエポックのためにモデルを微調整する時間です。

00:18:17.000 --> 00:18:29.000
モデルが微調整されたので、パレット化された重みをモデルの重みとして復元し、プロセスを完了するfinalize APIを呼び出します。

00:18:29.000 --> 00:18:32.000
次のステップは、モデルのサイズを確認することです。

00:18:32.000 --> 00:18:38.000
そのために、トーチモデルをCore MLモデルに変換します。

00:18:38.000 --> 00:18:42.000
Torch.jit.traceを使ってモデルをトレースすることから始めましょう。

00:18:42.000 --> 00:18:53.000
変換APIを呼び出し、今回はPassPipelineという追加のフラグを使用し、その値をDEFAULT_PALETTIZATIONに設定します。

00:18:53.000 --> 00:19:03.000
これは、変換された重みにパレット化された表現を使用するようにコンバーターに示します。

00:19:03.000 --> 00:19:08.000
テスト画像でモデルのサイズとその性能を見てみましょう。

00:19:08.000 --> 00:19:23.000
トレーニングタイムパレット化されたモデルも約3メガバイトであり、8倍の圧縮になることがわかりますが、トレーニング後のパレット化されたモデルとは異なり、このモデルはテスト画像の検出を正しく実行しています。

00:19:23.000 --> 00:19:29.000
これはデモだったので、2つのサンプル画像でモデルのパフォーマンスをテストしました。

00:19:29.000 --> 00:19:38.000
現実世界のシナリオでは、平均平均精度などのメトリックを使用し、検証データセットで評価します。

00:19:38.000 --> 00:19:40.000
要約しましょう。

00:19:40.000 --> 00:19:48.000
私は訓練されたモデルから始めて、Float16の重みを持つ23.6メガバイトのモデルを得るためにそれを変換しました。

00:19:48.000 --> 00:19:57.000
次に、palettize_weights APIを使用して、6ビットの重みを持つ小さなモデルをすばやく取得し、データでうまく機能しました。

00:19:57.000 --> 00:20:04.000
しかし、さらに2ビットにプッシュすると、パフォーマンスが明らかに低下しました。

00:20:04.000 --> 00:20:15.000
これを投稿し、私は optimize.torch APIsでトーチモデルを更新し、微分可能なkmeansアルゴリズムを使用していくつかのエポックを微調整しました。

00:20:15.000 --> 00:20:22.000
私はそれにより、2ビット圧縮オプションで良好な精度を得ることができました。

00:20:22.000 --> 00:20:40.000
デモでは特定のモデルと最適化アルゴリズムの組み合わせを採用していますが、このワークフローはユースケースに一般化され、必要な圧縮の量とモデルの再トレーニングに必要な時間とデータとの間のトレードオフを把握するのに役立ちます。

00:20:40.000 --> 00:20:44.000
これは私たちの最後のトピック、パフォーマンスに私たちをもたらします。

00:20:44.000 --> 00:20:55.000
アプリで展開されたときに、このようなモデルをより効率的に実行するために、Core MLランタイムに加えられた改善について簡単に触れたいと思います。

00:20:55.000 --> 00:21:02.000
iOS 16とiOS 17のランタイムのいくつかの重要な違いを見てみましょう。

00:21:02.000 --> 00:21:14.000
iOS 16では、重量のみの圧縮モデルがサポートされていましたが、iOS 17では、8ビットアクティベーション量子化モデルも実行できます。

00:21:14.000 --> 00:21:31.000
iOS 16では、ウェイト圧縮モデルはフロートウェイトを持つ対応するモデルと同じ速度で実行されますが、iOS 17ではCore MLランタイムが更新され、圧縮モデルは特定のシナリオでより速く実行されるようになりました。

00:21:31.000 --> 00:21:40.000
同様のランタイムの改善は、macOS、tvOS、watchOSの新しいバージョンでも利用可能です。

00:21:40.000 --> 00:21:43.000
しかし、これらの改善はどのように達成されますか?

00:21:43.000 --> 00:22:03.000
重みのみが圧縮されるモデルでは、活性化は浮動小数点精度であるため、畳み込みや行列乗算などの操作が発生する前に、他の入力の精度と一致するように重み値を解凍する必要があります。

00:22:03.000 --> 00:22:10.000
この解凍のステップは、iOS 16のランタイムで事前に行われます。

00:22:10.000 --> 00:22:19.000
したがって、この場合、モデルは実行前にメモリ内の完全浮動小数点式精度モデルに変換されます。

00:22:19.000 --> 00:22:23.000
したがって、推論遅延に変化は観察されません。

00:22:23.000 --> 00:22:33.000
ただし、iOS 17では、特定のシナリオでは、操作が実行される直前に、重みが解凍されます。

00:22:33.000 --> 00:22:44.000
これには、すべての推論呼び出しで解凍を行うコストで、メモリからより小さなビットウェイトをロードするという利点があります。

00:22:44.000 --> 00:22:55.000
ニューラルエンジンなどの特定のコンピューティングユニット、およびメモリにバインドされている特定のタイプのモデルでは、これは推論の利益につながる可能性があります。

00:22:55.000 --> 00:23:08.000
これらのランタイムの利点を説明するために、私はいくつかのモデルを選択してプロファイリングし、Float16バリアントと比較して推論が加速される相対的な量をプロットしました。

00:23:08.000 --> 00:23:14.000
予想通り、スピードアップの量はモデルとハードウェアに依存します。

00:23:14.000 --> 00:23:21.000
これらは、iPhone 14 Pro Maxの4ビットパレット化モデルのスピードアップの範囲です。

00:23:21.000 --> 00:23:27.000
改善点は大きく5%から30%です。

00:23:27.000 --> 00:23:39.000
スパースモデルについても、モデルタイプに基づいてさまざまな改善があり、一部のモデルはFloat16バリアントよりも75%速く実行されます。

00:23:39.000 --> 00:23:46.000
今、疑問が生じます:最高のレイテンシパフォーマンスを得るための戦略は何ですか?

00:23:46.000 --> 00:23:56.000
これは、フロートモデルから始めて、Optimize.coreml APIを使用して、モデルのさまざまな表現を探索することです。

00:23:56.000 --> 00:24:02.000
モデルの再トレーニングを必要としないため、これは迅速です。

00:24:02.000 --> 00:24:05.000
次に、興味のあるデバイスでプロファイルします。

00:24:05.000 --> 00:24:16.000
このため、XcodeのCore MLパフォーマンスレポートは、操作が実行される場所など、推論に多くの可視性を提供します。

00:24:16.000 --> 00:24:21.000
次に、どの構成があなたに最高の利益をもたらすかに基づいてショートリストします。

00:24:21.000 --> 00:24:36.000
この後、精度の評価と改善に集中できます。モデルを完成させる前に、トーチとコアMLツールでトレーニング時間の圧縮を適用する必要があるかもしれません。

00:24:36.000 --> 00:24:50.000
要約すると、モデルのサイズを小さくすることが重要であり、今では新しいCore ML Tools APIでこれまで以上に簡単にそれを行うことができ、より低いメモリフットプリントと推論のスピードアップを実現できます。

00:24:50.000 --> 00:24:56.000
より多くのオプションとベンチマークデータを確認するには、ドキュメントをご覧ください。

00:24:56.000 --> 00:25:08.000
また、今日のスライドで取り上げなかったCore MLフレームワークの改善について語る「非同期予測でCore ML統合を改善する」ビデオにチューニングすることを強くお勧めします。

00:25:08.000 --> 23:59:59.000
ありがとう、そして幸せな圧縮。

