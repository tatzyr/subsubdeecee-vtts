WEBVTT

00:00:00.000 --> 00:00:11.000
ロブ：こんにちは。

00:00:11.000 --> 00:00:20.000
私はカメラソフトウェアチームのロブ・シムティスと、写真チームのセバスチャン・メディナです。「より反応の良いカメラ体験を創造する」というセッションへようこそ。

00:00:20.000 --> 00:00:28.000
AVFoundationキャプチャクラスとPhotoKitフレームワークで、多くの新しい強力なAPIを紹介します。

00:00:28.000 --> 00:00:31.000
まず、延期された写真処理について話します。

00:00:31.000 --> 00:00:37.000
次に、シャッターラグゼロで本当に「瞬間を捉える」方法を紹介します。

00:00:37.000 --> 00:00:41.000
第三に、新しいレスポンシブキャプチャAPIについて説明します。

00:00:41.000 --> 00:00:47.000
そして最後に、更新されたビデオエフェクトで応答するための別の方法を見ていきます。

00:00:47.000 --> 00:01:02.000
iOS 13以降では、AVCapturePhotoSettingの写真品質優先順位付け列挙値を使用して、応答性を変更したり、処理された写真をキャプチャして取り戻したり、次の写真を撮影したりできます。

00:01:02.000 --> 00:01:09.000
アプリは、適切な応答性を達成するために、さまざまな列挙値から選択できますが、画質に影響を与えました。

00:01:09.000 --> 00:01:13.000
または、常に品質価値を使用したい場合は、ショットツーショットの時間に影響を与える可能性があります。

00:01:13.000 --> 00:01:26.000
iOS 17では、このAPIを引き続き使用できますが、新しい補完的なAPIを導入しているため、高品質の写真を取得しながら、必要なショットを取得する可能性を高めることができます。

00:01:26.000 --> 00:01:29.000
私のチームがセッションのために構築したこのアプリをご案内します。

00:01:29.000 --> 00:01:39.000
各トグルスイッチを「オン」に切り替えることで、今年は新機能を有効にすることができ、より応答性の高い写真体験を作るためにコンセプトを1つずつ構築します。

00:01:39.000 --> 00:01:43.000
では、延期された写真処理から始めましょう。

00:01:43.000 --> 00:01:54.000
今日、AVCapturePhotoOutputから最高品質の写真を取得するために、写真をキャプチャするときに設定で「品質」の写真品質の優先順位付け列挙値を使用します。

00:01:54.000 --> 00:02:00.000
私たちの「バランスのとれた」値と「品質」の値のために、これは通常、ある種のマルチフレーム融合とノイズリダクションを含みます。

00:02:00.000 --> 00:02:05.000
iPhone 11 Pro以降のモデルでは、私たちの最も先進的な技術の1つは「ディープフュージョン」と呼ばれています。

00:02:05.000 --> 00:02:09.000
これは、高解像度の写真で驚くほど鮮明なディテールを提供します。

00:02:09.000 --> 00:02:14.000
このディープフュージョンショットでは、オウムの羽は超シャープで、本当に際立っています。

00:02:14.000 --> 00:02:16.000
しかし、それにはコストがかかります。

00:02:16.000 --> 00:02:22.000
この処理は、次のキャプチャ要求が開始される前に完了する必要があり、完了するまでに時間がかかる場合があります。

00:02:22.000 --> 00:02:23.000
実際の例を見てみましょう。 

00:02:23.000 --> 00:02:29.000
カメラソフトウェアチームが仕事を終わらせるためにオフィスに行く方法についてのプレゼンテーションをまとめています。

00:02:29.000 --> 00:02:34.000
同僚のデビンが最新の自転車技術を使ってアップルパークを歩き回っています。

00:02:34.000 --> 00:02:41.000
タップすると、次のショットを撮る前に、シャッターボタンがワンショットから回転し終わるのを待っています。

00:02:41.000 --> 00:02:44.000
最終結果は素晴らしいディープフュージョンの写真です。

00:02:44.000 --> 00:02:46.000
そのひげの詳細をチェックしてください!

00:02:46.000 --> 00:02:52.000
しかし、私がタップしている間、処理は同期的に実行され、ショットからショットへの時間は少し遅い感じがします。

00:02:52.000 --> 00:02:59.000
だから、鮮明なディテールで良い写真を手に入れたかもしれませんが、私が探していたショットではないかもしれません。

00:02:59.000 --> 00:03:02.000
イベントの図を見てみましょう。 

00:03:02.000 --> 00:03:10.000
設定でAVCapturePhotoOutputのcapturePhotoメソッドを呼び出すと、デリゲートはプロセスのさまざまなポイントでコールバックを受け取ります...

00:03:10.000 --> 00:03:13.000
resolvedSettingsのwillBeginCaptureなど。

00:03:13.000 --> 00:03:21.000
カメラソフトウェアスタックは、センサーからフレームをつかみ、当社の処理技術を使用してそれらをディープフュージョン画像に融合させます...

00:03:21.000 --> 00:03:26.000
その後、didFinishProcessingPhotoデリゲートコールバックを介して写真が返送されます。

00:03:26.000 --> 00:03:32.000
この処理は、次のキャプチャが発生する前に完了する必要があり、それには時間がかかる場合があります。

00:03:32.000 --> 00:03:41.000
didFinishProcessingPhotoコールバックが発火する前にcapturePhotoを呼び出すこともできますが、前の写真の処理が完了するまで開始されません。

00:03:41.000 --> 00:03:44.000
延期された写真処理では、このタイムラインは縮小します。

00:03:44.000 --> 00:03:55.000
写真を要求し、適切な場合、カメラパイプラインは、新しいdidFinishCapturingDeferredPhotoProxyデリゲートコールバックを介して、軽く処理された「プロキシ」写真を配信します。

00:03:55.000 --> 00:03:59.000
このプロキシ写真をプレースホルダとしてライブラリに保存します。

00:03:59.000 --> 00:04:02.000
そして、次の写真はすぐに撮影できます。

00:04:02.000 --> 00:04:09.000
カメラセッションが却下されると、システムは後で処理を実行して最終的な写真を取得します。

00:04:09.000 --> 00:04:20.000
したがって、アプリの設定で遅延写真処理をオンにすると、キャプチャセッションは、必要に応じてキャプチャ時にプロキシ写真を配信するように再構成されます。

00:04:20.000 --> 00:04:29.000
そして、私は以前と同じシャープで詳細な写真を得ることができますが、最終的な処理を後で延期することで、その瞬間にいる間にそれらをもっと撮ることができます。

00:04:29.000 --> 00:04:34.000
ナイスウイリー。それらの自転車はしっかりしています。

00:04:34.000 --> 00:04:39.000
行くよ。それは私のプレゼンテーション、ひげ、そしてすべてのための素晴らしい写真です。

00:04:39.000 --> 00:04:44.000
だから、あなたに延期処理された写真を与えるために相互作用するすべての部分を見てみましょう。

00:04:44.000 --> 00:04:58.000
以前のWWプレゼンテーションの簡単な復習コースとして、写真をキャプチャするためにAVCaptureSessionを設定するときは、カメラであるAVCaptureDeviceでAVCaptureDeviceInputをセッションに追加します。

00:04:58.000 --> 00:05:10.000
次に、セッションにAVCapturePhotoOutputを追加し、特定のフォーマットまたはセッションプリセット、通常はアプリがphotoOutputでcapturePhotoを呼び出すときに「写真」セッションプリセットを選択します。

00:05:10.000 --> 00:05:18.000
遅延写真処理に適したタイプの写真の場合は、didFinishCapturing Deferred Photo Proxyで折り返しお電話します。

00:05:18.000 --> 00:05:22.000
そこから、プロキシデータをフォトライブラリに送信します。

00:05:22.000 --> 00:05:30.000
だから、あなたが今あなたのライブラリに持っているのはプロキシ写真ですが、最終的には最終的な画像を使用または共有したいと思うでしょう。

00:05:30.000 --> 00:05:42.000
最終的な写真処理は、ライブラリから画像データを要求するときにオンデマンドで、またはデバイスがアイドル状態であるなど、システムがそうするのが良いと判断したときにバックグラウンドで行われます。

00:05:42.000 --> 00:05:46.000
そして今、同僚のセバスチャンにこれをコード化する方法を教えてもらいます。

00:05:46.000 --> 00:05:48.000
あなたに、セバスチャン。

00:05:48.000 --> 00:05:49.000
セバスチャン:ありがとう、ロブ。

00:05:49.000 --> 00:05:52.000
こんにちは、私の名前はセバスチャン・メディナで、写真チームのエンジニアです。

00:05:52.000 --> 00:05:58.000
今日は、PhotoKitを介して最近キャプチャされた画像を撮影して、遅延処理をトリガーします。

00:05:58.000 --> 00:06:05.000
次に、PHImageManagerリクエストから画像を受信する際の新機能を示すために、同じ画像をリクエストします。

00:06:05.000 --> 00:06:13.000
ただし、PhotoKitを通じて資産を処理する前に、遅延処理用の新しいカメラAPIが設定されていることを確認する必要があります。

00:06:13.000 --> 00:06:18.000
これにより、私のアプリは、PhotoKitを介して送信できる遅延写真プロキシ画像を受け入れることができます。

00:06:18.000 --> 00:06:22.000
さて、私は先に進んで、これを利用するためのコードを書きます。

00:06:22.000 --> 00:06:27.000
ここでは、AVCapturePhotoOutputとAVCaptureSessionオブジェクトを設定しました。

00:06:27.000 --> 00:06:31.000
これで、セッションの設定を開始できます。

00:06:31.000 --> 00:06:38.000
この場合、遅延処理を利用できるように、セッションにタイプ写真のプリセットが必要です。

00:06:38.000 --> 00:06:44.000
次に、キャプチャデバイスをつかんで、デバイス入力を設定します。

00:06:44.000 --> 00:06:50.000
その後、可能であれば、デバイス入力を追加します。

00:06:50.000 --> 00:06:55.000
次に、photoOutputを追加できるかどうかを確認します。

00:06:55.000 --> 00:06:59.000
もしそうなら、それを追加します。

00:06:59.000 --> 00:07:00.000
さて、新しいもののために。

00:07:00.000 --> 00:07:10.000
新しいautoDeferredPhotoDeliverySupported値が真であるかどうかを確認し、遅延処理でキャプチャした写真を送信できることを確認します。

00:07:10.000 --> 00:07:22.000
これが合格した場合、私は先に進み、プロパティautoDeferredPhotoDeliveryEnabledで新しい延期写真の配信にオプトインすることができます。

00:07:22.000 --> 00:07:29.000
この延期された写真の配信チェックと有効化は、延期された写真を有効にするためにカメラコードに追加する必要があるすべてです。

00:07:29.000 --> 00:07:34.000
最後に、セッション設定をコミットします。

00:07:34.000 --> 00:07:42.000
したがって、capturePhotoメソッドに呼び出しが行われると、受信したデリゲートコールバックは遅延プロキシオブジェクトを保持します。

00:07:42.000 --> 00:07:50.000
これらのコールバックの例を見てみましょう。1つ。

00:07:50.000 --> 00:08:01.000
このフォトキャプチャコールバックでは、最近キャプチャした画像に関連するAVCapturePhotoOutputとAVCaptureDeferredPhotoProxyオブジェクトをカメラから受信しています。

00:08:01.000 --> 00:08:11.000
まず、適切な写真出力値を受信していることを確認することをお勧めしますので、エラーパラメータの値を確認します。

00:08:11.000 --> 00:08:16.000
では、PhotoKitを使用して画像をフォトライブラリに保存し始めます。

00:08:16.000 --> 00:08:20.000
共有されたPHPhotoLibraryで変更を実行します。

00:08:20.000 --> 00:08:26.000
ただし、フォトライブラリへの書き込みアクセスのみが必要です。 注意してください。

00:08:26.000 --> 00:08:36.000
次に、AVCaptureDeferredPhotoProxyオブジェクトから写真データをキャプチャします。

00:08:36.000 --> 00:08:44.000
フォトライブラリの変更を実行するため、関連するperformChangesインスタンスメソッドを設定する必要があります。

00:08:44.000 --> 00:08:50.000
資産を保存する場合と同様に、PHAssetCreationRequestを使用します。

00:08:50.000 --> 00:08:56.000
次に、リクエストに応じて「addResource」メソッドを呼び出します。

00:08:56.000 --> 00:09:01.000
パラメータについては、新しいPHAssetResourceType「.photoProxy」を使用します。

00:09:01.000 --> 00:09:05.000
これは、画像で遅延処理をトリガーするようにPhotoKitに指示するものです。

00:09:05.000 --> 00:09:10.000
その後、以前にキャプチャしたプロキシ画像データを追加できます。

00:09:10.000 --> 00:09:15.000
そして、この場合、私はオプションを使用しません。

00:09:15.000 --> 00:09:22.000
ここでは、遅延処理を必要としない画像データにこの新しいリソースタイプを使用すると、エラーが発生することを知っておくことが重要です。

00:09:22.000 --> 00:09:28.000
そして、エラーといえば、私は先に進み、完了ハンドラー内でそれらをチェックします。

00:09:28.000 --> 00:09:29.000
そして、それはそれと同じくらい簡単です。

00:09:29.000 --> 00:09:35.000
先に進み、アプリケーションが適切と判断した場合、完了ハンドラ内の成功とエラーを処理してください。

00:09:35.000 --> 00:09:38.000
さて、私たちの資産を取り戻したいと言ってください。

00:09:38.000 --> 00:09:43.000
PHImageManagerリクエストでそれを達成できるので、それを行うためにコードを調べます。

00:09:43.000 --> 00:09:52.000
パラメータについては、PhotoKit経由で送信した画像のPHAssetオブジェクト、返される画像のターゲットサイズ、およびコンテンツモードがあります。

00:09:52.000 --> 00:10:05.000
デフォルトのPHImageManagerオブジェクトを取得し、imageManagerオブジェクトを使用して、requestImageForAssetメソッドのリクエストイメージアセットを呼び出すことができます。

00:10:05.000 --> 00:10:16.000
パラメータについては、以前にフェッチしたアセット、ターゲットサイズ、コンテンツモードを使用します。この場合、オプションは使用しません。

00:10:16.000 --> 00:10:25.000
これで、resultImageがUIImageであり、infoが画像に関連する辞書であるresultHandlerを介してコールバックを処理できます。

00:10:25.000 --> 00:10:36.000
今日、最初のコールバックは、情報ディクショナリーキーPHImageResultIsDegradedKeyで低解像度の画像を保持しますが、最終的な画像コールバックは保持しません。

00:10:36.000 --> 00:10:41.000
だから、私はここでそれらのためにチェックすることができます。

00:10:41.000 --> 00:10:53.000
PhotoKitを介して処理された画像を作成する追加は、開発者がrequestImageForAssetメソッドからセカンダリ画像を受け取ることを可能にする新しいAPIを立ち上げる良い機会を提供します。

00:10:53.000 --> 00:11:01.000
遅延処理を通過する画像が完成するのに時間がかかる可能性があるため、その間にこの新しいセカンダリ画像を表示できます。

00:11:01.000 --> 00:11:08.000
この新しい画像を受け取るには、PHImageRequestOptionsの新しいallowSecondaryDegradedImageを使用します。

00:11:08.000 --> 00:11:14.000
この新しいイメージは、requestImageForAssetメソッドからの現在の2つのコールバックの間に入れられます。

00:11:14.000 --> 00:11:23.000
また、画像に関連する情報辞書には、今日最初の画像コールバックで使用されているPHImageResultIsDegradedKeyのエントリがあります。

00:11:23.000 --> 00:11:29.000
何が起こっているかをよりよく説明するために、今日、requestImageForAssetメソッドは2つの画像を提供します。

00:11:29.000 --> 00:11:36.000
1つ目は、最終的な高品質の画像を準備している間、一時的に表示するのに適した低品質の画像です。

00:11:36.000 --> 00:11:44.000
この新しいオプションでは、現在の2つのオプションの間に、最終的な処理中に表示する新しい高解像度の画像が提供されます。

00:11:44.000 --> 00:11:51.000
この新しい画像を表示すると、最終的な画像が処理されるのを待っている間、ユーザーはより快適な視覚体験が得られます。

00:11:51.000 --> 00:11:55.000
では、これを利用するためのコードを書いてみましょう。

00:11:55.000 --> 00:12:02.000
ただし、今回はPHImageRequestOptionsオブジェクトを作成します。

00:12:02.000 --> 00:12:06.000
次に、新しいallowSecondaryDegradedImageオプションをtrueに設定します。

00:12:06.000 --> 00:12:11.000
このようにして、リクエストは新しいセカンダリイメージコールバックを返送することを知っています。

00:12:11.000 --> 00:12:22.000
ここでは、以前に書いたrequestImageForAssetメソッドを再利用することができますが、作成したばかりの画像リクエストオプションオブジェクトを追加します。

00:12:22.000 --> 00:12:33.000
新しいセカンダリ画像情報辞書は、最初のコールバックと同様に、PHImageResultIsDegradedKeyの真の値を保持するので、ここで確認します。

00:12:33.000 --> 00:12:37.000
そして、それは新しい二次画像表現を受け取るためのものです。

00:12:37.000 --> 00:12:41.000
アプリを最もよくサポートするために、結果ハンドラー内で画像を処理することを忘れないでください。

00:12:41.000 --> 00:12:54.000
これで、遅延処理でフォトライブラリに画像を追加する方法と、最終的な画像が処理を完了するのを待っている間にアプリに表示する画像リクエストから二次的な高品質の画像を受け取る方法がわかりました。

00:12:54.000 --> 00:13:04.000
これらの変更は、新しい延期処理PhotoKitの変更とともに、iOS 17、tvOS 17、macOS Sonomaから利用可能になります。

00:13:04.000 --> 00:13:10.000
では、より反応の良いカメラを作成するための新しいツールの詳細については、ロブに返します。

00:13:10.000 --> 00:13:12.000
ロブ：すごい。ありがとう、セバスチャン!

00:13:12.000 --> 00:13:17.000
延期された写真処理で素晴らしい経験を確実にするために、細かい詳細に入りましょう。

00:13:17.000 --> 00:13:19.000
フォトライブラリから始めます。

00:13:19.000 --> 00:13:31.000
遅延写真処理を使用するには、プロキシ写真を保存するためのフォトライブラリへの書き込み許可と、アプリが最終的な写真を表示する必要がある場合、または何らかの方法で変更したい場合は読み取り許可が必要です。

00:13:31.000 --> 00:13:38.000
しかし、顧客に代わって最大限のプライバシーと信頼を維持するために、顧客から必要に応じて図書館へのアクセスの最小量のみを要求する必要があることを覚えておいてください。

00:13:38.000 --> 00:13:46.000
また、プロキシを受け取ったら、できるだけ早くファイルデータ表現をライブラリに取り入れることを強くお勧めします。

00:13:46.000 --> 00:13:51.000
アプリがバックグラウンド化されている場合、システムが一時停止する前に実行する時間は限られています。

00:13:51.000 --> 00:13:58.000
メモリの圧力が大きすぎると、バックグラウンドのウィンドウ中にアプリがシステムによって自動的に強制的に終了される可能性があります。

00:13:58.000 --> 00:14:05.000
プロキシをできるだけ早くライブラリに取り込むことで、顧客のデータ損失の可能性を最小限に抑えることができます。

00:14:05.000 --> 00:14:22.000
次に、通常、フィルターの適用などの写真のピクセルバッファに変更を加える場合、またはAVCapturePhoto File Data Representation Customizerを使用してAVCapturePhotoのメタデータやその他のプロパティに変更を加える場合、処理が完了すると、これらはライブラリ内の完成した写真には有効になりません。

00:14:22.000 --> 00:14:28.000
PhotoKit APIを使用して写真を調整するため、後でこれを行う必要があります。

00:14:28.000 --> 00:14:34.000
また、コードは、同じセッションで遅延プロキシと非遅延写真の両方を処理できる必要があります。

00:14:34.000 --> 00:14:39.000
これは、すべての写真が必要な余分な手順で処理するのが理にかなっているわけではないからです。

00:14:39.000 --> 00:14:50.000
たとえば、「品質」の写真品質の優先順位付け列挙値の下で撮影されたフラッシュキャプチャは、Deep Fusion写真のようにショットツーショットの節約の恩恵を受ける方法で処理されません。

00:14:50.000 --> 00:14:55.000
また、AVCapturePhotoSettingsにオプトインまたはオプトアウトのプロパティがないことに気付くかもしれません。

00:14:55.000 --> 00:14:58.000
それは、延期された写真処理が自動だからです。

00:14:58.000 --> 00:15:05.000
オプトインすると、カメラパイプラインがより長い処理時間を必要とする写真を撮ると、プロキシが返送されます。

00:15:05.000 --> 00:15:11.000
適切でない場合は、最終的な写真が送信されますので、ショットごとにオプトインまたはオプトアウトする必要はありません。

00:15:11.000 --> 00:15:19.000
キャプチャセッションを開始する前に、isAutoDeferredPhotoProcessingEnabledをtrueとしてAVCapturePhotoOutputに伝えるだけです。

00:15:19.000 --> 00:15:22.000
最後に、ユーザーエクスペリエンスについて話しましょう。

00:15:22.000 --> 00:15:31.000
遅延写真処理は、迅速なショットツーショットタイムで最高の画質を提供しますが、最終的な処理が後になるまで遅れるだけです。

00:15:31.000 --> 00:15:42.000
あなたのアプリが、ユーザーが共有や編集のためにすぐに画像を欲しがる可能性があり、私たちが提供する最高品質の写真にそれほど興味がない場合は、延期された写真処理の使用を避けることは理にかなっているかもしれません。

00:15:42.000 --> 00:15:49.000
この機能は、iPhone 11 Proと11 Pro Max、およびそれ以降のiPhoneから利用できます。

00:15:49.000 --> 00:15:57.000
そして、AVCapturePhotoOutputでの作業とライブラリ権限の処理に関する素晴らしい関連ビデオをいくつか紹介します。

00:15:57.000 --> 00:16:03.000
そして今、ゼロシャッターラグに目を向けて、スケートボードについて話しましょう。

00:16:03.000 --> 00:16:10.000
カメラソフトウェアチームの輸送モードについての私の今後のプレゼンテーションのために、私たちはいくつかの映像をつかむためにスケートパークに行きました。

00:16:10.000 --> 00:16:20.000
私はiPhone 14 Proでアクションモードで同僚を撮影していますが、スライド用に高品質のヒーローアクションショットも撮りたいです。

00:16:20.000 --> 00:16:25.000
しかし、スポイラーアラート、私はスケートボードをしません。

00:16:25.000 --> 00:16:29.000
シャッターボタンをタップして、空気を吸っている同僚のトモの写真を撮ります。

00:16:29.000 --> 00:16:35.000
写真を調べるためにカメラロールに行くと、これが私が得たものでした。

00:16:35.000 --> 00:16:40.000
彼がジャンプの高さにいたとき、私はシャッターボタンをタップしましたが、写真は彼の着陸です。

00:16:40.000 --> 00:16:42.000
それはまさに私が望んでいたものではありません。

00:16:42.000 --> 00:16:46.000
それで、何が起こったの?シャッターラグ。

00:16:46.000 --> 00:16:49.000
シャッターラグが発生しました。

00:16:49.000 --> 00:16:58.000
「シャッターラグ」は、キャプチャをリクエストしてから、センサーから1つ以上のフレームを読み出して写真に融合して配信するまでの遅延と考えることができます。

00:16:58.000 --> 00:17:04.000
ここでは、時間は左から右へ、左は古いフレーム、右は新しいフレームです。

00:17:04.000 --> 00:17:07.000
フレーム5はカメラのファインダーにあるものだとしましょう。

00:17:07.000 --> 00:17:17.000
今日、AVCapturePhotoOutputの設定でcapturePhoto:を呼び出すと、カメラパイプラインはセンサーからフレームをつかみ始め、処理技術を適用します。

00:17:17.000 --> 00:17:22.000
しかし、キャプチャされたフレームのブラケットは、タッチダウン後、フレーム5の後に始まります。

00:17:22.000 --> 00:17:26.000
あなたが得るのは、フレーム6から9、またはそれ以降のフレームに基づく写真です。

00:17:26.000 --> 00:17:31.000
毎秒30フレームで、各フレームは33ミリ秒間ファインダーにあります。

00:17:31.000 --> 00:17:35.000
あまり聞こえませんが、行動が終わるのに本当に時間はかかりません。

00:17:35.000 --> 00:17:40.000
それはトモが着陸するのに十分な長さであり、私はそのヒーローショットを得るのを逃しました。

00:17:40.000 --> 00:17:47.000
ゼロシャッターラグを有効にすると、カメラパイプラインは過去のフレームのローリングリングバッファを保持します。

00:17:47.000 --> 00:18:03.000
さて、フレーム5はファインダーで見るもので、タップしてキャプチャし、カメラパイプラインは少しタイムトラベルを行い、リングバッファからフレームをつかみ、それらを融合させ、あなたが望む写真を取得します。

00:18:03.000 --> 00:18:18.000
だから今、アプリの設定ペインで2番目のトグルを使用してゼロシャッターラグを有効にすると、トモが空気をキャッチするので、シャッターボタンをタップすると、プレゼンテーションに欲しかった「ヒーロー」ショットの1つを手に入れました。

00:18:18.000 --> 00:18:24.000
アプリでゼロシャッターラグを取得するために何をする必要があるかについて話しましょう。

00:18:24.000 --> 00:18:27.000
絶対に何もない！

00:18:27.000 --> 00:18:39.000
AVCaptureSessionPresetsとAVCaptureDeviceFormatsのiOS 17以降にリンクするアプリで、ゼロシャッターラグを有効にしました。

00:18:39.000 --> 00:18:49.000
しかし、テスト中に望む結果が得られないことがわかった場合は、AVCapturePhotoOutput.isZeroShutter LagEnabledをfalseに設定してオプトアウトすることができます。

00:18:49.000 --> 00:19:00.000
また、出力がセッションに接続されると、isZeroShutterLagSupportedがtrueかどうかを確認することで、photoOutputが設定されたプリセットまたはフォーマットのゼロシャッターラグをサポートしているかどうかを確認できます。

00:19:00.000 --> 00:19:17.000
フラッシュキャプチャ、手動露出用のAVCaptureDeviceの設定、ブラケットキャプチャ、複数のカメラからフレームを同期した構成写真配信などの特定の種類の静止画キャプチャは、ゼロシャッターラグを取得しません。

00:19:17.000 --> 00:19:30.000
カメラパイプラインはリングバッファからフレームをつかむために時間をさかのぼっているため、キャプチャを開始するジェスチャーと写真出力写真設定を送信するときに長い遅延がある場合、ユーザーは写真にカメラの揺れを誘発する可能性があります。

00:19:30.000 --> 00:19:36.000
したがって、タップイベントと写真出力のcapturePhoto API呼び出しの間に行う作業を最小限に抑えたいと思うでしょう。

00:19:36.000 --> 00:19:44.000
よりレスポンシブな写真体験を作るための機能を締めくくり、レスポンシブキャプチャAPIについて説明します。

00:19:44.000 --> 00:19:56.000
これは、顧客が重複するキャプチャを撮影し、写真の品質を調整することでショットツーショットの時間に優先順位を付け、次の写真を撮ることができるときに優れたUIフィードバックを提供するAPIのグループです。

00:19:56.000 --> 00:19:59.000
まず、メインAPI、レスポンシブキャプチャ。

00:19:59.000 --> 00:20:05.000
スケートパークに戻って、以前に2つの機能を有効にすると、毎秒約2枚の写真を撮ることができます。

00:20:05.000 --> 00:20:09.000
明確にするために、映像を遅くしました。

00:20:09.000 --> 00:20:18.000
毎秒2フレームでは、空中でのトモの行動はあまり見えず、これは私が終わった最高の写真でした。

00:20:18.000 --> 00:20:21.000
かなり良いですが、もっとうまくできるかどうか見てみましょう。

00:20:21.000 --> 00:20:26.000
次に、3番目と4番目のスイッチをオンにして、レスポンシブキャプチャ機能を有効にします。

00:20:26.000 --> 00:20:30.000
ファストキャプチャの優先順位付けを少し確認します。

00:20:30.000 --> 00:20:32.000
しかし、まず、公園に戻ってください!

00:20:32.000 --> 00:20:36.000
そして、もう一度やってみましょう。

00:20:36.000 --> 00:20:44.000
レスポンシブキャプチャを使用すると、同じ時間でより多くの写真を撮ることができ、適切な写真を撮る可能性が高くなります。

00:20:44.000 --> 00:20:47.000
そして、私のプレゼンテーションの開始のための「ヒーロー」ショットがあります。

00:20:47.000 --> 00:20:50.000
チームはきっと気に入るよ！

00:20:50.000 --> 00:21:05.000
設定方法を使用したAVCapturePhotoOutput.capturePhotoへの呼び出しは、センサーからフレームをキャプチャし、それらのフレームを最終的な非圧縮画像に処理し、写真をHEICまたはJPEGにエンコードする3つの異なるフェーズを経ると考えることができます。

00:21:05.000 --> 00:21:20.000
エンコーディングが完了した後、写真の出力はデリゲートのdidFinishProcessingPhotoコールバックを呼び出すか、遅延写真処理APIにオプトインした場合は、適切なショットであれば、おそらくdidFinishCapturing Deferred Photo Proxyを呼び出します。

00:21:20.000 --> 00:21:26.000
しかし、「キャプチャ」フェーズが完了し、「処理」が開始されると、写真の出力は理論的には別のキャプチャを開始する可能性があります。

00:21:26.000 --> 00:21:31.000
そして今、その理論は現実であり、あなたのアプリで利用可能です。

00:21:31.000 --> 00:21:45.000
メインのレスポンシブキャプチャAPIを選択すると、写真出力がこれらのフェーズと重なり、別のリクエストが処理段階にある間に新しい写真キャプチャリクエストを開始でき、顧客により速く、より一貫性のあるバックツーバックショットを提供します。

00:21:45.000 --> 00:21:57.000
これにより、写真出力で使用されるピークメモリが増加するため、アプリも多くのメモリを使用している場合は、システムに圧力をかけます。その場合、オプトアウトを好むか、オプトアウトする必要があるかもしれません。

00:21:57.000 --> 00:22:02.000
タイムライン図に戻ると、ここで2枚の写真を連続して撮ります。

00:22:02.000 --> 00:22:11.000
あなたの代理人は、willBeginCaptureFor resolvedSettingsのために呼び戻され、写真AのdidFinishCaptureFor resolvedSettingsのために呼び戻されます。

00:22:11.000 --> 00:22:24.000
しかし、写真がエンコードされて配信される写真AのdidFinish Processing Photoコールバックを取得する代わりに、写真Bの最初のwillBeginCapture For resolvedSettingsを取得できます。

00:22:24.000 --> 00:22:32.000
現在、機内写真リクエストが2つあるため、コードがインターリーブ写真のコールバックを適切に処理していることを確認する必要があります。

00:22:32.000 --> 00:22:38.000
これらの重複したレスポンシブキャプチャを取得するには、サポートされているときにまずゼロシャッターラグを有効にします。

00:22:38.000 --> 00:22:42.000
レスポンシブキャプチャサポートを受けるには、オンになっていなければなりません。

00:22:42.000 --> 00:22:53.000
次に、AVCapturePhotoOutput isResponsiveCaptureSupported APIを使用して、写真出力がプリセットまたはフォーマットでサポートしていることを確認し、AVCapturePhotoOutputを設定してオンにします。

00:22:53.000 --> 00:22:56.000
.isResponsiveCaptureEnabled to true。

00:22:56.000 --> 00:23:02.000
先ほど、「高速キャプチャの優先順位付け」を有効にしたので、簡単に説明します。

00:23:02.000 --> 00:23:17.000
写真出力のためにオンにすると、短期間で複数のキャプチャが撮影されているときに検出され、それに応じて、ショットツーショット時間を維持するために、最高品質の設定からより多くの「バランスのとれた」品質設定に写真の品質を適応させます。

00:23:17.000 --> 00:23:22.000
しかし、これは写真の品質に影響を与える可能性があるため、デフォルトではオフになっています。

00:23:22.000 --> 00:23:26.000
Camera.appの設定ペインでは、これは「より速い撮影を優先する」と呼ばれています。

00:23:26.000 --> 00:23:38.000
デフォルトでは、一貫したショットツーショット時間がより重要であると考えているため、Camera.appでデフォルトでオンにすることを選択しましたが、アプリと顧客によって異なる選択をするかもしれません。

00:23:38.000 --> 00:23:53.000
予想通り、サポートされているときに写真出力の「高速キャプチャの優先順位付けがサポートされています」プロパティを確認できます。また、サポートされている場合は、あなたや顧客がこの機能を使用したい場合は、「高速キャプチャの優先順位付けが有効になっています」をtrueに設定できます。

00:23:53.000 --> 00:23:57.000
では、ボタンの状態と外観の管理についておしゃべりしましょう。

00:23:57.000 --> 00:24:06.000
写真の出力は、次のキャプチャを開始する準備ができているとき、または処理されているときの指標を与えることができ、写真のキャプチャボタンを適切に更新することができます。

00:24:06.000 --> 00:24:11.000
これは、AVCapturePhotoOutput CaptureReadinessと呼ばれる値の列挙型を介して行われます。

00:24:11.000 --> 00:24:21.000
写真の出力は、「実行中」、「準備完了」、および「一時的に」、「キャプチャを待っている」、または「処理を待っている」の3つの「準備ができていない」状態にすることができます。

00:24:21.000 --> 00:24:33.000
「準備ができていない」列挙型は、設定でcapturePhotoを呼び出すと、キャプチャと写真の配信の間に待ち時間が長くなり、以前に話したシャッターラグが増加することを示しています。

00:24:33.000 --> 00:24:40.000
アプリは、新しいクラスAVCapturePhotoOutputReadinessCoordinatorを使用して、この状態の変化を聞くことができます。

00:24:40.000 --> 00:24:45.000
これにより、写真出力の準備状況が変更されたときに、指定したデリゲートオブジェクトにコールバックされます。

00:24:45.000 --> 00:24:52.000
レスポンシブキャプチャまたはファストキャプチャ優先順位付けAPIを使用していなくても、このクラスを使用できます。

00:24:52.000 --> 00:25:01.000
Readiness CoordinatorとReadiness列挙型を使用して、シャッターの可用性を伝え、ボタンの外観を変更する方法は次のとおりです。

00:25:01.000 --> 00:25:14.000
私たちのセッションのアプリは、「準備ができていない」列挙値を処理するときにキャプチャボタンのユーザーインタラクションイベントをオフにし、追加のリクエストが複数のタップで誤ってキューに入れられるのを防ぎ、シャッターラグが長くなります。

00:25:14.000 --> 00:25:24.000
タップし、設定要求付きの capturePhoto がキューにされた後、captureReadiness 状態は .ready と .notReadyMomentarily 列挙値の間になります。

00:25:24.000 --> 00:25:28.000
フラッシュキャプチャは.notReadyWaitingForCapture状態にヒットします。

00:25:28.000 --> 00:25:34.000
フラッシュが発火するまで、写真の出力はセンサーからフレームを取得していないので、ボタンは暗くなります。

00:25:34.000 --> 00:25:49.000
最後に、今年はゼロシャッターラグのみを使用し、他の機能を使用しない場合は、各写真のキャプチャと処理が完了しているため、.notReadyWaitingForProcessingの列挙値が現在の準備である間にスピナーを表示する場合があります。

00:25:49.000 --> 00:25:53.000
コードで準備コーディネーターを利用する方法は次のとおりです。

00:25:53.000 --> 00:26:02.000
まず、写真出力の準備コーディネーターを作成し、準備状態に関するコールバックを受信する適切なデリゲートオブジェクトを設定します。

00:26:02.000 --> 00:26:07.000
次に、各キャプチャ時に、通常どおり写真の設定を設定します。

00:26:07.000 --> 00:26:14.000
次に、準備コーディネーターに、これらの設定のキャプチャ要求の準備状態の追跡を開始するよう伝えます。

00:26:14.000 --> 00:26:18.000
そして、写真の出力でcapturePhotoを呼び出します。

00:26:18.000 --> 00:26:23.000
その後、準備コーディネーターはcaptureReadinessDidChangeデリゲートコールバックを呼び出します。

00:26:23.000 --> 00:26:34.000
受信した準備列挙値に基づいてキャプチャボタンの状態と外観を更新し、顧客が次にキャプチャできる時期について最善のフィードバックを提供します。

00:26:34.000 --> 00:26:47.000
レスポンシブキャプチャとファストキャプチャ優先順位付けAPIは、A12 Bionicチップ以降を搭載したiPhoneで利用でき、準備コーディネーターはAVCapturePhotoOutputがサポートされている場所ならどこでも利用できます。

00:26:47.000 --> 00:26:56.000
そして今、私はアプリですべての新機能を有効にし、可能な限り最も反応の良いカメラ体験を提供し、超シャープで高品質の写真を提供します。

00:26:56.000 --> 00:27:00.000
しかし、あなたは改善された経験を得るためにそれらすべてを使用する必要はありません。

00:27:00.000 --> 00:27:04.000
アプリに適したものだけを使用できます。

00:27:04.000 --> 00:27:07.000
本日、更新されたビデオエフェクトでセッションを終了します。

00:27:07.000 --> 00:27:19.000
以前は、macOSのコントロールセンターは、センターステージ、ポートレート、スタジオライトなどのカメラストリーミング機能のオプションを提供していました。

00:27:19.000 --> 00:27:25.000
macOS Sonomaでは、ビデオエフェクトをコントロールセンターから独自のメニューに移動しました。

00:27:25.000 --> 00:27:32.000
カメラや画面共有のプレビューが表示され、ポートレートモードやスタジオライトなどのビデオエフェクトを有効にすることができます。

00:27:32.000 --> 00:27:39.000
ポートレートとスタジオライトのエフェクトは強度が調整可能になり、スタジオライトはより多くのデバイスで利用できます。

00:27:39.000 --> 00:27:43.000
そして、「リアクション」と呼ばれる新しいエフェクトタイプがあります。

00:27:43.000 --> 00:27:52.000
ビデオ通話をしているときは、スピーカーを中断することなく継続させながら、アイデアを愛していることを表現したり、良いニュースについて親指を立てたりしたいと思うかもしれません。

00:27:52.000 --> 00:27:58.000
反応は、あなたのビデオと風船、紙吹雪などをシームレスにブレンドします。

00:27:58.000 --> 00:28:09.000
反応は、ポートレートとスタジオの照明効果のテンプレートに従います。これはシステムレベルのカメラ機能であり、アプリにコードの変更を必要とせずに、箱から出してすぐに利用できます。

00:28:09.000 --> 00:28:17.000
ポートレートとスタジオライトの効果の詳細については、2021年のセッション「カメラキャプチャの新機能」をご覧ください。

00:28:17.000 --> 00:28:27.000
ビデオストリームで反応を表示する3つの方法があります。まず、macOSの新しいビデオエフェクトメニューの下部ペインで反応効果をクリックすることができます。

00:28:27.000 --> 00:28:39.000
第二に、アプリはAVCaptureDevice.performEffectを呼び出すことができます: 反応タイプ。たとえば、アプリのビューの1つに一連のリアクションボタンがあり、参加者はクリックしてリアクションを実行できます。

00:28:39.000 --> 00:28:44.000
そして第三に、反応が有効になると、ジェスチャーをすることで送信できます。

00:28:44.000 --> 00:28:47.000
これをチェックしてみましょう。

00:28:47.000 --> 00:29:14.000
あなたは親指アップ、親指ダウン、2つの親指アップで花火、ハート、1つの勝利サインで風船、2つの親指ダウンで雨、2つの勝利サインで紙吹雪、そして私の個人的なお気に入り、レーザー、角の2つのサインを使用して行うことができます。

00:29:14.000 --> 00:29:18.000
ねえ、それは素敵な効果の束です。

00:29:18.000 --> 00:29:27.000
キャプチャセッションで使用するAVCaptureDeviceFormatのreactionEffectsSupportedプロパティを見ることで、反応効果のサポートを確認できます。

00:29:27.000 --> 00:29:36.000
AVCaptureDeviceには、ジェスチャー認識がオンになっているときと反応効果が有効になっているときを知るために、読み取ることができるプロパティまたはキー値観察があります。

00:29:36.000 --> 00:29:42.000
これらはユーザーの管理下にあるため、アプリはオンまたはオフを切り替えることができないことを覚えておいてください。

00:29:42.000 --> 00:29:44.000
iOSでは、それは同じ考えです。

00:29:44.000 --> 00:29:51.000
参加者はコントロールセンターに行き、ジェスチャー認識のオン/オフを切り替え、これが起こったときに値を観察することができます。

00:29:51.000 --> 00:29:55.000
ただし、iOSのアプリでエフェクトをトリガーするには、プログラムで行う必要があります。

00:29:55.000 --> 00:29:59.000
では、今、それを行う方法を見てみましょう。

00:29:59.000 --> 00:30:07.000
「canPerformReactionEffects」プロパティがtrueの場合、reactionTypeメソッドのperformEffectを呼び出すと、リアクションがビデオフィードにレンダリングされます。

00:30:07.000 --> 00:30:10.000
アプリは、効果をトリガーするボタンを提供する必要があります。

00:30:10.000 --> 00:30:19.000
ジェスチャーを介して入ってくる反応は、検出に使用されるキューに応じて、performEffect\を呼び出すときとは異なる場所でビデオでレンダリングされる場合があります。

00:30:19.000 --> 00:30:32.000
AVCaptureDeviceがキャプチャセッションで認識し、ビデオコンテンツにレンダリングできるサムズアップやバルーンなど、さまざまな反応効果のすべてのAVCaptureReactionTypeと呼ばれる新しい列挙型があります。

00:30:32.000 --> 00:30:41.000
また、「AVCaptureDevice.availableReactionTypes」プロパティは、設定された形式またはセッションプリセットに基づいてAVCaptureReactionTypesのセットを返します。

00:30:41.000 --> 00:30:47.000
これらのエフェクトには、独自のビューに配置できるシステムUIImagesも組み込まれています。

00:30:47.000 --> 00:31:02.000
AVCaptureReactionTypeを取り込み、UIImage systemNameコンストラクタで使用する適切な文字列を返す新しい関数AVCaptureReactionType.systemImageNameからリアクションのsystemNameを取得できます。

00:31:02.000 --> 00:31:10.000
そして、反応効果が進行中であるAVCaptureDevice.reactionEffectsInProgressという適切な名前のAPIがあります。

00:31:10.000 --> 00:31:18.000
ユーザーが複数の反応効果を順番に実行すると、一時的に重なる可能性があるため、ステータスオブジェクトの配列が返されます。

00:31:18.000 --> 00:31:22.000
キー値観察を使用して、これらがいつ始まり、いつ終わるかを知ることができます。

00:31:22.000 --> 00:31:33.000
Voice-over-IP会議アプリの場合は、この情報を使用して、特に発信者が帯域幅の理由でビデオをオフにしている場合に、エフェクトに関するメタデータをリモートビューに送信することもできます。

00:31:33.000 --> 00:31:38.000
たとえば、別の発信者に代わってUIにエフェクトアイコンを表示することができます。

00:31:38.000 --> 00:31:43.000
ビデオストリームにエフェクトアニメーションをレンダリングすることは、ビデオエンコーダにとって難しい場合があります。

00:31:43.000 --> 00:31:49.000
それらはコンテンツの複雑さを増大させ、それをエンコードするためにより大きなビットレートの予算が必要になる場合があります。

00:31:49.000 --> 00:31:57.000
キー値がreactionEffectsInProgressを観察することで、レンダリング中にエンコーダの調整を行うことができます。

00:31:57.000 --> 00:32:02.000
アプリで実現可能であれば、エフェクトがレンダリングされている間にエンコーダのビットレートを上げることができます。

00:32:02.000 --> 00:32:27.000
または、VideoToolboxを介して低遅延ビデオエンコーダを使用し、MaxAllowedFrameQP VTCompressionPropertyKeyを設定している場合は、サポートされている解像度、フレームレート、ビットレート層などのさまざまなビデオ構成を使用してアプリでテストを実行し、効果の進行中に最大許容FrameQPを調整することをお勧めします。

00:32:27.000 --> 00:32:35.000
MaxAllowedFrameQP値が低いと、エフェクトのフレームレートが損なわれる可能性があり、ビデオフレームレートが低くなることに注意してください。

00:32:35.000 --> 00:32:44.000
2021年のセッション「VideoToolboxで低遅延ビデオエンコーディングを探索する」には、この機能の操作に関するより優れた情報があります。

00:32:44.000 --> 00:32:49.000
また、エフェクトが進行中の場合、ビデオフレームレートが変化する可能性があることも知っておく必要があります。

00:32:49.000 --> 00:32:58.000
たとえば、AVCaptureSessionを毎秒60フレームで実行するように設定した場合、エフェクトが実行されていない間は毎秒60フレームになります。

00:32:58.000 --> 00:33:04.000
しかし、エフェクトが進行中ですが、毎秒30フレームなど、異なるフレームレートが得られる場合があります。

00:33:04.000 --> 00:33:11.000
これは、エンドフレームレートが指定したよりも低くなる可能性があるポートレートとスタジオライトエフェクトのモデルに従います。

00:33:11.000 --> 00:33:22.000
そのフレームレートを確認するには、デバイスで設定しているフォーマットのAVCaptureDeviceFormat.videoFrameRateRange ForReactionEffectsInProgressをチェックしてください。

00:33:22.000 --> 00:33:29.000
他のAVCaptureDeviceFormatプロパティと同様に、これはあなたが制御できるものではなく、あなたのアプリに情報を提供するものです。

00:33:29.000 --> 00:33:35.000
macOSとContinuity Cameraを使用するtvOSアプリでは、反応効果は常に有効になっています。

00:33:35.000 --> 00:33:41.000
iOSとiPad OSでは、アプリケーションはInfo.plistを変更してオプトインできます。

00:33:41.000 --> 00:33:54.000
UIBackgroundModes配列のVoIPアプリケーションカテゴリであることを宣伝するか、NSCameraReactionEffectsEnabledをYESの値で追加することで、オプトインします。

00:33:54.000 --> 00:34:18.000
反応効果とジェスチャー認識は、iPhone 12、Apple Silicon Mac、Intel Mac、Continuity Cameraデバイスを使用したApple TV、USB-C iPadまたはApple Silicon Macに接続されたApple Studioディスプレイ、USB-C iPadまたはApple Silicon Macに接続されたサードパーティ製カメラなど、A14チップ以降を搭載したiPhoneおよびiPadで利用できます。

00:34:18.000 --> 00:34:23.000
そして、今年の新しいAPIを使用したレスポンシブカメラ体験に関するセッションを締めくくります。

00:34:23.000 --> 00:34:44.000
遅延写真処理、ゼロシャッターラグ、レスポンシブキャプチャAPIについて話し、画質が向上した最も応答性の高い写真アプリを作成するための新しい可能性を提供し、新しい「リアクション」を含む更新されたビデオエフェクトでユーザーが本当に自分自身を表現する方法についても取り上げました。

00:34:44.000 --> 00:34:48.000
私はあなたがすべての素晴らしい新機能にどのように反応するかを見るのが待ちきれません。

00:34:48.000 --> 00:34:49.000
見てくれてありがとう。

00:34:49.000 --> 23:59:59.000
。

