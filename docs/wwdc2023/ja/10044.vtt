WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:13.000
デビッド：こんにちは。私の名前はデビッド・フィンドレイです。

00:00:13.000 --> 00:00:15.000
私はCreate MLチームの機械学習エンジニアです。

00:00:15.000 --> 00:00:19.000
私たちは、Create MLアプリとフレームワークのいくつかの大きな改善に取り組んできました。

00:00:19.000 --> 00:00:22.000
あなたを新機能のツアーに連れて行くのが楽しみです。

00:00:22.000 --> 00:00:28.000
大規模なモデルをゼロから訓練するには、数千時間、何百万もの注釈付きファイル、および専門的なドメイン知識が必要です。

00:00:28.000 --> 00:00:33.000
私たちの目標は、すべてのオーバーヘッドなしで機械学習を使用する優れたアプリを構築するためのツールを提供することです。

00:00:33.000 --> 00:00:42.000
私たちは、写真アプリの検索体験やアクセシビリティのカスタムサウンド認識など、多くの機能を強化する最先端のモデルを作成するプロセスを経てきました。

00:00:42.000 --> 00:00:49.000
Create MLを使用すると、最新のテクノロジーにアクセスできるので、手間をかけずに独自のカスタム機械学習体験を構築できます。

00:00:49.000 --> 00:00:53.000
MLを作成するために行った改善から始めます。 

00:00:53.000 --> 00:00:59.000
次に、複数のラベルを持つシーンを理解するための機械学習モデルを構築するまったく新しい方法を紹介します。

00:00:59.000 --> 00:01:06.000
最後に、トレーニングデータが制限されている場合に、モデルの品質を向上させるために設計された新しいオーグメンテーションAPIについて説明します。

00:01:06.000 --> 00:01:09.000
テキスト分類の改善から始めましょう。

00:01:09.000 --> 00:01:15.000
テキスト分類器は、自然言語テキストのパターンを認識するように設計された機械学習タスクです。

00:01:15.000 --> 00:01:20.000
このようなモデルを訓練するには、テキストとラベルのペアの表を提供するだけです。

00:01:20.000 --> 00:01:26.000
この例では、スポーツ、エンターテイメント、自然のラベルがあります。

00:01:26.000 --> 00:01:31.000
事前に訓練された埋め込みモデルを特徴抽出器として使用する転送学習アルゴリズムを選択できます。

00:01:31.000 --> 00:01:37.000
今年は、新しい埋め込みモデルを設計し、何十億ものラベル付きテキストの例でトレーニングしました。

00:01:37.000 --> 00:01:43.000
これは、変圧器モデル、略してBERTからの双方向エンコーダ表現です。

00:01:43.000 --> 00:01:48.000
新しいオプションは、Create MLアプリの[設定]タブのモデルパラメータセクションにあります。

00:01:48.000 --> 00:01:55.000
BERT埋め込みモデルは多言語であるため、トレーニングデータに複数の言語を含めることができます。

00:01:55.000 --> 00:02:02.000
多言語テキスト分類器をサポートすることに加えて、BERTはモノリンガルテキスト分類器の精度を高めることもできます。

00:02:02.000 --> 00:02:07.000
iOS 17、iPadOS 17、macOS SonomaでBERTを活用できます。

00:02:07.000 --> 00:02:10.000
私たちはすべての詳細をカバーするビデオ全体を持っています。

00:02:10.000 --> 00:02:16.000
詳細については、「Explore Natural Language多言語モデル」を必ずご覧ください。

00:02:16.000 --> 00:02:21.000
次に、画像分類タスクで転送学習をどのように使用するかについて話したいと思います。

00:02:21.000 --> 00:02:29.000
Create MLの画像分類器は、質問に答えるためのモデルを構築するのに役立つように設計されています。画像の内容を説明するのに最適なラベルは何ですか?

00:02:29.000 --> 00:02:36.000
テキスト分類器と同様に、画像分類器は事前に訓練されたモデルを活用して、画像から関連情報を抽出します。

00:02:36.000 --> 00:02:43.000
Apple Neural Scene Analyzerの最新バージョンは、トレーニングデータがほとんどない最先端のモデルを構築するために利用可能になりました。

00:02:43.000 --> 00:02:48.000
OSの画像理解モデルは、可能な限り最高の体験を提供するために進化し続けています。

00:02:48.000 --> 00:02:53.000
詳細については、機械学習研究ウェブサイトの記事をご覧ください。

00:02:53.000 --> 00:03:00.000
Create MLアプリでは、[設定]タブのモデルパラメータセクションに新しい機能抽出オプションが表示されます。

00:03:00.000 --> 00:03:05.000
新機能抽出器は、以前のバージョンと比較して出力埋め込みサイズが小さくなります。

00:03:05.000 --> 00:03:14.000
一般的な改善に加えて、これは分類器の精度を高め、トレーニング時間を短縮し、抽出された機能を保存するために必要なメモリを減らすことができます。

00:03:14.000 --> 00:03:19.000
Create MLの改善点を取り上げたので、新しいマルチラベル画像分類器について話したいと思います。

00:03:19.000 --> 00:03:27.000
私がそこに着く前に、シングルラベル画像分類は、画像の内容を記述する最適なラベルを予測するように設計されていることを思い出してください。

00:03:27.000 --> 00:03:32.000
たとえば、この画像を犬または屋外と表現するかもしれません。

00:03:32.000 --> 00:03:34.000
しかし、あなたは1つを選ぶ必要があります。

00:03:34.000 --> 00:03:40.000
オブジェクトに興味がある場合は、オブジェクト検出器を使用してシーン内のオブジェクトを見つけることができます。

00:03:40.000 --> 00:03:44.000
例えば、私は犬の周りにバウンディングボックスを描き、ボールの周りに別のバウンディングボックスを描きました。

00:03:44.000 --> 00:03:48.000
さて、これは素晴らしいですが、オブジェクトが入っているシーンにも興味があります。

00:03:48.000 --> 00:03:53.000
犬が公園や屋外にいることを表すためにバウンディングボックスを描くことはできません。

00:03:53.000 --> 00:03:56.000
そこで、新しいマルチラベル画像分類器が登場します。

00:03:56.000 --> 00:04:01.000
これにより、画像の一連のオブジェクト、属性、またはラベルを予測できます。

00:04:01.000 --> 00:04:07.000
例えば、この画像には犬、おもちゃ、草、公園が含まれています。

00:04:07.000 --> 00:04:09.000
Create MLを使って1つ作りに行きましょう。

00:04:09.000 --> 00:04:13.000
いつものように、私が最初にする必要があるのは、いくつかのトレーニングデータを収集することです。

00:04:13.000 --> 00:04:20.000
私は少し楽しんで、さまざまなシーンで複数の多肉植物を検出する分類器を構築することにしました。

00:04:20.000 --> 00:04:27.000
例えば、ここには、窓枠の上の鉢の中のハワーシア、ジェイド、アロエの画像があります。

00:04:27.000 --> 00:04:31.000
次の画像では、鍋にサボテンを持っている人がいます。

00:04:31.000 --> 00:04:39.000
トレーニング画像を収集しながら、アロエの写真のように、ラベルが1つしかない画像を含めることもできます。

00:04:39.000 --> 00:04:43.000
注釈をJSONファイルに整理する必要があります。

00:04:43.000 --> 00:04:48.000
あなたがする必要があるのは、注釈のセットで各ファイルに注釈を付けることだけです。

00:04:48.000 --> 00:04:53.000
では、Create MLアプリでモデルを構築するデモをしましょう。

00:04:53.000 --> 00:05:02.000
Create MLアプリで、新しいマルチラベル画像分類テンプレートを選択します。

00:05:02.000 --> 00:05:13.000
そして、Succulent Classifierという名前のプロジェクトを作成します。

00:05:13.000 --> 00:05:16.000
これにより、設定タブに移動します。

00:05:16.000 --> 00:05:26.000
まず、トレーニングデータをドラッグします。これにより、クラス数とトレーニング画像の概要が表示されます。

00:05:26.000 --> 00:05:33.000
検証データをドラッグするオプションもありますが、今のところトレーニングデータをランダムに分割することを選択します。

00:05:33.000 --> 00:05:37.000
デフォルトの反復回数を使用し、拡張も省きます。

00:05:37.000 --> 00:05:45.000
モデルの設定が終わったので、先に進んで「電車」をクリックします。

00:05:45.000 --> 00:05:49.000
このモデルは、私のMacでトレーニングするのに数分しかかかりません。

00:05:49.000 --> 00:05:54.000
すぐに、モデルは先ほど導入した新しい機能抽出器を使用して機能を抽出し始めます。

00:05:54.000 --> 00:05:58.000
それが完了すると、アプリは分類器のトレーニングを開始します。

00:05:58.000 --> 00:06:06.000
トレーニングプロセス中、アプリは平均平均精度スコア、略してMAPを計算することで、私のモデルの品質を測定します。

00:06:06.000 --> 00:06:15.000
一般的に、私のモデルは、私のデータセット内のすべてのラベルの平均でより高い精度とより高いリコールの両方を持っていることを意味するので、私はMAPスコアを最大化したいです。

00:06:15.000 --> 00:06:24.000
私のモデルはトレーニングを終了し、74回の反復で早期に収束し、MAPスコアはトレーニングセットで97%、検証セットで93%でした。

00:06:24.000 --> 00:06:30.000
次のステップは、テストデータで私のモデルを評価することです。

00:06:30.000 --> 00:06:35.000
デスクトップからフォルダをドラッグして、テストボタンをクリックします。

00:06:35.000 --> 00:06:40.000
テストデータには、モデルのトレーニングに使用したのと同じクラスラベルのセットが含まれている必要があります。

00:06:40.000 --> 00:06:48.000
このアプリは、MAPスコアや、どのクラスラベルが最高と最低の精度とリコールを持つかなど、いくつかの高レベルの統計を計算しました。

00:06:48.000 --> 00:06:51.000
メトリクスタブに焦点を当てましょう。

00:06:51.000 --> 00:07:00.000
このアプリは、偽陽性、偽陰性、精度、リコール、信頼しきい値など、各クラスラベルのメトリクスを計算します。

00:07:00.000 --> 00:07:06.000
私のモデルで予測を行う場合、信頼度が特定のクラスラベルのしきい値を上回っている場合、予測は正しいです。

00:07:06.000 --> 00:07:13.000
モデルがアロエの信頼しきい値を上回って予測した画像を探りましょう。

00:07:13.000 --> 00:07:17.000
次に、例をクリックします。

00:07:17.000 --> 00:07:25.000
モデルは、この画像には90%の信頼度を持つアロエが含まれていると予測し、これは40%のアロエ信頼しきい値を上回っている。

00:07:25.000 --> 00:07:29.000
他のラベルについては、モデルはそれぞれのしきい値を下回る信頼度を予測しました。

00:07:29.000 --> 00:07:32.000
言い換えれば、モデルはそれらを予測しなかった。

00:07:32.000 --> 00:07:38.000
次に、私のモデルがアロエを予測しなかったが、アロエとラベル付けされている画像を調べたいと思います。

00:07:38.000 --> 00:07:43.000
偽陰性の結果タイプを選択することで、それを行うことができます。

00:07:43.000 --> 00:07:47.000
この画像は面白いです。さらに探検しましょう。

00:07:47.000 --> 00:07:53.000
ここでは、アロエはバレルサボテンとムーンサボテンの背後にあるので、モデルはアロエを予測するのに苦労しています。

00:07:53.000 --> 00:07:57.000
しかし、良いニュースは、モデルが他の2つのラベルを正しく予測していることです。

00:07:57.000 --> 00:08:01.000
次に、プレビュータブに進みます。

00:08:01.000 --> 00:08:06.000
これは、まだラベル付けしていない画像のモデル予測をプレビューできる場所です。

00:08:06.000 --> 00:08:12.000
私は自分で植えた多肉植物のアレンジメントを持っていて、試してみるのが楽しいと思います。

00:08:12.000 --> 00:08:13.000
それを釘付けにした。

00:08:13.000 --> 00:08:18.000
私のモデルは、私の台所で私の月のサボテン、ウサギの耳のサボテン、バレルサボテンを正しく予測しました。

00:08:18.000 --> 00:08:28.000
私は私のモデルの品質にかなり満足していますが、私は間違いなく私のモデルの限界を理解するだけでなく、私のデータセットにより多くの多肉植物やシーンを追加するために実験を続けます。

00:08:28.000 --> 00:08:31.000
とりあえず、先に進みましょう。

00:08:31.000 --> 00:08:38.000
[出力]タブから、トレーニングしたモデルをディスクに保存できます。

00:08:38.000 --> 00:08:42.000
予測を作成するために書く必要があるコードを見てみましょう。 予測を作成するために書く必要があります。

00:08:42.000 --> 00:08:46.000
最初のステップは、コンパイルされたCore MLモデルからVisionモデルを作成することです。

00:08:46.000 --> 00:08:54.000
次に、ビジョンフレームワークを使用して、ソース画像で画像要求ハンドラを作成し、要求を実行します。

00:08:54.000 --> 00:09:01.000
最後に、分類観測値を取得し、興味のある精度とリコール値を使用してフィルタリングすることができます。

00:09:01.000 --> 00:09:11.000
ユースケースで機能する精度とリコール値を選択する方法の詳細については、WWDC 2019「Understanding Images in Vision Framework」のビデオをご覧ください。

00:09:11.000 --> 00:09:22.000
先に進む前に、少し時間を取って、画像分類器やマルチラベル画像分類器と同様に、オブジェクト検出の探索オプションで評価タブを強化したことに言及したいと思います。

00:09:22.000 --> 00:09:24.000
必ず確認してください。

00:09:24.000 --> 00:09:31.000
あなたの焦点を私の最後のトピックに移し、拡張を使用して限られたデータで機械学習モデルを訓練したいと思います。

00:09:31.000 --> 00:09:39.000
うまく一般化するモデルを取得するには、トレーニングセット内の画像には、さまざまな照明条件、向き、背景など、さまざまな特性が必要です。

00:09:39.000 --> 00:09:44.000
しかし、さまざまな状況でトレーニング画像をキャプチャしてラベル付けするには時間がかかる場合があります。

00:09:44.000 --> 00:09:51.000
データ拡張は、変換を適用することにより、既存のトレーニング例から新しいトレーニング例を生成する手法です。

00:09:51.000 --> 00:09:58.000
画像の場合、変換は、いくつかの例を挙げると、水平または垂直の反転、トリミング、またはコントラストにすることができます。

00:09:58.000 --> 00:10:03.000
この例では、多肉植物の画像から始めて、4つのバリエーションを生成します。

00:10:03.000 --> 00:10:08.000
そして、反転やコントラストの増加など、変換を組み合わせることができます。

00:10:08.000 --> 00:10:13.000
拡張は、特に小さなトレーニングデータセットがある場合、モデルの品質を向上させることができます。

00:10:13.000 --> 00:10:21.000
モデルがシーン内のオブジェクトの正確な位置などの属性を学習するのを防ぐため、モデルの一般化を改善するためにそれらを使用することができます。

00:10:21.000 --> 00:10:28.000
ただし、各トレーニング反復で特徴抽出が行われるため、トレーニングは通常遅くなることを考慮することが重要です。

00:10:28.000 --> 00:10:35.000
まだ見ていない場合は、Create ML Componentsを導入したWWDC 2022のビデオを必ずご覧ください。

00:10:35.000 --> 00:10:41.000
トランスフォーマーや見積もりなどのコンポーネントを使用して、カスタム機械学習モデルを構築するのに役立つフレームワークを設計しました。

00:10:41.000 --> 00:10:47.000
今年は、独自のカスタム拡張パイプラインを設計するために使用できる新しい拡張APIを追加しました。

00:10:47.000 --> 00:10:51.000
SwiftUIの経験があるなら、これはおなじみかもしれません。

00:10:51.000 --> 00:10:53.000
最初のステップは、オーグメンターを作成することです。

00:10:53.000 --> 00:10:58.000
SwiftUIと同様に、オーグザは結果ビルダーを使用します。

00:10:58.000 --> 00:11:03.000
オーグメンターの本体では、データを強化するための変換を追加できます。

00:11:03.000 --> 00:11:09.000
オーグメンターは汎用的であるため、データは画像、ラベル付きサウンド、または何か他のものにラベルを付けることができます。

00:11:09.000 --> 00:11:14.000
重要な部分は、各変換の入力タイプと出力タイプが一致する必要があることです。

00:11:14.000 --> 00:11:18.000
例えば、画像を撮って画像を生成します。

00:11:18.000 --> 00:11:23.000
50%の確率で水平に反転して画像を増強したい。

00:11:23.000 --> 00:11:26.000
オーグメンターにApplyRandomlyを追加することから始めます。

00:11:26.000 --> 00:11:29.000
これは、与えられた確率の変換を適用します。

00:11:29.000 --> 00:11:34.000
次に、本体に水平反転トランスを追加します。

00:11:34.000 --> 00:11:41.000
さて、オーグメンターが手になったので、適用されたメソッドを呼び出すことで、それを使ってオーグメンテーションを作成できます。

00:11:41.000 --> 00:11:46.000
拡張を設計するときは、データの性質を慎重に検討することが重要です。

00:11:46.000 --> 00:11:49.000
逆さまの多肉植物に遭遇する可能性は低い。

00:11:49.000 --> 00:11:54.000
したがって、この場合、垂直反転増強を適用することは意味がありません。

00:11:54.000 --> 00:11:58.000
または、代わりに交通標識を分類したいと想像してみてください。

00:11:58.000 --> 00:12:02.000
フリップ増強を適用すると、ラベルが画像を正しく表現できなくなる可能性があります。

00:12:02.000 --> 00:12:07.000
したがって、カスタム拡張を設計する前に、データの性質を考慮することを忘れないでください。

00:12:07.000 --> 00:12:13.000
さて、次のステップは、オーグメンターにより多くの変換を追加することです。

00:12:13.000 --> 00:12:18.000
今回は、UniformRandomFloatingPointParameterを使用して画像をランダムに回転させます。

00:12:18.000 --> 00:12:22.000
これにより、増強を適用するたびにランダムな角度が生成されます。

00:12:22.000 --> 00:12:26.000
そして最後に、画像をランダムにトリミングします。

00:12:26.000 --> 00:12:30.000
オーグマーの各変換は順番に適用されることに注意してください。

00:12:30.000 --> 00:12:33.000
まず、私の画像はランダムに反転します。

00:12:33.000 --> 00:12:37.000
結果はランダムに回転し、ランダムにトリミングされます。

00:12:37.000 --> 00:12:41.000
オーグメンターでできることの表面を引っ掻いただけです。

00:12:41.000 --> 00:12:45.000
ここでは、あなたが始めるために私たちが提供するコンポーネントのいくつかを紹介します。

00:12:45.000 --> 00:12:47.000
しかし、さらに進みたい場合はどうなりますか?

00:12:47.000 --> 00:12:54.000
カスタム変換を構築し、それらを使用して画像を拡張する方法の例を見てみましょう。

00:12:54.000 --> 00:13:00.000
堅牢な分類器を構築するには、さまざまなシーンや環境でトレーニング画像をキャプチャすることが重要です。

00:13:00.000 --> 00:13:07.000
この例では、ランダムなシーンのランダムな場所に多肉植物を配置するカスタム増強を作成します。

00:13:07.000 --> 00:13:10.000
RandomImageBackgroundを定義することから始めます。

00:13:10.000 --> 00:13:18.000
これは、トランスフォーマーに似ていますが、乱数発生器を必要とする新しいプロトコル、RandomTransformerに準拠しています。

00:13:18.000 --> 00:13:27.000
拡張は、異なる背景シーンで多肉植物をランダムに配置したいので、背景画像を撮影する初期化子を作成します。

00:13:27.000 --> 00:13:31.000
RandomTransformerプロトコルに準拠するには、適用されたメソッドを実装する必要があります。

00:13:31.000 --> 00:13:37.000
拡張を適用するとき、最初のステップは背景をランダムに選択することです。

00:13:37.000 --> 00:13:44.000
次に、背景画像で入力画像を配置したいランダムな場所を選択し、トリミングしないように注意します。

00:13:44.000 --> 00:13:48.000
次に、入力画像をランダムな場所に翻訳します。

00:13:48.000 --> 00:13:53.000
そして最後に、ランダムに選択された背景の上に入力画像を配置します。

00:13:53.000 --> 00:14:02.000
異なる背景に多肉植物を配置する前に、反転、回転、トリミングが行われるように、最後に新しいカスタム拡張を追加します。

00:14:02.000 --> 00:14:04.000
そして、これは私の最後の増強です。

00:14:04.000 --> 00:14:10.000
私のオーグメンターを使用して、私は分類器を訓練するためのいくつかの本当に興味深い画像を生成することができます。

00:14:10.000 --> 00:14:11.000
それが次のステップです。

00:14:11.000 --> 00:14:17.000
拡張を使用するときは、更新方法を使用して徐々にトレーニングする方が理にかなっています。

00:14:17.000 --> 00:14:21.000
それを念頭に置いて、増強を使用したトレーニングの例を見てみましょう。

00:14:21.000 --> 00:14:26.000
そして、私は遠慮しません。また、バッチ処理、ランダム化、早期停止も組み込みます。

00:14:26.000 --> 00:14:31.000
空の画像分類器モデルを作成することから始めます。

00:14:31.000 --> 00:14:33.000
次に、トレーニングループを作成します。

00:14:33.000 --> 00:14:37.000
トレーニングループでは、最初のステップは、トレーニング画像をシャッフルして拡張することです。

00:14:37.000 --> 00:14:44.000
拡張する前にシャッフルして、各反復でバッチに異なる画像が含まれるようにします。

00:14:44.000 --> 00:14:50.000
オーグザの結果は非同期シーケンスであり、変換が怠惰に行われることを意味します。

00:14:50.000 --> 00:14:56.000
バッチメソッドを使用すると、拡張の非同期シーケンスをグループにグループ化できます。

00:14:56.000 --> 00:15:00.000
この場合、私は16のバッチサイズを使用しました。

00:15:00.000 --> 00:15:04.000
最後に、拡張画像のバッチごとにモデルを更新します。

00:15:04.000 --> 00:15:08.000
これが、データ拡張を行う際に更新方法を使用することが良い選択である理由です。

00:15:08.000 --> 00:15:12.000
反復ごとに、新しい画像セットを取得します。

00:15:12.000 --> 00:15:18.000
オーグザによっては、更新方法がまったく同じ画像に複数回遭遇する可能性は低いです。

00:15:18.000 --> 00:15:22.000
これは、暗記するのではなく、モデルを一般化することを奨励します。

00:15:22.000 --> 00:15:28.000
私は100回の反復を選択しましたが、理想的には、検証精度の向上が停止したときにトレーニングを中止する必要があります。

00:15:28.000 --> 00:15:37.000
この例では、トレーニングの精度は向上し続けていますが、数回の反復後に検証精度が低下し始めることに注意してください。

00:15:37.000 --> 00:15:45.000
これは、モデルがトレーニングデータを記憶し、新しいデータへの一般化が少なくなり、検証とテストデータのパフォーマンスが悪化していることを意味します。

00:15:45.000 --> 00:15:48.000
この後もトレーニングを続けることは有害です。

00:15:48.000 --> 00:15:52.000
最後の例として、トレーニングループに早期停止を追加します。

00:15:52.000 --> 00:15:56.000
更新ステップの後、検証メトリクスを計算できます。

00:15:56.000 --> 00:16:01.000
検証精度を使用してトレーニングを早期に停止しています。これは分類器モデルで機能します。

00:16:01.000 --> 00:16:03.000
しかし、それは本当にあなた次第です。

00:16:03.000 --> 00:16:08.000
検証損失を使用したり、独自のメトリックを設計してモデルの品質を評価したりすることもできます。

00:16:08.000 --> 00:16:14.000
その後、過去5回の反復で精度が向上していない場合は、トレーニングループから抜け出します。

00:16:14.000 --> 00:16:17.000
そして、私の拡張画像分類器はこれで終わりです。

00:16:17.000 --> 00:16:26.000
このビデオでは、アプリで優れた機械学習体験を構築するために使用できる、OSに出荷された新しい最先端のモデルを紹介しました。

00:16:26.000 --> 00:16:32.000
次に、新しいマルチラベル画像分類器を使用してシーンを理解するためのモデルを構築する方法の例を挙げました。

00:16:32.000 --> 00:16:39.000
最後に、モデルの品質を向上させるために独自のカスタム拡張を構築する方法について詳しく説明しました。

00:16:39.000 --> 00:16:40.000
そして、それはラップです。

00:16:40.000 --> 23:59:59.000
Create MLでアプリを強化する時が来ました。

