WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:13.000
オヌール・タキン:こんにちは。私の名前はオヌール・タッキです。

00:00:13.000 --> 00:00:21.000
私はエンジニアリングマネージャーですが、今日はDockKitを使用した電動iPhoneスタンドとの統合を紹介します。

00:00:21.000 --> 00:00:38.000
このビデオでは、DockKitとは何かについて話し、箱から出してすぐにどのように機能するかを掘り下げ、DockKitを使用してアプリをカスタマイズする方法を見て、最後にデバイスのアニメーションを使用してアフォーダンスとして動きます。

00:00:38.000 --> 00:00:42.000
では、DockKitとは何ですか?

00:00:42.000 --> 00:00:48.000
DockKitは、iPhoneが電動カメラスタンドの中央コンピューティングとして機能することを可能にするフレームワークです。

00:00:48.000 --> 00:00:59.000
DockKitスタンドは、ピッチとヨーモーションモデルと自動システムトラッカーをサポートすることで、iPhoneカメラの視野を360度のパンと90度の傾きに拡張します。

00:00:59.000 --> 00:01:08.000
これにより、ユーザーはカメラアプリのフレームにいることを心配することなく、自分のコンテンツに集中することができます。

00:01:08.000 --> 00:01:18.000
これらのスタンドには、電源と追跡を無効にするための簡単なボタンと、追跡がアクティブでフレーム内にあるかどうかを知らせるLEDインジケーターが含まれている場合があります。

00:01:18.000 --> 00:01:22.000
あなたは電話をスタンドとペアリングし、それは行って良いです。

00:01:22.000 --> 00:01:28.000
すべての魔法は、iPhone自体のアプリケーションとシステムサービスで起こります。

00:01:28.000 --> 00:01:39.000
モーター制御と被写体追跡がシステムレベルで処理されているため、iOSカメラAPIを使用するすべてのアプリはDockKitを利用できます。

00:01:39.000 --> 00:01:52.000
これにより、ビデオキャプチャ、ライブストリーミング、ビデオ会議、フィットネス、エンタープライズ、教育、ヘルスケアなどのアプリで、より良い体験と新機能の機会が生まれます。

00:01:52.000 --> 00:02:01.000
DockKitについて話すのではなく、あなたにそれを実演させてください。

00:02:01.000 --> 00:02:04.000
目の前には、DockKitスタンドのプロトタイプがあります。

00:02:04.000 --> 00:02:06.000
私はすでにそれをiPhoneとペアリングしました。

00:02:06.000 --> 00:02:10.000
スタンドにいるとき、電話はDockKitを介してペアリングされたドックと通信します。

00:02:10.000 --> 00:02:12.000
では、試してみましょう。

00:02:12.000 --> 00:02:19.000
内蔵のカメラアプリを起動し、ここでこのボタンを押すと、追跡が開始されます。

00:02:19.000 --> 00:02:22.000
テーブルの周りを動き回ると、ドックが私を追跡します。

00:02:22.000 --> 00:02:26.000
私がフレームの中にいることを示すために緑色のLEDが点滅していることに注目してください。

00:02:26.000 --> 00:02:34.000
では、バックカメラに切り替えさせてください。

00:02:34.000 --> 00:02:39.000
私を視野に入れるために180度回転させることで、ドックがどのように反応するかに注目してください。

00:02:39.000 --> 00:02:40.000
超かっこいい。

00:02:40.000 --> 00:02:44.000
しかし、これは内蔵のカメラアプリだけでは機能しません。

00:02:44.000 --> 00:02:51.000
カメラAPIを使用するアプリは、ドックで使用できます。

00:02:51.000 --> 00:02:54.000
例えば、FILMICPROを試してみましょう。

00:02:54.000 --> 00:03:02.000
これは、今日App Storeからダウンロードできるのと同じFILMICPROです。

00:03:02.000 --> 00:03:06.000
アプリを起動すると、デフォルトで背面カメラになります。

00:03:06.000 --> 00:03:09.000
そして、これはすべて箱から出して機能します。

00:03:09.000 --> 00:03:13.000
DockKitスタンドを使用すると、周囲の空間や物体と対話できます。

00:03:13.000 --> 00:03:26.000
例えば、私はここで私の本のスタックに行き、私の本について話すか、オブジェクトと対話して私のスペースを再配置することができます。

00:03:26.000 --> 00:03:42.000
DockKitアプリケーションを使用すると、ストーリーテラーは視野を心配するのではなく、ストーリーに集中できます。

00:03:42.000 --> 00:03:49.000
iPhoneの内蔵画像インテリジェンスと滑らかなモーターにより、これらのスタンドは本当に生き生きとしています。

00:03:49.000 --> 00:03:52.000
これがどのように機能するかを掘り下げてみましょう。

00:03:52.000 --> 00:04:08.000
DockKitシステムトラッカーは、カメラ処理パイプライン内で実行され、組み込みの推論を通じてカメラフレームを分析し、どの対象を追跡するかを決定し、対象の軌道を推定し、モーターを駆動して対象を適切にフレーミングします。

00:04:08.000 --> 00:04:22.000
撮影監督が現実世界の出来事を観察し、カメラの視点を調整する方法と同様に、DockKitはカメラフレームからオブジェクトを推測し、ドックへの作動コマンドを介してモーターを調整します。

00:04:22.000 --> 00:04:28.000
モーター制御は、DockKitスタンドを管理し、通信するDockKitデーモンを介して達成されます。

00:04:28.000 --> 00:04:37.000
作動コマンドはDockKitプロトコルを介してスタンドに送信され、センサーフィードバックを使用してモータ制御のループを閉じます。

00:04:37.000 --> 00:04:45.000
追跡を処理するために、カメラフレームはISP推論を介して分析され、毎秒30フレームでDockKitに渡されます。

00:04:45.000 --> 00:04:50.000
フードの下には、複数人のシナリオに役立つ視覚的な理解フレームワークがあります。

00:04:50.000 --> 00:04:57.000
フェイスとボディのバウンディングボックスが生成され、マルチモデルのシステムレベルのトラッカーに供給されます。

00:04:57.000 --> 00:05:06.000
トラッカーは、追跡された各人またはオブジェクトのトラックを生成し、統計的なEKFフィルターを実行して、推論からのギャップやエラーを滑らかにします。

00:05:06.000 --> 00:05:10.000
結局のところ、現実世界の信号は必ずしも完璧ではありません。

00:05:10.000 --> 00:05:21.000
被験者のトラッカーの見積もりは、モーターの位置と速度のフィードバック、および電話IMUと組み合わせて、最終的な軌道とアクチュエータコマンドに到達します。

00:05:21.000 --> 00:05:25.000
多くの場合、複数の被験者がフレーム内に存在することがあります。

00:05:25.000 --> 00:05:30.000
デフォルトでは、DockKitは中央のフレーミングで主要な被写体を追跡します。

00:05:30.000 --> 00:05:37.000
彼らが2人目またはオブジェクトと関わった場合、DockKitもその人またはオブジェクトをフレーム化しようとします。

00:05:37.000 --> 00:05:43.000
例えば、ここにはMahmut、Steve、Dhruv、Vamsiがいます。

00:05:43.000 --> 00:05:46.000
Mahmutは、緑色の境界ボックスで示される主な主題です。

00:05:46.000 --> 00:05:52.000
検出された顔だけでなく、体にもバウンディングボックスがあることに注意してください。

00:05:52.000 --> 00:06:03.000
他のチームメンバーがMahmutを妨害し、道を渡っても、トラッカーは主要な主題を追跡し続けます。

00:06:03.000 --> 00:06:12.000
認識または推論が障害物のために誤った結果を与える場合、統計トラッカーはエラーを修正し、Mahmutを追跡し続けることができます。

00:06:12.000 --> 00:06:17.000
それが、アプリに追加のコードを追加しなくても、DockKitの仕組みです。

00:06:17.000 --> 00:06:24.000
しかし、DockKitと統合して、顧客が気に入る新機能を提供すると、物事は本当にエキサイティングになります。

00:06:24.000 --> 00:06:30.000
DockKitアクセサリをコントロールする方法を探りましょう。

00:06:30.000 --> 00:06:33.000
これには、ドックへの参照を取得することが含まれます。

00:06:33.000 --> 00:06:43.000
そこから、フレーミングを変更するか、追跡されているものを指定するか、システム追跡を停止した後にモーターを直接制御するかを選択できます。

00:06:43.000 --> 00:06:47.000
まず、ドックアクセサリの状態の変更を登録します。

00:06:47.000 --> 00:06:54.000
ドッキングまたはドッキング解除通知は、人が互換性のあるドックアクセサリからiPhoneをドッキングまたは取り外すときに発生します。

00:06:54.000 --> 00:07:01.000
通知は、追跡動作を変更するための前提条件です。

00:07:01.000 --> 00:07:05.000
DockAccessoryManager Stateイベントを通じて、ドックの状態を照会できます。

00:07:05.000 --> 00:07:09.000
関連する状態は、ドッキング済みとドッキング解除のみです。

00:07:09.000 --> 00:07:12.000
接続性はDockKit自体によって管理されます。

00:07:12.000 --> 00:07:18.000
ドッキング状態は、iPhoneがDockKitプロトコルを介してドックに接続されていることも意味します。

00:07:18.000 --> 00:07:23.000
状態イベントを通じて、アプリケーションは追跡ボタンの状態を取得することもできます。

00:07:23.000 --> 00:07:29.000
iPhoneがドッキングされていることがわかったら、アクセサリの他の側面を制御できます。

00:07:29.000 --> 00:07:36.000
最も有用なものの1つは、アプリがビデオのトリミング方法を管理することです。

00:07:36.000 --> 00:07:40.000
カメラの視野のトリミングを制御するには2つの方法があります。

00:07:40.000 --> 00:07:50.000
まず、自動フレーミングの左揃え、中央揃え、右揃えを選択するか、関心のある特定の領域を指定することができます。

00:07:50.000 --> 00:07:54.000
どちらかを選びたいケースを見てみましょう。 いずれかを選択したい場合があります。

00:07:54.000 --> 00:07:59.000
デフォルトでは、DockKitは被写体をフレームの中央に保ちます。

00:07:59.000 --> 00:08:06.000
これはビデオストリーミングのような単純なものにはうまく機能しますが、これが理想的ではないかもしれないユースケースがあります。

00:08:06.000 --> 00:08:13.000
たとえば、アプリがこのロゴのようなカスタムグラフィックオーバーレイをビデオフレームに注入した場合はどうなりますか?

00:08:13.000 --> 00:08:19.000
この場合、被写体がアートワークによって隠されていないことを確認する必要があります。

00:08:19.000 --> 00:08:23.000
フレーミングモードを変更するだけで、これを修正できます。

00:08:23.000 --> 00:08:30.000
ここでは、フレームの左3分の1に整列されたグラフィックとのバランスを取るために「右」を指定しています。

00:08:30.000 --> 00:08:36.000
そして、そのシンプルなコードで、コンポジションはこのビデオのオープニングシーケンスに最適です。

00:08:36.000 --> 00:08:43.000
トリミングを制御するもう1つの方法は、ビデオフレームに関心のある特定の領域を指定することです。

00:08:43.000 --> 00:08:46.000
例えば、このビデオ会議アプリを見てみましょう。

00:08:46.000 --> 00:08:50.000
すべてのビデオフレームは、正方形のアスペクト比にトリミングされることを意図しています。

00:08:50.000 --> 00:08:57.000
ただし、DockKitのデフォルトのフレーミングにより、誰かの顔が切断される可能性があります。

00:08:57.000 --> 00:09:02.000
DockKitアクセサリに関心のある領域を設定することで、これを修正できます。

00:09:02.000 --> 00:09:07.000
iPhoneのディスプレイの左上隅は原点と見なされます。

00:09:07.000 --> 00:09:11.000
関心のある領域は、正規化された座標で定義されます。

00:09:11.000 --> 00:09:18.000
この例では、DockKitアクセサリに、関心のある領域は中央の正方形であることを伝えています。

00:09:18.000 --> 00:09:24.000
関心のある領域への調整により、被写体はフレーム内で完全にトリミングされます。

00:09:24.000 --> 00:09:31.000
DockKitを使用すると、ユーティリティまたはアフォーダンスとして、アプリでカスタムモーター制御を行うこともできます。

00:09:31.000 --> 00:09:35.000
これにより、多くの新機能の機会を開くことができます。

00:09:35.000 --> 00:09:46.000
DockKitはデフォルトでシステムトラッキングを有効にしているため、モーターを制御したり、独自のカスタムトラッキングを実行したりする前に、この値をfalseに設定する必要があります。

00:09:46.000 --> 00:09:51.000
DockKitスタンドは、XとYの2つの回転軸で動作します。

00:09:51.000 --> 00:10:00.000
傾き、具体的にはピッチ回転は、マグネットドックポイントの背後にあるモーターと整列したX軸の周りです。

00:10:00.000 --> 00:10:08.000
ヨーとして知られるパンの場合、回転はスタンドの基部にあるモーターと一直線に沿ったY軸の周りです。

00:10:08.000 --> 00:10:12.000
コードでこれを制御する例を見てみましょう。

00:10:12.000 --> 00:10:21.000
毎秒0.1ラジアンをピッチダウンしながら、モーターを毎秒0.2ラジアンの低速で右に動かしたいとします。

00:10:21.000 --> 00:10:28.000
まず、最初の速度ベクトルを定義し、そのベクトルをドックに送信します。

00:10:28.000 --> 00:10:32.000
タスクをスリープ状態にすることで、2秒間動き続けることができます。

00:10:32.000 --> 00:10:41.000
そして、異なるベクトルで方向を逆にして、0.2 rad/sで左に移動し、0.1 rad/sでピッチアップします。

00:10:41.000 --> 00:10:45.000
さて、アプリがDockKitと統合されている場合、それがすべてではありません。

00:10:45.000 --> 00:10:51.000
モーターを直接制御するだけでなく、推論を制御することもできます。

00:10:51.000 --> 00:10:57.000
ビジョンフレームワーク、独自のカスタムMLモデル、またはアプリケーションが必要とする知覚アルゴリズムを使用できます。

00:10:57.000 --> 00:11:03.000
カスタム推論出力から、オブジェクトを追跡するためにDockKitにフィードするオブザベーションを構築します。

00:11:03.000 --> 00:11:06.000
では、観察とは何ですか?

00:11:06.000 --> 00:11:13.000
オブザベーションは、カメラフレームに関心のある主題を表す長方形のバウンディングボックスです。

00:11:13.000 --> 00:11:16.000
つまり、あなたが追跡したいものです。

00:11:16.000 --> 00:11:23.000
それは顔、動物、あるいは手かもしれません。

00:11:23.000 --> 00:11:28.000
正直なところ、時間の経過とともに観察できるオブジェクトやポイント。

00:11:28.000 --> 00:11:33.000
バウンディングボックスは、原点の左下に基づいて正規化された座標で定義されます。

00:11:33.000 --> 00:11:45.000
たとえば、この検出された顔の観察を作成するには、幅と高さが全体の画像フレームのパーセンテージを持つ約0.25、0.5のバウンディングボックスを使用します。

00:11:45.000 --> 00:11:53.000
観測を作成するには、正規化された座標でCGRectを構築し、そこから観測を構築します。

00:11:53.000 --> 00:12:00.000
その際、観察タイプが「ヒューマンフェイス」または「オブジェクト」のいずれかであることを指定します。

00:12:00.000 --> 00:12:09.000
「humanFace」オプションを使用することで、システムレベルの複数人の追跡とフレーミングの最適化がまだ有効であることを確認できます。

00:12:09.000 --> 00:12:13.000
オブザベーションが作成されると、トラッカーに入力できます。

00:12:13.000 --> 00:12:16.000
まず、現在のカメラ情報を取得することから始めます。

00:12:16.000 --> 00:12:34.000
この場合、座標が画面の左下隅に相対的であり、変換が不要であることをDockKitに知らせるために、向きが「修正」されることを指定し、観測配列とカメラ情報をドックアクセサリに渡します。

00:12:34.000 --> 00:12:42.000
さて、良いニュースは、あなたが観察し、手で完全に追跡したいかもしれない多くのものの境界ボックスを計算する必要がないということです。

00:12:42.000 --> 00:13:00.000
ビジョンフレームワークは、ボディポーズ検出、動物のボディポーズ検出、さらにはバーコード認識など、追跡可能な観察に簡単に変換できるバウンディングボックスを返す多くの組み込み要求がすでに含まれているため、カスタム推論に最適です。

00:13:00.000 --> 00:13:08.000
また、Visionの座標系はDockKitでも同じなので、変換せずに直接渡すことができます。

00:13:08.000 --> 00:13:12.000
デバイスの向きが何であるかをDockKitに知らせるだけです。

00:13:12.000 --> 00:13:17.000
では、コードでのカスタムオブザベーションの実装について説明しましょう。

00:13:17.000 --> 00:13:25.000
この場合、手のポーズ検出リクエストを使用して、デフォルトの顔と体の追跡を手の追跡に置き換えたいです。

00:13:25.000 --> 00:13:37.000
まず、ビジョンリクエストとリクエストハンドラを作成します。このユースケースでは、VNDetectHumanHandPoseRequestを使用しています。その後、リクエストを実行します。

00:13:37.000 --> 00:13:41.000
認識されたポイントに基づいて観測を構築できます。

00:13:41.000 --> 00:13:52.000
簡単にするために、私は親指の先端のポイントに焦点を当てていますが、観察を構築するために他の指の関節や手全体を使用することができます。

00:13:52.000 --> 00:13:57.000
そして最後に、観測をDockKitに渡して追跡します。

00:13:57.000 --> 00:14:14.000
それで、それを実際に見てみましょう!

00:14:14.000 --> 00:14:19.000
カスタムカメラアプリを起動して、追跡を開始します。

00:14:19.000 --> 00:14:24.000
手を左に動かすと、左にパンするスタンドに注意してください。

00:14:24.000 --> 00:14:29.000
そして、手を右に動かすと、スタンドは反対方向に続きます。

00:14:29.000 --> 00:14:36.000
手を上に掃くと、スタンドが傾いて手をフレーム内に保ち、手を後ろに掃くときに再び続きます。

00:14:36.000 --> 00:14:41.000
完璧。

00:14:41.000 --> 00:14:49.000
DockKit APIを介してモーターを直接制御する機能により、アニメーションを通じてデバイスに命を吹き込むことができます!

00:14:49.000 --> 00:14:55.000
ドックの動きは、確認のためのアフォーダンスとして、または感情を伝える方法として使用できます。

00:14:55.000 --> 00:15:08.000
ダイレクトモーターコントロールを使用して独自のカスタムアニメーションを作成して、プッシュやプルなどの電話との物理的な相互作用をエミュレートしたり、組み込みのアニメーションの1つを活用したりできます。

00:15:08.000 --> 00:15:15.000
内蔵のアニメーションは、Yes、No、Wakeup、Kapowです。

00:15:15.000 --> 00:15:20.000
デバイスの起動が発生するたびに、ウェイクアップアニメーションが動作しているのを見ることができます。

00:15:20.000 --> 00:15:30.000
先ほど実演したカスタムハンドトラッキングアプリに戻ると、特定のハンドジェスチャーが発生するといつでも組み込みアニメーションをトリガーできます。

00:15:30.000 --> 00:15:36.000
これを行うには、まずカスタムハンドアクション分類モデルをトレーニングする必要があります。

00:15:36.000 --> 00:15:39.000
これはCreate MLアプリを使って簡単にできます。

00:15:39.000 --> 00:15:48.000
これらのモデルの1つを作成する方法の詳細については、2021年の「手のポーズとアクションを分類する」ビデオを必ずご覧ください。

00:15:48.000 --> 00:15:53.000
カスタムモデルがハンドジェスチャーが発生したと予測するときはいつでも、アニメーションをトリガーできます。

00:15:53.000 --> 00:16:01.000
最初にシステムトラッキングを無効にしてから、アニメーションを開始します。この場合は、Kapowです。

00:16:01.000 --> 00:16:08.000
アニメーションはスタンドの現在の位置から始まり、非同期に実行されます。

00:16:08.000 --> 00:16:13.000
アニメーションが実行されたら、システムトラッキングを再度有効にします。

00:16:13.000 --> 00:16:20.000
さて、デモに戻って、動作を確認します。

00:16:20.000 --> 00:16:24.000
私が少し後退するにつれて、アプリは私の手を追跡し続けます。

00:16:24.000 --> 00:16:29.000
カメラを押しているというジェスチャーをしたら、カポウ!

00:16:29.000 --> 00:16:33.000
カメラは振り子の動きで前後に揺れる。

00:16:33.000 --> 00:16:36.000
それはとても楽しいので、もう一度やりましょう。

00:16:36.000 --> 00:16:39.000
そしてカポウ!

00:16:39.000 --> 00:16:44.000
このようなアニメーションを使用して、体験を終わらせたり、何かを表現したりすることができます。

00:16:44.000 --> 00:16:51.000
これを試してみることをお勧めします。ぜひ、カスタムモーターコントロールを使用して独自のアニメーションを作成することを検討してください。

00:16:51.000 --> 00:16:59.000
DockKitでは、電動スタンドを使用してオブジェクトを追跡する機能が導入され、アプリケーションに360度の視野を提供します。

00:16:59.000 --> 00:17:04.000
オブジェクトは、システムレベルまたはカスタム推論を使用してアプリケーションで追跡できます。

00:17:04.000 --> 00:17:11.000
そして、感情や実用性を伝えるためのアフォーダンスとして、アプリケーションで機械的な動きを使用することができます。

00:17:11.000 --> 00:17:20.000
DockKitでカスタムトラッキングにVisionフレームワークを使用する方法の詳細については、動物のポーズの検出に関するビデオを必ず確認してください。

00:17:20.000 --> 23:59:59.000
私はあなたがこれらの素晴らしいアクセサリーを手に入れ、あなたが1つを使用するときにあなたのアプリにもたらす経験を見るのが待ちきれません。

