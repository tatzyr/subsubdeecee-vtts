WEBVTT

00:00:00.000 --> 00:00:03.000
♪まろやかなインストゥルメンタルヒップホップ♪

00:00:03.000 --> 00:00:10.000
♪

00:00:10.000 --> 00:00:12.000
オミド・カリリ:こんにちは!私の名前はオミッドです。

00:00:12.000 --> 00:00:25.000
オリバーと私はARKitチームのエンジニアであり、iOS ARアプリを新しいプラットフォームに持ち込む際に知っておく必要がある概念（馴染みのあるものと新しいもの）を見直すことに興奮しています。

00:00:25.000 --> 00:00:35.000
ARKitは2017年にiOSで導入され、拡張現実アプリケーションを構築するための3つの重要な概念を導入しました。

00:00:35.000 --> 00:00:42.000
ワールドトラッキングにより、ARKitは6つの自由度で世界におけるデバイスの位置を追跡することができます。

00:00:42.000 --> 00:00:48.000
これにより、現実世界への位置と向きで仮想コンテンツを固定することができます。

00:00:48.000 --> 00:00:52.000
シーンの理解は、あなたの周りの現実世界についての洞察を提供します。

00:00:52.000 --> 00:01:02.000
提供されたジオメトリとセマンティック知識を使用して、コンテンツをインテリジェントに配置し、環境と現実的に対話することができます。

00:01:02.000 --> 00:01:14.000
最後に、レンダリングエンジンは、ARKitが提供するカメラ変換と本質を利用して、キャプチャされた画像上に仮想コンテンツを正しく登録して合成することができます。

00:01:14.000 --> 00:01:22.000
当初は、iOSでARKitのカメラ変換とレンダリング3Dコンテンツを使用するためのSceneKitビューから始めました。

00:01:22.000 --> 00:01:34.000
その後、RealityKitを導入し、非常に現実的な物理ベースのレンダリングと周囲との正確なオブジェクトシミュレーションが可能なエンジンの基礎をレイアウトしました。

00:01:34.000 --> 00:01:42.000
空間コンピューティングを可能にするために、ARKitとRealityKitは成熟し、オペレーティングシステムに深く統合されています。

00:01:42.000 --> 00:01:53.000
たとえば、ARKitのトラッキングとシーン理解は現在、システムサービスとして実行されており、ウィンドウの配置から空間オーディオまですべてをバックアップしています。

00:01:53.000 --> 00:01:59.000
このシステムは、以前はアプリケーションに属していた責任を負います。

00:01:59.000 --> 00:02:09.000
ユーザーの手のカメラパススルーとマットが組み込まれているため、アプリケーションはこれらの機能を無料で利用できます。

00:02:09.000 --> 00:02:20.000
もう1つの組み込み機能は、ARKitワールドマップがシステムサービスによって継続的に永続化されるため、アプリケーションがもうそれを行う必要がないことです。

00:02:20.000 --> 00:02:29.000
これにより、このプラットフォームで可能な限り最高のアプリケーションとコンテンツの構築に集中できると信じています。

00:02:29.000 --> 00:02:35.000
以下は、このプラットフォームで導入された新しい機能とともに、これらの機能を実証する例です。

00:02:35.000 --> 00:02:47.000
たとえば、ARKitはアプリにハンドトラッキングを提供するようになりました。これにより、人々は周囲と対話できる仮想コンテンツに手を差し伸べて直接対話することができます。

00:02:47.000 --> 00:02:56.000
この新しいプラットフォームが提供するすべての新機能と没入型体験を活用するには、iOS ARKitベースのエクスペリエンスを更新する必要があります。

00:02:56.000 --> 00:03:03.000
これは、空間コンピューティングのためのアプリとAR体験を再考する絶好の機会です。

00:03:03.000 --> 00:03:11.000
この移行の一環として、ARKitとRealityKitで導入した使い慣れた概念を使用します。

00:03:11.000 --> 00:03:18.000
これらの概念がどのように引き継がれたか、どのように進化したか、そしてそれらをどのように活用できるかについて説明します。

00:03:18.000 --> 00:03:20.000
始めましょう！

00:03:20.000 --> 00:03:30.000
まず、空間コンピューティングのためにアプリを提示するいくつかの新しい方法を探り、利用可能な新しいコンテンツツールを紹介します。

00:03:30.000 --> 00:03:38.000
次に、コンテンツをレンダリングして操作するために使用するエンジンであるリアリティキットについて説明します。

00:03:38.000 --> 00:03:46.000
RealityViewが、iOSのARViewと同様に、アプリが空間コンピューティングをどのように活用できるかを見ていきます。

00:03:46.000 --> 00:03:53.000
次に、アプリがコンテンツを人々の環境に持ち込むさまざまな方法について説明します。

00:03:53.000 --> 00:04:00.000
レイキャスティングは、多くのiOSアプリケーションがコンテンツを配置するために使用するものです。

00:04:00.000 --> 00:04:09.000
ARKitデータとRealityKitを組み合わせて、空間コンピューティングのレイキャスティングを有効にする方法の例を紹介します。

00:04:09.000 --> 00:04:19.000
そして最後に、ARKitのアップデートを確認し、iOSから使い慣れた概念を活用する新しい方法を見ていきます。

00:04:19.000 --> 00:04:25.000
空間コンピューティングのためにあなたの経験を移行する準備をしましょう。

00:04:25.000 --> 00:04:32.000
空間コンピューティングを使用すると、iOS ARエクスペリエンスをウィンドウを超えて拡張できます。

00:04:32.000 --> 00:04:41.000
このプラットフォームは、iOS体験をもたらす際に検討したいアプリケーションを提示する新しい方法を提供します。

00:04:41.000 --> 00:04:44.000
これは、Hello Worldのサンプルアプリの例です。

00:04:44.000 --> 00:04:51.000
ウィンドウや3次元コンテンツを含むUIを周りのどこにでも表示できるようになりました。

00:04:51.000 --> 00:04:56.000
デフォルトでは、このプラットフォーム上のアプリケーションは共有スペースに起動します。

00:04:56.000 --> 00:05:02.000
共有スペースは、Macデスクトップ上の複数のアプリのように、アプリが並んで存在する場所です。

00:05:02.000 --> 00:05:08.000
共有スペース内では、アプリは1つ以上のウィンドウを開いてコンテンツを表示できます。

00:05:08.000 --> 00:05:13.000
さらに、アプリは3次元ボリュームを作成できます。

00:05:13.000 --> 00:05:24.000
たとえば、利用可能なボードゲームのリストを1つのウィンドウに表示し、ルールを別のウィンドウに表示し、選択したゲームを独自のボリュームで開くことができます。

00:05:24.000 --> 00:05:30.000
このゲームは、サファリのウィンドウを開いたままプレイして、勝利戦略を読むことができます。

00:05:30.000 --> 00:05:39.000
ウィンドウとボリュームに追加するコンテンツは、他のアプリケーションとスペースを共有できるように、その範囲内に含まれるままです。

00:05:39.000 --> 00:05:49.000
場合によっては、あなたのアプリがあなたの経験への没入のレベルをよりコントロールしたいかもしれません - 多分あなたの部屋と相互作用するゲームをプレイするために。

00:05:49.000 --> 00:05:59.000
このため、アプリは、アプリのウィンドウ、ボリューム、および3Dオブジェクトのみが表示される専用のフルスペースを開くことができます。

00:05:59.000 --> 00:06:05.000
フルスペースに入ると、アプリケーションはより多くの機能にアクセスできます。

00:06:05.000 --> 00:06:17.000
RealityKitのアンカーエンティティを使用すると、テーブル、床、さらには手のひらや手首などの手の一部などの周囲にオブジェクトをターゲットにして取り付けることができます。

00:06:17.000 --> 00:06:22.000
アンカーエンティティは、ユーザーの許可を必要とせずに動作します。

00:06:22.000 --> 00:06:26.000
ARKitデータは、アプリがフルスペースでしかアクセスできない他のものです。

00:06:26.000 --> 00:06:42.000
許可を得て、ARKitは現実世界の表面、シーンジオメトリ、骨格ハンドトラッキングに関するデータを提供し、現実的な物理学と自然な相互作用のためのアプリの能力を拡大します。

00:06:42.000 --> 00:06:47.000
Windows、ボリューム、スペースはすべてSwiftUIシーンタイプです。

00:06:47.000 --> 00:06:50.000
これらについて学ぶべきことはもっとたくさんあります。

00:06:50.000 --> 00:06:55.000
まず第一に、ここで言及されているセッションに行くことができます。

00:06:55.000 --> 00:07:03.000
次に、空間コンピューティングに持ち込むためのコンテンツを準備するために必要な主なステップを確認しましょう。

00:07:03.000 --> 00:07:12.000
iOSでの思い出に残るAR体験は、素晴らしい3Dコンテンツから始まります。このプラットフォームでの空間体験にも同じことが言えます。

00:07:12.000 --> 00:07:21.000
そして、3Dコンテンツに関しては、Universal Scene Description、略してUSDのようなオープンスタンダードに頼るのは素晴らしいことです。

00:07:21.000 --> 00:07:31.000
USDは生産が証明されており、単一の資産を作るクリエイターからAAAゲームや映画に取り組む大規模なスタジオまで拡大しています。

00:07:31.000 --> 00:07:39.000
Appleは米ドルを早期に採用し、2017年に当社のプラットフォームに追加し、それ以来サポートが増加しています。

00:07:39.000 --> 00:07:44.000
今日、USDは空間コンピューティングの3Dコンテンツの中心です。

00:07:44.000 --> 00:07:56.000
USDアセットの準備ができたら、新しい開発者ツールであるReality Composer Proに持ち込んで、3Dコンテンツを構成、編集、プレビューすることができます。

00:07:56.000 --> 00:08:03.000
iOSで3DコンテンツにCustomMaterialsを使用している場合は、シェーダーグラフを使用して再構築する必要があります。

00:08:03.000 --> 00:08:08.000
また、UIから直接RealityKitコンポーネントを編集することもできます。

00:08:08.000 --> 00:08:24.000
そして最後に、Reality Composer ProプロジェクトをXcodeに直接インポートして、すべてのUSDアセット、マテリアル、カスタムコンポーネントをXcodeプロジェクトに簡単にバンドルできます。

00:08:24.000 --> 00:08:34.000
Reality Composer Proの詳細と、空間コンピューティング用の独自のカスタム資料を構築する方法を学ぶのに役立つ素晴らしいセッションがいくつかあります。

00:08:34.000 --> 00:08:44.000
アプリケーションを提示するさまざまな方法を見たので、体験をもたらす際にRealityViewが提供する機能について詳しく学びましょう。

00:08:44.000 --> 00:08:48.000
空間コンピューティングにより、アプリがあなたのスペースにコンテンツを表示する方法を見ました。

00:08:48.000 --> 00:08:55.000
iOSから生じる重要な違いの1つは、異なる要素を並べて提示する方法です。

00:08:55.000 --> 00:09:03.000
3Dコンテンツと2D要素がどのように表示され、互いに連携するかに注目してください。

00:09:03.000 --> 00:09:08.000
iOSから来て、あなたはこれらのそれぞれを作成するために使い慣れたフレームワークを使用します。

00:09:08.000 --> 00:09:15.000
SwiftUIを使用して、最高の2D UIを構築し、iOSのようなシステムジェスチャーイベントを取得します。

00:09:15.000 --> 00:09:21.000
また、RealityKitを使用して、空間体験のために3Dコンテンツをレンダリングします。

00:09:21.000 --> 00:09:34.000
これら両方と同時にインターフェースする方法は、空間コンピューティングのユニークなニーズに応えるために導入している新しいSwiftUIビューであるRealityViewです。

00:09:34.000 --> 00:09:45.000
RealityViewはSwiftUIとRealityKitを真に橋渡しし、2Dと3Dの要素を組み合わせて、思い出に残る空間体験を作り出すことができます。

00:09:45.000 --> 00:09:52.000
RealityViewを使用して、表示および対話するすべてのエンティティを保持します。

00:09:52.000 --> 00:09:57.000
ジェスチャーイベントを取得し、ビューのエンティティに接続して制御できます。

00:09:57.000 --> 00:10:09.000
また、ARKitのシーン理解にアクセスすることで、RealityKitの衝突コンポーネントを使用して、人々の周囲や手との現実的なシミュレーションを有効にすることができます。

00:10:09.000 --> 00:10:20.000
RealityKitの使用がiOSからどのように引き継がれているかを見る前に、RealityKitのエンティティコンポーネントシステムの操作方法について簡単に復習しましょう。

00:10:20.000 --> 00:10:27.000
リアリティキットエンティティコンポーネントシステムでは、各エンティティは3Dコンテンツのコンテナです。

00:10:27.000 --> 00:10:31.000
さまざまなコンポーネントがエンティティに追加され、その外観と動作が定義されます。

00:10:31.000 --> 00:10:41.000
これには、レンダリング方法のモデルコンポーネント、他のエンティティと衝突する方法の衝突コンポーネントなどが含まれます。

00:10:41.000 --> 00:10:50.000
RealityComposer Proを使用して、衝突コンポーネントなどのRealityKitコンポーネントを準備し、エンティティに追加することができます。

00:10:50.000 --> 00:10:55.000
システムには、必要なコンポーネントを持つエンティティに作用するコードが含まれています。

00:10:55.000 --> 00:11:05.000
たとえば、ジェスチャーサポートに必要なシステムは、CollisionComponentとInputTargetComponentを持つエンティティでのみ動作します。

00:11:05.000 --> 00:11:14.000
RealityViewが空間コンピューティングに使用する概念の多くは、iOSのARViewの概念から引き継がれています。

00:11:14.000 --> 00:11:17.000
この2つがどのように積み重なるか見てみましょう。

00:11:17.000 --> 00:11:24.000
どちらのビューも、アプリに表示したいエンティティを保持するためのイベント対応コンテナです。

00:11:24.000 --> 00:11:30.000
ビューにジェスチャーサポートを追加して、エンティティとの選択とインタラクションを有効にすることができます。

00:11:30.000 --> 00:11:37.000
空間コンピューティング用のSwiftUIを使用すると、エンティティを選択またはドラッグすることができます。

00:11:37.000 --> 00:11:43.000
ARViewとRealityViewの両方が、エンティティのコレクションを提供します。

00:11:43.000 --> 00:11:46.000
ARViewはこれのためにシーンを使用します。

00:11:46.000 --> 00:11:50.000
RealityViewには、エンティティを追加するコンテンツがあります。

00:11:50.000 --> 00:11:56.000
AnchorEntitiesを追加して、コンテンツを現実世界に固定することができます。

00:11:56.000 --> 00:12:03.000
どちらのプラットフォームでも、コンテンツモデルをロードするエンティティと、それを配置するAnchorEntityを作成します。

00:12:03.000 --> 00:12:09.000
プラットフォームの主な違いの1つは、アンカーエンティティの動作です。

00:12:09.000 --> 00:12:21.000
iOS上のARViewはARSessionを使用しており、アプリはアンカーエンティティが機能するために必要なシーン理解アルゴリズムを実行する許可を受ける必要があります。

00:12:21.000 --> 00:12:26.000
RealityViewは、システムサービスを使用してanchorEntitiesを有効にしています。

00:12:26.000 --> 00:12:34.000
これは、空間体験が許可を必要とせずに周囲にコンテンツを固定できることを意味します。

00:12:34.000 --> 00:12:41.000
このアプローチを使用するアプリは、基礎となるシーンの理解データや変換を受信しません。

00:12:41.000 --> 00:12:50.000
アプリがコンテンツを配置するための変換データを持っていないことは、オリバーが後で彼のセクションで話すいくつかの意味合いがあります。

00:12:50.000 --> 00:13:01.000
ご覧のように、iOSから引き継がされる多くのおなじみの概念がありますが、RealityKitが空間コンピューティングに提供する新機能もあります。

00:13:01.000 --> 00:13:12.000
私たちは、この新しいプラットフォームでRealityKitで何が可能かの表面を引っ掻いただけであり、あなたはより多くのフォローアップのために以下のセッションをチェックしたいかもしれません。

00:13:12.000 --> 00:13:19.000
さて、オリバーがRealityViewとiOSからコンテンツを持ち込む方法について詳しく話します。

00:13:19.000 --> 00:13:20.000
オリバー・ダンクレー:ありがとう、オミッド!

00:13:20.000 --> 00:13:26.000
既存のコンテンツを空間コンピューティングに持ち込むさまざまな方法を探り続けましょう。

00:13:26.000 --> 00:13:28.000
共有スペースから始めましょう。

00:13:28.000 --> 00:13:34.000
ウィンドウやボリュームに3Dコンテンツを追加し、システムジェスチャーを使用して操作することができます。

00:13:34.000 --> 00:13:39.000
アセットを表示するには、RealityViewのコンテンツに直接追加するだけです。

00:13:39.000 --> 00:13:46.000
これを行うには、モデルコンポーネントを保持するエンティティを作成し、変換コンポーネントを設定して配置します。

00:13:46.000 --> 00:13:51.000
ジェスチャーサポートを設定して、変換コンポーネントを変更することもできます。

00:13:51.000 --> 00:14:01.000
ビューのコンテンツに追加されたすべてのエンティティは、スペースの原点に対して同じスペースに存在するため、互いに対話できることに注意してください。

00:14:01.000 --> 00:14:05.000
共有スペースでは、コンテンツを周囲に固定することはできません。

00:14:05.000 --> 00:14:09.000
アプリをフルスペースに移行する場合は、オプションを検討しましょう。

00:14:09.000 --> 00:14:16.000
共有スペースからの主な違いの1つは、アプリが人々の周囲にコンテンツをさらに固定できるようになったことです。

00:14:16.000 --> 00:14:19.000
ここでコンテンツを固定することは、2つの方法で行うことができます。

00:14:19.000 --> 00:14:28.000
まず、RealityKitのAnchorEntityを使用して、アプリでARKitデータを使用する許可を必要とせずにコンテンツを配置することを見てみましょう。

00:14:28.000 --> 00:14:36.000
RealityKitのAnchorEntitiesを使用すると、システムがコンテンツを見つけて自動的にアンカーするターゲットを指定できます。

00:14:36.000 --> 00:14:46.000
たとえば、目の前のテーブル面に3Dモデルを配置するには、ターゲットをテーブルに設定したRealityKit AnchorEntityを使用できます。

00:14:46.000 --> 00:14:52.000
iOSとは異なり、AnchorEntitiesはユーザー許可を求めることなく使用できます。

00:14:52.000 --> 00:15:00.000
人々のプライバシーは、AnchorEntityの根底にある変換をアプリケーションと共有しないことによって保護されます。

00:15:00.000 --> 00:15:05.000
注：これは、異なるアンカーエンティティの子がお互いを認識していないことを意味します。

00:15:05.000 --> 00:15:12.000
anchorEntitiesの初心者は、手をターゲットにすることができ、興味深い相互作用の機会の全く新しい領域を開きます。

00:15:12.000 --> 00:15:19.000
たとえば、コンテンツを手のひらに固定して、手を動かすときに手を追うことができます。

00:15:19.000 --> 00:15:25.000
これはすべて、人の手が実際にどこにあるかをアプリに伝えることなく、システムによって行われます。

00:15:25.000 --> 00:15:32.000
AnchorEntitysは、アプリがコンテンツを人々の周囲に固定するための迅速でプライバシーに優しい方法を提供します。

00:15:32.000 --> 00:15:39.000
フルスペースに戻ると、ARKitを活用して、人々の環境に関するシステムレベルの知識を組み込むこともできます。

00:15:39.000 --> 00:15:43.000
これにより、独自のカスタム配置ロジックを構築できます。

00:15:43.000 --> 00:15:46.000
これがどのように機能するかを見てみましょう。

00:15:46.000 --> 00:15:51.000
iOSと同様に、アプリケーションはシーン理解データのアンカー更新を受け取ります。

00:15:51.000 --> 00:15:58.000
このアンカーデータをアプリのロジックに統合して、あらゆる種類の素晴らしい体験を実現できます。

00:15:58.000 --> 00:16:03.000
たとえば、平面の境界を使用して、コンテンツを中央に配置して配布することができます。

00:16:03.000 --> 00:16:11.000
または、飛行機とその分類を使用して、2つの壁と床の交差点を探して部屋の隅を見つけることもできます。

00:16:11.000 --> 00:16:20.000
コンテンツをどこに配置するかを決めたら、ARKitが追跡するためのワールドアンカーを追加し、それを使用してエンティティの変換コンポーネントを更新します。

00:16:20.000 --> 00:16:31.000
これにより、基礎となる世界地図が更新されるにつれて、コンテンツが現実世界に固定されたままになるだけでなく、アンカーの永続性への扉も開きます。

00:16:31.000 --> 00:16:37.000
あなたのスペースに追加されたすべてのエンティティは、周囲だけでなく、互いに相互作用することができます。

00:16:37.000 --> 00:16:43.000
シーン理解アンカーは、空間の原点に対する変換で配信されるため、これはすべて機能します。

00:16:43.000 --> 00:16:48.000
ARKit機能を使用するには、ユーザーの許可が必要です。

00:16:48.000 --> 00:16:54.000
ARKitデータをアプリロジックに統合することで、より高度な機能を有効にする方法を見ました。

00:16:54.000 --> 00:16:57.000
これまでのところ、あなたのアプリにコンテンツを配置させることについて話しました。

00:16:57.000 --> 00:17:01.000
人々に配置を指導させる方法を探りましょう。

00:17:01.000 --> 00:17:07.000
iOSでは、レイキャストを使用して2D入力を3D位置に変換できます。

00:17:07.000 --> 00:17:16.000
しかし、この新しいプラットフォームでは、手を使って経験と直接対話できるため、この2D-3Dブリッジはもう必要ありません。

00:17:16.000 --> 00:17:22.000
レイキャスティングは依然として強力です。それは人々が腕の長さを超えて手を差し伸べることを可能にします。

00:17:22.000 --> 00:17:25.000
レイキャスティングを設定するにはさまざまな方法があります。

00:17:25.000 --> 00:17:30.000
基本的に、RealityKitの衝突コンポーネントをレイキャストに設定する必要があります。

00:17:30.000 --> 00:17:37.000
衝突コンポーネントは、ARKitのメッシュアンカーから人々の周囲に対してレイキャストするために作成することもできます。

00:17:37.000 --> 00:17:47.000
空間コンピューティングのためにレイキャストする方法の2つの例を探りましょう。1つ目はシステムジェスチャーを使用し、2つ目は針データを使用します。

00:17:47.000 --> 00:17:54.000
ポジションを取得した後、コンテンツを固定したままにするためにARKit worldAnchorを配置することができます。

00:17:54.000 --> 00:17:56.000
次の例を考えてみましょう。

00:17:56.000 --> 00:18:01.000
私たちのアプリがモデラーのための感動的な3Dアセットを配置することを中心に展開していると想像してみてください。

00:18:01.000 --> 00:18:10.000
たぶん、この特定のシナリオでは、ある人が私たちのアプリを使用して、モデリングプロジェクトのためにワークベンチに仮想船を配置したいと考えています。

00:18:10.000 --> 00:18:13.000
これが私たちの船を置きたい作業台です。

00:18:13.000 --> 00:18:17.000
空のRealityViewから始めます。

00:18:17.000 --> 00:18:22.000
ARKitのシーン理解は、周囲を表現するために使用するメッシュアンカーを提供します。

00:18:22.000 --> 00:18:26.000
それらは私たちが使用できる幾何学と意味情報を提供します。

00:18:26.000 --> 00:18:32.000
シーン再構築データのメッシュは、一連のチャンクとして配信されることを忘れないでください。

00:18:32.000 --> 00:18:42.000
このメッシュチャンクを表すエンティティを作成し、メッシュアンカーの変換を使用して、このエンティティを完全なスペースに正しく配置します。

00:18:42.000 --> 00:18:46.000
その後、私たちのエンティティは、テストを打つために衝突コンポーネントを必要とします。

00:18:46.000 --> 00:18:54.000
RealityKitのShapeResourcesメソッドを使用して、エンティティのmeshAnchorから衝突形状を生成します。

00:18:54.000 --> 00:18:59.000
次に、ヒットテストをサポートする正しく配置されたエンティティを追加します。

00:18:59.000 --> 00:19:06.000
受け取った各メッシュチャンクのエンティティと衝突コンポーネントを構築し、すべての環境を表します。

00:19:06.000 --> 00:19:11.000
シーンの再構築が洗練されるにつれて、メッシュを更新したり、チャンクを削除したりすることがあります。

00:19:11.000 --> 00:19:16.000
これらの変更についても、エンティティを更新する準備ができているはずです。

00:19:16.000 --> 00:19:20.000
私たちは今、周囲を代表するエンティティのコレクションを持っています。

00:19:20.000 --> 00:19:25.000
これらのエンティティはすべて衝突コンポーネントを持ち、レイキャストテストをサポートできます。

00:19:25.000 --> 00:19:32.000
まず、システムジェスチャーを使用してレイキャストを探索し、その後、手のデータを使用して例を続けましょう。

00:19:32.000 --> 00:19:37.000
レイキャストし、システムのジェスチャーを使用して船を配置する位置を得ることができます。

00:19:37.000 --> 00:19:46.000
ジェスチャーは、CollisionコンポーネントとInputTargetコンポーネントの両方を持つエンティティとのみ対話できるため、各メッシュエンティティに1つを追加します。

00:19:46.000 --> 00:19:53.000
RealityViewにSpatialTapGestureを追加することで、人々はエンティティを見てタップすることでレイキャストすることができます。

00:19:53.000 --> 00:20:00.000
この結果として生じるイベントは、人々がタップしたときに見た場所を表す世界空間での地位を保持しています。

00:20:00.000 --> 00:20:05.000
システムのジェスチャーを使用する代わりに、ARKitのハンドアンカーを使用してレイを構築することもできます。

00:20:05.000 --> 00:20:08.000
一歩下がって、このオプションを探りましょう。

00:20:08.000 --> 00:20:12.000
人々がどこを指しているのかを知るには、まずその人の手の表現が必要です。

00:20:12.000 --> 00:20:16.000
ARkitの新しいハンドアンカーは、私たちが必要とするすべてを提供します。

00:20:16.000 --> 00:20:23.000
フィンガージョイント情報を使用して、クエリの光線の起源と方向を構築できます。

00:20:23.000 --> 00:20:30.000
光線の起源と方向がわかったので、シーンのエンティティに対してレイキャストを行うことができます。

00:20:30.000 --> 00:20:37.000
結果として得られるCollisionCastHitは、ヒットしたエンティティとその位置と表面法線を提供します。

00:20:37.000 --> 00:20:46.000
コンテンツを配置するための世界の位置を特定したら、ARKitのワールドアンカーを追加して、この位置を継続的に追跡します。

00:20:46.000 --> 00:20:51.000
ARKitは、世界地図が洗練されるにつれて、この世界アンカーの変換を更新します。

00:20:51.000 --> 00:21:00.000
船のモデルをロードするための新しいエンティティを作成し、ワールドアンカーアップデートを使用してその変換を設定し、ユーザーが望む場所に配置することができます。

00:21:00.000 --> 00:21:05.000
最後に、エンティティをコンテンツに追加して、ワークベンチ上でレンダリングすることができます。

00:21:05.000 --> 00:21:15.000
ARKitが追加したワールドアンカーを更新するたびに、船舶エンティティの変換コンポーネントを更新し、現実世界に固定されたままであることを確認します。

00:21:15.000 --> 00:21:16.000
そして、それだけです!

00:21:16.000 --> 00:21:21.000
私たちは手を使って周囲の場所を指し、そこにコンテンツを配置しました。

00:21:21.000 --> 00:21:26.000
レイキャスティングは、コンテンツを配置するだけでなく、コンテンツとのやり取りにも役立ちます。

00:21:26.000 --> 00:21:30.000
私たちの仮想船に対してレイキャストするのに何が必要か見てみましょう。

00:21:30.000 --> 00:21:33.000
RealityKitの衝突コンポーネントは非常に強力です。

00:21:33.000 --> 00:21:42.000
Reality Composer Proが私たちを助けることができる適切な衝突コンポーネントを追加するだけで、船のエンティティを衝突に参加させることができます。

00:21:42.000 --> 00:21:53.000
船の衝突コンポーネントを有効にし、最新のハンドジョイント位置から新しい光線を構築した後、別のレイキャストを行い、ユーザーが船とテーブルのどちらを指しているかを知ることができます。

00:21:53.000 --> 00:22:03.000
前の例では、RealityKitの機能とARKitのシーン理解を組み合わせて、真に魅力的な体験を構築するパワーと汎用性を示しました。

00:22:03.000 --> 00:22:08.000
ARkitの使用が空間コンピューティングでどのように変化したかを見てみましょう。

00:22:08.000 --> 00:22:15.000
基本的に、iOSと同様に、ARKitはアンカーの更新を受信するセッションを実行することで機能します。

00:22:15.000 --> 00:22:21.000
セッションの設定と実行、アンカーの更新の受信、ワールドアンカーの永続化方法は、この新しいプラットフォームで変更されました。

00:22:21.000 --> 00:22:23.000
見てみましょう！ 

00:22:23.000 --> 00:22:28.000
iOSでは、ARKitはさまざまな構成から選択できます。

00:22:28.000 --> 00:22:33.000
各構成には、あなたの経験に必要な機能がバンドルされています。

00:22:33.000 --> 00:22:43.000
たとえば、ここではARWorldTrackingConfigurationを選択し、メッシュのsceneReconstructionと平面のplaneDetectionを有効にします。

00:22:43.000 --> 00:22:49.000
その後、ARSessionを作成し、選択した構成で実行できます。

00:22:49.000 --> 00:22:56.000
この新しいプラットフォームでは、ARKitは各シーン理解機能のデータプロバイダーを公開するようになりました。

00:22:56.000 --> 00:23:02.000
ハンドトラッキングは、ARKitが提供する新しい機能であり、独自のプロバイダーも取得します。

00:23:02.000 --> 00:23:08.000
各データプロバイダーの初期化子は、そのプロバイダーインスタンスを設定するために必要なパラメータを取ります。

00:23:08.000 --> 00:23:18.000
プリセット構成のカタログから選択する代わりに、アプリケーションに必要なプロバイダーをアラカルトで選択できます。

00:23:18.000 --> 00:23:26.000
たとえば、メッシュアンカーを受信するSceneReconstructionProviderと、平面アンカーを受信するPlaneDetectionProviderを選択します。

00:23:26.000 --> 00:23:32.000
プロバイダーを作成し、受信したいメッシュ分類と平面タイプを初期化します。

00:23:32.000 --> 00:23:40.000
次に、ARKitSessionを作成し、インスタンス化されたプロバイダーで実行します。

00:23:40.000 --> 00:23:50.000
セッションの設定がどのように簡素化されたかを見たので、これらの新しいデータプロバイダーがアプリが実際にARKitデータを受信する方法をどの方法で変更するかを理解しましょう。

00:23:50.000 --> 00:23:54.000
iOSでは、1人のデリゲートがアンカーとフレームの更新を受け取ります。

00:23:54.000 --> 00:24:00.000
アンカーは、カメラフレームとアンカーを同期させるためにARFramesで集約され、配信されます。

00:24:00.000 --> 00:24:09.000
アプリケーションは、カメラのピクセルバッファを表示し、カメラ変換を使用して追跡された仮想コンテンツを登録およびレンダリングする責任があります。

00:24:09.000 --> 00:24:17.000
メッシュアンカーとプレーンアンカーはベースアンカーとして配信され、どれがどれであるかを把握するためにそれらを曖昧にするのはあなた次第です。

00:24:17.000 --> 00:24:22.000
私たちの新しいプラットフォームでは、アンカーアップデートを提供するのはデータプロバイダーです。

00:24:22.000 --> 00:24:25.000
以前に設定したプロバイダーは次のとおりです。

00:24:25.000 --> 00:24:33.000
ARKitSessionを実行すると、各プロバイダーはすぐにアンカーアップデートの非同期公開を開始します。

00:24:33.000 --> 00:24:39.000
SceneReconstructionProviderはmeshAnchorsを提供し、planeDetectionProviderはPlaneAnchorsを提供します。

00:24:39.000 --> 00:24:42.000
曖昧さ回避は必要ありません!

00:24:42.000 --> 00:24:48.000
アンカーの更新は、利用可能になるとすぐに提供され、他のデータプロバイダーの更新から切り離されます。

00:24:48.000 --> 00:24:52.000
ARFrameはもはや提供されていないことに注意することが重要です。

00:24:52.000 --> 00:25:00.000
空間コンピューティングアプリケーションは、システムによって自動的に行われるため、コンテンツを表示するためにフレームやカメラデータを必要としません。

00:25:00.000 --> 00:25:12.000
ARFrameでアンカーアップデートをパッケージ化することなく、ARKitはそれらをすぐに配信し、レイテンシを削減し、アプリケーションが周囲のアップデートにすばやく反応できるようにします。

00:25:12.000 --> 00:25:16.000
次に、ワールドアンカーの永続性について話しましょう。

00:25:16.000 --> 00:25:18.000
あなたはこれらの変化を気に入るはずです!

00:25:18.000 --> 00:25:24.000
レイキャスティングの例では、ワールドアンカーを使用して、仮想コンテンツを現実世界の位置に配置およびアンカーしました。

00:25:24.000 --> 00:25:32.000
アプリはこれらのアンカーを永続化することができ、デバイスが同じ環境に戻ったときに自動的に再び受信できるようにします。

00:25:32.000 --> 00:25:37.000
まず、iOSで永続性がどのように機能したかを簡単に要約しましょう。

00:25:37.000 --> 00:25:43.000
iOSでは、世界地図とアンカーの永続性を処理するのはアプリケーションの責任です。

00:25:43.000 --> 00:26:00.000
これには、追加されたアンカーを使用してARKitの世界地図を要求して保存し、適切なタイミングで正しい世界地図をリロードするロジックを追加し、以前に持続したアンカーを受け取り、アプリケーションエクスペリエンスを継続する前に再ローカリゼーションが完了するのを待つことが含まれます。

00:26:00.000 --> 00:26:13.000
この新しいプラットフォームでは、システムは世界地図をバックグラウンドで継続的に保持し、人々が移動するにつれて既存の地図にシームレスにロード、アンロード、作成、再ローカライズします。

00:26:13.000 --> 00:26:18.000
あなたのアプリケーションはもうマップを処理する必要はありません、システムは今あなたのためにそれを行います!

00:26:18.000 --> 00:26:24.000
ワールドアンカーを使用して仮想コンテンツの場所を永続化することに集中するだけです。

00:26:24.000 --> 00:26:31.000
コンテンツを配置するときは、新しいWorldTrackingProviderを使用してWorldAnchorsを世界地図に追加します。

00:26:31.000 --> 00:26:34.000
システムはこれらを自動的に保存します。

00:26:34.000 --> 00:26:40.000
WorldTrackingProviderは、これらの世界アンカーの追跡ステータスと変換を更新します。

00:26:40.000 --> 00:26:47.000
WorldAnchor識別子を使用して、対応する仮想コンテンツをロードまたはアンロードできます。

00:26:47.000 --> 00:26:54.000
私たちは、あなたがiOSから知っていたARKitの原則のいくつかのアップデートを強調しましたが、探求すべきことはもっとたくさんあります!

00:26:54.000 --> 00:27:01.000
コード例など、より深く掘り下げるには、「空間コンピューティングのためのARKitの出会い」を見ることをお勧めします。

00:27:01.000 --> 00:27:02.000
このセッションを締めくくりましょう!

00:27:02.000 --> 00:27:14.000
このセッションでは、ARKitとRealityKitの概念がiOSからどのように進化したか、考慮する必要がある変更点、および詳細についてはどのセッションを見るべきかについて、高レベルの理解を提供しました。

00:27:14.000 --> 00:27:26.000
このプラットフォームは、iOSアプリが処理しなければならなかった多くのタスクを引き受け、すでに慣れ親しんでいるフレームワークやコンセプトを使用して、美しいコンテンツや体験を構築することに本当に集中することができます。

00:27:26.000 --> 00:27:32.000
私たちは、あなたがあなたのアプリを進化させるために空間コンピューティングとそのすべての驚くべき機能をどのように活用するかを見て興奮しています!

00:27:32.000 --> 00:27:33.000
見てくれてありがとう!

00:27:33.000 --> 23:59:59.000
♪

