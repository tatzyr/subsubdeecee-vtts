WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:13.000
アダム：ようこそ、私はVisionKitのエンジニアであるアダムです。

00:00:13.000 --> 00:00:20.000
今日は、チームが今年取り組んでいる新機能やAPIについてお話しできることを嬉しく思います。

00:00:20.000 --> 00:00:31.000
要約すると、昨年、Live TextサポートがVisionKitに追加され、テキスト選択、翻訳、QRサポートなどのインタラクションがアプリ内の画像に対応しました。

00:00:31.000 --> 00:00:44.000
VisionKitはまた、DataScannerViewControllerを導入しました。データスキャナーはライブカメラフィードを使用して、特定のテキストタイプと機械読み取り可能なコードの多くのバリアントをキャプチャするためのシンプルでフル機能の方法を提供します。

00:00:44.000 --> 00:00:50.000
これらのAPIに関する情報は、これらのWWDC22セッションに含まれています。

00:00:50.000 --> 00:01:00.000
開発者からの反応は素晴らしく、今年はVisionKitがSubject LiftingとVisual Look Upの両方のサポートを追加したことを発表できることを嬉しく思います。

00:01:00.000 --> 00:01:09.000
また、テキスト選択のための新しいLive Text API、Catalystの拡張プラットフォームサポート、ネイティブmacOSアプリのコンテキストメニュー統合もあります。

00:01:09.000 --> 00:01:13.000
そして今、私はサブジェクトリフティングを始めるつもりです。

00:01:13.000 --> 00:01:26.000
画像の被写体を長押しするだけで、周囲から持ち上げ、この美しいアニメーションの輝きで強調表示され、それを共有したり、ビジュアルルックアップを呼び出すためのいくつかのオプションが表示されます。

00:01:26.000 --> 00:01:35.000
iOS 17の新機能で、持ち上げられた被写体を使用してステッカーを作成し、光沢のある、ふくらんでいるなどの楽しい効果で、友人や家族と共有できるようになりました。

00:01:35.000 --> 00:01:40.000
さて、良いニュースは、サブジェクトリフティングの統合は非常に簡単だということです。

00:01:40.000 --> 00:01:44.000
実際、あなたはすでに終わっている可能性が高いです。

00:01:44.000 --> 00:01:50.000
これは、画像を分析してインタラクションに設定した昨年のビデオと同じコードスニペットです。

00:01:50.000 --> 00:01:54.000
しかし、今では、コードを変更することなく、サブジェクトリフティングをサポートしています。

00:01:54.000 --> 00:01:56.000
さらに探検しましょう。

00:01:56.000 --> 00:02:00.000
アナライザの構成に特別なことは何も渡していないことに注意してください。

00:02:00.000 --> 00:02:10.000
これは、パワーとパフォーマンスを維持するために、サブジェクトリフティング分析は、初期分析が完了した後、相互作用によって別々に処理されるためです。

00:02:10.000 --> 00:02:18.000
iOSの場合、このプロセスは数秒間画面に表示された後に発生し、macOSの場合、メニューが初めて表示されたときに発生します。

00:02:18.000 --> 00:02:22.000
これは、ユーザーが多くの写真をスワイプするケースを処理する必要がないことを意味します。

00:02:22.000 --> 00:02:25.000
インタラクションはあなたのためにこれを処理します。

00:02:25.000 --> 00:02:33.000
必要なのは、適切なインタラクションタイプ（この場合は自動）が設定されていることを確認するだけで、残りはインタラクションによって処理されます。

00:02:33.000 --> 00:02:38.000
サブジェクトリフティングの互換性のあるインタラクションタイプをもう少し詳しく調べてみましょう。

00:02:38.000 --> 00:02:45.000
自動は、テキストインタラクション、サブジェクトリフティングなどを組み合わせて、箱から出してデフォルトのエクスペリエンスを提供します。

00:02:45.000 --> 00:02:56.000
テキスト選択やデータ検出器ではなく、サブジェクトリフティングのみが必要な場合は、インタラクションタイプを.imageSegmentationに設定するか、他のタイプと組み合わせることができます。

00:02:56.000 --> 00:03:07.000
そして最後に、サブジェクトリフティングがアプリにとって意味をなさないが、iOS 16からの以前の自動動作が必要な場合は、問題ありません。新しいタイプの.automaticTextOnlyを使用できます。

00:03:07.000 --> 00:03:13.000
これは、テキスト選択やデータ検出器などの機能を提供しますが、サブジェクトリフティングは提供しません。

00:03:13.000 --> 00:03:23.000
VisionKitとVisionの両方でこの驚くべき新技術に関する高度なトピックを学びたい場合は、特にサブジェクトリフティングに関する詳細なセッションがあります。

00:03:23.000 --> 00:03:28.000
今年はVisionKitがVisual Look Upもサポートしています。

00:03:28.000 --> 00:03:35.000
ビジュアルルックアップを使用すると、ユーザーはペット、自然、ランドマーク、アート、メディアについて簡単に識別して学ぶことができます。

00:03:35.000 --> 00:03:42.000
また、iOS 17では、Visual Look Upは、食品、製品、記号や記号などの追加ドメインをサポートします。

00:03:42.000 --> 00:03:46.000
さて、最後に、洗濯タグのこれらのシンボルが何を意味するのかを簡単に調べることができます。

00:03:46.000 --> 00:03:49.000
つまり、あなたが私に尋ねるなら、それはかなりクールです。

00:03:49.000 --> 00:03:55.000
ビジュアルルックアップの可用性は言語に基づいており、これらの言語で利用できます。

00:03:55.000 --> 00:03:59.000
ボンネットの下をちょっと覗いて、Visual Look Upの仕組みを探りましょう。

00:03:59.000 --> 00:04:02.000
それは実際には2部構成のプロセスです。

00:04:02.000 --> 00:04:07.000
初期処理は、分析時にデバイス上で完全に行われます。

00:04:07.000 --> 00:04:15.000
.visualLookUpタイプがアナライザ構成に存在する場合、Visual Look Upは結果の境界ボックスとそのトップレベルドメインを見つけます。

00:04:15.000 --> 00:04:18.000
例えば、それが猫、本、または植物の場合。

00:04:18.000 --> 00:04:22.000
このステップには、特徴の抽出も含まれます。

00:04:22.000 --> 00:04:32.000
ユーザーがオブジェクトの検索をリクエストすると、機能抽出からのドメインと画像の埋め込みが追加の処理のためにサーバーに送信されます。

00:04:32.000 --> 00:04:40.000
ビジュアルルックアップの仕組みがわかったので、その使用方法と、アプリに追加するために必要なアクションをすばやく探りましょう。

00:04:40.000 --> 00:04:43.000
ビジュアルルックアップは2つの異なる方法で呼び出すことができます。

00:04:43.000 --> 00:04:57.000
1つ目は、サブジェクトリフティングと組み合わせて、現在持ち上げられたサブジェクトに1つだけ相関するビジュアルルックアップ結果が含まれている場合、ルックアップオプションがメニューで提供され、それを選択すると、完全なルックアップ結果が表示されます。

00:04:57.000 --> 00:05:00.000
VisionKitは、このインタラクションを自動的に処理します。

00:05:00.000 --> 00:05:07.000
採用担当者として必要なのは、分析時にアナライザ構成に.visualLookUpを追加することだけです。

00:05:07.000 --> 00:05:14.000
第二に、各視覚的な検索結果の上にバッジが配置されるモーダルインタラクションがあります。

00:05:14.000 --> 00:05:22.000
ビューポートを離れると、バッジがどのようにコーナーに移動するかに注目してください。ユーザーはこれらのバッジをタップして検索結果を表示できます。

00:05:22.000 --> 00:05:28.000
これは、たとえば、写真アプリの情報ボタンやクイックルックをクリックするのと同じインタラクションです。

00:05:28.000 --> 00:05:34.000
このモードは、インタラクションの優先InteractionTypeとして.visualLookUpを設定することによって呼び出されます。

00:05:34.000 --> 00:05:38.000
ご注意：このタイプは他のインタラクションタイプよりも優先されます。

00:05:38.000 --> 00:05:44.000
たとえば、visualLookupモードが設定されていると同時にテキストやデータ検出器を選択することはできません。

00:05:44.000 --> 00:05:52.000
そのため、これは通常、ボタン、またはこのモードに出入りするための他のオーダーメイドの方法と組み合わせて使用されます。

00:05:52.000 --> 00:05:57.000
たとえば、クイックルックは情報ボタンを使用してビジュアルルックアップモードに入ります。

00:05:57.000 --> 00:06:03.000
では、ギアをシフトして、データスキャナーとライブテキストの新しいAPIと機能について話し合いましょう。

00:06:03.000 --> 00:06:11.000
iOS 16で導入されたDataScannerViewControllerは、ライブカメラビューファインダーでOCRを使用する最も簡単な方法であるように設計されています。

00:06:11.000 --> 00:06:18.000
iOS 17では、光フロートラッキングと通貨サポートで強化されています。

00:06:18.000 --> 00:06:23.000
オプティカルフロートラッキングは、ライブカメラ体験のためのテキストトラッキングを強化することができます。

00:06:23.000 --> 00:06:25.000
これがiOS 16にあるものです。

00:06:25.000 --> 00:06:30.000
highFrameRateTrackingを有効にした状態でテキストをスキャンしています。

00:06:30.000 --> 00:06:33.000
そして、これは光学フロートラッキングで得られるものです。

00:06:33.000 --> 00:06:37.000
今、ハイライトは以前よりもはるかに安定し、接地されていると感じています。

00:06:37.000 --> 00:06:47.000
光学フロートラッキングは、DataScannerViewControllerを使用するたびに無料で提供されますが、テキストを認識する場合にのみ使用でき、機械読み取り可能なコードは使用できません。

00:06:47.000 --> 00:06:53.000
また、特定のテキストコンテンツタイプが設定されていないテキストをスキャンする必要があります。

00:06:53.000 --> 00:06:57.000
そして最後に、もう一度、高いフレームレートの追跡が有効になっていることを確認してください。

00:06:57.000 --> 00:07:01.000
これは、便利なことに、デフォルトです。

00:07:01.000 --> 00:07:12.000
どのように設定しても、データスキャナは優れたテキストトラッキングを提供します。しかし、ユースケースがこの構成を可能にする場合、新しい光フロートラッキングはそれをさらに強化することができます。

00:07:12.000 --> 00:07:18.000
次に、データスキャナには、ユーザーが金銭的価値を見つけて操作できる新しいオプションがあります。

00:07:18.000 --> 00:07:21.000
有効にするのは信じられないほど簡単です。

00:07:21.000 --> 00:07:32.000
データスキャナの初期化子でテキスト認識を指定するときは、電子メールアドレスや電話番号などの他のコンテンツタイプと同様に、テキストコンテンツタイプを通貨に設定するだけです。

00:07:32.000 --> 00:07:37.000
次に、この新しいタイプを簡単な例でより詳細に探求します。

00:07:37.000 --> 00:07:44.000
データスキャナがテキスト内の通貨を認識すると、境界とトランスクリプトの両方が含まれます。

00:07:44.000 --> 00:07:49.000
トランスクリプトには通貨記号と金額の両方があります。

00:07:49.000 --> 00:07:54.000
これは、領収書のようなもののすべての値の合計を見つける例です。

00:07:54.000 --> 00:07:58.000
まず、現在のロケールを使用して通貨記号を取得します。

00:07:58.000 --> 00:08:07.000
認識されたアイテムストリームでデータスキャナの結果を待っている間、私は認識された各アイテムをループし、そのトランスクリプトをつかむことができます。

00:08:07.000 --> 00:08:13.000
トランスクリプトに興味のある通貨記号が含まれている場合は、先に進んで合計値を更新します。

00:08:13.000 --> 00:08:16.000
そして、ちょうどそのように、今、あなたはすべての値の合計を持つことになります。

00:08:16.000 --> 00:08:23.000
これは単なる簡単な例ですが、これは非常に強力です、私はあなたがこれで何を構築できるかを見て興奮しています。

00:08:23.000 --> 00:08:27.000
そして今、私はライブテキストの強化について話します。

00:08:27.000 --> 00:08:36.000
まず、ライブテキストは、サポートされている言語をタイ語とベトナム語を含むように拡大することで、より多くの地域に来ています。

00:08:36.000 --> 00:08:40.000
ライブテキストには、今年もドキュメント構造検出のための機能強化が含まれています。

00:08:40.000 --> 00:08:43.000
文書構造の検出?それはどういう意味ですか? 

00:08:43.000 --> 00:08:49.000
たとえば、iOS 16のライブテキストでは、リスト検出がサポートされています。

00:08:49.000 --> 00:08:57.000
これにより、メモなどのリストを理解するアプリにリストを簡単にコピーして貼り付けることができ、リストの書式設定が維持されます。

00:08:57.000 --> 00:09:01.000
ライブテキストは、数字や箇条書きなど、いくつかのリストスタイルを処理します。

00:09:01.000 --> 00:09:12.000
そして今、Live Textはテーブルに同じサポートを提供しているため、画像からNotesやNumbersなどのアプリケーションに構造化されたテーブルデータをはるかに簡単に取得できます。

00:09:12.000 --> 00:09:18.000
これで、この表を選択、コピー、Numbersに貼り付けることができ、構造が維持されます。

00:09:18.000 --> 00:09:22.000
必要に応じて、セルを自動的にマージする方法に注目してください。

00:09:22.000 --> 00:09:27.000
そして、ちょうどそのように、私は今、グラフでこの情報を視覚化することから数回クリックするだけです。

00:09:27.000 --> 00:09:29.000
いいね。

00:09:29.000 --> 00:09:31.000
そして、それだけではありません。

00:09:31.000 --> 00:09:35.000
また、ライブテキストにコンテキスト認識データ検出器を追加しています。

00:09:35.000 --> 00:09:41.000
この機能では、連絡先を追加するときにデータ検出器とその視覚的関係が使用されます。

00:09:41.000 --> 00:09:53.000
メールアドレスからこの連絡先を追加すると、周囲のデータ検出器からの追加情報が含まれ、このすべての情報を一度に簡単に追加できるようになったことに注意してください。

00:09:53.000 --> 00:09:57.000
名刺やチラシから連絡先を追加することは、かつてないほど簡単になりました。

00:09:57.000 --> 00:10:04.000
無料で入手できるこれらの素晴らしい機能に加えて、VisionKitにはテキスト専用の新しいAPIもあります。

00:10:04.000 --> 00:10:12.000
昨年は、画像分析のトランスクリプトプロパティにアクセスすることで、テキストコンテンツ全体を取得できました。

00:10:12.000 --> 00:10:20.000
フィードバックに基づいて、プレーンテキストと帰属テキスト、選択した範囲へのフルアクセス、および選択したテキストへの簡単なアクセスが可能になりました。

00:10:20.000 --> 00:10:27.000
新しいデリゲートメソッドもあるので、テキストの選択が変更されたときに気づき、必要に応じてUIを更新することができます。

00:10:27.000 --> 00:10:31.000
ユーザーが選択したものに依存する機能を簡単に追加できるようになりました。

00:10:31.000 --> 00:10:38.000
たとえば、メニュービルダーAPIを使用すると、現在のテキスト選択に基づいてリマインダーを作成するメニュー項目を挿入できます。

00:10:38.000 --> 00:10:42.000
画像分析インタラクションを所有するビューコントローラーから始めます。

00:10:42.000 --> 00:10:47.000
まず、選択したテキストをつかみ、空でないことを確認します。

00:10:47.000 --> 00:10:54.000
次に、選択したときにハンドラを呼び出すコマンドを作成し、コマンドを保持するメニューオブジェクトを作成します。

00:10:54.000 --> 00:10:59.000
そして最後に、共有メニューオプションの後にそのメニューを兄弟として挿入します。

00:10:59.000 --> 00:11:04.000
これで、コピーや共有などのシステム項目の横にカスタムメニューがあります。

00:11:04.000 --> 00:11:08.000
次に、拡張されたプラットフォームのサポートについて話します。

00:11:08.000 --> 00:11:10.000
そして今年、それはすべてMacについてです。

00:11:10.000 --> 00:11:16.000
iOSアプリからライブテキストをMacに簡単に持ち込むために、Catalystサポートを展開しています。

00:11:16.000 --> 00:11:26.000
また、ネイティブのmacOS APIとImageAnalysisOverlayViewを初めて使用する場合は、いくつかの詳細と、それらを採用するためのヒントについて説明しますので、お楽しみに。

00:11:26.000 --> 00:11:34.000
最後に、コンテキストメニューへのVisionKitのシンプルでシームレスな統合を提供する、メニューの新しいシステムについて説明します。

00:11:34.000 --> 00:11:37.000
触媒の採用は非常に簡単です。

00:11:37.000 --> 00:11:42.000
Catalystで画像分析のインタラクションを動作させるには、簡単な再コンパイルが必要です。

00:11:42.000 --> 00:11:53.000
ライブテキスト、サブジェクトリフティング、ビジュアルルックアップをサポートしていますが、残念ながらQRコードのサポートは、Catalyst環境またはVisionKitのネイティブmacOS APIでは利用できません。

00:11:53.000 --> 00:12:05.000
しかし、共有実装がある場合、Catalystのアナライザ構成に.machineReadableCodesを残すことは完全に安全であり、ノーオペになることをお知らせしたいと思います。

00:12:05.000 --> 00:12:14.000
また、Macでこの機能が必要な場合は、Vision FrameworkでQR検出サポートが利用可能であることに注意してください。

00:12:14.000 --> 00:12:18.000
今、私はネイティブのmacOS APIに移行しています。

00:12:18.000 --> 00:12:27.000
iOSと同様に、VisionKitを採用する際に注意する必要がある2つの主要なクラスがあります。ImageAnalyzerとImageAnalysisOverlayViewです。

00:12:27.000 --> 00:12:29.000
まず、簡単な部分です。

00:12:29.000 --> 00:12:34.000
Macのイメージアナライザーと分析プロセスはiOSと同じです。

00:12:34.000 --> 00:12:41.000
先に述べたように、機械読み取り可能なコードがno-opであることを除いて、すべてが同じであり、同じ方法で使用されます。

00:12:41.000 --> 00:12:51.000
iOS ImageAnalysisInteractionとmacOSのImageAnalysisOverlayViewの主な違いは、インタラクションがアプリケーションにどのように追加されるかです。

00:12:51.000 --> 00:13:00.000
iOSの場合、ImageAnalysisInteractionは、アプリのビュー階層にすでに存在するビューに追加されるUIInteractionです。

00:13:00.000 --> 00:13:03.000
しかし、UIInteractionはMacには存在しません。

00:13:03.000 --> 00:13:05.000
それで、あなたは何をしますか?

00:13:05.000 --> 00:13:11.000
この場合、名前が示すように、画像分析オーバーレイビューはNSViewのサブクラスです。

00:13:11.000 --> 00:13:17.000
画像コンテンツの上にあるビュー階層にオーバーレイビューを追加するだけです。

00:13:17.000 --> 00:13:20.000
例えば、ここに追加できます。

00:13:20.000 --> 00:13:21.000
あるいはここでも。

00:13:21.000 --> 00:13:26.000
しかし、最も簡単な方法は、それを私のコンテンツビューのサブビューとして追加することです。

00:13:26.000 --> 00:13:38.000
選択した方法は完全に問題ありませんが、サブビューとして追加することは、コンテンツビューの位置が変更されたときにオーバーレイビューの再配置を処理する必要がないため、一般的に簡単で管理が簡単であることがわかりました。

00:13:38.000 --> 00:13:44.000
そして今、あなたはそれをあなたのアプリに追加する方法と場所を知っています、長方形について話しましょう。

00:13:44.000 --> 00:13:51.000
OverlayViewはコンテンツをホストまたはレンダリングしないため、その境界に関連してコンテンツが存在する場所を正確に知る必要があります。

00:13:51.000 --> 00:13:58.000
これは、左上の原点を持つ単位座標空間にあるcontentsRectによって記述されます。

00:13:58.000 --> 00:14:00.000
うわー、それは一口でした。

00:14:00.000 --> 00:14:02.000
簡単な例は、これを明確にするのに役立つはずです。

00:14:02.000 --> 00:14:07.000
オーバーレイビューはimageViewの上に直接配置されるため、同じ境界があります。

00:14:07.000 --> 00:14:10.000
この長方形で境界を表示します。 境界を表示します。

00:14:10.000 --> 00:14:13.000
そして、一致する内容のrectも追加します。

00:14:13.000 --> 00:14:16.000
最も簡単なケースは、コンテンツが境界に一致する場合です。

00:14:16.000 --> 00:14:19.000
ここでは、単にユニットの長方形です。

00:14:19.000 --> 00:14:21.000
さて、ここにアスペクトフィットがあります。

00:14:21.000 --> 00:14:24.000
imageViewのこの部分には、その下にコンテンツがないことに注意してください。

00:14:24.000 --> 00:14:27.000
そして、内容のrectに反映されます。

00:14:27.000 --> 00:14:29.000
そして、ここにアスペクトフィルがあります。

00:14:29.000 --> 00:14:36.000
画像のこの部分はユーザーには表示されなくなりました。コンテンツがここでどのように変化するかに注目してください。

00:14:36.000 --> 00:14:38.000
さて、良いニュースがあります。

00:14:38.000 --> 00:14:50.000
iOSのUIImageViewと同様に、NSImageViewを使用している場合は、オーバーレイビューでtrackingImageViewプロパティを設定するだけで、これらすべてが自動的に計算されます。

00:14:50.000 --> 00:14:53.000
NSImageViewを使用していない場合は、心配しないでください。

00:14:53.000 --> 00:15:03.000
デリゲートメソッドcontentsRect(overlayView用:)を実装することで、コンテンツrectを提供できます。オーバーレイビューは、境界が変更されたときにレイアウト中にこれを要求します。

00:15:03.000 --> 00:15:10.000
ただし、overlayViewでsetContentsRectNeedsUpdateを呼び出すことで、この更新を手動で要求できます。

00:15:10.000 --> 00:15:13.000
さて、コンテキストメニューに移りましょう。

00:15:13.000 --> 00:15:18.000
ご存知のように、コンテクストメニューはMac体験の大きな部分を占めています。

00:15:18.000 --> 00:15:27.000
ライブテキスト、ルックアップ、サブジェクトリフティングなどの機能のために、VisionKitが提供する機能をメニューに直接簡単に追加できるようになりました。

00:15:27.000 --> 00:15:30.000
あなたが持つかもしれない1つの質問は、なぜですか?

00:15:30.000 --> 00:15:32.000
macOSの写真アプリを調べてみましょう。

00:15:32.000 --> 00:15:39.000
この象徴的な道路標識のテキストを右クリックすると、VisionKitのテキストメニューのみが表示されます。

00:15:39.000 --> 00:15:44.000
テキストオーバーでなければ、テキスト項目なしで、代わりにアプリメニューが提供されます。

00:15:44.000 --> 00:15:46.000
これは理想的ではありません。

00:15:46.000 --> 00:15:51.000
macOS Sonomaでは、アイテムを同じメニューにまとめることができます。

00:15:51.000 --> 00:15:56.000
メニューイベントがどこで開始されたかに関係なく、テキストと画像の両方の機能に簡単にアクセスできます。

00:15:56.000 --> 00:16:01.000
これはユーザーにとってはるかに優れた体験であり、実装が簡単です。

00:16:01.000 --> 00:16:04.000
これを自分のアプリでどのように達成できるかを探りましょう。

00:16:04.000 --> 00:16:09.000
これで、overlayview:updatedmenu:forevent:atpointという新しいデリゲートメソッドが利用可能になりました。

00:16:09.000 --> 00:16:18.000
引数には、メニューをトリガーしたイベントと、オーバーレイビュー境界のポイントがスペースを調整するため、必要なメニューを作成できます。

00:16:18.000 --> 00:16:22.000
そこから、表示したいメニューを返すだけです。

00:16:22.000 --> 00:16:25.000
デフォルトの実装は、VisionKitメニューを返します。

00:16:25.000 --> 00:16:32.000
ただし、そのメニューに独自のアイテムを追加したり、そのメニューからアイテムを取り出すこともできます。

00:16:32.000 --> 00:16:38.000
VisionKitのメニュー項目はタグで識別され、これらのタグを含む構造体があります。

00:16:38.000 --> 00:16:44.000
画像と被写体をコピーして共有できるアイテムと、ルックアップ用のアイテムがいくつかあります。

00:16:44.000 --> 00:16:52.000
また、VisionKitが提供するメニューにアイテムを追加するための推奨インデックスを見つけるために使用できる特別なアイテムもありますが、それについては後で詳しく説明します。

00:16:52.000 --> 00:16:55.000
これがどのように使用されるかの例をいくつか紹介します。

00:16:55.000 --> 00:17:02.000
既存のメニューがあり、私が興味を持っていたのはcopySubjectアイテムを追加することだけであれば、このように簡単に追加できます。

00:17:02.000 --> 00:17:05.000
まず、アプリのメニューを入手してください。

00:17:05.000 --> 00:17:06.000
その後、興味のあるアイテムを入手してください。

00:17:06.000 --> 00:17:09.000
この場合、copySubject。

00:17:09.000 --> 00:17:11.000
そして、それをあなたのメニューに挿入してください。

00:17:11.000 --> 00:17:15.000
さて、アイテムは実際に有効な場合にのみ利用可能であることを覚えておくことが重要です。

00:17:15.000 --> 00:17:22.000
たとえば、サブジェクトインタラクション可能なタイプが存在しない場合、copySubject項目はメニューにありません。

00:17:22.000 --> 00:17:29.000
また、システムが提供するテキスト項目については、該当する場合に含まれていますが、すべてがタグで識別できるわけではありません。

00:17:29.000 --> 00:17:33.000
これらのアイテムを好きなようにカスタマイズすることもできます。

00:17:33.000 --> 00:17:37.000
たとえば、アイテムをコピー画像からコピー写真に変更しました。

00:17:37.000 --> 00:17:39.000
そして、これらのプロパティを変更することを心配しないでください。

00:17:39.000 --> 00:17:44.000
これらのアイテムは毎回再作成され、好きなように変更できます。

00:17:44.000 --> 00:17:51.000
既存のメニューにアイテムを追加することを説明したので、VisionKitメニューにアイテムを追加する方法の例を探ってみようと思います。

00:17:51.000 --> 00:17:59.000
前述のように、overlayViewは、推奨インデックスにタグが付いたアイテムを提供し、sendedAppItemsというアイテムを挿入します。

00:17:59.000 --> 00:18:05.000
このアイテムのインデックスを尋ねて、そのインデックスにアイテムを挿入するだけです。

00:18:05.000 --> 00:18:08.000
このインデックスの使用はオプションであり、必須ではありません。

00:18:08.000 --> 00:18:13.000
しかし、それはあなたのユーザーのために物事の一貫性を保つための良い方法です。

00:18:13.000 --> 00:18:16.000
これらのメニュー項目の一部には特別な特性があることに気付くでしょう。

00:18:16.000 --> 00:18:29.000
たとえば、件名に関連するメニュー項目が強調表示されると、私の猫のKiKiを取り巻く領域が暗くなり、グローアニメーションが始まり、コピーまたは共有される前にユーザーに被写体を示します。

00:18:29.000 --> 00:18:35.000
VisionKitは、まだ開始されていない場合は、トリガーとして表示されるメニューを使用して主題分析を開始します。

00:18:35.000 --> 00:18:38.000
これはすべて自動的に処理されます。

00:18:38.000 --> 00:18:46.000
これらの機能を提供するために、VisionKitは、更新メニューメソッドから返されるメニューのデリゲートとして設定します。

00:18:46.000 --> 00:18:58.000
以前にこれらのNSMenuDelegateコールバックに依存していた場合、VisionKitは独自のデリゲートコールバックを提供し、以前に使用していた場合はメニューアイテムの機能を保持できるようになりました。

00:18:58.000 --> 00:19:00.000
そして、ここに簡単なヒントがあります。

00:19:00.000 --> 00:19:06.000
このような状況にある場合、メニューがどこから開始されたかによっては、VisionKitからではない可能性があります。

00:19:06.000 --> 00:19:10.000
したがって、既存の実装を維持したいと思うでしょう。

00:19:10.000 --> 00:19:21.000
一般的に、これをすべて同期させる最も簡単な方法は、OverlayViewDelegateの実装が一致するNSMenuDelegateの実装を呼び出し、必要に応じて調整することです。

00:19:21.000 --> 00:19:26.000
もちろん、これがあなたのアプリにとって理にかなっていることを確認してくださいが、一般的に、これは通常トリックを行います。

00:19:26.000 --> 00:19:29.000
そして、それはVisionKitの新機能の簡単な概要です。

00:19:29.000 --> 00:19:38.000
今日は、Subject LiftingとVisual Look Up、新しいmacOS APIと関連情報についてお話しできて光栄です。

00:19:38.000 --> 00:19:44.000
これらの新機能を使用して、顧客を喜ばせ、驚かせる方法を楽しみにしています。

00:19:44.000 --> 00:19:48.000
そして真面目な話、いつものように、楽しんでください!

00:19:48.000 --> 23:59:59.000
ありがとう！

