WEBVTT

00:00:00.000 --> 00:00:10.000
♪ ♪

00:00:10.000 --> 00:00:14.000
ベン：こんにちは、私はCore MLチームのエンジニアであるBen Levineです。

00:00:14.000 --> 00:00:19.000
今日は、Core MLをアプリに統合することに関する新機能について話します。

00:00:19.000 --> 00:00:23.000
アプリでインテリジェントな体験を構築することは、かつてないほど簡単になりました。

00:00:23.000 --> 00:00:30.000
Xcode SDKは、機械学習を活用し、展開するための強固な基盤を提供します。

00:00:30.000 --> 00:00:36.000
ドメイン固有のフレームワークのセットにより、シンプルなAPIを通じて組み込みのインテリジェンスにアクセスできます。

00:00:36.000 --> 00:00:42.000
彼らが提供する機能は、Appleによって訓練され、最適化されたモデルによって強化されています。

00:00:42.000 --> 00:00:45.000
これらのモデルはCore MLを介して実行されます。

00:00:45.000 --> 00:00:50.000
Core MLフレームワークは、デバイス上で機械学習モデルを実行するためのエンジンを提供します。

00:00:50.000 --> 00:00:55.000
アプリに合わせてカスタマイズされたモデルを簡単に展開できます。

00:00:55.000 --> 00:01:05.000
AccelerateとMetalファミリーのフレームワークの助けを借りて、Appleシリコンの高性能コンピューティング機能を活用しながら、ハードウェアの詳細を抽象化します。

00:01:05.000 --> 00:01:10.000
Core MLの使命は、機械学習モデルをアプリに統合できるようにすることです。

00:01:10.000 --> 00:01:15.000
今年、Core MLの焦点はパフォーマンスと柔軟性でした。

00:01:15.000 --> 00:01:21.000
ワークフロー、APIサーフェス、および基礎となる推論エンジンを改善しました。

00:01:21.000 --> 00:01:36.000
ワークフローに飛び込み、Core ML統合を最適化するための新しい機会を強調する前に、最新のOSにアップデートするだけで自動的に得られる潜在的なパフォーマンスのメリットのアイデアがあります。

00:01:36.000 --> 00:01:46.000
iOS 16と17の間の相対的な予測時間を比較すると、iOS 17は多くのモデルで単に高速であることがわかります。

00:01:46.000 --> 00:01:54.000
推論エンジンのこのスピードアップはOSに付属しており、モデルの再コンパイルやコードの変更は必要ありません。

00:01:54.000 --> 00:01:58.000
同じことが他のプラットフォームにも当てはまります。

00:01:58.000 --> 00:02:02.000
当然のことながら、スピードアップの量はモデルとハードウェアに依存します。

00:02:02.000 --> 00:02:08.000
議題に移り、Core MLをアプリに統合する際のワークフローの概要から始めます。

00:02:08.000 --> 00:02:14.000
その過程で、ワークフローのさまざまな部分の最適化の機会を強調します。

00:02:14.000 --> 00:02:28.000
次に、モデル統合に焦点を当て、コンピューティングの可用性、モデルライフサイクル、非同期予測に関する新しいAPIと動作について説明します。Core MLワークフローの概要から始めます。

00:02:28.000 --> 00:02:32.000
Core MLをアプリに統合するには2つの段階があります。

00:02:32.000 --> 00:02:37.000
1つ目はモデルの開発で、2つ目はアプリ内でそのモデルを使用することです。

00:02:37.000 --> 00:02:41.000
モデル開発には、いくつかの選択肢があります。

00:02:41.000 --> 00:02:46.000
独自のモデルを開発する最も便利な方法の1つは、Create MLを使用することです。

00:02:46.000 --> 00:02:55.000
Create MLは、一般的な機械学習タスクにさまざまなテンプレートを提供し、OSに組み込まれた高度に最適化されたモデルを活用できます。

00:02:55.000 --> 00:03:00.000
モデル開発ワークフローを案内し、結果をインタラクティブに評価できます。

00:03:00.000 --> 00:03:05.000
もっと詳しく知りたい場合は、今年のCreate MLビデオをチェックしてください。

00:03:05.000 --> 00:03:12.000
モデルを開発するもう1つの方法は、いくつかのPython機械学習フレームワークの1つを使用してモデルを訓練することです。

00:03:12.000 --> 00:03:18.000
次に、CoreMLTools pythonパッケージを使用して、Core MLモデル形式に変換します。

00:03:18.000 --> 00:03:25.000
最後に、Appleハードウェアの精度とパフォーマンスの両方の観点からモデルを評価することが重要です。

00:03:25.000 --> 00:03:32.000
評価からのフィードバックを使用すると、モデルをさらに最適化するために、これらのステップのいくつかを再検討することがよくあります。

00:03:32.000 --> 00:03:36.000
これらのステップでは、最適化の機会がたくさんあります。

00:03:36.000 --> 00:03:41.000
トレーニングでは、トレーニングデータをどのように収集して選択するかが重要です。

00:03:41.000 --> 00:03:47.000
展開時にモデルに渡されるデータと一致し、ユーザーの手にある必要があります。

00:03:47.000 --> 00:03:51.000
あなたが選んだモデルアーキテクチャも重要です。

00:03:51.000 --> 00:04:00.000
複数のオプションを検討している可能性があり、それぞれがトレーニングデータの要件、精度、サイズ、およびパフォーマンスの間で独自のトレードオフがあります。

00:04:00.000 --> 00:04:07.000
これらのトレードオフの多くは、トレーニング時に完全に表示されない可能性があり、完全な開発フローを通じていくつかの反復が必要です。

00:04:07.000 --> 00:04:10.000
次はモデル変換です。

00:04:10.000 --> 00:04:18.000
Core MLツールは、変換されたモデルの精度、フットプリント、および計算コストを最適化するのに役立つさまざまなオプションを提供します。

00:04:18.000 --> 00:04:25.000
不要なコピーを避けるために、アプリのデータフローに最も適した入出力フォーマットを選択できます。

00:04:25.000 --> 00:04:36.000
入力形状が異なる場合は、1つの形状を選択したり、複数の形状固有のモデルを切り替えたりするのではなく、そのバリエーションを指定できます。

00:04:36.000 --> 00:04:41.000
計算精度は、モデル全体または個々の操作に対して明示的に設定することもできます。

00:04:41.000 --> 00:04:46.000
Float32とfloat16の両方が利用可能です。

00:04:46.000 --> 00:04:53.000
計算の精度に加えて、モデルパラメータの表現方法もある程度制御できます。

00:04:53.000 --> 00:05:00.000
CoreMLToolsには、トレーニング後の重量の量子化と圧縮のための一連のユーティリティが付属しています。

00:05:00.000 --> 00:05:06.000
これらのユーティリティは、モデルのフットプリントを大幅に削減し、デバイス上のパフォーマンスを向上させるのに役立ちます。

00:05:06.000 --> 00:05:12.000
しかし、これらの利点を達成するためには、正確性にはいくつかのトレードオフがあります。

00:05:12.000 --> 00:05:20.000
このスペースに役立つ新しいツールがいくつかあります。CoreMLToolsパッケージに新しい最適化サブモジュールがあります。

00:05:20.000 --> 00:05:28.000
トレーニング後の圧縮ユーティリティを統一および更新し、PyTorchの新しい定量化対応トレーニング拡張機能を追加します。

00:05:28.000 --> 00:05:35.000
これにより、データ駆動型の最適化にアクセスして、トレーニング中に量子化されたモデルの精度を維持できます。

00:05:35.000 --> 00:05:43.000
これは、Core MLのMLプログラムモデルタイプのアクティベーション量子化をサポートする新しい操作と組み合わされています。

00:05:43.000 --> 00:05:50.000
詳細については、Core MLを使用した機械学習モデルの圧縮に関する今年のセッションをご覧ください。

00:05:50.000 --> 00:05:52.000
次は評価です。

00:05:52.000 --> 00:06:00.000
モデルを評価するオプションの1つは、CoreMLToolsを使用してPythonコードから直接変換されたモデルの予測を実行することです。

00:06:00.000 --> 00:06:11.000
アプリコードが使用するのと同じCore ML推論スタックを使用し、モデル変換中の選択がモデルの精度とパフォーマンスにどのように影響するかをすばやく確認できます。

00:06:11.000 --> 00:06:18.000
Xcodeは、モデルの評価と探索に関して役立つツールも提供しています。

00:06:18.000 --> 00:06:22.000
モデルプレビューは、多くの一般的なモデルタイプで利用できます。

00:06:22.000 --> 00:06:30.000
これにより、モデルにいくつかのサンプル入力を提供し、コードを書くことなく予測された出力をプレビューできます。

00:06:30.000 --> 00:06:40.000
Core MLパフォーマンスレポートは、接続されたデバイスの負荷、予測、およびコンパイル時間のモデル計算パフォーマンスの内訳を提供します。

00:06:40.000 --> 00:06:46.000
これは、モデルアーキテクチャをトレーニングする前であっても、モデルアーキテクチャを評価するのに役立つことに注意してください。

00:06:46.000 --> 00:06:52.000
さて、全体的なワークフローに戻ると、次のトピックはモデル統合です。

00:06:52.000 --> 00:06:55.000
モデル統合は、アプリの開発の一部です。

00:06:55.000 --> 00:07:04.000
アプリで使用する他のリソースと同様に、Core MLモデルの使用方法を慎重に管理し、最適化したいと考えています。

00:07:04.000 --> 00:07:06.000
モデル統合には3つのステップがあります。

00:07:06.000 --> 00:07:09.000
まず、モデルを使用するためのアプリケーションコードを作成します。

00:07:09.000 --> 00:07:18.000
モデルをいつロードするか、モデルの入力データを準備する方法、予測を行う方法、結果を使用する方法に関するコードがあります。

00:07:18.000 --> 00:07:21.000
次に、このコードをモデルと一緒にコンパイルします。

00:07:21.000 --> 00:07:27.000
そして第三に、アプリ内で実行されているモデルをテスト、実行、プロファイリングします。

00:07:27.000 --> 00:07:33.000
プロファイリングに関しては、Core MLとNeural Engineの機器が役に立つかもしれません。

00:07:33.000 --> 00:07:39.000
これはまた、出荷の準備が整うまでの設計と最適化の反復プロセスです。

00:07:39.000 --> 00:07:44.000
今年は、モデル統合を最適化するための新しい追加がいくつかあります。

00:07:44.000 --> 00:07:46.000
まず、コンピューティングの可用性です。

00:07:46.000 --> 00:07:54.000
Core MLはすべてのAppleプラットフォームでサポートされており、デフォルトでは実行を最適化するために利用可能なすべてのコンピューティングを考慮します。

00:07:54.000 --> 00:07:59.000
これには、利用可能な場合はCPU、GPU、およびニューラルエンジンが含まれます。

00:07:59.000 --> 00:08:08.000
ただし、これらのコンピューティングデバイスのパフォーマンス特性と可用性は、アプリが実行される可能性のあるサポートされているハードウェアによって異なります。

00:08:08.000 --> 00:08:15.000
これは、MLを搭載した機能に関するユーザーエクスペリエンスに影響を与えたり、モデルや構成の選択に影響を与える可能性があります。

00:08:15.000 --> 00:08:24.000
たとえば、一部の経験では、パフォーマンスや電力要件を満たすためにニューラルエンジンで実行されているモデルが必要になる場合があります。

00:08:24.000 --> 00:08:29.000
コンピューティングデバイスの可用性のランタイム検査のための新しいAPIができるようになりました。

00:08:29.000 --> 00:08:39.000
MLComputeDevice列挙型は、関連する値内のコンピューティングデバイスのタイプと特定のコンピューティングデバイスのプロパティをキャプチャします。

00:08:39.000 --> 00:08:47.000
MLModelのavailableComputeDevicesプロパティを使用すると、Core MLで利用可能なデバイスを検査できます。

00:08:47.000 --> 00:08:52.000
たとえば、このコードは、利用可能なニューラルエンジンがあるかどうかをチェックします。

00:08:52.000 --> 00:09:00.000
具体的には、利用可能なすべてのコンピューティングデバイスのコレクションに、タイプがNeural Engineであるものが含まれているかどうかをチェックします。

00:09:00.000 --> 00:09:04.000
モデル統合の次のトピックは、モデルのライフサイクルを理解することです。

00:09:04.000 --> 00:09:08.000
さまざまなモデル資産タイプを確認することから始めます。

00:09:08.000 --> 00:09:12.000
ソースモデルとコンパイルされたモデルの2種類があります。

00:09:12.000 --> 00:09:17.000
ソースモデルのファイル拡張子はMLModelまたはMLPackageです。

00:09:17.000 --> 00:09:21.000
これは、構築と編集のために設計されたオープンフォーマットです。

00:09:21.000 --> 00:09:25.000
コンパイルされたモデルのファイル拡張子はMLModelCです。

00:09:25.000 --> 00:09:28.000
ランタイムアクセス用に設計されています。

00:09:28.000 --> 00:09:37.000
ほとんどの場合、ソースモデルをアプリのターゲットに追加し、Xcodeがモデルをコンパイルしてアプリのリソースに入れます。

00:09:37.000 --> 00:09:43.000
実行時に、モデルを使用するには、MLModelをインスタンス化します。

00:09:43.000 --> 00:09:49.000
インスタンス化は、コンパイルされた形式のURLとオプションの設定を取ります。

00:09:49.000 --> 00:10:00.000
結果のMLModelは、指定された構成とデバイス固有のハードウェア機能に基づいて、最適な推論に必要なすべてのリソースをロードしました。

00:10:00.000 --> 00:10:04.000
この負荷中に何が起こるかを詳しく見てみましょう。

00:10:04.000 --> 00:10:12.000
まず、Core MLはキャッシュをチェックして、構成とデバイスに基づいてモデルをすでに専門化しているかどうかを確認します。

00:10:12.000 --> 00:10:17.000
あれば、キャッシュから必要なリソースをロードして返します。

00:10:17.000 --> 00:10:20.000
これはキャッシュされたロードと呼ばれます。

00:10:20.000 --> 00:10:27.000
設定がキャッシュに見つからなかった場合、デバイスに特化したコンパイルがトリガーされます。

00:10:27.000 --> 00:10:34.000
このプロセスが完了すると、キャッシュに出力を追加し、そこからロードを終了します。

00:10:34.000 --> 00:10:37.000
これはキャッシュされていない負荷と呼ばれます。

00:10:37.000 --> 00:10:42.000
特定のモデルでは、キャッシュされていない負荷にかなりの時間がかかることがあります。

00:10:42.000 --> 00:10:50.000
しかし、それはデバイスのモデルを最適化し、その後の負荷をできるだけ速くすることに重点を置いています。

00:10:50.000 --> 00:10:58.000
デバイスの特殊化中、Core MLは最初にモデルを解析し、一般的な最適化パスを適用します。

00:10:58.000 --> 00:11:06.000
次に、推定パフォーマンスとハードウェアの可用性に基づいて、特定のコンピューティングデバイスのオペレーションチェーンをセグメントします。

00:11:06.000 --> 00:11:09.000
その後、このセグメンテーションはキャッシュされます。

00:11:09.000 --> 00:11:17.000
最後のステップは、各セグメントが割り当てられたコンピューティングデバイスのコンピューティングデバイス固有のコンパイルを通過することです。

00:11:17.000 --> 00:11:26.000
このコンパイルには、特定のコンピューティングデバイスのさらなる最適化が含まれており、コンピューティングデバイスが実行できるアーティファクトを出力します。

00:11:26.000 --> 00:11:33.000
完了すると、Core MLはこれらのアーティファクトをキャッシュして、後続のモデルロードに使用します。

00:11:33.000 --> 00:11:38.000
Core MLは、特殊なアセットをディスクにキャッシュします。

00:11:38.000 --> 00:11:42.000
それらはモデルのパスと構成に結びついています。

00:11:42.000 --> 00:11:48.000
これらのアセットは、アプリの起動とデバイスの再起動にわたって持続することを目的としています。

00:11:48.000 --> 00:11:59.000
デバイスの空きディスク容量が不足している場合、システムアップデートがあった場合、またはコンパイルされたモデルが削除または変更された場合、オペレーティングシステムはキャッシュを削除します。

00:11:59.000 --> 00:12:05.000
これが発生すると、次のモデルロードはデバイスの特殊化を再び実行します。

00:12:05.000 --> 00:12:13.000
モデルのロードがキャッシュに当たっているかどうかを調べるには、Core ML Instrumentでアプリを追跡し、ロードイベントを見ることができます。

00:12:13.000 --> 00:12:23.000
「準備とキャッシュ」というラベルがある場合、それはキャッシュされていないロードだったので、Core MLはデバイスの特殊化を実行し、結果をキャッシュしました。

00:12:23.000 --> 00:12:30.000
ロードイベントに「キャッシュ」というラベルがある場合、それはキャッシュされたロードであり、デバイスの特殊化は発生しませんでした。

00:12:30.000 --> 00:12:34.000
これは特にMLProgramモデル向けの新しいものです。

00:12:34.000 --> 00:12:40.000
コアMLパフォーマンスレポートは、負荷のコストを可視化することもできます。

00:12:40.000 --> 00:12:45.000
デフォルトでは、キャッシュされた負荷の中央値が表示されます。

00:12:45.000 --> 00:12:50.000
キャッシュされていないロード時間も表示するオプションが追加されました。

00:12:50.000 --> 00:12:57.000
モデルの読み込みはレイテンシとメモリの面で費用がかかる可能性があるため、一般的なベストプラクティスをいくつか紹介します。

00:12:57.000 --> 00:13:01.000
まず、アプリの起動時にUIスレッドでモデルをロードしないでください。

00:13:01.000 --> 00:13:08.000
代わりに、非同期ロードAPIを使用するか、モデルを怠惰にロードすることを検討してください。

00:13:08.000 --> 00:13:18.000
次に、アプリケーションがシーケンス内の各予測のモデルをリロードするのではなく、多くの予測を連続して実行する可能性が高い場合は、モデルをロードしたままにします。

00:13:18.000 --> 00:13:23.000
最後に、アプリがしばらく使用しない場合は、モデルをアンロードできます。

00:13:23.000 --> 00:13:29.000
これはメモリの圧力を軽減するのに役立ち、キャッシングのおかげで、その後の負荷はより速くなるはずです。

00:13:29.000 --> 00:13:33.000
モデルがロードされたら、モデルで予測を実行することを考える時です。

00:13:33.000 --> 00:13:38.000
デモに飛び込んで、新しい非同期オプションを表示します。

00:13:38.000 --> 00:13:46.000
新しい非同期予測APIを表示するには、画像のギャラリーを表示し、画像にフィルターを適用できるアプリを使用します。

00:13:46.000 --> 00:13:55.000
グレースケール画像を入力として取り、画像のカラーバージョンを出力するCore MLモデルを使用するカラーリングフィルターに焦点を当てます。

00:13:55.000 --> 00:13:58.000
これは、動作中のアプリの例です。

00:13:58.000 --> 00:14:08.000
グレースケールの元の画像をロードすることから始まり、カラー化画像モードを選択すると、Core MLを使用して画像をカラー化します。

00:14:08.000 --> 00:14:15.000
下にスクロールすると、モデルは間違いなく機能していますが、予想より少し遅いです。

00:14:15.000 --> 00:14:24.000
また、遠く下にスクロールすると、画像が色付けされるのにかなり時間がかかることに気づきます。

00:14:24.000 --> 00:14:30.000
上にスクロールすると、途中ですべての画像を着色するのに時間を費やしていたように見えます。

00:14:30.000 --> 00:14:38.000
しかし、私のSwiftUIコードでは、画像を保持するためにLazyVGridを使用しているので、ビューが画面から外れたときにタスクをキャンセルする必要があります。

00:14:38.000 --> 00:14:47.000
現在の実装を見て、パフォーマンスが不足している理由と、タスクがキャンセルされることを尊重しない理由を理解してみましょう。

00:14:47.000 --> 00:14:50.000
これが実装です。

00:14:50.000 --> 00:14:58.000
同期予測APIはスレッドセーフではないため、アプリは予測がモデル上で連続して実行されるようにしなければならない。

00:14:58.000 --> 00:15:06.000
これは、ColorizingServiceをアクターにすることで達成され、一度にcolorizeメソッドへの1回の呼び出ししか許可されません。

00:15:06.000 --> 00:15:14.000
このアクターは、アプリにバンドルされているモデル用に生成される自動生成されたインターフェイスであるcolorizerModelを所有しています。

00:15:14.000 --> 00:15:18.000
Colorizeメソッドは現在、2つの操作を実行します。

00:15:18.000 --> 00:15:24.000
最初にモデルの入力を準備します。これには、モデルの入力サイズに合わせて画像のサイズを変更する必要があります。

00:15:24.000 --> 00:15:29.000
その後、モデルを介して入力を実行し、色付きの出力を取得します。

00:15:29.000 --> 00:15:36.000
私は先に進み、Core ML Instrumentsテンプレートで実行されているアプリのInstrumentsトレースをキャプチャしました。

00:15:36.000 --> 00:15:44.000
インスツルメントのトレースを見ると、予測が連続して実行されていることが示され、これはアクターの分離によって保証されます。

00:15:44.000 --> 00:15:52.000
しかし、次の予測が実行される前に、各予測の周りにギャップがあり、パフォーマンスの欠如に寄与しています。

00:15:52.000 --> 00:15:59.000
これらは、アクターの分離がモデル予測だけでなく、入力準備にも巻き付けられた結果です。

00:15:59.000 --> 00:16:08.000
1つの改善点は、入力準備を非分離方法としてマークすることで、次の着色要求がアクターに入るのをブロックしないようにします。

00:16:08.000 --> 00:16:15.000
これは役立ちますが、Core MLの予測自体はまだシリアル化され、これは私の処理のボトルネックです。

00:16:15.000 --> 00:16:23.000
コアML予測自体の並行性を利用するために、私が検討できるオプションはバッチ予測APIです。

00:16:23.000 --> 00:16:27.000
入力のバッチを取り込み、モデルを介して実行します。

00:16:27.000 --> 00:16:32.000
内部では、Core MLは可能な限り並行性を利用します。

00:16:32.000 --> 00:16:35.000
着色方法のバッチバージョンを作成するのはとても簡単です。

00:16:35.000 --> 00:16:43.000
しかし、難しい部分は、入力をバッチに収集し、この方法に渡す方法を理解することです。

00:16:43.000 --> 00:16:49.000
このユースケースには、バッチ予測APIの使用を困難にする複数の側面があります。

00:16:49.000 --> 00:16:54.000
バッチAPIは、完了すべき作業の既知の量がある場合に最もよく使用されます。

00:16:54.000 --> 00:17:02.000
この場合、処理する画像の量は固定ではなく、画面サイズとスクロール量の機能です。

00:17:02.000 --> 00:17:11.000
バッチサイズは自分で選ぶことができますが、バッチサイズが満たされていないが、まだ処理する必要があるケースを処理する必要があります。

00:17:11.000 --> 00:17:17.000
また、画像がバッチで色付けされる別のUI体験ができます。

00:17:17.000 --> 00:17:23.000
最後に、ユーザーがそこから離れてスクロールしても、バッチをキャンセルすることはできません。

00:17:23.000 --> 00:17:29.000
これらの課題のために、私はむしろ一度に1つの予測を処理するAPIに固執したいと思います。

00:17:29.000 --> 00:17:33.000
これは、新しい非同期予測APIが非常に役立つ場所です。

00:17:33.000 --> 00:17:38.000
スレッドセーフで、Swiftの並行性と一緒にCore MLを使用するのに適しています。

00:17:38.000 --> 00:17:45.000
コードの非同期デザインに切り替えるには、まずcolorizeメソッドを非同期に変更しました。

00:17:45.000 --> 00:17:53.000
次に、APIの新しい非同期バージョンを使用するために必要な予測呼び出しの前にawaitキーワードを追加しました。

00:17:53.000 --> 00:17:57.000
その後、ColorizingServiceを俳優ではなくクラスに変更しました。

00:17:57.000 --> 00:18:00.000
そうすれば、複数の画像を同時に色付けすることができます。

00:18:00.000 --> 00:18:05.000
最後に、方法の開始にキャンセルチェックを追加しました。

00:18:05.000 --> 00:18:16.000
非同期予測APIは、特に複数の予測が同時に要求された場合、キャンセルに対応するために最善を尽くしますが、この場合は開始時に追加のチェックを含めるのが最善です。

00:18:16.000 --> 00:18:23.000
そうすれば、colorizeメソッドが入力される前にタスクがキャンセルされた場合、入力の準備も回避できます。

00:18:23.000 --> 00:18:27.000
次に、これらの変更を行い、アプリを再実行します。

00:18:27.000 --> 00:18:31.000
以前と同じように、カラーモードに設定します。

00:18:31.000 --> 00:18:35.000
私はすでに画像がはるかに速く色付けされているのを見ることができます。

00:18:35.000 --> 00:18:41.000
そして、一番下まですばやくスクロールすると、画像はほぼすぐに読み込まれます。

00:18:41.000 --> 00:18:55.000
少し上にスクロールすると、上にスクロールすると画像が色付けされていることを確認できます。つまり、最初に一番下まですばやくスワイプしたときに、色分けの呼び出しが正常にキャンセルされました。

00:18:55.000 --> 00:19:04.000
この新しい非同期設計を使用してトレースを見ると、予測が複数の画像で同時に実行されていることがわかります。

00:19:04.000 --> 00:19:08.000
これは、垂直に積み重ねられた複数の予測間隔で示されます。

00:19:08.000 --> 00:19:16.000
このモデルはニューラルエンジンで部分的に動作するため、ニューラルエンジン機器でも観察できます。

00:19:16.000 --> 00:19:26.000
画像を連続的に色付けした最初の実装では、スクロールせずに画像の初期ビューを色付けするのに約2秒かかりました。

00:19:26.000 --> 00:19:35.000
画像を同時に色付けする非同期実装に切り替えた後、その時間は約1秒に半分に短縮されました。

00:19:35.000 --> 00:19:46.000
全体として、非同期予測APIとColorizerモデルとの並行性を利用することで、総スループットの約2倍の改善を達成することができました。

00:19:46.000 --> 00:20:05.000
ただし、特定のモデルとユースケースが同時設計の恩恵を受ける可能性がある金額は、モデルの操作、コンピューティングユニットとハードウェアの組み合わせ、およびコンピューティングデバイスが忙しい可能性のあるその他の作業など、いくつかの要因に大きく依存していることに注意することが重要です。

00:20:05.000 --> 00:20:14.000
また、MLプログラムとパイプラインモデルタイプは、予測を同時に実行することから最高のパフォーマンス改善を提供します。

00:20:14.000 --> 00:20:23.000
全体として、アプリに並行性を追加するときは、ワークロードを慎重にプロファイリングして、実際にユースケースに利益をもたらしていることを確認する必要があります。

00:20:23.000 --> 00:20:29.000
アプリに並行性を追加するときに覚えておくべきもう1つの重要なことは、メモリ使用量です。

00:20:29.000 --> 00:20:37.000
多くのモデル入力と出力が同時にメモリにロードされると、アプリケーションのピークメモリ使用量が大幅に増加します。

00:20:37.000 --> 00:20:44.000
コアMLインストゥルメントと割り当てインストゥルメントを組み合わせることで、これをプロファイリングできます。

00:20:44.000 --> 00:20:53.000
トレースは、カラーライザーモデルを実行するために多くの入力をメモリにロードするにつれて、私のアプリのメモリ使用量が急速に増加していることを示しています。

00:20:53.000 --> 00:21:03.000
潜在的な問題は、私のコードのカラーライズメソッドにはフロー制御がないため、同時にカラー化される画像の量には固定された制限がないことです。

00:21:03.000 --> 00:21:07.000
モデルの入力と出力が小さい場合、これは問題にならないかもしれません。

00:21:07.000 --> 00:21:17.000
しかし、それらが大きい場合、これらの入力と出力の多くのセットを同時にメモリに持つと、アプリのピークメモリ使用量を大幅に増加させることができます。

00:21:17.000 --> 00:21:24.000
これを改善する方法は、飛行中の予測の最大量を制限するロジックを追加することです。

00:21:24.000 --> 00:21:32.000
これにより、メモリに同時にロードされる入力と出力が少なくなり、予測の実行中にピークメモリ使用量が減少します。

00:21:32.000 --> 00:21:41.000
この例では、すでに2つの項目が作業されている場合、以前の作業項目が完了するまで新しい作業項目を延期します。

00:21:41.000 --> 00:21:45.000
最善の戦略は、あなたのユースケースに依存します。

00:21:45.000 --> 00:21:51.000
たとえば、カメラからデータをストリーミングするときは、作業を延期するのではなく、単に作業をドロップしたいと思うかもしれません。

00:21:51.000 --> 00:21:58.000
このようにして、フレームを蓄積したり、もはや一時的に関連性のない作業をしたりすることを避けることができます。

00:21:58.000 --> 00:22:04.000
少し下がって、さまざまな予測APIを使用するタイミングに関する一般的なガイダンスをいくつか紹介します。

00:22:04.000 --> 00:22:15.000
同期コンテキストにあり、利用可能な各入力間の時間がモデルのレイテンシに比べて大きい場合、同期予測APIはうまく機能します。

00:22:15.000 --> 00:22:22.000
入力がバッチで利用可能になった場合、バッチ予測APIは自然に適合します。

00:22:22.000 --> 00:22:32.000
非同期コンテキストにあり、大量の入力が時間の経過とともに個別に利用可能になる場合、非同期APIが最も有用です。

00:22:32.000 --> 00:22:42.000
最後に、Core MLワークフローを移動すると、モデル開発とモデル統合の両方で最適化の機会がたくさんあります。

00:22:42.000 --> 00:22:51.000
新しいコンピューティング可用性APIは、デバイスで利用可能なハードウェアに基づいて、実行時に決定を下すのに役立ちます。

00:22:51.000 --> 00:22:59.000
モデルのライフサイクルとキャッシングの動作を理解することは、モデルをいつ、どこでロードおよびアンロードするかを最適に決定するのに役立ちます。

00:22:59.000 --> 00:23:09.000
そして最後に、非同期予測APIは、Core MLを他の非同期Swiftコードと統合し、同時予測をサポートすることでスループットを向上させるのに役立ちます。

00:23:09.000 --> 23:59:59.000
これはCore MLチームのBenで、私はAIではありません。

