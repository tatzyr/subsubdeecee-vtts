WEBVTT

00:00:00.000 -> 00:00:10.000
♪ ♪

00:00:10.000 -> 00:00:15.000
リジー：こんにちは！私はリジーで、ここアップルでVisionKitに取り組んでいるエンジニアです。

00:00:15.000 -> 00:00:20.000
私は今日、あなたのアプリにサブジェクトリフティングを持ち込む方法についてあなたと話すことに興奮しています。

00:00:20.000 -> 00:00:28.000
サブジェクトリフティングはiOS 16で導入され、ユーザーは画像の被写体を選択、持ち上げ、共有できるようになりました。

00:00:28.000 -> 00:00:32.000
まず、サブジェクトリフティングの基本について確認します。

00:00:32.000 -> 00:00:39.000
次に、新しいVisionKit APIを使用してサブジェクトリフティングを追加する方法を説明します。

00:00:39.000 -> 00:00:46.000
最後に、同僚のSaumitroはより深く掘り下げ、新しい基盤となるVision APIを紹介します。

00:00:46.000 -> 00:00:49.000
では、正確には主題とは何ですか?

00:00:49.000 -> 00:00:54.000
被写体は、写真の前景オブジェクト、またはオブジェクトです。

00:00:54.000 -> 00:00:57.000
これは必ずしも人やペットではありません。

00:00:57.000 -> 00:01:03.000
それは建物、食べ物のプレート、または靴のペアから何でもかまいません。

00:01:03.000 -> 00:01:08.000
画像は、ここでこれらの3杯のコーヒーのように、複数の被写体を持つことができます。

00:01:08.000 -> 00:01:13.000
被験者は必ずしも個々のオブジェクトではないことに注意することが重要です。

00:01:13.000 -> 00:01:20.000
この例では、男と彼の犬が一緒に画像の焦点であり、それらを1つの組み合わせた被写体になります。

00:01:20.000 -> 00:01:23.000
では、どうやってこれをアプリに入れるのですか?

00:01:23.000 -> 00:01:28.000
アプリにサブジェクトリフティングを追加するのに役立つ2つの別々のAPIがあります。

00:01:28.000 -> 00:01:30.000
VisionKit。

00:01:30.000 -> 00:01:31.000
そしてビジョン。

00:01:31.000 -> 00:01:38.000
VisionKitを使用すると、箱から出してすぐにシステムのような被写体持ち上げ動作を非常に簡単に採用できます。

00:01:38.000 -> 00:01:45.000
ほんの数行のコードで、私たち全員が知っていて愛している被写体リフティングUIを簡単に再現できます。

00:01:45.000 -> 00:01:54.000
VisionKitはまた、これらの被写体に関するいくつかの基本的な情報を公開するので、人々に画像の被写体と対話する新しい方法を与えることができます。

00:01:54.000 -> 00:02:00.000
これはすべてプロセス外で発生し、パフォーマンス上の利点がありますが、画像サイズが制限されていることを意味します。

00:02:00.000 -> 00:02:05.000
ビジョンは低レベルのフレームワークであり、すぐに使えるUIはありません。

00:02:05.000 -> 00:02:10.000
これは、ビューに縛られていないことを意味し、より多くの柔軟性を提供します。

00:02:10.000 -> 00:02:17.000
画像分析はプロセス内で行われ、VisionKitほど画像の解像度に制限はありません。

00:02:17.000 -> 00:02:24.000
最後に、このAPIは、CoreImageを使用するような、より高度な画像編集パイプラインの一部になることができます。

00:02:24.000 -> 00:02:28.000
まず、VisionKitのサブジェクトリフティングAPIに飛び込みましょう。

00:02:28.000 -> 00:02:36.000
VisionKitで被写体リフティングを追加するには、ImageAnalysisInteractionを初期化し、画像を含むビューに追加するだけです。

00:02:36.000 -> 00:02:40.000
これはUIImageViewにすることができますが、そうである必要はありません。

00:02:40.000 -> 00:02:41.000
それはとても簡単です。

00:02:41.000 -> 00:02:45.000
今、あなたの画像はシステム被写体リフティングインタラクションを持つことになります。

00:02:45.000 -> 00:02:52.000
同様に、macOSでは、ImageAnalysisOverlayViewを作成し、画像を含むNSViewのサブビューとして追加します。

00:02:52.000 -> 00:03:03.000
ImageAnalysisInteractionまたはImageAnalysisOverlayViewの優先インタラクションタイプを設定して、サポートするVisionKitインタラクションの種類を選択できます。

00:03:03.000 -> 00:03:07.000
デフォルトのインタラクションタイプは.automaticで、システムの動作を反映します。

00:03:07.000 -> 00:03:12.000
サブジェクトリフティング、ライブテキスト、データ検出器が必要な場合は、このタイプを使用してください。

00:03:12.000 -> 00:03:19.000
新しい画像サブジェクトタイプには、テキストをインタラクティブにしたくない場合に、サブジェクトリフティングのみが含まれます。

00:03:19.000 -> 00:03:27.000
これらのUIインタラクションに加えて、VisionKitでは、ImageAnalysisを使用して画像の被写体にプログラムでアクセスすることもできます。

00:03:27.000 -> 00:03:34.000
画像分析を生成するには、ImageAnalyzerを作成し、分析関数を呼び出すだけです。

00:03:34.000 -> 00:03:39.000
目的の画像とアナライザの設定を渡します。

00:03:39.000 -> 00:03:46.000
ImageAnalysisの被写体プロパティを使用して、すべての画像の被写体リストに非同期にアクセスできます。

00:03:46.000 -> 00:03:51.000
これは、画像とその境界を含む新しいサブジェクト構造体を使用します。

00:03:51.000 -> 00:03:57.000
強調表示された主題プロパティは、強調表示された主題のセットを返します。

00:03:57.000 -> 00:04:01.000
この例では、下の2つの科目が強調されています。

00:04:01.000 -> 00:04:09.000
ユーザーは、それを長押しすることで主題を強調表示できますが、コードで設定された強調表示された主題を更新することで、選択状態を変更することもできます。

00:04:09.000 -> 00:04:14.000
非同期サブジェクト(at:)メソッドを使用して、ポイントごとにサブジェクトを検索できます。

00:04:14.000 -> 00:04:19.000
この例では、ここをタップすると真ん中の主語が返されます。

00:04:19.000 -> 00:04:23.000
その時点で主題がない場合、このメソッドはnilを返します。

00:04:23.000 -> 00:04:27.000
最後に、2つの方法で被写画像を生成できます。

00:04:27.000 -> 00:04:31.000
単一の被写体については、被写体の画像プロパティにアクセスするだけです。

00:04:31.000 -> 00:04:40.000
複数の被写体で構成される画像が必要な場合は、非同期画像(for:)メソッドを使用し、含めたい被写体を渡します。

00:04:40.000 -> 00:04:48.000
この例では、下の2つの被写体の画像が必要な場合は、この方法を使用してこの画像を生成できます。

00:04:48.000 -> 00:04:50.000
このすべてがデモで一緒になるのを見てみましょう。

00:04:50.000 -> 00:04:53.000
私はパズルアプリに取り組んでいます。

00:04:53.000 -> 00:04:57.000
ピースをパズルにドラッグしたいのですが、まだ持ち上げられません。

00:04:57.000 -> 00:04:58.000
それを直しましょう。

00:04:58.000 -> 00:05:05.000
まず、ピースと対話できるように、この画像で被写体リフティングインタラクションを有効にする必要があります。

00:05:05.000 -> 00:05:10.000
ImageAnalysisInteractionを作成することでこれを行うことができます...

00:05:10.000 -> 00:05:14.000
...そして、単に私の見解に追加するだけです。

00:05:14.000 -> 00:05:26.000
ライブテキストを含める必要がないので、ここではimageSubjectインタラクションタイプを使用しました。

00:05:26.000 -> 00:05:27.000
すごい！

00:05:27.000 -> 00:05:30.000
今、私はパズルのピースを選択し、このようにそれらと対話することができます。

00:05:30.000 -> 00:05:33.000
この画像はいかなる方法でも前処理されていません。

00:05:33.000 -> 00:05:36.000
これは、被写体を持ち上げるだけで行われます。

00:05:36.000 -> 00:05:48.000
私はパズルのピースをパズルに落とすのを処理するためにいくつかのコードを追加し、さらにそれらを所定の位置に調整しました。

00:05:48.000 -> 00:05:52.000
かなりクールに見えますが、アプリをさらに魅力的に感じさせたいです。

00:05:52.000 -> 00:05:59.000
わずかな3D効果を与えるために、その上にカーソルを合わせると、各パズルピースの下にドロップシャドウを追加することを考えています。

00:05:59.000 -> 00:06:05.000
ここにはすでにホバージェスチャーハンドラーがあります。影を追加するだけです。

00:06:05.000 -> 00:06:10.000
画像を簡単に編集できないので、代わりに画像レイヤーのトリックでやります。

00:06:10.000 -> 00:06:18.000
まず、imageAnalysis.subject(at point:)を呼び出して、被写体の上にカーソルを合わせていることを確認します。

00:06:18.000 -> 00:06:29.000
被写体画像のコピーを挿入し、それを灰色にし、元の被写体位置からわずかにオフセットするaddShadow(被写体:)メソッドがあります。

00:06:29.000 -> 00:06:36.000
次に、影の上に被写体画像のコピーを追加して、3次元に見えるようにします。

00:06:36.000 -> 00:06:44.000
最後に、ホバーポイントが被写体と交差しなかった場合、私は影をクリアします。

00:06:44.000 -> 00:06:49.000
試してみましょう。

00:06:49.000 -> 00:07:00.000
すごい。作品の上にカーソルを合わせると、影の効果が得られます。

00:07:00.000 -> 00:07:10.000
VisionKitを使用すると、アプリでサブジェストリフティングを設定し、わずか数行のコードで楽しいサブジェクト効果を追加することさえできました。

00:07:10.000 -> 00:07:17.000
次に、同僚のSaumitroに伝えます。Saumitroは、新しいVision APIとそれをアプリに統合する方法について話します。

00:07:17.000 -> 00:07:19.000
サウミトロ:ありがとう、リジー!

00:07:19.000 -> 00:07:23.000
こんにちは、私はサウミトロで、ビジョンチームのエンジニアです。

00:07:23.000 -> 00:07:28.000
VisionKitのAPIは、サブジェクトリフティングを始める最も簡単な方法です。

00:07:28.000 -> 00:07:32.000
より高度な機能を必要とするアプリケーションについては、ビジョンがカバーします。

00:07:32.000 -> 00:07:40.000
サブジェクトリフティングは、顕著性や人物セグメンテーションなどのビジョンの既存のセグメンテーションAPIのコレクションに加わります。

00:07:40.000 -> 00:07:45.000
それぞれの強みをすばやく見直し、被写体リフティングがどのように適合するかを見てみましょう。

00:07:45.000 -> 00:07:52.000
注意や客観性のためのもののような顕著な要求は、粗い地域ベースの分析に最適です。

00:07:52.000 -> 00:07:59.000
生成された顕著性マップはかなり低い解像度であるため、セグメンテーションには適していないことに注意してください。

00:07:59.000 -> 00:08:04.000
代わりに、画像の自動トリミングなどのタスクに顕著な領域を使用できます。

00:08:04.000 -> 00:08:11.000
人物セグメンテーションAPIは、現場の人々のために詳細なセグメンテーションマスクを生産することに輝いています。

00:08:11.000 -> 00:08:16.000
人々をセグメント化することに特に集中したい場合は、これを使用してください。

00:08:16.000 -> 00:08:24.000
新しい人物インスタンスセグメンテーションAPIは、シーン内の各人に個別のマスクを提供することで、物事をさらに進めます。

00:08:24.000 -> 00:08:28.000
詳細については、個人セグメンテーションに関するこのセッションをチェックしてください。

00:08:28.000 -> 00:08:35.000
人のセグメンテーションとは対照的に、新しく導入されたサブジェクトリフティングAPIは「クラス不可知論者」です。

00:08:35.000 -> 00:08:40.000
フォアグラウンドオブジェクトは、そのセマンティッククラスに関係なく、潜在的にセグメント化される可能性があります。

00:08:40.000 -> 00:08:46.000
例えば、この画像の人々に加えて、それがどのように車を拾うかに注目してください。

00:08:46.000 -> 00:08:49.000
それでは、関連する重要な概念のいくつかを見てみましょう。

00:08:49.000 -> 00:08:51.000
入力画像から始めます。

00:08:51.000 -> 00:08:59.000
被写体リフティングリクエストは、この画像を処理し、同じ解像度でソフトセグメンテーションマスクを生成します。

00:08:59.000 -> 00:09:04.000
このマスクを取ってソース画像に適用すると、マスクされた画像になります。

00:09:04.000 -> 00:09:09.000
それぞれの異なるセグメント化されたオブジェクトはインスタンスと呼ばれます。

00:09:09.000 -> 00:09:14.000
ビジョンは、これらのインスタンスに関するピクセル単位の情報も提供します。

00:09:14.000 -> 00:09:19.000
このインスタンスマスクは、ソース画像のピクセルをインスタンスインデックスにマッピングします。

00:09:19.000 -> 00:09:27.000
ゼロインデックスはバックグラウンド用に予約されており、各フォアグラウンドインスタンスは1から順次ラベル付けされます。

00:09:27.000 -> 00:09:33.000
連続してラベル付けされる以外に、これらのIDの順序は保証されません。

00:09:33.000 -> 00:09:39.000
これらのインデックスを使用して、ソース画像内のフォアグラウンドオブジェクトのサブセットをセグメント化できます。

00:09:39.000 -> 00:09:44.000
インタラクティブなアプリを設計する場合、このインスタンスマスクはヒットテストにも役立ちます。

00:09:44.000 -> 00:09:47.000
これらのタスクの両方を実行する方法を少し実演します。

00:09:47.000 -> 00:09:50.000
APIに飛び込みましょう。

00:09:50.000 -> 00:09:54.000
被写体リフティングは、ビジョンにおける画像ベースのリクエストの使い慣れたパターンに従います。

00:09:54.000 -> 00:10:03.000
まず、フォアグラウンドインスタンスマスク要求をインスタンス化し、続いて入力画像で画像要求ハンドラをインスタンス化します。

00:10:03.000 -> 00:10:05.000
次に、リクエストを実行します。

00:10:05.000 -> 00:10:11.000
ボンネットの下で、これはビジョンが画像を分析して被写体を把握するときです。

00:10:11.000 -> 00:10:21.000
効率のためにAppleのハードウェアを利用するように最適化されていますが、それでもリソースを大量に消費する作業であり、UIをブロックしないようにバックグラウンドスレッドに延期するのが最善です。

00:10:21.000 -> 00:10:27.000
これを行う一般的な方法の1つは、このステップを別のDispatchQueueで非同期に実行することです。

00:10:27.000 -> 00:10:35.000
入力画像で1つ以上の被写体が検出された場合、結果配列には単一のオブザベーションが入力されます。

00:10:35.000 -> 00:10:40.000
ここから、マスクとセグメント化された画像の観察を照会することができます。

00:10:40.000 -> 00:10:47.000
どのインスタンスがセグメント化され、結果がどのようにトリミングされるかを制御する2つのパラメータを詳しく見てみましょう。

00:10:47.000 -> 00:10:55.000
instancesパラメータは、最終的なセグメント化された画像またはマスクで抽出されるオブジェクトを制御するIndexSetです。

00:10:55.000 -> 00:11:01.000
例として、この画像には、バックグラウンドインスタンスを含まない2つのフォアグラウンドインスタンスが含まれています。

00:11:01.000 -> 00:11:15.000
検出されたすべてのフォアグラウンドインスタンスをセグメント化することは非常に一般的な操作であるため、Visionは、すべてのフォアグラウンドインスタンスインデックスを含むIndexSetを返す便利なallInstancesプロパティを提供します。

00:11:15.000 -> 00:11:19.000
ここの画像には、インデックス1と2が含まれます。

00:11:19.000 -> 00:11:23.000
バックグラウンドインスタンス0は含まれていないことに注意してください。

00:11:23.000 -> 00:11:26.000
また、これらのインデックスのサブセットのみを提供することもできます。

00:11:26.000 -> 00:11:29.000
これはちょうど例1です。

00:11:29.000 -> 00:11:31.000
そして、ちょうど例2。

00:11:31.000 -> 00:11:34.000
また、最終的なマスクされた画像がどのようにトリミングされるかを制御することもできます。

00:11:34.000 -> 00:11:40.000
このパラメータがfalseに設定されている場合、出力画像の解像度は入力画像と一致します。

00:11:40.000 -> 00:11:48.000
これは、下流の合成操作のために、セグメント化されたオブジェクトの相対的な位置を保持したい場合にいいです。

00:11:48.000 -> 00:11:53.000
Trueに設定すると、選択したインスタンスのタイトなクロップになります。

00:11:53.000 -> 00:11:58.000
これまでの例では、私は完全にマスクされた画像出力で作業してきました。

00:11:58.000 -> 00:12:07.000
ただし、マスク効果を適用するなど、一部の操作では、代わりにセグメンテーションマスクだけで作業する方が便利です。

00:12:07.000 -> 00:12:13.000
オブザベーションでcreateScaledMaskメソッドを呼び出すことで、これらのマスクを生成できます。

00:12:13.000 -> 00:12:16.000
パラメータは以前と同じように動作します。

00:12:16.000 -> 00:12:23.000
出力は、ソフトセグメンテーションマスクを含むシングルチャンネル浮動小数点ピクセルバッファです。

00:12:23.000 -> 00:12:27.000
先ほど生成したマスクは、CoreImageでの使用に最適です。

00:12:27.000 -> 00:12:32.000
Visionは、VisionKitと同様に、SDR出力を生成します。

00:12:32.000 -> 00:12:38.000
ただし、CoreImageでマスキングを実行すると、入力の高いダイナミックレンジが維持されます。

00:12:38.000 -> 00:12:43.000
これの詳細については、アプリにHDRを追加するためのセッションをチェックすることを検討してください。

00:12:43.000 -> 00:12:47.000
このマスキングを実行する1つの方法は、CIBlendWithMaskフィルターを使用することです。

00:12:47.000 -> 00:12:50.000
マスクする必要があるソース画像から始めます。

00:12:50.000 -> 00:12:54.000
これは通常、ビジョンに渡したのと同じ画像になります。

00:12:54.000 -> 00:12:58.000
ビジョンのcreateScaledMaskコールから得られたマスク。

00:12:58.000 -> 00:13:03.000
そして最後に、被写体が上に合成される新しい背景画像。

00:13:03.000 -> 00:13:07.000
これに空の画像を使用すると、背景が透明になります。

00:13:07.000 -> 00:13:14.000
あるいは、新しい背景の上に結果を合成する予定がある場合は、ここに直接渡すことができます。

00:13:14.000 -> 00:13:15.000
そして、それはほとんどそれです。

00:13:15.000 -> 00:13:21.000
出力は、HDRで保存されたマスクされた合成された画像になります。

00:13:21.000 -> 00:13:26.000
では、すべてをまとめ、クールな被写体リフティングビジュアルエフェクトアプリを構築しましょう。

00:13:26.000 -> 00:13:32.000
背景を削除してその下にビューを表示したり、何か他のものに置き換えたりできます。

00:13:32.000 -> 00:13:36.000
さらに、プリセットエフェクトの1つを適用できます。

00:13:36.000 -> 00:13:40.000
そして、エフェクトは選択した背景で構成されています。

00:13:40.000 -> 00:13:45.000
フォアグラウンドインスタンスをタップして、選択的に持ち上げることもできます。

00:13:45.000 -> 00:13:48.000
このアプリの作成にどのようにアプローチするかの概要を見てみましょう。 見てみましょう。

00:13:48.000 -> 00:13:58.000
私たちのアプリのコアは、UIからの入力を受け入れ、最終的な出力を生成するために必要なすべての作業を実行するエフェクトパイプラインに依存しています。

00:13:58.000 -> 00:14:02.000
ソース画像で被写体リフティングを行うことから始めます。

00:14:02.000 -> 00:14:06.000
オプションのタップで、個々のインスタンスを選択できます。

00:14:06.000 -> 00:14:10.000
結果のマスクはソース画像に適用されます。

00:14:10.000 -> 00:14:18.000
そして最後に、選択した背景と視覚効果が適用され、合成され、最終的な出力画像が生成されます。

00:14:18.000 -> 00:14:21.000
これらの最後の2つのステップは、CoreImageを使用して達成されます。

00:14:21.000 -> 00:14:31.000
私たちのトップレベル機能は、入力画像、選択した背景画像と効果、および潜在的にインスタンスの1つを選択するためのユーザーからのタップ位置を取ります。

00:14:31.000 -> 00:14:35.000
ここのエフェクトタイプは、プリセットの単純な列挙型です。

00:14:35.000 -> 00:14:39.000
その出力は最終的な合成画像であり、UIに表示する準備ができています。

00:14:39.000 -> 00:14:43.000
このタスクは2つの段階に分解できます。

00:14:43.000 -> 00:14:48.000
まず、選択したインスタンスのサブジェクトマスクを生成します。

00:14:48.000 -> 00:14:52.000
そして第二に、そのマスクを使用して選択した効果を適用します。

00:14:52.000 -> 00:14:54.000
最初の段階から始めましょう。

00:14:54.000 -> 00:14:59.000
この段階への入力は、ソース画像とオプションのタップ位置です。

00:14:59.000 -> 00:15:06.000
私たちはすでにここで、単にビジョン要求を実行し、マスクを返すコードのほとんどに遭遇しました。

00:15:06.000 -> 00:15:12.000
興味深いのは、ラベルマスクを使用してタップ位置を一連のインデックスにマッピングするこの行です。

00:15:12.000 -> 00:15:15.000
詳しく見てみましょう。 

00:15:15.000 -> 00:15:19.000
タップが見つからない場合は、デフォルトですべてのインスタンスを使用します。

00:15:19.000 -> 00:15:24.000
タップ位置をインスタンスマスクのピクセルにマッピングしたい。

00:15:24.000 -> 00:15:27.000
ここには関連する情報が2つあります。

00:15:27.000 -> 00:15:33.000
まず、UIは、それを渡す前に、タップ位置を0、1に正規化します。

00:15:33.000 -> 00:15:38.000
ディスプレイの解像度やスケーリング要因などの詳細を心配する必要がないので、これはいいですね。

00:15:38.000 -> 00:15:44.000
第二に、左上に原点を持つデフォルトのUIKit座標系を使用します。

00:15:44.000 -> 00:15:48.000
これは、ピクセルバッファの画像空間座標と一致しています。

00:15:48.000 -> 00:15:53.000
そのため、この既存のビジョンヘルパー機能を使用して、この変換を実行できます。

00:15:53.000 -> 00:15:58.000
私は今、タップされたインスタンスラベルを検索するために必要なすべての情報を持っています。

00:15:58.000 -> 00:16:04.000
これには、ピクセルバッファのデータに直接アクセスすることが含まれており、次にその方法を紹介します。

00:16:04.000 -> 00:16:08.000
ラベルを手に入れたら、ゼロかどうかを確認します。 

00:16:08.000 -> 00:16:12.000
ゼロラベルは、ユーザーが背景ピクセルをタップしたことを意味することを思い出してください。

00:16:12.000 -> 00:16:15.000
この場合、すべてのインスタンスを選択することにフォールバックします。

00:16:15.000 -> 00:16:20.000
それ以外の場合は、選択したラベルだけでシングルトンセットを返します。

00:16:20.000 -> 00:16:24.000
このコードは、インスタンスラベルのルックアップの実行方法を入力します。

00:16:24.000 -> 00:16:29.000
他のピクセルバッファと同様に、データにアクセスする前にまずロックする必要があります。

00:16:29.000 -> 00:16:33.000
読み取り専用アクセスは、私たちの目的には十分です。

00:16:33.000 -> 00:16:42.000
ピクセルバッファの行はアライメントのためにパディングされる可能性があるため、ピクセルのバイトオフセットを計算する最も堅牢な方法は、そのbytesPerRow値を使用することです。

00:16:42.000 -> 00:16:50.000
instanceMaskはシングルチャネルのUInt8バッファなので、それ以上のスケーリングを心配する必要はありません。

00:16:50.000 -> 00:16:53.000
インスタンスマスクから読み終わったので、バッファのロックを解除できます。

00:16:53.000 -> 00:16:58.000
そして、それを包んで、私は選択したインスタンスを分離したマスクを持っています。

00:16:58.000 -> 00:17:01.000
私は今、効果の適用に進むことができます。

00:17:01.000 -> 00:17:04.000
ここでの最初のステップは、選択したエフェクトを背景に適用することです。

00:17:04.000 -> 00:17:11.000
それが完了したら、CoreImageを使用して、変換された背景の上にマスクされた被写体を合成します。

00:17:11.000 -> 00:17:17.000
最初のいくつかの効果は、既存のCoreImageフィルターの非常に簡単で直接的なアプリケーションです。

00:17:17.000 -> 00:17:23.000
たとえば、被写体を強調表示するために、露出調整フィルターを使用して背景を暗くしました。

00:17:23.000 -> 00:17:26.000
ボケ効果は少し複雑です。

00:17:26.000 -> 00:17:31.000
背景をぼかすことに加えて、選択した被写体を強調するハローが欲しいです。

00:17:31.000 -> 00:17:35.000
ぼやける前に被写体のための白い切り抜きがトリックを行います。

00:17:35.000 -> 00:17:43.000
これを達成するための迅速な方法は、現在の機能を再利用し、被写体に真っ白な画像を渡すことです。

00:17:43.000 -> 00:17:46.000
そして、それで、私は合成のためのベースレイヤーを持っています。

00:17:46.000 -> 00:17:51.000
最後に、先ほどのCoreImageブレンドスニペットをドロップします。

00:17:51.000 -> 00:17:57.000
これは、新しく変換された背景の上に持ち上げられた被写体を合成します。

00:17:57.000 -> 00:18:02.000
そして、エフェクトパイプラインの最後の部分が整ったことで、アプリは完成しました。

00:18:02.000 -> 00:18:07.000
新しいサブジェクトリフティングAPIで何が可能かを味わうことができることを願っています。

00:18:07.000 -> 00:18:13.000
要約すると、VisionKitは、被写体リフティングをアプリに組み込む最速の方法です。

00:18:13.000 -> 00:18:18.000
より高度なアプリケーションについては、VisionのAPIにドロップダウンできます。

00:18:18.000 -> 00:18:25.000
そして最後に、CoreImageは、被写体リフティングでHDR対応の画像処理を実行するための完璧なコンパニオンです。

00:18:25.000 -> 00:18:30.000
リジーと私はあなたがこのビデオを楽しんだことを願っています、そして私たちはあなたが作るものを見ることにとても興奮しています。

00:18:30.000 -> 23:59:59.000
♪ ♪

