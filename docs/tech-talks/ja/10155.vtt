WEBVTT

00:00:02.000 --> 00:00:07.000
こんにちは。私はCreate MLチームのエンジニア、Shreya Jainです。

00:00:07.000 --> 00:00:16.000
今日は、オブジェクト検出テンプレートの機能強化を探り、それらを活用してより良いモデルを作成します。

00:00:16.000 --> 00:00:26.000
Create MLのオブジェクト検出にまだ慣れていない場合は、WWDC 2019のこのビデオを見ることをお勧めします。

00:00:26.000 --> 00:00:31.000
オブジェクト検出は、いくつかの魅力的なアプリ体験を可能にします。

00:00:31.000 --> 00:00:40.000
人々がゴミを分別するのに役立つアプリを構築し、ペットの猫に仮想メガネを試すことができます...

00:00:40.000 --> 00:00:46.000
そして、検出された成分に基づいてレシピを推薦できるアプリさえも。

00:00:46.000 --> 00:00:54.000
このアプリのモデルを構築することは、Create MLとその新機能の動作を見るのに最適な方法です。

00:00:54.000 --> 00:00:59.000
物体検出が大幅に改善されました。

00:00:59.000 --> 00:01:11.000
より少ないトレーニングデータで正確で小さなモデルをトレーニングし、トレーニングをカスタマイズするためのより多くの構成オプションを公開することができます。

00:01:11.000 --> 00:01:14.000
だから、すぐに飛び込みましょう。

00:01:14.000 --> 00:01:20.000
はじめに、SpotlightからCreate MLアプリを開きます。

00:01:20.000 --> 00:01:26.000
最初に会うのはテンプレートピッカーで、そこでオブジェクト検出を選択します。

00:01:26.000 --> 00:01:32.000
これにより、Create MLプロジェクトの詳細を入力するためのダイアログボックスが開きます。

00:01:32.000 --> 00:01:40.000
このプロジェクトに「FindMyRecipe」という名前を付け、成分を検出するのに役立つ説明を追加します。

00:01:40.000 --> 00:01:45.000
作成する前に、プロジェクトの場所を変更することを選択できます。

00:01:45.000 --> 00:01:49.000
次に、設定タブに着陸します。

00:01:49.000 --> 00:01:54.000
データと設定のオプションは、トレーニングの前にここで調整できます。

00:01:54.000 --> 00:02:00.000
データをロードする前に、このデータの準備方法を説明します。

00:02:00.000 --> 00:02:09.000
オブジェクト検出データは、すべてのトレーニング画像とJSONファイル内の注釈を含むフォルダに保存する必要があります。

00:02:09.000 --> 00:02:22.000
Annotations.jsonの内容は、この画像を例にとると理解できます。これには、パンのスライスとトマトの2つのオブジェクトがあります。

00:02:22.000 --> 00:02:29.000
各オブジェクトの注釈は、オブジェクトのラベルと画像内の位置で構成されています。

00:02:29.000 --> 00:02:34.000
場所は画像の左上隅に基づいています。

00:02:34.000 --> 00:02:41.000
トレーニングデータの画像内のすべてのオブジェクトは、この方法で注釈を付けることができます。

00:02:41.000 --> 00:02:48.000
これらの注釈はすべて、この形式で単一のJSONファイルに追加されます。

00:02:48.000 --> 00:02:54.000
この情報を使用してトレーニングデータを準備します。

00:02:54.000 --> 00:03:00.000
データの準備が整ったら、Create MLアプリに読み込むことができます。

00:03:00.000 --> 00:03:05.000
表示ボタンをクリックすると、データセットのクラス分布が表示されます。

00:03:05.000 --> 00:03:12.000
ご覧の通り、私のクラスはトマト、チーズ、パン、バジルです。

00:03:12.000 --> 00:03:23.000
[設定]タブに戻ると、モデルが目に見えないデータでうまく機能することを確認するために、検証データをオプションで提供できます。

00:03:23.000 --> 00:03:34.000
ここでは、検証データを自動に設定し、Create MLにデータセットのごく一部を使用できるようにします。

00:03:34.000 --> 00:03:41.000
また、モデルのトレーニングをより適切に制御できる新しいトレーニングパラメータもあります。

00:03:41.000 --> 00:03:49.000
それらはアルゴリズム、反復、バッチサイズ、グリッドサイズです。

00:03:49.000 --> 00:03:53.000
トレーニングには2つのアルゴリズムがあります。

00:03:53.000 --> 00:03:55.000
まず、完全なネットワークです。

00:03:55.000 --> 00:04:00.000
完全なネットワークアルゴリズムを詳しく見てみましょう。

00:04:00.000 --> 00:04:10.000
フルネットワークは2019年にCreate MLに導入され、それ以来デフォルトのトレーニングアルゴリズムとなっています。

00:04:10.000 --> 00:04:15.000
このアルゴリズムはYOLOv2アーキテクチャに基づいています。

00:04:15.000 --> 00:04:20.000
このネットワークのすべてのパラメータは、データを使用してトレーニングされます。

00:04:20.000 --> 00:04:27.000
結果として得られるCore MLモデルは、学習したすべてのパラメータをエンコードします。

00:04:27.000 --> 00:04:33.000
このCore MLモデルは、16ビットの精度で重みを格納するように量子化されています。

00:04:33.000 --> 00:04:38.000
結果として得られるモデルサイズは、先ほどの半分です。

00:04:38.000 --> 00:04:47.000
したがって、以前は約65メガバイトだったモデルは、現在33メガバイトになります。

00:04:47.000 --> 00:04:57.000
このアルゴリズムは、クラスごとに200以上の境界ボックスなど、大量のトレーニングデータがある場合に推奨されます。

00:04:57.000 --> 00:05:04.000
結果のモデルは下位互換性があり、iOS 12にまでさかのぼります。

00:05:04.000 --> 00:05:18.000
より少ないトレーニングデータで高精度のモデルを構築できるようにしたかったので、物体検出のための転送学習アルゴリズムを導入しています。

00:05:18.000 --> 00:05:24.000
転送学習は、すでにオペレーティングシステムにある機械学習モデルを活用します。

00:05:24.000 --> 00:05:32.000
例えば、写真アプリには、検索とメモリーを動かすモデルが含まれています。

00:05:32.000 --> 00:05:38.000
写真が使用する事前に訓練されたバックボーンの1つは、オブジェクトプリントと呼ばれています。

00:05:38.000 --> 00:05:42.000
これは膨大な量の多様なデータで訓練されています。

00:05:42.000 --> 00:05:50.000
転送学習では、これを利用してデータ要件を減らすことができます。

00:05:50.000 --> 00:05:57.000
Create MLの転送学習アルゴリズムは、ヘッドネットワークとともにオブジェクト印刷を使用します。

00:05:57.000 --> 00:06:06.000
ヘッドネットワークのみがデータでトレーニングされ、学習する必要があるパラメータの数が減少します。

00:06:06.000 --> 00:06:18.000
その結果、Core MLモデルにはヘッドネットワークパラメータのみが含まれているため、モデルはフルネットワークよりも5倍小さくなります。

00:06:18.000 --> 00:06:32.000
2019年に65メガバイト、量子化後に33メガバイトだったのと同じモデルは、転送学習アルゴリズムを使用してわずか7メガバイトになります。

00:06:32.000 --> 00:06:39.000
転送学習は、データが限られており、軽量モデルが必要な場合は素晴らしい選択肢です。

00:06:39.000 --> 00:06:45.000
クラスごとにわずか80のトレーニング例でうまくいきます。

00:06:45.000 --> 00:06:54.000
結果として得られるモデルでは、OSに同梱されているオブジェクトプリントを活用するためにiOS 14が必要です。

00:06:54.000 --> 00:06:59.000
アルゴリズムは新しい構成の1つにすぎません。

00:06:59.000 --> 00:07:05.000
反復やバッチサイズなどのパラメータも追加されました。

00:07:05.000 --> 00:07:10.000
反復は、モデルのパラメータが更新される回数です。

00:07:10.000 --> 00:07:15.000
デフォルト値は、データセットのサイズに基づいて選択されます。

00:07:15.000 --> 00:07:27.000
特定のユースケースでは、モデルがまだ収束していない場合は反復を増やしたり、モデルが早期にうまくいっている場合は反復を減らすことができます。

00:07:27.000 --> 00:07:34.000
バッチサイズとは、1回の反復で利用されるトレーニング例の数を指します。

00:07:34.000 --> 00:07:39.000
デフォルト値は、ハードウェアの制限に基づいて選択されます。

00:07:39.000 --> 00:07:49.000
バッチサイズが高い方が良いですが、デフォルト値を使用するか、パフォーマンス制限に基づいて減らすことをお勧めします。

00:07:49.000 --> 00:07:54.000
最後に、ネットワーク全体については、グリッドサイズをカスタマイズできます。

00:07:54.000 --> 00:08:00.000
グリッドサイズを理解するには、完全なネットワークで予測がどのように機能するかについての知識が必要です。

00:08:00.000 --> 00:08:04.000
詳しく見てみましょう。 

00:08:04.000 --> 00:08:07.000
この入力画像から始めて...

00:08:07.000 --> 00:08:11.000
訓練された完全なネットワークモデルにそれを渡す...

00:08:11.000 --> 00:08:16.000
結果、バウンディングボックスを持つ多くの予測オブジェクトが生成されます。

00:08:16.000 --> 00:08:24.000
画像内のオブジェクトを見つけるために、モデルはグリッドとアンカーボックスのセットを活用します。

00:08:24.000 --> 00:08:33.000
指定されたグリッドは、入力画像のアスペクト比と、モデルが検出されたオブジェクトを探す場所を定義します。

00:08:33.000 --> 00:08:41.000
たとえば、モデルが5×5のグリッド次元でどのように動作するかを見てみましょう。

00:08:41.000 --> 00:08:53.000
画像のサイズは、グリッド（この場合は正方形の画像）に合わせてサイズが変更され、定義されたセル数に分割されます。

00:08:53.000 --> 00:08:59.000
その後、ネットワークはグリッドセルごとに1つずつ予測を生成します。

00:08:59.000 --> 00:09:10.000
各予測には、セルにオブジェクトがあるかどうか、オブジェクトのクラス、およびその境界ボックスなどの情報が含まれています。

00:09:10.000 --> 00:09:17.000
YOLOは、各オブジェクトが1つのグリッドセルに関連付けられている複数のオブジェクトで正常に動作します。

00:09:17.000 --> 00:09:25.000
この画像でわかるように、バナナと犬の中心は同じ細胞に落ちます。

00:09:25.000 --> 00:09:34.000
各セルは1つのクラスしか予測できないので、バナナか犬のどちらかを選ぶことになっています。

00:09:34.000 --> 00:09:40.000
バナナと犬の両方を予測するために、アンカーボックスが定義されています。

00:09:40.000 --> 00:09:48.000
アンカーボックスにはアスペクト比が設定されており、グリッドセル内の複数のオブジェクトを検出します。

00:09:48.000 --> 00:09:57.000
Create MLは、13×13のデフォルトのグリッドディメンションを使用し、合計169セルです。

00:09:57.000 --> 00:10:05.000
さまざまなアスペクト比の15個のアンカーボックスの固定セットが、各セルで評価されます。

00:10:05.000 --> 00:10:15.000
したがって、デフォルトモデルは画像ごとに合計2,535の予測を行っています。

00:10:15.000 --> 00:10:24.000
このサイコロの画像と、3×3のグリッド寸法で物体検出がどのように機能するかを考えてみましょう。

00:10:24.000 --> 00:10:35.000
同様のアスペクト比の複数のサイコロが1つのセルに存在するため、そのうちの1つだけが検出されます。

00:10:35.000 --> 00:10:40.000
大きなグリッドサイズでは、より多くのサイコロが検出されます。

00:10:40.000 --> 00:10:46.000
ただし、これにより画像あたりの予測数が増えます。

00:10:46.000 --> 00:10:53.000
グリッドサイズを変更する際には、計算コストを考慮することが重要です。

00:10:53.000 --> 00:11:08.000
このような非正方形の入力画像では、寸法が1500×800で、この画像に8×8のグリッドを使用すると、情報が失われ、オブジェクトの自然な形状が歪めます。

00:11:08.000 --> 00:11:18.000
これにより、トレーニング中にモデルがより細かいパターンをキャプチャするのを防ぎ、予測力を妨げます。

00:11:18.000 --> 00:11:33.000
15×8のグリッドサイズを選択すると、入力画像の元のアスペクト比が保持され、より多くの情報を学習し、より良い結果をもたらすことができるモデルになります。

00:11:33.000 --> 00:11:46.000
FindMyRecipeプロジェクトのモデルのトレーニングに戻ると、転送学習アルゴリズムを選択し、1000回の反復を設定し、バッチサイズの自動を設定できます。

00:11:46.000 --> 00:11:51.000
再生ボタンをクリックすると、モデルはトレーニングを開始します。

00:11:51.000 --> 00:11:55.000
トレーニングタブには、バッチが準備中であることがわかります。

00:11:55.000 --> 00:12:05.000
このステップは、現実世界のデータに対する堅牢性と一般化を支援するために、一連の標準的な画像拡張を実行します。

00:12:05.000 --> 00:12:17.000
すぐにグラフが表示され、各反復の損失値がプロットされます。

00:12:17.000 --> 00:12:24.000
トレーニングが進むにつれて、スナップショットボタンをクリックして、その時点でモデルを取得できます。

00:12:24.000 --> 00:12:29.000
スナップショットは、トレーニングの進捗状況を確認するのに役立ちます。

00:12:29.000 --> 00:12:35.000
このモデルを使用して、いくつかの画像の予測をプレビューできます。

00:12:35.000 --> 00:12:41.000
すべての画像について、モデルの予測は[プレビュー]タブに表示されます。

00:12:41.000 --> 00:12:48.000
これらのバウンディングボックスをクリックすると、下部にある各クラスの自信を確認できます。

00:12:48.000 --> 00:12:55.000
スナップショットは、アプリ内での実験にも使用できます。

00:12:55.000 --> 00:13:04.000
トレーニングが完了すると、トレーニングと検証データの評価指標は、[評価] タブで確認できます。

00:13:04.000 --> 00:13:08.000
これらの数字はどういう意味ですか?

00:13:08.000 --> 00:13:13.000
物体検出モデルの評価は2倍である必要があります。

00:13:13.000 --> 00:13:20.000
正しいラベルが欲しいだけでなく、適切な場所にある必要があります。

00:13:20.000 --> 00:13:26.000
バウンディングボックスを注釈付きボックスと正確に一致させるのは難しいです。

00:13:26.000 --> 00:13:35.000
予測されたボックスが注釈付きボックスにどれだけ近いかをキャプチャする数字が必要になります。

00:13:35.000 --> 00:13:40.000
これは、intersection-over-unionと呼ばれるスコアによって測定されます。

00:13:40.000 --> 00:13:45.000
それは0%の間の値であり、重複はありません...

00:13:45.000 --> 00:13:51.000
100%に、これは完璧なオーバーラップです。

00:13:51.000 --> 00:14:03.000
予測が正しいと見なされるには、正しいクラスラベルと、事前定義されたしきい値よりも大きい交点オーバーユニオンスコアが必要です。

00:14:03.000 --> 00:14:14.000
ユニオン交差スコアがしきい値を下回っているか、予測されたクラスが正しくない場合、全体的な予測は正しくありません。

00:14:14.000 --> 00:14:23.000
この情報は、平均平均精度（mAP）と呼ばれるメトリックを計算するために使用されます。

00:14:23.000 --> 00:14:30.000
評価タブに戻って、これらの数字を確認します。

00:14:30.000 --> 00:14:37.000
これらの数字は、2つのしきい値で計算されたクラスごとの平均精度を表しています。

00:14:37.000 --> 00:14:43.000
1つは50%に固定され、もう1つは複数のしきい値で変化します。

00:14:43.000 --> 00:14:50.000
データセットの全体的な平均精度は、右上隅で見ることができます。

00:14:50.000 --> 00:14:55.000
より高いmAPは、より正確な予測を反映しています。

00:14:55.000 --> 00:14:59.000
私たちのモデルのmAPは全体的に良さそうです。

00:14:59.000 --> 00:15:10.000
モデルの予測が正しく見えることを確認するために、いくつかの例でモデルをプレビューします。

00:15:10.000 --> 00:15:12.000
すべてが素晴らしく見えます。

00:15:12.000 --> 00:15:16.000
これで、このモデルをアプリにドロップできます。

00:15:16.000 --> 00:15:25.000
先ほど見た追加機能により、Create MLを使用してオブジェクト検出モデルを作成するのは簡単です。

00:15:25.000 --> 00:15:32.000
Create MLは、トレーニングをより詳細に制御することで、モデルをカスタマイズするのに役立ちます。

00:15:32.000 --> 00:15:38.000
より少ないデータとより小さな出力サイズで正確なモデルを構築するのに役立ちます。

00:15:38.000 --> 23:59:59.000
これらの真新しい機能を使用して、あなたがテーブルにもたらす素晴らしいアイデアを見るのが待ちきれません。

