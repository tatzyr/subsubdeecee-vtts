WEBVTT

00:00:00.000 --> 00:00:04.000
こんにちは、ARの高度なシーン理解へようこそ。

00:00:04.000 --> 00:00:13.000
このビデオでは、新しいiPad ProのLiDARスキャナで有効になるARKitとRealityKitの新機能を紹介します。

00:00:13.000 --> 00:00:19.000
iOSとipadOSは、AR用のアプリを構築するのに役立つ2つの強力なフレームワークを開発者に提供します。

00:00:19.000 --> 00:00:32.000
ARKitは、位置追跡、シーン理解、レンダリング技術との統合を組み合わせて、背面カメラまたは前面カメラを使用してさまざまなAR体験を可能にします。

00:00:32.000 --> 00:00:48.000
そして、AR用に特別に構築された新しい高レベルフレームワークであるRealityKitは、フォトリアリスティックなレンダリングと特殊効果、スケーラブルなパフォーマンス、SwiftベースのAPIを提供し、優れたAR体験のプロトタイプと構築を容易にします。

00:00:48.000 --> 00:00:57.000
今日は、新しいiPad Proのハードウェア機能によってのみ可能になった両方のフレームワークの進歩について話すことに興奮しています。

00:00:57.000 --> 00:01:00.000
新しいiPad ProにはLiDARスキャナーが装備されています。

00:01:00.000 --> 00:01:10.000
これは、光が目の前の物体に到達して反射するのにかかる時間をナノ秒の速度で測定することによって距離を決定するために使用されます。

00:01:10.000 --> 00:01:16.000
これは最大5メートル先で有効で、屋内と屋外の両方で動作します。

00:01:16.000 --> 00:01:25.000
そして、この機能をワイドカメラとウルトラワイドカメラでキャプチャされた情報と組み合わせることで、あなたの環境を信じられないほど理解することができます。

00:01:25.000 --> 00:01:32.000
それでは、ARKitから始めて、これらのフレームワークのそれぞれで構築されたAR体験をどのように改善するかを見てみましょう。

00:01:32.000 --> 00:01:42.000
ARKitバージョン3.5の新しいアップデートは、新しいiPad ProのLiDARスキャナーを最大限に活用する多くの新機能と改善を提供します。

00:01:42.000 --> 00:01:49.000
シーンジオメトリは、周囲の環境の詳細なトポロジカルマップをアプリに提供する新しいAPIです。

00:01:49.000 --> 00:01:58.000
また、LiDARスキャナーは、サーフェスをより迅速かつ正確に検出することで、ARオンボーディング体験を簡素化します。

00:01:58.000 --> 00:02:08.000
また、モーションキャプチャ、ピープルオクルージョン、レイキャスティングなどの既存のARKit機能も、追加のアプリケーションの変更を必要とせずにメリットがあります。

00:02:08.000 --> 00:02:11.000
では、シーンジオメトリから始めましょう。

00:02:11.000 --> 00:02:18.000
シーンジオメトリは、環境のトポロジカルマッピングを表す三角形のメッシュを提供します。

00:02:18.000 --> 00:02:24.000
そして、オプションで、そのメッシュには、見られるものを分類するセマンティック情報を含めることができます。

00:02:24.000 --> 00:02:30.000
これには、テーブルや椅子、床、壁、天井、窓などが含まれます。

00:02:30.000 --> 00:02:42.000
この情報はすべて、現実世界のオブジェクト、環境に依存する物理学、およびシーン内の実際のオブジェクトと仮想オブジェクトの両方の照明による仮想コンテンツのオクルージョンを可能にするために使用できます。

00:02:42.000 --> 00:02:46.000
では、このマッピングの動作を見てみましょう。

00:02:46.000 --> 00:02:49.000
これは、屋内のシーンから取られた例です。

00:02:49.000 --> 00:02:56.000
LiDARセンサーを使用して、ARKitによって生成されるメッシュでARフレーム画像をオーバーレイしています。

00:02:56.000 --> 00:03:03.000
部屋を一掃すると、家具の形状や環境のレイアウトをどれだけ早く検出できるかがわかります。

00:03:03.000 --> 00:03:09.000
そして、色はメッシュがオーバーレイするものの分類に基づいています。

00:03:09.000 --> 00:03:12.000
では、APIを見てみましょう。

00:03:12.000 --> 00:03:19.000
シーンジオメトリは、ARWorldTtrackingConfigurationの新しいシーン再構築プロパティを通じて有効になります。

00:03:19.000 --> 00:03:22.000
さて、生成したいデータに応じて、2つのオプションがあります。

00:03:22.000 --> 00:03:29.000
最初のオプションは、メッシュのみを生成することです。つまり、トポロジカル情報のみが表面化されます。

00:03:29.000 --> 00:03:36.000
これは、周囲のオブジェクトの分類に依存しないオブジェクトの配置などを行うアプリを対象としています。

00:03:36.000 --> 00:03:39.000
もう1つのオプションは.meshWithClassificationです。

00:03:39.000 --> 00:03:45.000
そして、名前が示すように、これはすべてのシーンジオメトリのセマンティック分類を追加します。

00:03:45.000 --> 00:03:53.000
これは、床とテーブルの照明の違いなど、シーンに何があるかに応じて、異なる行動を望むアプリに役立ちます。

00:03:53.000 --> 00:03:56.000
そして、この下にコードが見えます。それはかなり簡単です。

00:03:56.000 --> 00:04:02.000
私たちはワールドトラッキングを使用しており、シーンの再構築をサポートするデバイスで実行されていることを確認するためにテストしています。

00:04:02.000 --> 00:04:07.000
もしそうなら、ここでメッシュオプションを選択し、セッションの実行を開始します。

00:04:07.000 --> 00:04:13.000
シーンの再構築を有効にしてセッションを実行すると、ARセッションはARメッシュアンカーを返します。

00:04:13.000 --> 00:04:22.000
これらは他のアンカーと変わり、add(anchor:)、update(anchor:)、remove(anchor:)など、通常のARセッションデリゲートメソッドを介して変更が行われます。

00:04:22.000 --> 00:04:27.000
そして、各メッシュアンカーは、メッシュジオメトリのローカル領域を表します。

00:04:27.000 --> 00:04:32.000
アンカーの変換とメッシュジオメトリオブジェクトで説明されています。

00:04:32.000 --> 00:04:38.000
ARMeshGeometryオブジェクトは、周囲の環境を表すために必要なすべての情報を保持します。

00:04:38.000 --> 00:04:45.000
各オブジェクトには、頂点、法線、面のリスト、および各フェーズで有効になっている場合の意味的分類が含まれています。

00:04:45.000 --> 00:04:51.000
これらはすべてMTLBuffersとして提供され、レンダラーに直接統合できます。

00:04:51.000 --> 00:04:56.000
今、シーンジオメトリと平面検出の間に起こるいくつかの興味深い相互作用があります。

00:04:56.000 --> 00:05:04.000
シーンの再構築と平面検出の両方が有効になっている場合、構築されたメッシュは重なり合う平面と一致するように平坦化されます。

00:05:04.000 --> 00:05:11.000
これは、滑らかなオブジェクトの動きを可能にするために、表面が一貫して平らである必要があるオブジェクトの配置に役立ちます。

00:05:11.000 --> 00:05:18.000
一方、シーンジオメトリを使用していて、平面検出が有効になっていない場合、メッシュは平坦化されなくなります。

00:05:18.000 --> 00:05:22.000
しかし、この組み合わせは、メッシュされた表面でより多くの詳細を保持します。

00:05:22.000 --> 00:05:25.000
さて、それはシーンジオメトリです。

00:05:25.000 --> 00:05:29.000
次に、LiDARスキャナによってさらにいくつかの改善が可能になります。

00:05:29.000 --> 00:05:33.000
1つ目は、はるかにシンプルで高速なオンボーディングです。

00:05:33.000 --> 00:05:39.000
LiDARスキャナーを使用すると、プレーナーの表面もほぼ瞬時に、より正確に検出されます。

00:05:39.000 --> 00:05:43.000
これは、白い壁のような低機能の表面でも当てはまります。

00:05:43.000 --> 00:05:53.000
その結果、以前は数秒かかり、ある程度のユーザーガイダンスが必要だった飛行機検出のオンボーディングが完全にシームレスに発生できるようになりました。

00:05:53.000 --> 00:05:55.000
そして、変更は必要ありません。

00:05:55.000 --> 00:06:00.000
新しいiPad Proで実行すると、すべてのARKitアプリが恩恵を受けます。

00:06:00.000 --> 00:06:04.000
既存のアプリも、改善されたレイキャスティングの恩恵を受けるでしょう。

00:06:04.000 --> 00:06:12.000
ARKitの強化されたシーン理解により、水平面と垂直面に対するより迅速かつ正確なレイキャストが可能になります。

00:06:12.000 --> 00:06:18.000
さらに、新しいiPad Proは、これまで以上に幅広い表面に対してレイキャストできます。

00:06:18.000 --> 00:06:28.000
許可ターゲットを推定Planeに設定するだけで、LiDARスキャナーからのデータは、周囲の環境に一致するレイキャスティング結果を提供します。

00:06:28.000 --> 00:06:34.000
たとえば、ここで見られるように、大きな椅子やソファのすべての表面にオブジェクトを配置できるようになりました。

00:06:34.000 --> 00:06:41.000
モーションキャプチャと人々の閉塞も、より正確な深度情報を提供するLiDARスキャナにより改善されています。

00:06:41.000 --> 00:06:49.000
モーションキャプチャを使用するアプリは、より正確なスケール推定の恩恵を受け、人々のオクルージョンの深さ値もより正確です。

00:06:49.000 --> 00:06:55.000
さらに、両方の機能が有効になっている場合、ピープルオクルージョンとシーンジオメトリAPIは連携できます。

00:06:55.000 --> 00:07:05.000
人々の非常にダイナミックなジオメトリは、シーンの再構築から除外することができ、その結果、現実世界の環境のより安定したメッシュを提供します。

00:07:05.000 --> 00:07:10.000
だから、新しいiPad ProのARKit 3.5を簡単に見てみましょう。

00:07:10.000 --> 00:07:14.000
シーンジオメトリは、あなたの周りの環境のトポロジカルマップを提供します。

00:07:14.000 --> 00:07:20.000
プレーナーサーフェスはほぼ瞬時に、より正確に検出され、オンボーディングが簡素化されます。

00:07:20.000 --> 00:07:29.000
レイキャスティングはより正確で、シーンジオメトリを考慮に入れることができ、モーションキャプチャと人々の閉塞も改善されます。

00:07:29.000 --> 00:07:34.000
ARKitは、RealityKitと呼ばれる高レベルのARフレームワークとも緊密に統合されています。

00:07:34.000 --> 00:07:41.000
RealityKitは、フォトリアリスティックなレンダリング、カメラエフェクト、アニメーション、物理学などを提供します。

00:07:41.000 --> 00:07:45.000
それはARのために特別にゼロから建てられました。

00:07:45.000 --> 00:07:56.000
RealityKitは、新しいARKit 3.5機能を活用し、新規または既存のRealityKitアプリに簡単に統合できるようにします。

00:07:56.000 --> 00:08:00.000
これらの機能は、新しいシーン理解APIを通じてアクセスできます。

00:08:00.000 --> 00:08:11.000
LiDARで強化された物理学、オクルージョン、照明を有効にするためのオプションを提供し、ARViewのいくつかの簡単な設定を介してすべてアクセスできるので、見てみましょう。

00:08:11.000 --> 00:08:22.000
新しいiPad Proでは、RealityKitは、現実世界で検出したサーフェスからシーンジオメトリの仮想オブジェクト間の物理相互作用を決定することができます。

00:08:22.000 --> 00:08:26.000
そのため、現実世界の家具から仮想ボールが跳ね返ることができます。

00:08:26.000 --> 00:08:33.000
これを行うには、まず仮想コンテンツであるModelEntityの衝突形状を生成し、その物理ボディを初期化します。

00:08:33.000 --> 00:08:41.000
次に、ARViewのsceneUnderstanding.optionsセットに物理オプションを追加するだけで、残りはRealityKitが処理します。

00:08:41.000 --> 00:08:53.000
オクルージョンと同様に、RealityKitは、出入り口、テーブル、椅子などの現実世界のオブジェクトから検出されたシーンジオメトリを使用して、シーン内の仮想オブジェクトを閉塞します。

00:08:53.000 --> 00:08:55.000
これは完全に自動です。

00:08:55.000 --> 00:09:01.000
これを有効にするには、ARViewに設定されたsceneUnderstanding.optionsにオクルージョンを追加するだけです。

00:09:01.000 --> 00:09:08.000
RealityKitはボンネットの下の他のすべてを処理し、あなたの仮想コンテンツは環境内のすべての主要なオブジェクトによって遮断されます。

00:09:08.000 --> 00:09:10.000
これがその例です。

00:09:10.000 --> 00:09:13.000
私たちの仮想ロボットは床を歩き回っています。

00:09:13.000 --> 00:09:17.000
しかし、カメラが柱の後ろを移動すると、ロボットが閉塞していることがわかります。

00:09:17.000 --> 00:09:26.000
そのジオメトリはレンダリングから消えており、それはシーンの適切な深さの錯覚を維持するのに役立ちます。

00:09:26.000 --> 00:09:28.000
さて、3番目の作品は照明用です。

00:09:28.000 --> 00:09:34.000
新しいRealityKitを使用すると、仮想光源が現実世界の表面を照らすことができます。

00:09:34.000 --> 00:09:42.000
これは、LiDARスキャナーの助けを借りて、これらの表面に非常に正確にフィットするシーンジオメトリを照らすことができるためです。

00:09:42.000 --> 00:09:49.000
そして、以前と同様に、これを有効にすることは、ARViewのsceneUnderstanding.optionsセットにreceivesLightingを追加するのと同じくらい簡単です。

00:09:49.000 --> 00:09:55.000
そして最後に、これらの機能のサポートは、Reality Composerで構築されたシーンにも拡張されます。

00:09:55.000 --> 00:10:06.000
物理は、シーンジオメトリメッシュを使用して仮想オブジェクトと現実世界を衝突するように構成でき、現実世界のオブジェクトによる仮想コンテンツのオクルージョンはインスペクタパネルで有効にすることができます。

00:10:06.000 --> 00:10:16.000
詳細については、developer.apple.comにアクセスして、ドキュメント、サンプルコード、またはこのような開発者ビデオへのリンクを見つけることができます。

00:10:16.000 --> 23:59:59.000
見てくれてありがとう!

