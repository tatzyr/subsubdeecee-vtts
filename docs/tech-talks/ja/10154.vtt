WEBVTT

00:00:01.000 --> 00:00:05.000
こんにちは。私の名前はスティーブで、アップルのエンジニアです。

00:00:05.000 --> 00:00:09.000
こんにちは。私はポールです。私もエンジニアです。

00:00:09.000 --> 00:00:19.000
このビデオでは、PyTorchモデルをCore MLに変換するという、Core MLの新しい側面の1つを深く掘り下げます。

00:00:19.000 --> 00:00:31.000
WWDC 2020では、変換プロセスの多くの側面を改善したCore MLコンバータのオーバーホールを発表しました。

00:00:31.000 --> 00:00:37.000
私たちは、ディープラーニングコミュニティで最も一般的に使用されるライブラリのサポートを拡大しました。

00:00:37.000 --> 00:00:45.000
新しいインメモリ表現を活用して、ユーザーエクスペリエンスを向上させるためにコンバーターアーキテクチャを再設計しました。

00:00:45.000 --> 00:00:51.000
そして、APIを統合したので、任意のモデルソースから変換を呼び出すための単一の呼び出しがあります。

00:00:51.000 --> 00:01:01.000
まだ見ていない場合は、この新しいコンバータアーキテクチャの詳細に入るビデオをチェックすることをお勧めします。 

00:01:01.000 --> 00:01:10.000
しかし、このビデオでは、PyTorchディープラーニングフレームワークに組み込まれたモデルから始めて、モデル変換に焦点を当てます。

00:01:10.000 --> 00:01:15.000
だから、あなたはPyTorchを使ってモデルを訓練するのに苦労しているMLエンジニアかもしれません。

00:01:15.000 --> 00:01:24.000
あるいは、あなたはオンラインでキラーPyTorchモデルを見つけたアプリ開発者で、そのモデルをアプリにドロップしたいのかもしれません。

00:01:24.000 --> 00:01:31.000
問題は、そのPyTorchモデルをCore MLモデルにどのように変換するかです。

00:01:31.000 --> 00:01:38.000
さて、古いCore MLコンバーターでは、プロセスのステップとしてモデルをONNXにエクスポートする必要がありました。

00:01:38.000 --> 00:01:43.000
そして、そのコンバーターを使用したことがあるなら、その制限のいくつかに遭遇したかもしれません。

00:01:43.000 --> 00:01:49.000
ONNXはオープンスタンダードであるため、進化や新機能の導入が遅くなる可能性があります。

00:01:49.000 --> 00:01:58.000
それに加えて、PyTorchのようなMLフレームワークは、最新のモデル機能をONNXにエクスポートするためのサポートを追加する時間が必要です。

00:01:58.000 --> 00:02:08.000
したがって、古いコンバータを使用すると、ONNXにエクスポートできないPyTorchモデルがあり、Core MLへの変換がブロックされている可能性があります。

00:02:08.000 --> 00:02:17.000
さて、この余分な依存関係を削除することは、新しいCore MLコンバータで変更されたことの1つにすぎません。

00:02:17.000 --> 00:02:23.000
したがって、このビデオでは、真新しいPyTorchモデル変換パスの詳細を掘り下げます。

00:02:23.000 --> 00:02:32.000
実際の変換例など、PyTorchモデルをCore MLに変換するさまざまな方法について説明します。

00:02:32.000 --> 00:02:40.000
そして最後に、途中でトラブルに遭遇した場合に従うべきいくつかの役立つヒントを共有します。

00:02:40.000 --> 00:02:45.000
それでは、新しい変換プロセスに飛び込みましょう。

00:02:45.000 --> 00:02:54.000
変換するPyTorchモデルから始めて、PyTorchのJITモジュールを使用して、TorchScriptと呼ばれる表現に変換します。

00:02:54.000 --> 00:03:00.000
興味があれば、JITはJust In Timeの頭字語です。

00:03:00.000 --> 00:03:09.000
次に、TorchScriptモデルを手にして、新しいCore MLコンバータを呼び出して、アプリにドロップできるMLモデルを生成します。

00:03:09.000 --> 00:03:14.000
ビデオの後半で、そのTorchScript変換プロセスがどのように見えるかを掘り下げます。

00:03:14.000 --> 00:03:20.000
しかし、では、新しいCore MLコンバータがどのように機能するかを見てみましょう。

00:03:20.000 --> 00:03:25.000
コンバーターはPythonで書かれており、それを呼び出すには数行のコードしかかかりません。

00:03:25.000 --> 00:03:36.000
TorchScriptオブジェクトまたはディスクに保存されているオブジェクトへのパス、およびモデルへの入力の説明を提供するだけです。

00:03:36.000 --> 00:03:42.000
モデルの出力に関する情報を含めることもできますが、それはオプションです。

00:03:42.000 --> 00:03:51.000
コンバーターは、TorchScriptグラフの操作を反復処理し、それらを1つずつCore MLと同等のものに変換することによって機能します。

00:03:51.000 --> 00:03:57.000
1つのTorchScript操作が複数のCore ML操作に変換されることがあります。

00:03:57.000 --> 00:04:09.000
また、グラフ最適化パスは既知のパターンを検出し、いくつかの操作を1つに融合する可能性があります。

00:04:09.000 --> 00:04:14.000
さて、モデルには、コンバーターが理解していないカスタム操作が含まれている場合があります。

00:04:14.000 --> 00:04:22.000
でも、それは大丈夫です。コンバーターは拡張可能に設計されているため、新しい操作の定義を簡単に追加できます。

00:04:22.000 --> 00:04:29.000
多くの場合、その操作を既存の操作の組み合わせとして表現することができ、私たちはそれを「composite op」と呼びます。

00:04:29.000 --> 00:04:36.000
しかし、それが十分でない場合は、カスタムSwift実装を作成し、変換中にそれをターゲットにすることもできます。

00:04:36.000 --> 00:04:45.000
このビデオでは、その方法の詳細については説明しませんが、例やウォークスルーについては、オンラインリソースをご覧ください。

00:04:45.000 --> 00:04:56.000
変換プロセス全体の概要を説明したので、PyTorchモデルからTorchScriptモデルを取得する方法を掘り下げる時が来ました。

00:04:56.000 --> 00:04:58.000
PyTorchには2つの方法があります。

00:04:58.000 --> 00:05:05.000
1つ目は「トレース」と呼ばれ、2つ目は「スクリプト」と呼ばれます。

00:05:05.000 --> 00:05:10.000
まず、モデルをトレースすることの意味を見てみましょう。

00:05:10.000 --> 00:05:17.000
トレースは、このコードスニペットに示すように、PyTorchのJITモジュールのトレースメソッドを呼び出すことによって行われます。

00:05:17.000 --> 00:05:26.000
入力例とともにPyTorchモデルを渡し、モデルとTorchScript表現を返します。

00:05:26.000 --> 00:05:29.000
では、この呼び出しは実際に何をしますか?

00:05:29.000 --> 00:05:41.000
アクティブトレースは、モデルのフォワードパスを介して入力例を実行し、入力がモデルのレイヤーを通過するときに呼び出される操作をキャプチャします。

00:05:41.000 --> 00:05:49.000
その後、これらすべての操作のコレクションは、モデルのTorchScript表現になります。

00:05:49.000 --> 00:05:57.000
さて、トレースする入力の例を選ぶとき、使用する最善のことは、モデルが通常の使用中で見るものと同様のデータです。

00:05:57.000 --> 00:06:06.000
たとえば、検証データの1つのサンプルを使用するか、アプリがモデルに提示するのと同じ方法でキャプチャされたデータを使用できます。

00:06:06.000 --> 00:06:09.000
ランダムなデータを使用することもできます。

00:06:09.000 --> 00:06:18.000
その場合は、入力値の範囲とテンソルの形状がモデルが期待するものと一致していることを確認してください。

00:06:18.000 --> 00:06:22.000
例を通して作業することで、このすべてをもう少し具体的にしましょう。

00:06:22.000 --> 00:06:31.000
同僚のポールを紹介したいと思います。彼は、セグメンテーションモデルをPyTorchからCore MLに変換する完全なプロセスを案内します。

00:06:31.000 --> 00:06:33.000
ありがとう、スティーブ。

00:06:33.000 --> 00:06:38.000
セグメンテーションモデルを持っていて、それをデバイス上で実行したいとします。

00:06:38.000 --> 00:06:48.000
セグメンテーションモデルが何をするのかに精通していない場合は、画像を取得し、その画像の各ピクセルにクラス確率スコアを割り当てます。

00:06:48.000 --> 00:06:51.000
では、モデルをデバイス上で実行させるにはどうすればよいですか?

00:06:51.000 --> 00:06:55.000
私は自分のモデルをCore MLモデルに変換するつもりです。

00:06:55.000 --> 00:07:04.000
これを行うには、まずPyTorchモデルをトレースして、PyTorchのJITトレースモジュールを使用してTorchScriptフォームに変換します。

00:07:04.000 --> 00:07:12.000
次に、新しいCore MLコンバータを使用して、TorchScriptモデルをCore MLモデルに変換します。

00:07:12.000 --> 00:07:18.000
最後に、結果のCore MLモデルがXcodeにシームレスに統合される方法を紹介します。

00:07:18.000 --> 00:07:23.000
このプロセスがコードでどのように見えるか見てみましょう。

00:07:23.000 --> 00:07:31.000
このJupyter Notebookでは、スライドに記載されているPyTorchセグメンテーションモデルをCore MLモデルに変換します。

00:07:31.000 --> 00:07:38.000
このコードを自分で試してみたい場合は、このビデオに関連付けられたコードスニペットで入手できます。

00:07:38.000 --> 00:07:46.000
まず、このデモに使用するいくつかの依存関係をインポートします。

00:07:46.000 --> 00:08:04.000
次に、トーチビジョンとサンプル入力からResNet-101セグメンテーションモデルをロードします。この場合、犬と猫の画像です。

00:08:04.000 --> 00:08:10.000
PyTorchモデルは、PIL画像オブジェクトではなく、テンソルオブジェクトを取ります。

00:08:10.000 --> 00:08:15.000
だから私はtransforms.ToTensorで画像をテンソルに変換します。

00:08:15.000 --> 00:08:26.000
モデルはまた、バッチサイズを示すテンソルに余分な次元を期待しているので、私もそれを追加します。

00:08:26.000 --> 00:08:32.000
スライドで述べたように、Core MLコンバータはTorchScriptモデルを受け入れます。

00:08:32.000 --> 00:08:47.000
これを取得するには、PyTorchモデルをTorchScriptモデルに変換するTorch.JITモジュールのトレースメソッドを使用します。

00:08:47.000 --> 00:08:51.000
うーん、ああ。トレースは例外を投げました。

00:08:51.000 --> 00:08:59.000
例外メソッドで言うように、「テンソルのテンソルまたはタプルのみがトレース関数から出力できます。」

00:08:59.000 --> 00:09:03.000
これはPyTorchのJITモジュールの制限です。

00:09:03.000 --> 00:09:08.000
ここでの問題は、私のモデルが辞書を返していることです。

00:09:08.000 --> 00:09:18.000
出力辞書からテンソル値のみを抽出するPyTorchモジュールにモデルをラップすることでこれを解決します。

00:09:18.000 --> 00:09:26.000
ここでは、PyTorchのモジュールクラスから継承するクラスラッパーを宣言します。

00:09:26.000 --> 00:09:32.000
上記のように、ResNet-101を含むモデル属性を定義します。

00:09:32.000 --> 00:09:43.000
このラッピングクラスのforwardメソッドでは、返された辞書を「out」という名前のキーでインデックスし、テンソル出力のみを返します。

00:09:43.000 --> 00:09:57.000
モデルは辞書ではなくテンソルを返すので、正常にトレースします。

00:09:57.000 --> 00:10:01.000
今、新しいCore MLコンバーターを利用する時が来ました。

00:10:01.000 --> 00:10:09.000
まず、入力とその前処理を定義する必要があります。

00:10:09.000 --> 00:10:20.000
私は自分の入力を、ImageNet統計で画像を正規化し、その値を0から1の間に縮小する前処理を備えたImageTypeとして定義します。

00:10:20.000 --> 00:10:26.000
この前処理は、ResNet-101が期待するものです。

00:10:26.000 --> 00:10:43.000
次に、Core MLツール変換メソッドを呼び出すだけで、TorchScriptモデルと入力定義を渡します。

00:10:43.000 --> 00:10:51.000
変換後、Xcodeなどの他のプログラムで理解できるように、モデルのメタデータを設定します。

00:10:51.000 --> 00:11:05.000
モデルのタイプをセグメンテーションに設定し、モデルの順序でクラスを列挙します。

00:11:05.000 --> 00:11:09.000
それで、私の変換されたモデルは機能しますか?

00:11:09.000 --> 00:11:13.000
Xcodeを通じて、モデルの出力を簡単に視覚化できます。

00:11:13.000 --> 00:11:20.000
まず、モデルを保存します。

00:11:20.000 --> 00:11:27.000
今、私がする必要があるのは、Finderで保存したモデルをクリックするだけで、Xcodeによって開かれます。

00:11:27.000 --> 00:11:33.000
ここでは、入力図形や型を含むメタデータを表示できます。

00:11:33.000 --> 00:11:43.000
モデルの出力を視覚化するには、[プレビュー]タブに移動し、犬と猫のサンプル画像をドラッグします。

00:11:43.000 --> 00:11:49.000
私のモデルは、この画像のペットをうまくセグメント化しているようです。

00:11:49.000 --> 00:11:55.000
ResNet-101は追跡できましたが、一部のモデルを追跡することはできません。

00:11:55.000 --> 00:12:02.000
これらの他のモデルを変換する方法を説明するために、スティーブにキックバックします。

00:12:02.000 --> 00:12:03.000
ありがとう、ポール。

00:12:03.000 --> 00:12:08.000
わかりました。トレースを使用して変換がどのように機能するかについて、私たちはかなり良いハンドルを持っていると思います。

00:12:08.000 --> 00:12:11.000
しかし、PyTorchはTorchScriptを取得する2番目の方法を提供します。

00:12:11.000 --> 00:12:15.000
それでは、「スクリプティング」と呼ばれるものを掘り下げてみましょう。

00:12:15.000 --> 00:12:22.000
スクリプトは、PyTorchモデルを取り、TorchScript操作に直接コンパイルすることで機能します。

00:12:22.000 --> 00:12:27.000
トレースは、データが流れるにつれてモデルをキャプチャしたことを忘れないでください。

00:12:27.000 --> 00:12:31.000
しかし、トレースと同様に、モデルのスクリプト化も本当に簡単です。

00:12:31.000 --> 00:12:39.000
PyTorchのJITモジュールのスクリプトメソッドを呼び出して、モデルを提供するだけです。

00:12:39.000 --> 00:12:48.000
わかりました。TorchScript表現を取得する2つの異なる方法を紹介しましたが、いつ一方と他方を使用するのか疑問に思うかもしれません。

00:12:48.000 --> 00:12:53.000
スクリプトを使用する必要があるケースの1つは、モデルに制御フローが含まれている場合です。

00:12:53.000 --> 00:12:56.000
理由を理解するために例を見てみましょう。

00:12:56.000 --> 00:13:05.000
ここでは、このモデルには分岐とループがあり、モデルを直接コンパイルしているため、スクリプトはそのすべてをキャプチャします。

00:13:05.000 --> 00:13:15.000
モデルをトレースした場合、得られるのは、指定された入力のモデルを通るパスだけで、モデル全体をキャプチャしていないことがわかります。

00:13:15.000 --> 00:13:27.000
モデルをスクリプト化する必要がある場合は、通常、できるだけ多くのモデルを追跡し、それを必要とするモデルの部分のみをスクリプト化すると、最良の結果が得られます。

00:13:27.000 --> 00:13:33.000
これは、トレースが通常、スクリプトよりも簡単な表現を生成するためです。

00:13:33.000 --> 00:13:37.000
いくつかのコードを見て、このアイデアを適用する方法を見てみましょう。

00:13:37.000 --> 00:13:44.000
この例では、ループ内で一定回数のコードチャンクを実行するモデルがあります。

00:13:44.000 --> 00:13:53.000
ループの本体を自分で簡単に追跡できるものに分離し、モデル全体にスクリプトを適用することができます。

00:13:53.000 --> 00:14:02.000
私たちが基本的にやっていることは、スクリプトをそれを必要とする制御フローのビットだけに制限し、他のすべてをトレースすることです。

00:14:02.000 --> 00:14:11.000
このトレースとスクリプトの混合は、両方ともすでにTorchScriptに変換されたコードをスキップするため、機能します。

00:14:11.000 --> 00:14:15.000
今度は、スクリプトを使用する具体的な例を見てみましょう。

00:14:15.000 --> 00:14:20.000
私はそれをポールに返します。ポールは言語モデルの変換を案内します。

00:14:20.000 --> 00:14:22.000
やあ。

00:14:22.000 --> 00:14:30.000
デバイス上で実行できるように、Core MLモデルに変換したい文補完モデルがあるとします。

00:14:30.000 --> 00:14:41.000
いくつかの文脈では、文の完了は、文の断片を取り、モデルを使用してその後に来る可能性が高い単語を予測することを含むタスクです。

00:14:41.000 --> 00:14:45.000
では、これは計算ステップの観点からどのように見えますか?

00:14:45.000 --> 00:14:56.000
私は文の断片のいくつかの単語から始めて、それらの単語を私のモデルが理解できる表現に翻訳するエンコーダと呼ばれるものに渡します。

00:14:56.000 --> 00:14:59.000
この場合、整数トークンのシーケンス。

00:14:59.000 --> 00:15:07.000
次に、そのトークンのシーケンスをモデルに渡し、シーケンス内の次のトークンを予測します。

00:15:07.000 --> 00:15:21.000
私は私のモデルに部分的に構築された文を与え続け、私のモデルが特別な文末トークンを予測するまで、最後に新しいトークンを追加します。これは私の文が完了したことを意味します。

00:15:21.000 --> 00:15:30.000
トークンの完全な文がわかったので、トークンを言葉に戻すデコーダに渡します。

00:15:30.000 --> 00:15:38.000
トークンのリストを完成させるこの図の中央部分は、私がCore MLモデルに変換するものです。

00:15:38.000 --> 00:15:42.000
エンコーダとデコーダは別々に扱われます。

00:15:42.000 --> 00:15:47.000
疑似コードを見て、何が起こっているのかを確実に理解しましょう。 擬似コードを見てみましょう。

00:15:47.000 --> 00:15:51.000
私のモデルのコアは、私の次のトークン予測器です。

00:15:51.000 --> 00:15:55.000
このために、私はHugging FaceのGPT2モデルを使用します。

00:15:55.000 --> 00:16:02.000
予測器はトークンのリストを入力として受け取り、次のトークンの予測を提供します。

00:16:02.000 --> 00:16:10.000
次に、予測器の周りにいくつかの制御フローをラップして、文末トークンが表示されるまで続けます。

00:16:10.000 --> 00:16:19.000
ループ内では、予測トークンを実行リストに追加し、それをすべてのループの予測因子への入力として使用します。

00:16:19.000 --> 00:16:26.000
予測器が文末トークンを返すと、デコードするための完全な文を返します。

00:16:26.000 --> 00:16:31.000
さて、このプロセス全体がエンコードされるのを見るために、Jupyter Notebookに飛び込みましょう。

00:16:31.000 --> 00:16:38.000
このノートでは、文の断片を取り、文を完成させる言語モデルを構築します。

00:16:38.000 --> 00:16:45.000
輸入品を邪魔にならないようにしましょう。

00:16:45.000 --> 00:16:53.000
これが私のモデルのコードです。

00:16:53.000 --> 00:17:06.000
私のモデルはtarch.Moduleを継承し、文末トークン、next_token_predictorモデル、および文の先頭を示すデフォルトトークンの属性が含まれています。

00:17:06.000 --> 00:17:14.000
そのフォワードメソッドでは、スライドと同じように、トークンのリストを取り、次のトークンを予測するループボディを書きました。

00:17:14.000 --> 00:17:19.000
ループは、文末トークンが生成されるまで続きます。

00:17:19.000 --> 00:17:24.000
これが起こったら、文を返します。

00:17:24.000 --> 00:17:33.000
前述のように、私の次のトークン予測は、ループ本体に存在するGPT2になります。

00:17:33.000 --> 00:17:39.000
モデル全体のスクリプト化とは別に、ループ本体をトレースする練習に従います。

00:17:39.000 --> 00:17:44.000
だから、次のトークン予測器でのみJITトレーサーを実行します。

00:17:44.000 --> 00:17:58.000
トークンのリストを入力として取るので、トレースのために、ランダムなトークンのリストを渡すだけです。

00:17:58.000 --> 00:18:04.000
トレーサーは、このトレースが他の入力に一般化されない可能性があるという警告を発したことがわかります。

00:18:04.000 --> 00:18:10.000
この警告は、Core MLではなく、PyTorchのJITトレーサーからのものであることに注意してください。

00:18:10.000 --> 00:18:19.000
トラブルシューティングセクションで何が起こっているのかは後で説明しますが、実際には問題がないので、今のところこの警告を無視します。

00:18:19.000 --> 00:18:32.000
ループボディの大部分をトレースすると、センテンスフィニッシュモデルをインスタンス化し、JITスクリプターを適用してCore MLへの変換の準備をすることができます。

00:18:32.000 --> 00:18:46.000
今、私のTorchScriptモデルでは、セグメンテーションのデモと同じようにCore MLモデルに変換します。

00:18:46.000 --> 00:18:50.000
今、私のモデルが文章を終えることができるかどうかを確認します。

00:18:50.000 --> 00:18:56.000
私は文の断片を作成します。この場合、「マンハッタン橋は」です。

00:18:56.000 --> 00:19:12.000
次に、GPT2に含まれているエンコーダを介して実行してフラグメントのエンコーディングを取得し、そのトークンのリストをトーチテンソルに変換します。

00:19:12.000 --> 00:19:34.000
次に、Core MLモデルからの入力をパッケージ化し、そのモデルを実行し、GPT2に含まれているデコーダで出力をデコードします。

00:19:34.000 --> 00:19:38.000
いいね。コアMLモデルは文を完成させることができました。

00:19:38.000 --> 00:19:43.000
マンハッタン橋についての声明を生み出したようです。

00:19:43.000 --> 00:19:50.000
モデルをトレースしてスクリプト化してCore ML形式にすると、道路に沿ってバンプに遭遇する可能性があります。

00:19:50.000 --> 00:19:55.000
途中であなたを助けるためにそれをスティーブに返します。

00:19:55.000 --> 00:20:07.000
締めくくる前に、PyTorchモデルをCore MLに変換する際に気付いた障害を確認し、いくつかのトラブルシューティングのヒントとベストプラクティスを確認したいと思います。

00:20:07.000 --> 00:20:12.000
セグメンテーションのデモを振り返ってみると、トレース中にエラーが発生したことを覚えておいてください。

00:20:12.000 --> 00:20:21.000
これは、私たちのモデルが辞書を返し、JITトレースがテンソルまたはテンソルのタプルしか処理できないためです。

00:20:21.000 --> 00:20:29.000
デモで示した解決策は、モデルのネイティブ出力をアンパックするモデルの周りに薄いラッパーを作成することでした。

00:20:29.000 --> 00:20:40.000
この例では、モデルが辞書を返したので、ここでは推論結果を表す辞書キーにアクセスし、そのテンソルを返します。

00:20:40.000 --> 00:20:52.000
もちろん、このアイデアは、辞書から複数のアイテムにアクセスして返したい場合や、他のタイプのコンテナを解凍する必要がある場合にも機能します。

00:20:52.000 --> 00:21:00.000
言語モデルのデモ中に、トレースが他の入力に一般化されない可能性があるというトレーサーの警告に遭遇しました。

00:21:00.000 --> 00:21:05.000
そして、トレーサーが面倒なコード行を印刷するのに役立ちます。

00:21:05.000 --> 00:21:07.000
それで、実際に何が起こっているの?

00:21:07.000 --> 00:21:16.000
警告を理解するためにモデルのソースコードを見ると、モデルが別のテンソルのサイズに基づいて1つのテンソルをスライスしていることがわかります。

00:21:16.000 --> 00:21:31.000
テンソルのサイズを取得すると、PyTorchテンソルではなく、ベアPython値になり、トレーサーはこれらのベアPython値で実行されている数学操作をトレースできないと警告しています。

00:21:31.000 --> 00:21:40.000
しかし、この場合、トレーサーはこの警告を発するのに少し攻撃的であり、実際には問題はありません。

00:21:40.000 --> 00:21:51.000
裸のPython値で動作するトレースコードに関する良い経験則は、組み込みのPython操作のみがトレーサーによって正しくキャプチャされるということです。

00:21:51.000 --> 00:21:55.000
このアイデアを説明するのに役立つ例をいくつか紹介します。

00:21:55.000 --> 00:22:03.000
これらを通して考え、その経験則に基づいて、それらが正しく追跡されるかどうかを把握しましょう。

00:22:03.000 --> 00:22:15.000
最初の例は、デモ中に見たものと非常によく似ており、組み込み操作、この場合は追加が適用されているため、正しいトレースになります。

00:22:15.000 --> 00:22:25.000
2番目の例も正しくトレースします。この場合、モジュロ演算子を使用します。これは、これも組み込み操作です。

00:22:25.000 --> 00:22:29.000
しかし、3番目の例は正しくトレースできません。

00:22:29.000 --> 00:22:45.000
JITトレーサーは、ライブラリ関数math.sqrtが何をするのかを知りません。トレースされたグラフは、テンソルサイズと平方根を計算する操作の代わりに、一定の値が記録されます。

00:22:45.000 --> 00:22:55.000
しかし、math.sqrtをPythonの組み込みパワー演算子に置き換えるモデルの簡単な修正により、正しいトレースが得られます。

00:22:55.000 --> 00:22:59.000
では、モデルのスクリプティングが失敗する可能性があるケースを見てみましょう。

00:22:59.000 --> 00:23:05.000
このモデルは空のリストから始まり、固定された整数セットを連続して追加します。

00:23:05.000 --> 00:23:08.000
これはひどく有用なモデルではないことを覚えておいてください。

00:23:08.000 --> 00:23:12.000
故障状態を説明するために使っているだけです。

00:23:12.000 --> 00:23:18.000
このモデルをスクリプト化すると、型の不一致を示唆するランタイムエラーが表示されます。

00:23:18.000 --> 00:23:26.000
JITスクリプターは、モデルをTorchScriptに変えるために型情報を必要とし、コンテキストからオブジェクト型を推測するのにかなり良い仕事をします。

00:23:26.000 --> 00:23:35.000
ただし、それが不可能な場合があり、スクリプターがオブジェクトのタイプを把握できない場合は、オブジェクトがテンソルであると仮定します。

00:23:35.000 --> 00:23:44.000
この場合、このリストがテンソルのリストであると仮定していますが、実際には整数のリストとして構築されています。

00:23:44.000 --> 00:23:47.000
では、スクリプターを助けるために何ができますか?

00:23:47.000 --> 00:23:54.000
さて、変数の有意義な初期化を含めるか、型注釈を使用できます。

00:23:54.000 --> 00:23:59.000
ここでは、両方の例を示すためにモデルを調整しました。

00:23:59.000 --> 00:24:02.000
最後に1つ言及したいことがあります。

00:24:02.000 --> 00:24:08.000
トレースする前に、常にモデルが評価モードになっていることを確認する必要があります。

00:24:08.000 --> 00:24:13.000
これにより、すべてのレイヤーがトレーニングではなく推論用に構成されることが保証されます。

00:24:13.000 --> 00:24:16.000
ほとんどのレイヤーでは、これは問題ではありません。

00:24:16.000 --> 00:24:23.000
しかし、たとえば、モデルにドロップアウトレイヤーがある場合、評価モードを設定すると、それが無効になっていることを確認します。

00:24:23.000 --> 00:24:31.000
そして、コンバーターが無効になっている操作に遭遇すると、それらをパススルー操作として扱います。

00:24:31.000 --> 00:24:49.000
このビデオでは多くの資料を取り上げましたが、Core MLコンバータのドキュメント、カスタムオペ変換に関する情報、多くの詳細なTorchScriptの例など、ビデオに関連するリンクでさらに多くの情報を見つけることができます。

00:24:49.000 --> 00:24:54.000
PyTorchモデルを変換するための一流のサポートを提供することに本当に興奮しています。

00:24:54.000 --> 00:25:10.000
新しいCore MLコンバータが、PyTorchモデルのより広範なサポートを可能にし、デバイス上のモデル実行を最適化し、モデルを簡単に変換するための最大限のサポートを提供することを願っています。

00:25:10.000 --> 23:59:59.000
見てくれてありがとう。

