WEBVTT

00:00:00.000 --> 00:00:08.000
iOS 11では、iPhoneとiPad用の拡張現実アプリを作成するための新しいフレームワークであるARKitが導入されました。

00:00:08.000 --> 00:00:20.000
ARKitは、デジタルオブジェクトを周囲の環境に配置することで、アプリを画面を超えて、まったく新しい方法で現実世界と対話できるようにします。

00:00:20.000 --> 00:00:24.000
WWDCでは、ARKitの3つの主要な機能を導入しました。

00:00:24.000 --> 00:00:33.000
位置追跡はデバイスのポーズを検出し、iPhoneやiPadをあなたの周りのデジタル世界への窓として使うことができます。

00:00:33.000 --> 00:00:53.000
シーンの理解は、テーブルトップなどの水平面を検出し、安定したアンカーポイントを見つけ、周囲の照明条件の推定値を提供し、SpriteKit、SceneKit、Metalなどのレンダリング技術や、UnityやUnrealなどの一般的なゲームエンジンとの統合を提供します。

00:00:53.000 --> 00:01:00.000
iPhone Xでは、ARKitはあなたに焦点を合わせ、フロントカメラを使用した顔追跡を提供します。

00:01:00.000 --> 00:01:07.000
この新しい能力は、6つの自由度で堅牢な顔検出と位置追跡を可能にします。

00:01:07.000 --> 00:01:20.000
顔の表情もリアルタイムで追跡され、アプリには、検出された顔の50以上の特定の筋肉の動きを表すフィットトライアングルメッシュと加重パラメータが提供されます。

00:01:20.000 --> 00:01:27.000
ARの場合、カメラからの前面カラー画像と前面深度画像を提供します。

00:01:27.000 --> 00:01:36.000
また、ARKitは顔を光プローブとして使用して照明条件を推定し、レンダリングに適用できる球面高調波係数を生成します。

00:01:36.000 --> 00:01:42.000
そして、私が述べたように、これはすべてiPhone Xでのみサポートされています。

00:01:42.000 --> 00:01:45.000
フェイストラッキングでできる本当に楽しいことがいくつかあります。

00:01:45.000 --> 00:02:04.000
1つ目は自撮り効果で、バーチャルタトゥーやフェイスペイントなどの効果のためにフェイスメッシュに半透明のテクスチャをレンダリングしたり、化粧をしたり、ひげや口ひげを生やしたり、ジュエリー、マスク、帽子、メガネでメッシュを重ねたりします。

00:02:04.000 --> 00:02:16.000
2つ目はフェイスキャプチャで、顔の表情をリアルタイムでキャプチャし、それをリギングとして使用して、アバターやゲームのキャラクターに表情を投影します。

00:02:16.000 --> 00:02:20.000
では、詳細を掘り下げて、フェイストラッキングを始める方法を見てみましょう。

00:02:20.000 --> 00:02:24.000
最初にする必要があるのは、ARSessionを作成することです。

00:02:24.000 --> 00:02:33.000
ARSessionは、デバイスの設定からさまざまなARテクニックの実行まで、ARKitのために行われたすべての処理を処理するオブジェクトです。

00:02:33.000 --> 00:02:38.000
セッションを実行するには、まずこのアプリにどのようなトラッキングが必要なのかを説明する必要があります。

00:02:38.000 --> 00:02:44.000
これを行うには、フェイストラッキング用の特定のARConfigurationを作成して設定します。

00:02:44.000 --> 00:02:51.000
処理を開始するには、セッションで「実行」メソッドを呼び出して、実行する設定を提供するだけです。

00:02:51.000 --> 00:02:59.000
内部的には、ARKitはAVCaptureSessionとCMMotionManagerを設定して、カメラ画像とセンサーデータの受信を開始します。

00:02:59.000 --> 00:03:03.000
そして、処理後、結果はARFrameとして出力されます。

00:03:03.000 --> 00:03:12.000
各ARFrameは時間のスナップショットであり、カメラ画像、追跡データ、アンカーポイントを提供します。基本的にシーンをレンダリングするために必要なすべてです。

00:03:12.000 --> 00:03:16.000
それでは、顔追跡のためのARConfigurationを詳しく見てみましょう。

00:03:16.000 --> 00:03:21.000
ARFaceTrackingConfigurationという新しいサブクラスを追加しました。

00:03:21.000 --> 00:03:28.000
これは、正面カメラを介して顔追跡を有効にするようにARSessionに指示する簡単な設定サブクラスです。

00:03:28.000 --> 00:03:36.000
デバイスでの顔追跡の可用性と、照明推定を有効にするかどうかをチェックするための基本的なプロパティがいくつかあります。

00:03:36.000 --> 00:03:41.000
次に、「run」を呼び出すと、追跡を開始し、ARFrameの受信を開始します。

00:03:41.000 --> 00:03:45.000
顔が検出されると、セッションはARFaceAnchorを生成します。

00:03:45.000 --> 00:03:52.000
これは主要な顔を表しています - カメラのビューで単一の最大かつ最も近い顔。

00:03:52.000 --> 00:03:59.000
ARFaceAnchorは、スーパークラスの変換特性を通じて、世界座標のフェイスポーズを提供します。

00:03:59.000 --> 00:04:05.000
また、現在の表情の3Dトポロジとパラメータも提供します。

00:04:05.000 --> 00:04:12.000
そして、ご覧のとおり、それはすべて追跡され、メッシュとパラメータはリアルタイムで毎秒60回更新されます。

00:04:12.000 --> 00:04:26.000
今、トポロジーに焦点を当てて、ARKitは、寸法、形状、およびユーザーの表情にリアルタイムでフィットする顔の詳細な3Dメッシュを提供します。

00:04:26.000 --> 00:04:32.000
このデータは、いくつかの異なる形式で利用できます。1つ目はARFaceGeometryクラスです。

00:04:32.000 --> 00:04:41.000
これは本質的に三角形のメッシュなので、頂点、三角形のインデックス、テクスチャ座標の配列で、レンダラーで視覚化することができます。

00:04:41.000 --> 00:04:53.000
ARKitは、任意のSceneKitノードにアタッチできるジオメトリオブジェクトを定義するARSCNFaceGeometryクラスを通じて、SceneKitでメッシュを視覚化する簡単な方法も提供します。

00:04:53.000 --> 00:04:57.000
ジオメトリメッシュとは別に、ブレンドシェイプと呼ばれるものもあります。

00:04:57.000 --> 00:05:02.000
ブレンドシェイプは、現在の表情の高レベルモデルを提供します。

00:05:02.000 --> 00:05:13.000
それらは、まぶた、眉毛、顎、鼻など、特定の特徴のポーズを表す名前付き係数の辞書です。

00:05:13.000 --> 00:05:18.000
それらはゼロから1までの浮動小数点値として表現され、すべてライブで更新されます。

00:05:18.000 --> 00:05:28.000
したがって、これらのブレンド形状係数を使用して、ユーザーの顔の動きを直接反映する方法で、2Dまたは3Dのキャラクターをアニメーション化またはリグすることができます。

00:05:28.000 --> 00:05:32.000
利用可能なもののアイデアを与えるために、ここにブレンド形状係数のリストがあります。

00:05:32.000 --> 00:05:43.000
したがって、これらのそれぞれは、右と左の眉毛、あなたの目の位置、あなたの顎、あなたの笑顔の形など、独立して追跡され、更新されます。

00:05:43.000 --> 00:05:50.000
顔のジオメトリのレンダリングや3Dキャラクターのアニメーション化と密接に関連しているのは、リアルな照明です。

00:05:50.000 --> 00:06:02.000
そして、あなたの顔を光プローブとして使用することで、顔検出を実行しているARSessionは、世界空間における光の強度とその方向を表す指向性光の推定値を提供することができます。

00:06:02.000 --> 00:06:06.000
ほとんどのアプリでは、この照明ベクトルと強度は十分すぎるほどです。

00:06:06.000 --> 00:06:14.000
しかし、ARKitは、シーンで検出された光の強度を表す2度の球面高調波係数も提供します。

00:06:14.000 --> 00:06:19.000
したがって、より高度な要件を持つアプリの場合、これも活用できます。

00:06:19.000 --> 00:06:21.000
そして、言及すべきさらにいくつかの機能。

00:06:21.000 --> 00:06:29.000
カラーデータ付きのフロントカメラ画像に加えて、ARKitはアプリに前面深度画像も提供できます。

00:06:29.000 --> 00:06:31.000
そして、私はこれをグレースケール画像としてここに表示しています。

00:06:31.000 --> 00:06:37.000
データ自体は、タイムスタンプとともにAVDepthDataオブジェクトとして提供されます。

00:06:37.000 --> 00:06:47.000
しかし、これは15Hzでキャプチャされており、ARKitが60Hzでキャプチャするカラー画像よりも低い周波数であることに注意することが重要です。

00:06:47.000 --> 00:06:55.000
そして最後に、どのARKitセッションでも使用できますが、フェイストラッキングで特に興味深い機能は、オーディオキャプチャです。

00:06:55.000 --> 00:07:06.000
現在はデフォルトで無効になっていますが、有効になっている場合、ARSessionの実行中に、マイクからオーディオサンプルをキャプチャし、一連のCMSampleBuffersをアプリに配信します。

00:07:06.000 --> 00:07:12.000
したがって、これはユーザーの顔と声を同時にキャプチャしたい場合に便利です。

00:07:12.000 --> 00:07:20.000
フェイストラッキングの詳細、およびサンプルコードへのリンクについては、開発者のウェブサイトdeveloper.apple.com/arkitをご覧ください。

00:07:20.000 --> 23:59:59.000
見てくれてありがとう!

