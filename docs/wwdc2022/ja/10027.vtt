WEBVTT

00:00:00.000 -> 00:00:03.000
♪まろやかなインストゥルメンタルヒップホップ音楽♪

00:00:03.000 -> 00:00:09.000
♪

00:00:09.000 -> 00:00:13.000
こんにちは、私の名前はベンで、コアMLチームのエンジニアです。

00:00:13.000 -> 00:00:18.000
今日は、Core MLに追加されるエキサイティングな新機能のいくつかを紹介します。

00:00:18.000 -> 00:00:23.000
これらの機能の焦点は、Core MLの使用を最適化することです。

00:00:23.000 -> 00:00:34.000
このセッションでは、Core MLを使用する際にモデルのパフォーマンスを理解し、最適化するために必要な情報を提供するために、現在利用可能なパフォーマンスツールについて確認します。

00:00:34.000 -> 00:00:40.000
次に、これらの最適化を可能にするいくつかの強化されたAPIについて確認します。

00:00:40.000 -> 00:00:47.000
そして最後に、いくつかの追加のCore ML機能と統合オプションの概要を説明します。

00:00:47.000 -> 00:00:50.000
パフォーマンスツールから始めましょう。

00:00:50.000 -> 00:00:56.000
背景を伝えるために、アプリ内でCore MLを使用する際の標準的なワークフローを要約することから始めます。

00:00:56.000 -> 00:00:59.000
最初のステップは、モデルを選択することです。

00:00:59.000 -> 00:01:13.000
これは、Core MLツールを使用してPyTorchまたはTensorFlowモデルをCore ML形式に変換する、既存のCore MLモデルを使用する、Create MLを使用してモデルをトレーニングおよびエクスポートするなど、さまざまな方法で行うことができます。

00:01:13.000 -> 00:01:20.000
モデル変換の詳細やCreate MLについては、これらのセッションをチェックすることをお勧めします。

00:01:20.000 -> 00:01:23.000
次のステップは、そのモデルをアプリに統合することです。

00:01:23.000 -> 00:01:33.000
これには、モデルをアプリケーションにバンドルし、Core ML APIを使用して、アプリの実行中にそのモデルの推論をロードして実行することが含まれます。

00:01:33.000 -> 00:01:39.000
最後のステップは、Core MLの使用方法を最適化することです。

00:01:39.000 -> 00:01:41.000
まず、モデルの選択について確認します。

00:01:41.000 -> 00:01:47.000
アプリ内でそのモデルを使用するかどうかを決定する際に考慮すべきモデルの多くの側面があります。

00:01:47.000 -> 00:01:53.000
また、選択したいモデルの候補が複数あるかもしれませんが、どちらを使用するかをどのように決めますか?

00:01:53.000 -> 00:01:58.000
有効にしたい機能の要件に一致する機能を持つモデルが必要です。

00:01:58.000 -> 00:02:03.000
これには、モデルの精度とパフォーマンスを理解することが含まれます。

00:02:03.000 -> 00:02:07.000
Core MLモデルについて学ぶ素晴らしい方法は、Xcodeで開くことです。

00:02:07.000 -> 00:02:12.000
任意のモデルをダブルクリックするだけで、以下が表示されます。

00:02:12.000 -> 00:02:19.000
上部には、モデルタイプ、サイズ、およびオペレーティングシステムの要件があります。

00:02:19.000 -> 00:02:30.000
[一般]タブでは、モデルのメタデータ、計算精度とストレージ精度、および予測できるクラスラベルなどの情報でキャプチャされた追加の詳細が表示されます。

00:02:30.000 -> 00:02:37.000
プレビュータブは、サンプル入力を提供し、それが何を予測するかを見て、モデルをテストするためのものです。

00:02:37.000 -> 00:02:45.000
[予測]タブには、モデルの入力と出力、およびCore MLが実行時に期待するタイプとサイズが表示されます。

00:02:45.000 -> 00:02:52.000
そして最後に、ユーティリティタブは、モデルの暗号化と展開タスクに役立ちます。

00:02:52.000 -> 00:02:58.000
全体として、これらのビューは、モデルの機能の概要と精度のプレビューを提供します。

00:02:58.000 -> 00:03:02.000
しかし、あなたのモデルのパフォーマンスはどうですか?

00:03:02.000 -> 00:03:12.000
モデルのロードコスト、単一の予測にかかる時間、または使用するハードウェアは、ユースケースにとって重要な要素である可能性があります。

00:03:12.000 -> 00:03:22.000
リアルタイムのストリーミングデータの制約に関連するハードターゲットがある場合や、知覚されるレイテンシに応じて、ユーザーインターフェイスに関する重要な設計決定を行う必要がある場合があります。

00:03:22.000 -> 00:03:32.000
モデルのパフォーマンスに関する洞察を得る方法の1つは、アプリへの初期統合を行うか、測定および測定できる小さなプロトタイプを作成することです。

00:03:32.000 -> 00:03:39.000
また、パフォーマンスはハードウェアに依存するため、サポートされているさまざまなハードウェアでこれらの測定を行いたいと思うでしょう。

00:03:39.000 -> 00:03:45.000
XcodeとCore MLは、1行のコードを書く前でも、このタスクを支援できるようになりました。

00:03:45.000 -> 00:03:47.000
Core MLでは、パフォーマンスレポートを作成できるようになりました。

00:03:47.000 -> 00:03:52.000
お見せしましょう。

00:03:52.000 -> 00:03:59.000
YOLOv3オブジェクト検出モデルのXcodeモデルビューアが開きました。

00:03:59.000 -> 00:04:04.000
[予測] タブと [ユーティリティ] タブの間に、[パフォーマンス] タブがあります。

00:04:04.000 -> 00:04:20.000
パフォーマンスレポートを生成するには、左下のプラスアイコンを選択し、実行したいデバイスを選択します。これは私のiPhoneです。[次へ]をクリックし、Core MLを使用するコンピューティングユニットを選択します。

00:04:20.000 -> 00:04:26.000
Core MLが利用可能なすべてのコンピューティングユニットでレイテンシを最適化できるように、すべてにしておくつもりです。

00:04:26.000 -> 00:04:31.000
では、Run Testを押して終了します。

00:04:31.000 -> 00:04:36.000
テストを確実に実行できるように、選択したデバイスのロックが解除されていることを確認してください。

00:04:36.000 -> 00:04:40.000
パフォーマンスレポートの生成中に回転するアイコンが表示されます。

00:04:40.000 -> 00:04:50.000
レポートを作成するには、モデルがデバイスに送信され、モデルで実行されるコンパイル、ロード、および予測のいくつかの反復があります。

00:04:50.000 -> 00:04:55.000
これらが完了すると、パフォーマンスレポートのメトリクスが計算されます。

00:04:55.000 -> 00:05:00.000
今、それは私のiPhoneでモデルを実行し、パフォーマンスレポートを表示します。

00:05:00.000 -> 00:05:09.000
上部には、テストが実行されたデバイスと、どのコンピューティングユニットが選択されたかについての詳細が表示されます。

00:05:09.000 -> 00:05:12.000
次に、実行に関する統計が表示されます。

00:05:12.000 -> 00:05:20.000
予測時間の中央値は22.19ミリ秒で、ロード時間の中央値は約400ミリ秒でした。

00:05:20.000 -> 00:05:28.000
また、デバイス上でモデルをコンパイルする予定の場合、これはコンパイル時間が約940ミリ秒だったことを示しています。

00:05:28.000 -> 00:05:39.000
約22ミリ秒の予測時間は、リアルタイムで実行したい場合、このモデルは毎秒約45フレームをサポートできることを示しています。

00:05:39.000 -> 00:05:45.000
このモデルにはニューラルネットワークが含まれているため、パフォーマンスレポートの下部にレイヤービューが表示されます。

00:05:45.000 -> 00:05:53.000
これは、すべてのレイヤーの名前とタイプ、および各レイヤーが実行された計算ユニットを示します。

00:05:53.000 -> 00:05:59.000
塗りつぶされたチェックマークは、その計算ユニットでレイヤーが実行されたことを意味します。

00:05:59.000 -> 00:06:06.000
記入されていないチェックマークは、レイヤーがそのコンピューティングユニットでサポートされていることを意味しますが、Core MLはそこで実行することを選択しませんでした。

00:06:06.000 -> 00:06:12.000
そして、空のダイヤモンドは、そのコンピューティングユニットでレイヤーがサポートされていないことを意味します。

00:06:12.000 -> 00:06:19.000
この場合、54層がGPUで実行され、32層がNeural Engineで実行されました。

00:06:19.000 -> 00:06:29.000
クリックして、計算ユニットでレイヤーをフィルタリングすることもできます。

00:06:29.000 -> 00:06:35.000
それが、Xcode 14を使用してCore MLモデルのパフォーマンスレポートを生成する方法でした。

00:06:35.000 -> 00:06:45.000
これはiPhoneで実行するために示されましたが、1行のコードを書くことなく、複数のオペレーティングシステムとハードウェアの組み合わせでテストすることができます。

00:06:45.000 -> 00:06:50.000
モデルを選択したので、次のステップは、このモデルをアプリに統合することです。

00:06:50.000 -> 00:06:59.000
これには、モデルをアプリにバンドルし、Core ML APIを使用してモデルをロードし、予測を行うことが含まれます。

00:06:59.000 -> 00:07:08.000
この場合、Core MLスタイル転送モデルを使用して、ライブカメラセッションからフレームでスタイル転送を実行するアプリを構築しました。

00:07:08.000 -> 00:07:15.000
正常に動作しています。しかし、フレームレートは予想よりも遅く、その理由を理解したいと思います。

00:07:15.000 -> 00:07:20.000
これは、Core MLの使用を最適化するためのステップ3に進む場所です。

00:07:20.000 -> 00:07:32.000
パフォーマンスレポートを生成すると、モデルがスタンドアロン環境で達成できるパフォーマンスを示すことができます。ただし、アプリでライブで実行されているモデルのパフォーマンスをプロファイリングする方法も必要です。

00:07:32.000 -> 00:07:38.000
このため、Xcode 14のInstrumentsアプリにあるCore ML Instrumentを使用できるようになりました。

00:07:38.000 -> 00:07:46.000
このインストゥルメントは、アプリでライブで実行されるモデルのパフォーマンスを視覚化し、潜在的なパフォーマンスの問題を特定するのに役立ちます。

00:07:46.000 -> 00:07:50.000
それがどのように使用できるかをお見せしましょう。

00:07:50.000 -> 00:07:56.000
だから私はスタイル転送アプリのワークスペースを開いた状態でXcodeにいて、アプリをプロファイリングする準備ができています。

00:07:56.000 -> 00:08:02.000
実行ボタンを強引にクリックし、プロファイルを選択します。

00:08:02.000 -> 00:08:10.000
これにより、最新バージョンのコードがデバイスにインストールされ、ターゲットのデバイスとアプリが選択された状態でInstrumentsが開きます。

00:08:10.000 -> 00:08:17.000
Core MLの使用状況をプロファイリングしたいので、Core MLテンプレートを選択します。

00:08:17.000 -> 00:08:24.000
このテンプレートには、Core MLインストゥルメントと、Core MLの使用状況をプロファイリングするのに役立つ他のいくつかの便利なインストゥルメントが含まれています。

00:08:24.000 -> 00:08:32.000
トレースをキャプチャするには、レコードを押すだけです。

00:08:32.000 -> 00:08:35.000
そのアプリは今、私のiPhoneで実行されています。

00:08:35.000 -> 00:08:39.000
私はそれを数秒間実行させ、いくつかの異なるスタイルを使用します。

00:08:39.000 -> 00:08:42.000
そして今、私は停止ボタンを押してトレースを終了します。

00:08:42.000 -> 00:08:44.000
今、私は楽器の痕跡を持っています。

00:08:44.000 -> 00:08:48.000
コアMLインストゥルメントに焦点を当てます。

00:08:48.000 -> 00:08:53.000
Core ML Instrumentは、トレースでキャプチャされたすべてのCore MLイベントを表示します。

00:08:53.000 -> 00:09:01.000
最初のビューは、すべてのイベントをアクティビティ、データ、計算の3つのレーンにグループ化します。

00:09:01.000 -> 00:09:14.000
アクティビティレーンは、負荷や予測など、直接呼び出す実際のCore ML APIと1対1の関係を持つトップレベルのCore MLイベントを表示します。

00:09:14.000 -> 00:09:25.000
データレーンは、モデルの入出力を安全に操作できることを確認するために、Core MLがデータチェックまたはデータ変換を実行しているイベントを示します。

00:09:25.000 -> 00:09:33.000
計算レーンは、Core MLがニューラルエンジンやGPUなどの特定の計算ユニットに計算要求を送信するタイミングを示します。

00:09:33.000 -> 00:09:42.000
また、イベントタイプごとに個別のレーンがあるグループ化されていないビューを選択することもできます。

00:09:42.000 -> 00:09:46.000
下部には、モデルアクティビティアグリゲーションビューがあります。

00:09:46.000 -> 00:09:51.000
このビューは、トレースに表示されるすべてのイベントの集計統計情報を提供します。

00:09:51.000 -> 00:10:02.000
たとえば、このトレースでは、平均モデル負荷は17.17ミリ秒かかり、平均予測は7.2ミリ秒かかりました。

00:10:02.000 -> 00:10:05.000
もう1つの注意点は、期間でイベントを並べ替えることができるということです。

00:10:05.000 -> 00:10:19.000
ここでは、リストは、わずか2.69秒の予測と比較して、合計6.41秒の負荷で、実際に予測を行うよりも多くの時間がモデルのロードに費やされていることを教えてくれています。

00:10:19.000 -> 00:10:22.000
おそらく、これは低フレームレートと何か関係があります。

00:10:22.000 -> 00:10:28.000
これらすべての負荷がどこから来ているのか調べてみます。

00:10:28.000 -> 00:10:33.000
各予測を呼び出す前に、Core MLモデルをリロードしていることに気づいています。

00:10:33.000 -> 00:10:38.000
モデルを一度ロードしてメモリに保持できるため、これは一般的に良い習慣ではありません。

00:10:38.000 -> 00:10:47.000
私は自分のコードに戻って、これを修正しようとします。

00:10:47.000 -> 00:10:50.000
モデルをロードするコードの領域を見つけました。

00:10:50.000 -> 00:11:01.000
ここでの問題は、これが適切に計算されていることです。つまり、styleTransferModel変数を参照するたびに、プロパティを再計算します。これは、この場合、モデルをリロードすることを意味します。

00:11:01.000 -> 00:11:14.000
これを遅延変数に変更することで、これをすぐに修正できます。

00:11:14.000 -> 00:11:27.000
次に、アプリを再プロファイアプ化して、繰り返しロードの問題が解決したかどうかを確認します。

00:11:27.000 -> 00:11:34.000
もう一度Core MLテンプレートを選択し、トレースをキャプチャします。

00:11:34.000 -> 00:11:36.000
これは私が期待していることとはるかに一致しています。

00:11:36.000 -> 00:11:49.000
カウント列は、アプリで使用したスタイルの数と一致する合計5つのロードイベントがあり、ロードの合計期間は予測の合計期間よりもはるかに小さいことを示しています。

00:11:49.000 -> 00:11:54.000
また、スクロールしながら...

00:11:54.000 -> 00:12:02.000
...それぞれの間に負荷をかけずに繰り返し予測イベントを正しく表示します。

00:12:02.000 -> 00:12:07.000
もう1つの注意点は、これまでのところ、すべてのCore MLモデルアクティビティを表示するビューしか見ていないということです。

00:12:07.000 -> 00:12:14.000
このアプリでは、スタイルごとに1つのCore MLモデルがあるので、モデルごとにCore MLアクティビティを分解したいと思うかもしれません。

00:12:14.000 -> 00:12:17.000
楽器はこれを簡単にします。

00:12:17.000 -> 00:12:27.000
メイングラフでは、左上の矢印をクリックすると、トレースで使用されるモデルごとに1つのサブトラックが作成されます。

00:12:27.000 -> 00:12:32.000
ここでは、使用されたさまざまなスタイル転送モデルがすべて表示されます。

00:12:32.000 -> 00:12:43.000
集計ビューは、統計をモデル別に分解できるようにすることで、同様の機能も提供します。

00:12:43.000 -> 00:12:50.000
次に、それがどのように実行されているかをよりよく理解するために、私のモデルの1つの予測に飛び込みたいと思います。

00:12:50.000 -> 00:12:54.000
水彩画のモデルをもっと深く見てみます。

00:12:54.000 -> 00:13:03.000
この予測では、コンピュートレーンは、私のモデルがニューラルエンジンとGPUの組み合わせで実行されたことを教えてくれます。

00:13:03.000 -> 00:13:16.000
Core MLはこれらの計算要求を非同期に送信しているので、これらの計算ユニットがいつモデルを積極的に実行しているかを知りたいなら、Core MLインストゥルメントとGPUインストゥルメントと新しいニューラルエンジンインストゥルメントを組み合わせることができます。

00:13:16.000 -> 00:13:23.000
これを行うには、ここに3つの楽器を固定しています。

00:13:23.000 -> 00:13:33.000
コアMLインストゥルメントは、モデルが実行された地域全体を示しています。

00:13:33.000 -> 00:13:47.000
そして、この領域内では、ニューラルエンジンインストゥルメントは、最初にニューラルエンジンで実行されているコンピューティングを示し、次にGPUインストゥルメントは、モデルがニューラルエンジンから引き継がれ、GPUで実行を終了したことを示しています。

00:13:47.000 -> 00:13:54.000
これにより、私のモデルが実際にハードウェアでどのように実行されているかをよりよく知ることができる。

00:13:54.000 -> 00:14:02.000
要約すると、Xcode 14のCore ML Instrumentを使用して、アプリでライブで実行しているときのモデルのパフォーマンスについて学びました。

00:14:02.000 -> 00:14:07.000
次に、モデルを頻繁にリロードしすぎている問題を特定しました。

00:14:07.000 -> 00:14:14.000
コードの問題を修正し、アプリケーションを再プロファイプロファイし、問題が修正されたことを確認しました。

00:14:14.000 -> 00:14:24.000
また、Core ML、GPU、新しいNeural Engine Instrumentを組み合わせて、モデルが実際に異なるコンピューティングユニットでどのように実行されているかについての詳細を得ることができました。

00:14:24.000 -> 00:14:29.000
これは、パフォーマンスを理解するのに役立つ新しいツールの概要でした。

00:14:29.000 -> 00:14:34.000
次に、そのパフォーマンスを最適化するのに役立つ強化されたAPIをいくつか確認します。

00:14:34.000 -> 00:14:39.000
まず、Core MLがモデルの入力と出力をどのように処理するかを見てみましょう。

00:14:39.000 -> 00:14:47.000
Core MLモデルを作成すると、そのモデルには一連の入出力機能があり、それぞれにタイプとサイズがあります。

00:14:47.000 -> 00:14:55.000
実行時に、Core ML APIを使用して、モデルのインターフェイスに準拠した入力を提供し、推論を実行した後に出力を取得します。

00:14:55.000 -> 00:15:00.000
もう少し詳細に画像とマルチアレイに集中させてください。

00:15:00.000 -> 00:15:08.000
画像の場合、Core MLは、コンポーネントあたり8ビットの8ビットグレースケールと32ビットのカラー画像をサポートしています。

00:15:08.000 -> 00:15:16.000
また、多次元配列の場合、Core MLはスカラー型としてInt32、Double、およびFloat32をサポートしています。

00:15:16.000 -> 00:15:21.000
アプリがすでにこれらのタイプで動作している場合は、単にモデルに接続するだけです。

00:15:21.000 -> 00:15:24.000
しかし、あなたのタイプが異なる場合があります。

00:15:24.000 -> 00:15:26.000
例をお見せしましょう。

00:15:26.000 -> 00:15:30.000
画像処理とスタイルアプリに新しいフィルターを追加したいのですが。

00:15:30.000 -> 00:15:35.000
このフィルターは、単一チャンネルの画像を操作することで画像をシャープにするために機能します。

00:15:35.000 -> 00:15:43.000
私のアプリには、GPUでいくつかの前処理と後処理操作があり、この単一のチャネルをFloat16精度で表しています。

00:15:43.000 -> 00:15:51.000
これを行うには、coremltoolsを使用して、ここに示すように、画像シャープニングトーチモデルをCore ML形式に変換しました。

00:15:51.000 -> 00:15:54.000
このモデルは、Float16の精密計算を使用するように設定されました。

00:15:54.000 -> 00:15:59.000
また、画像入力を受け取り、画像出力を生成します。

00:15:59.000 -> 00:16:02.000
私はこのようなモデルを手に入れました。

00:16:02.000 -> 00:16:07.000
Core MLでは8ビットのグレースケール画像が撮影されることに注意してください。

00:16:07.000 -> 00:16:19.000
これを機能させるには、入力をOneComponent16HalfからOneComponent8にダウンキャストし、出力をOneComponent8からOneComponent16Halfにアップキャストするコードを書く必要がありました。

00:16:19.000 -> 00:16:22.000
しかし、これは全体の話ではありません。

00:16:22.000 -> 00:16:33.000
このモデルはFloat16精度で計算を実行するように設定されているため、ある時点で、Core MLはこれらの8ビット入力をFloat16に変換する必要があります。

00:16:33.000 -> 00:16:39.000
変換を効率的に行いますが、アプリを実行しているInstrumentsのトレースを見ると、これが表示されます。

00:16:39.000 -> 00:16:47.000
Core MLがNeural Engineの計算の前後に実行しているデータステップに注目してください。

00:16:47.000 -> 00:16:57.000
データレーンにズームインすると、Core MLがニューラルエンジンでの計算に備えてデータをコピーしていることを示しています。これは、この場合、Float16に変換することを意味します。

00:16:57.000 -> 00:17:02.000
元のデータはすでにFloat16だったので、これは残念なようです。

00:17:02.000 -> 00:17:12.000
理想的には、これらのデータ変換は、モデルをFloat16の入力と出力で直接動作させることで、アプリ内とCore ML内の両方で回避できます。

00:17:12.000 -> 00:17:24.000
iOS 16とmacOS Ventura以降、Core MLは1つのOneComponent16Halfグレースケール画像とFloat16 MultiArraysをネイティブにサポートします。

00:17:24.000 -> 00:17:36.000
coremltools変換メソッドを呼び出しながら、画像の新しいカラーレイアウトまたはMultiArraysの新しいデータタイプを指定することで、Float16の入出力を受け入れるモデルを作成できます。

00:17:36.000 -> 00:17:43.000
この場合、モデルの入力と出力をグレースケールのFloat16画像に指定しています。

00:17:43.000 -> 00:17:56.000
Float16のサポートはiOS 16とmacOS Venturaから利用可能であるため、これらの機能は最小展開ターゲットがiOS 16として指定されている場合にのみ利用できます。

00:17:56.000 -> 00:17:59.000
これがモデルの再変換されたバージョンの外観です。

00:17:59.000 -> 00:18:04.000
入力と出力はGrayscale16Halfとしてマークされていることに注意してください。

00:18:04.000 -> 00:18:16.000
このFloat16のサポートにより、私のアプリはFloat16画像をCore MLに直接フィードできるため、入力をダウンキャストしたり、アプリ内の出力をアップキャストしたりする必要がなくなります。

00:18:16.000 -> 00:18:18.000
これがコードでどのように見えるかです。

00:18:18.000 -> 00:18:27.000
OneComponent16Half CVPixelBufferの形式で入力データを持っているので、ピクセルバッファをCore MLに直接送信するだけです。

00:18:27.000 -> 00:18:31.000
これにより、データのコピーや変換は発生しません。

00:18:31.000 -> 00:18:36.000
次に、出力としてOneComponent16Half CVPixelBufferを取得します。

00:18:36.000 -> 00:18:42.000
これにより、コードが簡素化され、データ変換が不要になります。

00:18:42.000 -> 00:18:53.000
あなたができるもう一つのクールなことは、Core MLに各予測に新しいバッファを割り当てるのではなく、出力のために事前に割り当てられたバッファを埋めるようにCore MLに依頼することです。

00:18:53.000 -> 00:18:59.000
出力バッキングバッファを割り当て、予測オプションに設定することで、これを行うことができます。

00:18:59.000 -> 00:19:06.000
私のアプリでは、OneComponent1 HalfCVPixelBufferを返すoutputBackingBufferという関数を書きました。

00:19:06.000 -> 00:19:14.000
次に、予測オプションでこれを設定し、最後にそれらの予測オプションを使用してモデルの予測メソッドを呼び出します。

00:19:14.000 -> 00:19:21.000
出力バッキングを指定することで、モデル出力のバッファ管理をより適切に制御できます。

00:19:21.000 -> 00:19:31.000
したがって、これらの変更を要約すると、8ビットの入力と出力を持つモデルのオリジナルバージョンを使用するときに、Instrumentsトレースに表示されたものは次のとおりです。

00:19:31.000 -> 00:19:42.000
そして、新しいFloat16バージョンのモデルにIOSurfaceバックアップのFloat16バッファを提供するためにコードを変更した後、最終的なInstrumentsトレースがどのように見えるかは次のとおりです。

00:19:42.000 -> 00:19:49.000
Core MLがそれらを実行する必要がなくなったため、以前にデータレーンに表示されていたデータ変換はなくなりました。

00:19:49.000 -> 00:19:55.000
要約すると、Core MLは現在、Float16データをエンドツーエンドのネイティブサポートを持っています。

00:19:55.000 -> 00:20:03.000
これは、Float16入力をCore MLに提供し、Core MLにFloat16出力を返すことができることを意味します。

00:20:03.000 -> 00:20:11.000
また、新しい出力バッキングAPIを使用して、新しい出力バッファを作成する代わりに、Core MLが事前に割り当てられた出力バッファを埋めることもできます。

00:20:11.000 -> 00:20:25.000
そして最後に、Core MLはユニファイドメモリを利用してデータコピーなしで異なるコンピューティングユニット間でデータを転送できるため、可能な限りIOSurfaceバックアップバッファを使用することをお勧めします。

00:20:25.000 -> 00:20:31.000
次に、Core MLに追加される追加機能のいくつかの簡単なツアーを見ていきます。

00:20:31.000 -> 00:20:33.000
まずは重量圧縮です。

00:20:33.000 -> 00:20:39.000
モデルの重みを圧縮すると、モデルを小さくしながら同様の精度を達成できる場合があります。

00:20:39.000 -> 00:20:48.000
iOS 12では、Core MLは、Core MLニューラルネットワークモデルのサイズを縮小できるトレーニング後の重量圧縮を導入しました。

00:20:48.000 -> 00:20:59.000
現在、16ビットと8ビットのサポートをMLプログラムモデルタイプに拡張し、さらに、まばらな表現で重みを保存する新しいオプションを導入しています。

00:20:59.000 -> 00:21:09.000
coremltoolsユーティリティを使用すると、MLプログラムモデルの重みを量子化、パレット化、スパース化できるようになります。

00:21:09.000 -> 00:21:12.000
次は新しいコンピューティングユニットオプションです。

00:21:12.000 -> 00:21:18.000
Core MLは常に、特定のコンピューティングユニットの好みに対する推論レイテンシを最小限に抑えることを目指しています。

00:21:18.000 -> 00:21:24.000
アプリは、MLModelConfiguration computeUnitsプロパティを設定することで、この設定を指定できます。

00:21:24.000 -> 00:21:31.000
既存の3つのコンピューティングユニットオプションに加えて、cpuAndNeuralEngineと呼ばれる新しいものがあります。

00:21:31.000 -> 00:21:44.000
これは、GPUで計算をディスパッチしないようにCore MLに指示します。これは、アプリが他の計算にGPUを使用する場合に役立ち、したがって、Core MLがCPUとニューラルエンジンにフォーカスを制限することを好みます。

00:21:44.000 -> 00:21:54.000
次に、モデルのシリアル化に関して追加の柔軟性を提供するCore MLモデルインスタンスを初期化する新しい方法を追加します。

00:21:54.000 -> 00:22:01.000
これにより、カスタム暗号化スキームでモデルデータを暗号化し、読み込み直前に復号化することができます。

00:22:01.000 -> 00:22:11.000
これらの新しいAPIを使用すると、コンパイルされたモデルをディスクに必要とせずに、インメモリCore MLモデル仕様をコンパイルしてロードできます。

00:22:11.000 -> 00:22:16.000
最後の更新は、Swiftパッケージと、それらがCore MLとどのように連携するかについてです。

00:22:16.000 -> 00:22:20.000
パッケージは、再利用可能なコードをバンドルして配布するのに最適な方法です。

00:22:20.000 -> 00:22:28.000
Xcode 14を使用すると、SwiftパッケージにCore MLモデルを含めることができ、誰かがパッケージをインポートすると、モデルが機能します。

00:22:28.000 -> 00:22:36.000
Xcodeは、Core MLモデルを自動的にコンパイルしてバンドルし、作業に慣れているのと同じコード生成インターフェイスを作成します。

00:22:36.000 -> 00:22:43.000
Swiftエコシステムでモデルを配布することがはるかに簡単になるため、この変更に興奮しています。

00:22:43.000 -> 00:22:46.000
これで、このセッションは終わりです。

00:22:46.000 -> 00:22:56.000
Xcode 14のCore MLパフォーマンスレポートとInstrumentは、アプリのML搭載機能のパフォーマンスを分析して最適化するのに役立ちます。

00:22:56.000 -> 00:23:03.000
新しいFloat16サポートと出力バッキングAPIを使用すると、データがCore MLに出入りする方法をより詳細に制御できます。

00:23:03.000 -> 00:23:09.000
重量圧縮の拡張サポートは、モデルのサイズを最小限に抑えるのに役立ちます。

00:23:09.000 -> 00:23:18.000
また、インメモリモデルとSwiftパッケージのサポートにより、Core MLモデルの表現、統合、共有方法に関しては、さらに多くのオプションがあります。

00:23:18.000 -> 00:23:22.000
これはコアMLチームのベンで、WWDCの素晴らしい残りを持っています。

00:23:22.000 -> 23:59:59.000
♪

