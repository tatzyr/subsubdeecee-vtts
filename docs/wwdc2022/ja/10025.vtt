WEBVTT

00:00:00.000 --> 00:00:13.000
ロン・サントス:ねえ、あなたが元気であることを願っています。入力エンジニアのロン・サントスです。

00:00:13.000 --> 00:00:21.000
今日は、ビデオフィードから機械で読み取り可能なコードとテキストをキャプチャすること、または私たちが呼びたいように、データスキャンについてお話しします。

00:00:21.000 --> 00:00:24.000
データスキャンとは正確にはどういう意味ですか?

00:00:24.000 --> 00:00:29.000
これは単に、カメラのようなセンサーを使用してデータを読み取る方法です。

00:00:29.000 --> 00:00:32.000
通常、そのデータはテキストの形で提供されます。

00:00:32.000 --> 00:00:39.000
例えば、電話番号、日付、価格などの興味深い情報を含む領収書。

00:00:39.000 --> 00:00:45.000
あるいは、データは、ユビキタスQRコードのように、機械で読み取り可能なコードとして来るかもしれません。

00:00:45.000 --> 00:00:53.000
おそらく、カメラアプリで、またはiOS 15で導入されたライブテキスト機能を使用して、以前にデータスキャナを使用したことがあるでしょう。

00:00:53.000 --> 00:00:59.000
そして、あなたは自分のカスタムスキャン体験で日常生活の中でアプリを使ったことがあるに違いない。

00:00:59.000 --> 00:01:02.000
しかし、独自のデータスキャナを構築しなければならない場合はどうなりますか?

00:01:02.000 --> 00:01:04.000
どうやってやるの？

00:01:04.000 --> 00:01:09.000
iOS SDKには、ニーズに応じて複数のソリューションがあります。

00:01:09.000 --> 00:01:22.000
1つのオプションは、AVFoundationフレームワークを使用してカメラグラフを設定し、入力と出力をセッションに接続し、機械読み取り可能なコードのようなAVMetadataObjectsを生成するように設定できることです。

00:01:22.000 --> 00:01:30.000
テキストをキャプチャしたい場合、もう1つのオプションは、AVFoundationとVisionフレームワークの両方を組み合わせることです。

00:01:30.000 --> 00:01:35.000
この図では、メタデータ出力の代わりに、ビデオデータ出力を作成します。

00:01:35.000 --> 00:01:47.000
ビデオデータ出力は、テキストおよびバーコード認識要求で使用するためにビジョンフレームワークに供給できるサンプルバッファの配信をもたらし、その結果、ビジョン観測オブジェクトが得られます。

00:01:47.000 --> 00:01:54.000
データスキャンにVisionを使用する方法の詳細については、WWDC21の「Visionを使用してドキュメントデータを抽出する」をご覧ください。

00:01:54.000 --> 00:01:58.000
さて、それはデータスキャンにAVFoundationとVisionを使用しています。

00:01:58.000 --> 00:02:03.000
iOS 16では、そのすべてをカプセル化する新しいオプションがあります。

00:02:03.000 --> 00:02:07.000
VisionKitフレームワークのDataScannerViewControllerを紹介します。

00:02:07.000 --> 00:02:13.000
データスキャンの目的で、AVFoundationとVisionの機能を組み合わせています。

00:02:13.000 --> 00:02:34.000
DataScannerViewControllerユーザーは、ライブカメラのプレビュー、便利なガイダンスラベル、アイテムの強調表示、選択にも使用されるタップツーフォーカス、そして最後にピンチツーズームなどの機能に扱われます。

00:02:34.000 --> 00:02:37.000
そして、あなたのような開発者のための機能について話しましょう。

00:02:37.000 --> 00:02:42.000
DataScannerViewControllerは、選択した方法で提示できるUIViewControllerサブクラスです。

00:02:42.000 --> 00:02:51.000
認識されたアイテムの座標は常にビュー座標にあり、画像空間からビジョン座標に変換して座標を表示するのを省くことができます。

00:02:51.000 --> 00:02:59.000
また、ビュー座標にある関心のある領域を指定することで、ビューのアクティブな部分を制限することもできます。

00:02:59.000 --> 00:03:04.000
テキスト認識では、コンテンツタイプを指定して、検索するテキストの種類を制限できます。

00:03:04.000 --> 00:03:10.000
また、機械で読み取り可能なコードの場合は、どのシンボルを探すかを正確に指定できます。

00:03:10.000 --> 00:03:15.000
わかりました。私はあなたのアプリを使用しており、データスキャンはその機能のほんの一部に過ぎないことを理解しています。

00:03:15.000 --> 00:03:18.000
しかし、それは多くのコードを必要とするかもしれません。

00:03:18.000 --> 00:03:24.000
DataScannerViewControllerを使用すると、私たちの目標は、あなたが他の場所に時間を集中できるように、あなたのために共通のタスクを実行することです。

00:03:24.000 --> 00:03:28.000
次に、アプリに追加する方法を説明します。 アプリに追加します。

00:03:28.000 --> 00:03:31.000
プライバシーの使用状況の説明から始めましょう。

00:03:31.000 --> 00:03:38.000
アプリがビデオをキャプチャしようとすると、iOSはユーザーにカメラにアクセスする明示的な許可を与えるよう求めます。

00:03:38.000 --> 00:03:42.000
あなたのニーズを正当化する説明的なメッセージを提供したいと思うでしょう。

00:03:42.000 --> 00:03:48.000
これを行うには、アプリのInfo.plistファイルに「プライバシー - カメラの使用状況の説明」を追加します。

00:03:48.000 --> 00:03:53.000
ユーザーが何に同意しているかがわかるように、できるだけ説明的にしてください。

00:03:53.000 --> 00:03:55.000
さあ、コードに。

00:03:55.000 --> 00:04:01.000
データスキャナを提示したいところはどこでも、まずVisionKitをインポートしてください。

00:04:01.000 --> 00:04:14.000
次に、データスキャンはすべてのデバイスでサポートされていないため、isSupportedクラスプロパティを使用して、機能を公開するボタンやメニューを非表示にすると、ユーザーに使用できないものが表示されません。

00:04:14.000 --> 00:04:22.000
興味があれば、Apple Neural Engineを搭載した2018年以降のiPhoneおよびiPadデバイスは、データスキャンをサポートしています。

00:04:22.000 --> 00:04:24.000
また、空き状況も確認する必要があります。

00:04:24.000 --> 00:04:27.000
プライバシーの使用に関する説明を思い出しますか?

00:04:27.000 --> 00:04:41.000
スキャンは、ユーザーがカメラアクセスのためにアプリを承認し、デバイスがスクリーンタイムのコンテンツとプライバシー制限でここで設定されたカメラアクセス制限のような制限を受けていない場合に利用できます。

00:04:41.000 --> 00:04:43.000
これで、インスタンスを設定する準備が整いました。

00:04:43.000 --> 00:04:47.000
これは、最初に関心のあるデータの種類を指定することによって行われます。

00:04:47.000 --> 00:04:52.000
たとえば、QRコードとテキストの両方をスキャンできます。

00:04:52.000 --> 00:05:01.000
オプションで、テキストリコグナイザが言語修正などのさまざまな処理面のヒントとして使用する言語のリストを渡すことができます。

00:05:01.000 --> 00:05:04.000
どの言語が予想されるか考えがあるなら、それらをリストアップしてください。

00:05:04.000 --> 00:05:08.000
2つの言語に似た見た目のスクリプトがある場合に特に便利です。

00:05:08.000 --> 00:05:13.000
言語を提供しない場合は、デフォルトでユーザーの優先言語が使用されます。

00:05:13.000 --> 00:05:17.000
特定のテキストコンテンツタイプをリクエストすることもできます。

00:05:17.000 --> 00:05:20.000
この例では、スキャナにURLを探してもらいたい。

00:05:20.000 --> 00:05:26.000
認識するデータの種類を述べたので、DataScannerインスタンスを作成できます。

00:05:26.000 --> 00:05:33.000
前の例では、バーコードシンボル、認識言語、およびテキストコンテンツタイプを指定しました。

00:05:33.000 --> 00:05:37.000
それぞれの他のオプションについて少し説明させてください。

00:05:37.000 --> 00:05:43.000
バーコードシンボルについては、ビジョンのバーコード検出器と同じシンボルをすべてサポートしています。

00:05:43.000 --> 00:05:50.000
言語の面では、LiveText機能の一部として、DataScannerViewControllerはまったく同じ言語をサポートしています。

00:05:50.000 --> 00:05:55.000
そして、iOS 16では、日本語と韓国語のサポートを追加していることを嬉しく思います。

00:05:55.000 --> 00:05:58.000
もちろん、これは将来再び変わる可能性があります。

00:05:58.000 --> 00:06:04.000
したがって、supportedTextRecognitionLanguagesクラスプロパティを使用して、最新のリストを取得します。

00:06:04.000 --> 00:06:11.000
最後に、特定の意味的な意味を持つテキストをスキャンするとき、DataScannerViewControllerはこれらの7つのタイプを見つけることができます。

00:06:11.000 --> 00:06:14.000
これで、データスキャナをユーザーに提示する準備が整いました。

00:06:14.000 --> 00:06:22.000
他のビューコントローラーと同じように提示したり、フルスクリーンにしたり、シートを使用したり、別のビュー階層に完全に追加したりします。

00:06:22.000 --> 00:06:24.000
それはすべてあなた次第です。

00:06:24.000 --> 00:06:29.000
その後、プレゼンテーションが完了したら、startScanning()を呼び出してデータの探しを開始します。

00:06:29.000 --> 00:06:35.000
だから今、私は一歩下がって、データスキャナの初期化パラメータに時間を費やしたいと思います。

00:06:35.000 --> 00:06:38.000
私はここで1つ、認識されたDataTypesを使用しました。

00:06:38.000 --> 00:06:43.000
しかし、あなたの経験をカスタマイズするのに役立つ他のものがあります。

00:06:43.000 --> 00:06:44.000
それぞれに目を通しましょう。

00:06:44.000 --> 00:06:49.000
recognizedDataTypesを使用すると、認識するデータの種類を指定できます。

00:06:49.000 --> 00:06:52.000
テキスト、機械で読み取り可能なコード、およびそれぞれのタイプ。

00:06:52.000 --> 00:06:56.000
qualityLevelは、バランス、高速、または正確です。

00:06:56.000 --> 00:07:04.000
Fastは、看板のテキストなど、大きくて読みやすいアイテムを期待するシナリオでは、スピードを優先して解像度を犠牲にします。

00:07:04.000 --> 00:07:11.000
精度は、マイクロQRコードや小さなシリアル番号などの小さなアイテムでも、最高の精度を提供します。

00:07:11.000 --> 00:07:15.000
バランスの取れたものから始めることをお勧めします。これはほとんどのケースでうまくいくはずです。

00:07:15.000 --> 00:07:23.000
recognizesMultipleItemsは、一度に複数のバーコードをスキャンする場合など、フレーム内の1つ以上のアイテムを探すオプションを提供します。

00:07:23.000 --> 00:07:29.000
Falseの場合、ユーザーが他の場所をタップするまで、最中央の項目はデフォルトで認識されます。

00:07:29.000 --> 00:07:33.000
ハイライトを描くときに高フレームレートトラッキングを有効にします。

00:07:33.000 --> 00:07:39.000
カメラが動いたり、シーンが変わったりすると、ハイライトができるだけ密接にアイテムを追うことができます。

00:07:39.000 --> 00:07:43.000
ピンチツーズームを有効にするか、無効にします。

00:07:43.000 --> 00:07:47.000
また、ズームレベルを自分で変更する方法もあります。

00:07:47.000 --> 00:07:52.000
ガイダンスを有効にすると、ラベルが画面の上部に表示され、ユーザーを指示するのに役立ちます。

00:07:52.000 --> 00:08:00.000
そして最後に、必要に応じてシステムハイライトを有効にすることも、それを無効にして独自のカスタムハイライトを描画することもできます。

00:08:00.000 --> 00:08:08.000
データスキャナーの提示方法がわかったので、認識されたアイテムをどのように取り込むか、また、独自のカスタムハイライトをどのように描画するかについて話しましょう。

00:08:08.000 --> 00:08:12.000
まず、データスキャナにデリゲートを提供します。

00:08:12.000 --> 00:08:20.000
デリゲートができたら、ユーザーがアイテムをタップしたときに呼び出されるdataScanner didTapOnメソッドを実装できます。

00:08:20.000 --> 00:08:24.000
これにより、この新しいタイプのRecognizeItemのインスタンスを受け取ります。

00:08:24.000 --> 00:08:29.000
RecognizedItemは、テキストまたはバーコードを関連する値として保持する列挙型です。

00:08:29.000 --> 00:08:33.000
テキストの場合、転写プロパティは認識された文字列を保持します。

00:08:33.000 --> 00:08:39.000
バーコードの場合、ペイロードに文字列が含まれている場合は、payloadStringValueで取得できます。

00:08:39.000 --> 00:08:48.000
RecognizedItemについて知っておくべき他の2つのこと：まず、認識された各アイテムには、生涯を通じてアイテムを追跡するために使用できる一意の識別子があります。

00:08:48.000 --> 00:08:54.000
その寿命は、アイテムが最初に表示されたときに始まり、表示されなくなったときに終了します。

00:08:54.000 --> 00:08:57.000
そして第二に、各RecognizedItemにはboundsプロパティがあります。

00:08:57.000 --> 00:09:01.000
境界は直線ではありませんが、各コーナーに1つずつ4つのポイントで構成されています。

00:09:01.000 --> 00:09:07.000
次に、シーンの変更で認識されたアイテムが呼び出される3つの関連するデリゲートメソッドについて話しましょう。

00:09:07.000 --> 00:09:12.000
1つ目はdidAddで、シーン内のアイテムが新たに認識されたときに呼び出されます。

00:09:12.000 --> 00:09:18.000
独自のカスタムハイライトを作成したい場合は、新しいアイテムごとにここに1つ作成します。

00:09:18.000 --> 00:09:23.000
関連するアイテムのIDを使用して、ハイライトを追跡できます。

00:09:23.000 --> 00:09:35.000
また、新しいビューをビュー階層に追加するときは、DataScannerのoverlayContainerViewに追加して、カメラプレビューの上に表示されますが、他の補足クロムの下に表示されます。

00:09:35.000 --> 00:09:40.000
次のデリゲートメソッドはdidUpdateで、アイテムが移動したり、カメラが移動したりするときに呼び出されます。

00:09:40.000 --> 00:09:44.000
また、認識されたテキストの転写が変更されたときに呼び出すこともできます。

00:09:44.000 --> 00:09:50.000
スキャナーがテキストを見る時間が長ければ長いほど、転写がより正確になるため、それらは変わります。

00:09:50.000 --> 00:10:00.000
更新されたアイテムのIDを使用して、作成したばかりの辞書からハイライトを取得し、ビューを新しく更新された境界にアニメーション化します。

00:10:00.000 --> 00:10:07.000
そして最後に、didRemoveデリゲートメソッドは、アイテムがシーンに表示されなくなったときに呼び出されます。

00:10:07.000 --> 00:10:15.000
この方法では、削除されたアイテムに関連付けられたハイライトビューを忘れることができ、ビュー階層から削除できます。

00:10:15.000 --> 00:10:26.000
要約すると、アイテムの上に独自のハイライトを描く場合、これらの3つのデリゲートメソッドは、シーンにハイライトをアニメーション化し、その動きをアニメーション化し、それらをアニメーション化するために非常に重要です。

00:10:26.000 --> 00:10:33.000
また、以前の3つのデリゲートメソッドのそれぞれについて、現在認識されているすべてのアイテムの配列も与えられます。

00:10:33.000 --> 00:10:45.000
これは、アイテムが自然な読み取り順序で配置されるため、テキスト認識に役立つ場合があります。つまり、ユーザーはインデックス1のアイテムの前にインデックス0でアイテムを読みます。

00:10:45.000 --> 00:10:48.000
これは、DataScannerViewControllerの使用方法の概要です。

00:10:48.000 --> 00:10:55.000
締めくくる前に、写真を撮影するなど、他のいくつかの機能について簡単に言及したかった。

00:10:55.000 --> 00:11:02.000
高品質のUIImageを非同期に返すcapturePhotoメソッドを呼び出すことができます。

00:11:02.000 --> 00:11:07.000
また、カスタムハイライトを作成していない場合は、これら3つのデリゲートメソッドを必要としないかもしれません。

00:11:07.000 --> 00:11:10.000
代わりに、cognizemcogniztItemプロパティを使用できます。

00:11:10.000 --> 00:11:17.000
これは、シーンが変化するにつれて継続的に更新されるAsyncStreamです。

00:11:17.000 --> 00:11:19.000
ぶらぶらしてくれてありがとう。

00:11:19.000 --> 00:11:26.000
iOS SDKは、AVFoundationとVisionフレームワークを使用してコンピュータビジョンワークフローを作成するためのオプションを提供することを忘れないでください。

00:11:26.000 --> 00:11:36.000
しかし、ピックアンドパックアプリ、バックオブザウェアハウスアプリ、POSアプリなど、ライブビデオフィードでテキストや機械で読み取り可能なコードをスキャンするアプリを作成しているのかもしれません。

00:11:36.000 --> 00:11:40.000
もしそうなら、VisionKitのDataScannerViewControllerを見てみましょう。

00:11:40.000 --> 00:11:50.000
今日調べたように、アプリのスタイルとニーズに合ったカスタムエクスペリエンスを提供するために使用できる多くの初期化パラメータとデリゲートメソッドがあります。

00:11:50.000 --> 00:12:01.000
そして最後に、静的画像のVisionKitのライブテキスト機能について学ぶことができる「アプリにライブテキストインタラクションを追加する」セッションに叫びたいと思いました。

00:12:01.000 --> 00:12:03.000
次回まで、平和。

00:12:03.000 --> 23:59:59.000
。

