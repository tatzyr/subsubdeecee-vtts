WEBVTT

00:00:00.000 -> 00:00:03.000
♪インストゥルメンタルヒップホップ音楽♪

00:00:03.000 -> 00:00:09.000
♪

00:00:09.000 -> 00:00:14.000
こんにちは、私の名前はDavid Findlayで、Create MLチームのエンジニアです。

00:00:14.000 -> 00:00:20.000
このセッションでは、独自の機械学習タスクを構築するための強力な新しい方法であるCreate ML Componentsについてです。

00:00:20.000 -> 00:00:25.000
同僚のアレハンドロは、「Create ML Componentsを知ろう」というセッションで紹介しました。

00:00:25.000 -> 00:00:31.000
彼は、Create MLタスクをコンポーネントに分解し、カスタムモデルを構築するのがいかに簡単かを明らかにしました。

00:00:31.000 -> 00:00:40.000
トランスフォーマーとエスティメータは、画像回帰などのカスタムモデルを構築するために一緒に構成できる主な構成要素です。

00:00:40.000 -> 00:00:46.000
このセッションでは、基本をはるかに超えて、Create ML Componentsで何が可能かを実証したいと思います。

00:00:46.000 -> 00:00:49.000
議題を確認しましょう。カバーすべきことがたくさんあります。

00:00:49.000 -> 00:00:57.000
まず、ビデオデータについて話し、時間の経過とともに値を処理するように設計された新しいコンポーネントについて詳しく説明します。

00:00:57.000 -> 00:01:03.000
次に、これらの概念を機能させ、変圧器のみを使用して人間の行動の繰り返しカウンターを構築します。

00:01:03.000 -> 00:01:07.000
最後に、カスタムサウンド分類モデルのトレーニングに進みます。

00:01:07.000 -> 00:01:13.000
バッチでモデルを更新し、トレーニングを早期に停止し、モデルをチェックポイントできる増分フィッティングについて説明します。

00:01:13.000 -> 00:01:17.000
このレベルの柔軟性には非常に多くの機会があります。

00:01:17.000 -> 00:01:19.000
飛び込むのが待ちきれません。

00:01:19.000 -> 00:01:21.000
始めましょう。

00:01:21.000 -> 00:01:28.000
WWDC 2020では、Create MLでアクション分類を導入しました。これにより、ビデオからアクションを分類できます。

00:01:28.000 -> 00:01:37.000
そして、ジャンピングジャック、ランジ、スクワットなどの人のトレーニングルーチンを認識するためのフィットネス分類器を作成する方法を実証しました。

00:01:37.000 -> 00:01:44.000
たとえば、アクション分類器を使用して、このビデオのアクションをジャンピングジャックとして認識できます。

00:01:44.000 -> 00:01:47.000
しかし、ジャンピングジャックを数えたい場合はどうなりますか?

00:01:47.000 -> 00:01:55.000
最初に考慮する必要があるのは、ジャンピングジャックが連続したフレームにまたがることであり、時間の経過とともに値を処理する方法が必要になります。

00:01:55.000 -> 00:01:59.000
ありがたいことに、SwiftのAsyncSequenceはこれを本当に簡単にします。

00:01:59.000 -> 00:02:06.000
非同期シーケンスに慣れていない場合は、セッション「Meet AsyncSequence」をチェックする必要があります。

00:02:06.000 -> 00:02:13.000
Create ML Componentsを使用すると、ビデオリーダーを使用して、ビデオをフレームの非同期シーケンスとして読み取ることができます。

00:02:13.000 -> 00:02:19.000
また、AsyncSequenceは、ビデオから受信したフレームを反復する方法を提供します。

00:02:19.000 -> 00:02:26.000
たとえば、マップメソッドを使用して、各ビデオフレームを非同期に簡単に変換できます。

00:02:26.000 -> 00:02:30.000
これは、フレームを1つずつ処理したい場合に便利です。

00:02:30.000 -> 00:02:34.000
しかし、一度に複数のフレームを処理したい場合はどうなりますか?

00:02:34.000 -> 00:02:36.000
そこで時間的変圧器の出番です。

00:02:36.000 -> 00:02:42.000
たとえば、ビデオのアクションを高速化するためにフレームをダウンサンプリングしたい場合があります。

00:02:42.000 -> 00:02:48.000
非同期シーケンスを取り、ダウンサンプリングされた非同期シーケンスを返すものに対してダウンサンプラーを使用できます。

00:02:48.000 -> 00:02:54.000
または、アクションの繰り返しを数えるために重要なウィンドウにフレームをグループ化することもできます。

00:02:54.000 -> 00:02:58.000
そこでスライディングウィンドウトランスを使用できます。

00:02:58.000 -> 00:03:06.000
ウィンドウの長さを指定できます。これは、ウィンドウでグループ化するフレーム数であり、ストライドはスライディング間隔を制御する方法です。

00:03:06.000 -> 00:03:15.000
入力は、再び、非同期シーケンスであり、この場合の出力はウィンドウ化された非同期シーケンスです。

00:03:15.000 -> 00:03:22.000
一般的に言えば、時間トランスフォーマーは、非同期シーケンスを新しい非同期シーケンスに処理する方法を提供します。

00:03:22.000 -> 00:03:25.000
では、これらの概念を機能させましょう。

00:03:25.000 -> 00:03:30.000
あなたのことは知りませんが、運動しているとき、私はいつも担当者の数を失います。

00:03:30.000 -> 00:03:36.000
だから私は物事を少し揺るがして、Create ML Componentsでアクション繰り返しカウンターを構築することにしました。

00:03:36.000 -> 00:03:42.000
この例では、変圧器と時間変圧器を一緒に構成する方法について説明します。

00:03:42.000 -> 00:03:45.000
ポーズ抽出から始めましょう。

00:03:45.000 -> 00:03:49.000
人体のポーズ抽出器を使ってポーズを抽出できます。

00:03:49.000 -> 00:03:54.000
入力は画像であり、出力は人体のポーズの配列です。

00:03:54.000 -> 00:03:59.000
舞台裏では、ビジョンフレームワークを活用してポーズを抽出します。

00:03:59.000 -> 00:04:05.000
画像には複数の人を含めることができることに注意してください。これはグループワークアウトで一般的です。

00:04:05.000 -> 00:04:08.000
そのため、出力はポーズの配列です。

00:04:08.000 -> 00:04:13.000
しかし、私は一度に1人のアクションの繰り返しを数えることにしか興味がありません。

00:04:13.000 -> 00:04:19.000
だから私はポーズセレクターで人体のポーズ抽出器を構成します。

00:04:19.000 -> 00:04:26.000
ポーズセレクターは、一連のポーズと選択戦略を取り、単一のポーズを返します。

00:04:26.000 -> 00:04:33.000
選択できる選択戦略はいくつかありますが、この例では、rightMostJointLocation戦略を使用します。

00:04:33.000 -> 00:04:38.000
次のステップは、ポーズをウィンドウにグループ化することです。

00:04:38.000 -> 00:04:42.000
そのためにスライディングウィンドウトランスを追加します。

00:04:42.000 -> 00:04:49.000
そして、私は90のポーズの重複しないウィンドウを生成する90のウィンドウの長さとストライドを使用します。

00:04:49.000 -> 00:05:00.000
スライディングウィンドウトランスフォーマーは一時的であり、タスク全体が一時的になり、期待される入力はフレームの非同期シーケンスになったことを思い出してください。

00:05:00.000 -> 00:05:05.000
最後に、人体アクションカウンターを追加します。

00:05:05.000 -> 00:05:14.000
この時間的トランスフォーマーは、ウィンドウ化されたポーズの非同期シーケンスを消費し、これまでのアクションの繰り返しの累積カウントを返します。

00:05:14.000 -> 00:05:18.000
今では、カウントが浮動小数点数であることに気づいたかもしれません。

00:05:18.000 -> 00:05:21.000
それは、タスクが部分的なアクションもカウントされるからです。

00:05:21.000 -> 00:05:23.000
それはとても簡単です。

00:05:23.000 -> 00:05:27.000
今、私は私のトレーニングビデオで私の担当者を数え、私が浮気していないことを確認することができます。

00:05:27.000 -> 00:05:35.000
しかし、現在のトレーニングを追跡できるように、アプリで繰り返しをライブで数えるのがさらに良いでしょう。

00:05:35.000 -> 00:05:38.000
あなたがそれをする方法をお見せしましょう。

00:05:38.000 -> 00:05:45.000
まず、カメラ構成を取り、カメラフレームの非同期シーケンスを返すreadCameraメソッドを使用します。

00:05:45.000 -> 00:05:52.000
次に、より頻繁に更新されたカウントを取得できるように、ストライドパラメータを15フレームに調整します。

00:05:52.000 -> 00:05:58.000
私のカメラが毎秒30フレームの速度でフレームをキャプチャすると、半秒ごとにカウントされます。

00:05:58.000 -> 00:06:03.000
今、私はトレーニングすることができ、担当者を逃すことを心配する必要はありません。

00:06:03.000 -> 00:06:08.000
これまでのところ、私は非同期シーケンスを変換するための時間的コンポーネントを調査しました。

00:06:08.000 -> 00:06:14.000
次に、時間データに依存するカスタムモデルのトレーニングに焦点を当てたいと思います。

00:06:14.000 -> 00:06:19.000
2019年には、Create MLでサウンド分類器をトレーニングする方法を実演しました。

00:06:19.000 -> 00:06:24.000
そして2021年には、健全な分類の強化を導入しました。

00:06:24.000 -> 00:06:30.000
私はさらに進んで、カスタムサウンド分類器を段階的に訓練したい。

00:06:30.000 -> 00:06:36.000
Create MLフレームワークのMLSoundClassifierは、カスタムサウンド分類器モデルをトレーニングする最も簡単な方法です。

00:06:36.000 -> 00:06:42.000
しかし、より多くのカスタマイズ性と制御が必要な場合は、ボンネットの下のコンポーネントを使用できます。

00:06:42.000 -> 00:06:51.000
最も単純な形式では、サウンド分類器には、オーディオ機能印刷機能抽出器と選択した分類器の2つのコンポーネントがあります。

00:06:51.000 -> 00:06:59.000
AudioFeaturePrintは、オーディオバッファの非同期シーケンスからオーディオ機能を抽出する一時的なトランスフォーマーです。

00:06:59.000 -> 00:07:07.000
スライディングウィンドウトランスフォーマーと同様に、AudioFeaturePrintは非同期シーケンスをウィンドウし、機能を抽出します。

00:07:07.000 -> 00:07:20.000
選択できる分類器はいくつかありますが、この例では、ロジスティック回帰分類器を使用して、機能抽出器と一緒に構成して、カスタムサウンド分類器を構築します。

00:07:20.000 -> 00:07:25.000
次のステップは、カスタムサウンド分類器をラベル付けされたトレーニングデータに合わせることです。

00:07:25.000 -> 00:07:32.000
トレーニングデータの収集の詳細については、「MLコンポーネントの作成を知る」セッションは始めるのに良い場所です。

00:07:32.000 -> 00:07:34.000
これまでのところ、私は幸せな道をカバーしてきました。

00:07:34.000 -> 00:07:39.000
しかし、機械学習モデルの構築は反復的なプロセスになる可能性があります。

00:07:39.000 -> 00:07:46.000
たとえば、時間の経過とともに新しいトレーニングデータを発見して収集し、モデルを更新したいと思うかもしれません。

00:07:46.000 -> 00:07:49.000
モデルの品質を向上させることができる可能性があります。 

00:07:49.000 -> 00:07:53.000
しかし、モデルをゼロから再訓練するのは時間がかかります。

00:07:53.000 -> 00:07:58.000
これは、以前のすべてのデータに対して機能抽出をやり直す必要があるためです。

00:07:58.000 -> 00:08:04.000
新しく発見されたデータでモデルをトレーニングするときに時間を節約する方法の例を挙げましょう。

00:08:04.000 -> 00:08:09.000
重要なのは、モデルに適合させることとは別に、トレーニングデータを前処理することです。

00:08:09.000 -> 00:08:15.000
この例では、分類器のフィッティングとは別にオーディオ機能を抽出できます。

00:08:15.000 -> 00:08:17.000
そして、これは一般的にも機能します。

00:08:17.000 -> 00:08:26.000
一連の変圧器に続いて見積もりがあるときはいつでも、見積もりにつながる変圧器を介して入力を前処理できます。

00:08:26.000 -> 00:08:33.000
あなたがする必要があるのは、前処理メソッドを呼び出して、前処理された機能にモデルを合わせることだけです。

00:08:33.000 -> 00:08:38.000
サウンド分類器の構成を変更する必要がなかったので、これは便利だと思います。

00:08:38.000 -> 00:08:46.000
機能を別々に抽出したので、新しいデータのオーディオ機能のみを抽出する柔軟性があります。

00:08:46.000 -> 00:08:51.000
モデルの新しいトレーニングデータを発見すると、このデータを別々に簡単に前処理できます。

00:08:51.000 -> 00:08:56.000
そして、以前に抽出されたものに補足機能を追加します。

00:08:56.000 -> 00:09:01.000
これは、前処理が時間を節約できる最初の例にすぎません。

00:09:01.000 -> 00:09:04.000
モデル構築のライフサイクルに戻りましょう。

00:09:04.000 -> 00:09:09.000
モデルの品質に満足するまで、見積もりパラメータを調整する必要があるかもしれません。 

00:09:09.000 -> 00:09:18.000
フィーチャ抽出をフィッティングから分離することで、フィーチャを一度だけ抽出し、異なる推定パラメータでモデルに合わせることができます。

00:09:18.000 -> 00:09:24.000
特徴抽出をやり直さずに分類器パラメータを変更する例を見てみましょう。

00:09:24.000 -> 00:09:30.000
すでに機能を抽出したと仮定して、分類器のL2ペナルティパラメータを変更します。

00:09:30.000 -> 00:09:35.000
そして、新しい分類器を古い機能抽出器に追加する必要があります。

00:09:35.000 -> 00:09:43.000
見積もりをチューニングするときに機能抽出器を変更しないことが重要です。これは、以前に抽出された機能を無効にするからです。

00:09:43.000 -> 00:09:47.000
バッチでモデルを段階的にフィッティングすることに移りましょう。

00:09:47.000 -> 00:09:51.000
機械学習モデルは、一般的に大量のトレーニングデータの恩恵を受けます。

00:09:51.000 -> 00:09:55.000
ただし、アプリのメモリ制約が限られている可能性があります。

00:09:55.000 -> 00:09:56.000
それで、あなたは何をしますか?

00:09:56.000 -> 00:10:02.000
Create ML Componentsを使用して、一度にデータのバッチのみをメモリにロードすることで、モデルをトレーニングできます。

00:10:02.000 -> 00:10:07.000
最初にする必要があるのは、分類器を更新可能な分類器に置き換えることです。

00:10:07.000 -> 00:10:12.000
バッチでカスタムモデルをトレーニングするには、分類器が更新可能である必要があります。

00:10:12.000 -> 00:10:25.000
たとえば、完全に接続されたニューラルネットワーク分類器は、更新不可能なロジスティック回帰分類器の代わりに簡単に使用できます。

00:10:25.000 -> 00:10:28.000
さて、今からトレーニングループを書きます。

00:10:28.000 -> 00:10:32.000
デフォルトの初期化モデルを作成することから始めます。

00:10:32.000 -> 00:10:37.000
あなたはまだ予測をすることはできません。これはトレーニングの出発点にすぎないからです。

00:10:37.000 -> 00:10:41.000
その後、トレーニングが始まる前にオーディオ機能を抽出します。

00:10:41.000 -> 00:10:46.000
すべての反復で機能を抽出したくないので、これは重要なステップです。

00:10:46.000 -> 00:10:53.000
次のステップは、トレーニングループを定義し、トレーニングする反復回数を指定することです。

00:10:53.000 -> 00:10:57.000
続ける前に、アルゴリズムのSwiftパッケージをインポートします。

00:10:57.000 -> 00:11:01.000
トレーニングデータのバッチを作成するために必要です。

00:11:01.000 -> 00:11:10.000
詳細については、WWDC 2021のセッション「Meet the Swift Algorithms and Collections packages」をチェックしてください。

00:11:10.000 -> 00:11:13.000
トレーニングループ内では、バッチ処理が行われる場所です。

00:11:13.000 -> 00:11:18.000
チャンクメソッドを使用して、機能をトレーニング用のバッチにグループ化します。

00:11:18.000 -> 00:11:23.000
チャンクサイズは、一度にメモリにロードされる機能の数です。

00:11:23.000 -> 00:11:31.000
その後、バッチを反復して更新メソッドを呼び出すことで、モデルを更新できます。

00:11:31.000 -> 00:11:35.000
モデルを段階的にトレーニングすると、さらにいくつかのトレーニングテクニックのロックを解除できます。

00:11:35.000 -> 00:11:42.000
たとえば、このトレーニンググラフでは、約10回の反復の後、モデルの精度は95%で推移します。

00:11:42.000 -> 00:11:46.000
この時点で、モデルは収束しており、早期に停止することができます。

00:11:46.000 -> 00:11:50.000
トレーニングループで早期停止を実施しましょう。

00:11:50.000 -> 00:11:54.000
最初にすべきことは、検証セットの予測をすることです。

00:11:54.000 -> 00:12:01.000
検証予測とその注釈をペアリングする必要があるため、ここではmapFeaturesメソッドを使用しています。

00:12:01.000 -> 00:12:04.000
次のステップは、モデルの品質を測定することです。

00:12:04.000 -> 00:12:10.000
今のところ、組み込みのメトリクスを使用しますが、独自のカスタムメトリクスを実装するのを妨げるものは何もありません。

00:12:10.000 -> 00:12:16.000
そして最後に、私のモデルが95%の精度に達したら、トレーニングを中止します。

00:12:16.000 -> 00:12:22.000
トレーニングループ以外では、後で予測に使用できるように、モデルをディスクに書き出します。

00:12:22.000 -> 00:12:28.000
早めに停止することに加えて、モデルのチェックポイントについて話したいと思います。

00:12:28.000 -> 00:12:33.000
最後まで待つのではなく、トレーニング中にモデルの進捗状況を保存できます。

00:12:33.000 -> 00:12:41.000
また、トレーニングを再開するためにチェックポイントを使用することもできます。これは、特にモデルのトレーニングに長い時間がかかる場合に便利です。

00:12:41.000 -> 00:12:45.000
あなたがする必要があるのは、トレーニングループにモデルを書き出すことだけです。

00:12:45.000 -> 00:12:49.000
チェックポイント間隔を定義して、数回の反復ごとにこれを行うことをお勧めします。

00:12:49.000 -> 00:12:51.000
それはとても簡単です。

00:12:51.000 -> 00:13:00.000
このセッションでは、オーディオやビデオなどの時間的データを使用して機械学習タスクを構築する新しい方法である時間的コンポーネントを紹介しました。

00:13:00.000 -> 00:13:05.000
私は人間の行動の繰り返しカウンターを作るために一緒に時間的コンポーネントを構成しました。

00:13:05.000 -> 00:13:08.000
そして最後に、私はインクリメンタルフィッティングについて話しました。

00:13:08.000 -> 00:13:13.000
これにより、アプリに機械学習を組み込む新しい可能性が解き放ちます。

00:13:13.000 -> 00:13:16.000
参加してくれてありがとう、WWDCの残りの部分を楽しんでください。

00:13:16.000 -> 23:59:59.000
♪

