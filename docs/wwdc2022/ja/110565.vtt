WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:13.000
ケン・グリーンバウム:皆さん、こんにちは!WWDC 2022へようこそ。

00:00:13.000 --> 00:00:18.000
私の名前はケン・グリーンバウムで、アップルのカラー&amp;ディスプレイテクノロジーチームに所属しています。

00:00:18.000 --> 00:00:21.000
今年は3回のEDR講演ができることに興奮しています。

00:00:21.000 --> 00:00:33.000
iOSのEDR APIサポートを発表した「Explore EDR on iOS」と「Core Image、Metal、SwiftUIでEDRコンテンツを表示する」を見る機会があったことを願っています。

00:00:33.000 --> 00:00:44.000
昨年の私のEDRトークを見た人もいるかもしれません。そこでは、AVPlayerを使用してEDRを使用してHDRビデオを再生する方法を実演しました。

00:00:44.000 --> 00:00:59.000
この講演では、Core Mediaインターフェイスを使用して、EDR再生だけでなく、HDRビデオを独自のEDRレイヤーまたはビューにデコードして再生する方法を探ります。

00:00:59.000 --> 00:01:20.000
次に、単にコンテンツを再生するだけでなく、Core Videoのディスプレイリンクを介してリアルタイムでデコードされたビデオフレームにアクセスする方法を示し、それらのフレームをCoreImage Filters、またはMetal Shaderに送信して、カラー管理、視覚効果を追加したり、その他の信号処理を適用したり、最後に、結果のフレームをMetalにプラムしてレンダリングします。

00:01:20.000 --> 00:01:30.000
まず、EDR互換のビデオメディアフレームワークを見直して、アプリケーションの要件に最も適したフレームワークを決定するのに役立ちます。

00:01:30.000 --> 00:01:42.000
次に、アプリケーションが直接再生を必要とする場合、HDRビデオを再生するすべての作業を行うことができる高レベルのAVKitとAVFoundationフレームワークについて簡単に説明します。

00:01:42.000 --> 00:01:54.000
最後に、EDR再生、編集、または画像処理エンジンで、Core VideoとMetalを使用して、デコードされたビデオフレームを使用するためのベストプラクティスについて説明します。

00:01:54.000 --> 00:02:10.000
まず、Appleのビデオフレームワークの簡単な調査から始めましょう。最高レベルのインターフェイスから始めて、最も使いやすいです。そして、コードに複雑さを加えることを犠牲にして、より多くの機会を提供する低レベルのフレームワークを続けます。

00:02:10.000 --> 00:02:17.000
自動的に提供される最適化を利用するには、可能な限り最高レベルのフレームワークを使用するのが最善です。

00:02:17.000 --> 00:02:31.000
これにより、簡単なEDR再生から、デコードされたビデオフレームのより洗練された配管、リアルタイム処理のためのCoreImageやMetalまで、多くのシナリオを探求するトークの本体に飛び込む準備が整います。

00:02:31.000 --> 00:02:34.000
最高レベルでは、AVKitがあります。

00:02:34.000 --> 00:02:45.000
AVKitを使用すると、メディア再生用のユーザーインターフェイスを作成できます。トランスポートコントロール、チャプターナビゲーション、ピクチャーインピクチャーのサポート、字幕とクローズドキャプションの表示が付属しています。

00:02:45.000 --> 00:02:52.000
AVKitは、AVPlayerViewControllerを使用してデモンストレーションを行うように、HDRコンテンツをEDRとして再生できます。

00:02:52.000 --> 00:03:01.000
ただし、アプリケーションでビデオフレームのさらなる処理が必要な場合は、パイプラインをより詳細に制御できるメディアフレームワークを使用する必要があります。

00:03:01.000 --> 00:03:04.000
次はAVFoundationです。

00:03:04.000 --> 00:03:12.000
AVFoundationは、Appleプラットフォームで時間ベースのオーディオビジュアルメディアを扱うためのフル機能のフレームワークです。

00:03:12.000 --> 00:03:24.000
AVFoundationを使用すると、QuickTimeムービーやMPEG 4ファイルを簡単に再生、作成、編集したり、HLSストリームを再生したり、強力なメディア機能をアプリに組み込んだりできます。

00:03:24.000 --> 00:03:30.000
この講演では、AVPlayerと関連するAVPlayerLayerインターフェースの使用を探ります。

00:03:30.000 --> 00:03:35.000
コアビデオは、デジタルビデオのパイプラインモデルを提供するフレームワークです。

00:03:35.000 --> 00:03:40.000
プロセスを個別のステップに分割することで、ビデオの操作方法を簡素化します。

00:03:40.000 --> 00:03:51.000
また、Core Videoを使用すると、データタイプ間の変換やディスプレイの同期を心配することなく、個々のフレームへのアクセスと操作が容易になります。

00:03:51.000 --> 00:03:56.000
DisplayLinkとCVPixelBufferのCore Imageの使用を実演します。

00:03:56.000 --> 00:03:59.000
そして、金属を使ったCVMetalTextureCache。

00:03:59.000 --> 00:04:01.000
次に、ビデオツールボックスがあります。

00:04:01.000 --> 00:04:06.000
これは、ハードウェアエンコーダとデコーダへの直接アクセスを提供する低レベルのフレームワークです。

00:04:06.000 --> 00:04:15.000
Video Toolboxは、ビデオの圧縮と解凍、およびCore Videoピクセルバッファに保存されているラスター画像フォーマット間の変換のためのサービスを提供します。

00:04:15.000 --> 00:04:24.000
VTDecompressionSessionは、この講演の範囲外の強力な低レベルのインターフェースですが、高度な開発者はさらに調査したいかもしれません。

00:04:24.000 --> 00:04:26.000
そして最後に、コアメディアがあります。

00:04:26.000 --> 00:04:33.000
このフレームワークは、AVFoundationで使用されるメディアパイプライン、およびその他の高レベルのメディアフレームワークを定義します。

00:04:33.000 --> 00:04:41.000
Core Mediaの低レベルのデータタイプとインターフェイスをいつでも使用して、メディアサンプルを効率的に処理し、メディアデータのキューを管理できます。

00:04:41.000 --> 00:04:47.000
この講演の残りの部分では、アプリでこれらのフレームワークを使用する方法と時期を紹介します。

00:04:47.000 --> 00:04:55.000
まず、AVKitとAVFoundationを使用して、EDRとしてレンダリングされたHDRビデオを簡単に再生する方法。

00:04:55.000 --> 00:05:17.000
次に、AVPlayerの一連のより洗練されたアプリケーション：独自のレイヤーにレンダリングし、CADisplayLinkを介して個別にデコードされたフレームにアクセスし、結果のCVPixelBuffersをCore Imageに送信して処理し、最後に、Metalで処理およびレンダリングするためにCVMetalTextureCacheを介してMetalテクスチャとしてデコードされたフレームにアクセスします。

00:05:17.000 --> 00:05:26.000
Appleプラットフォームのビデオメディアレイヤーの概要がわかったので、AVKitとAVFoundationのフレームワークに焦点を当てます。

00:05:26.000 --> 00:05:33.000
まず、AVFoundationのAVPlayerインターフェイスを使用してHDRビデオコンテンツの再生について話し合うことから始めましょう。

00:05:33.000 --> 00:05:39.000
AVPlayerは、メディアアセットの再生とタイミングを管理するために使用されるコントローラーオブジェクトです。

00:05:39.000 --> 00:05:48.000
AVPlayerインターフェースは、HDRビデオの高性能再生に使用でき、可能であれば自動的にEDRとして結果をレンダリングします。

00:05:48.000 --> 00:05:58.000
AVPlayerを使用すると、QuickTimeムービーなどのローカルおよびリモートファイルベースのメディア、およびHLSを使用して提供されるストリーミングメディアを再生できます。

00:05:58.000 --> 00:06:02.000
基本的に、AVPlayerは一度に1つのメディアアセットを再生するために使用されます。

00:06:02.000 --> 00:06:17.000
プレーヤーインスタンスを再利用して、追加のメディアアセットを連続再生したり、複数のインスタンスを作成して複数のアセットを同時に再生することもできますが、AVPlayerは一度に1つのメディアアセットの再生のみを管理します。

00:06:17.000 --> 00:06:29.000
AVFoundationフレームワークは、シーケンシャルHDRメディアアセットのキューイングと再生を作成および管理するために使用できるAVQueuePlayerと呼ばれるAVPlayerのサブクラスも提供します。

00:06:29.000 --> 00:06:39.000
アプリケーションがEDRにレンダリングされたHDRビデオメディアの簡単な再生が必要な場合は、AVPlayerViewControllerを使用したAVPlayerが最善のアプローチかもしれません。

00:06:39.000 --> 00:06:46.000
AVPlayerLayerでAVPlayerを使用して、iOSまたはmacOSで独自のビューを再生します。

00:06:46.000 --> 00:06:49.000
これらはAVPlayerを使用する最も簡単な方法です。

00:06:49.000 --> 00:06:51.000
両方の例を見てみましょう。

00:06:51.000 --> 00:06:59.000
まず、AVKitのAVPlayer View Controllerと組み合わせて、AVFoundationのAVPlayerインターフェイスを使用する方法を見ていきます。

00:06:59.000 --> 00:07:06.000
ここでは、メディアのURLからAVPlayerをインスタンス化することから始めます。

00:07:06.000 --> 00:07:18.000
次に、AVPlayerViewControllerを作成し、ビューアコントローラーのプレーヤープロパティをメディアのURLから作成したプレーヤーに設定します。

00:07:18.000 --> 00:07:23.000
そして、ビデオの再生を開始するために、ビューコントローラーをモーダルに提示します。

00:07:23.000 --> 00:07:31.000
AVKitはすべての詳細を管理し、EDRをサポートするディスプレイでHDRビデオをEDRとして自動的に再生します。

00:07:31.000 --> 00:07:37.000
前述したように、一部のアプリケーションでは、HDRビデオメディアを独自のビューに再生する必要があります。

00:07:37.000 --> 00:07:42.000
AVPlayerLayerでAVPlayerを使用してこれを達成する方法を見てみましょう。

00:07:42.000 --> 00:07:51.000
自分のビューでHDRビデオメディアをEDRとして再生するには、メディアのURLでAVPlayerを作成することから始めます。

00:07:51.000 --> 00:07:57.000
しかし、今回は作成したばかりのプレーヤーでAVPlayerLayerをインスタンス化します。

00:07:57.000 --> 00:08:02.000
次に、ビューから取得したプレイヤーレイヤーの境界を設定する必要があります。

00:08:02.000 --> 00:08:10.000
プレイヤーレイヤーがビューからの境界を持つようになったので、プレイヤーレイヤーをサブレイヤーとしてビューに追加できます。

00:08:10.000 --> 00:08:15.000
最後に、HDRビデオメディアを再生するには、AVPlayerの再生方式と呼びます。

00:08:15.000 --> 00:08:24.000
AVPlayerとAVPlayerLayerを使用して、独自のレイヤーでHDRビデオメディアをEDRとして再生するために必要なのはそれだけです。

00:08:24.000 --> 00:08:29.000
AVPlayerを使用して、2つの最も簡単なHDRビデオ再生ワークフローを調査しました。

00:08:29.000 --> 00:08:35.000
しかし、多くのアプリケーションは単純なメディア再生以上のものを必要とします。

00:08:35.000 --> 00:08:43.000
たとえば、アプリケーションでは、カラーグレーディングやクロマキーイングなどの画像処理をビデオに適用する必要がある場合があります。

00:08:43.000 --> 00:08:55.000
AVPlayerからデコードされたビデオフレームを取得し、コアイメージフィルターまたはメタルシェーダーをリアルタイムで適用し、結果をEDRとしてレンダリングするワークフローを探りましょう。

00:08:55.000 --> 00:09:17.000
AVPlayerとAVPlayerItemを使用して、HDRビデオメディアからEDRフレームをデコードする方法を実演し、Core Videoディスプレイリンクからデコードされたフレームにアクセスし、結果のピクセルバッファをCore ImageまたはMetalに送信して処理し、EDRをサポートするディスプレイでCAMetalLayerでEDRとして結果をレンダリングします。

00:09:17.000 --> 00:09:28.000
これを念頭に置いて、まず、HDRメディアがEDRとして正しくレンダリングされるようにするために必要なCAMetalLayerにいくつかの重要なプロパティを設定することを実演しましょう。

00:09:28.000 --> 00:09:34.000
まず、HDRビデオコンテンツをレンダリングするCAMetalLayerを入手する必要があります。

00:09:34.000 --> 00:09:42.000
そのレイヤーでは、wansExtendedDynamicRangeContentフラグをtrueに設定してEDRを選択します。

00:09:42.000 --> 00:09:48.000
拡張ダイナミックレンジコンテンツをサポートするピクセル形式を必ず使用してください。

00:09:48.000 --> 00:10:01.000
以下のAVPlayerの例では、CAMetalLayerをハーフフロートピクセル形式を使用するように設定しますが、PQまたはHLG転送関数と組み合わせて使用される10ビット形式も機能します。

00:10:01.000 --> 00:10:10.000
結果をSDRに制限しないようにするには、レイヤーをEDR互換の拡張範囲の色空間に設定する必要があります。

00:10:10.000 --> 00:10:18.000
この例では、ハーフフロートメタルテクスチャを拡張リニアディスプレイP3色空間に設定します。

00:10:18.000 --> 00:10:23.000
EDR、色空間、ピクセルバッファフォーマットに関する表面を引っ掻いただけです。

00:10:23.000 --> 00:10:33.000
詳細については、昨年の私のセッション「EDRによるHDRレンダリング」と今年の「iOSのEDR」をチェックしてください。

00:10:33.000 --> 00:10:42.000
CAMetalLayerの基本的なプロパティを設定したので、Core ImageまたはMetalシェーダーを使用してリアルタイムの画像処理を追加して、デモンストレーションを続けましょう。

00:10:42.000 --> 00:10:49.000
AVPlayerと組み合わせてディスプレイリンクを使用して、デコードされたビデオフレームにリアルタイムでアクセスします。

00:10:49.000 --> 00:10:53.000
このワークフローでは、AVPlayerItemからAVPlayerを作成することから始めます。

00:10:53.000 --> 00:11:02.000
次に、EDRの適切なピクセルバッファ形式と色空間で構成されたAVPlayerItemVideoOutputをインスタンス化します。

00:11:02.000 --> 00:11:05.000
次に、表示リンクを作成して設定します。

00:11:05.000 --> 00:11:11.000
そして最後に、ディスプレイリンクを実行して、ピクセルバッファをコアイメージまたはメタルに取得して処理します。

00:11:11.000 --> 00:11:16.000
iOSで使用されているCADisplayLinkを実演します。

00:11:16.000 --> 00:11:21.000
macOS用に開発する場合は、同等のCVDisplayLinkインターフェースを使用してください。

00:11:21.000 --> 00:11:32.000
今回は、メディアのURLからAVPlayerItemを作成し、作成したばかりのAVPlayerItemでAVPlayerをインスタンス化することを選択します。

00:11:32.000 --> 00:11:39.000
次に、デコードされたフレームの色空間とピクセルバッファ形式を指定する辞書のペアを作成します。

00:11:39.000 --> 00:11:45.000
最初の辞書であるvideoColorPropertiesは、色空間と転送関数が指定されている場所です。

00:11:45.000 --> 00:12:00.000
この例では、ほとんどのAppleディスプレイの色空間に対応するディスプレイP3色空間と、AVFoundationがEDRに必要な拡張範囲値を維持できるようにする線形転送機能を要求します。

00:12:00.000 --> 00:12:11.000
2番目の辞書であるoutputVideoSettingsは、ピクセルバッファ形式の特性を指定し、作成したばかりのvideoColorProperties辞書への参照も提供します。

00:12:11.000 --> 00:12:17.000
この例では、ワイドカラーとハーフフロートピクセルバッファ形式を要求します。

00:12:17.000 --> 00:12:34.000
AVPlayerItemVideoOutputは、出力設定辞書で指定したピクセルバッファ形式にビデオをデコードするだけでなく、ピクセル転送セッションを介して必要な色変換を自動的に実行することは非常に役に立ちます。

00:12:34.000 --> 00:12:39.000
思い出してください、ビデオには複数のクリップが含まれている可能性があり、潜在的に異なる色空間を持つ可能性があります。

00:12:39.000 --> 00:12:57.000
AVFoundationはこれらを自動的に管理し、すぐに実演するように、この動作により、結果としてデコードされたビデオフレームを、ディスプレイの色空間への自動色空間変換を提供しないMetalのような低レベルのフレームワークに送信することもできます。

00:12:57.000 --> 00:13:03.000
次に、outputVideoSettings辞書を使用してAVPlayerItemVideoOutputを作成します。

00:13:03.000 --> 00:13:10.000
3番目のステップとして、デコードされたフレームにリアルタイムでアクセスするために使用されるディスプレイリンクを設定します。

00:13:10.000 --> 00:13:15.000
CADisplayLinkは、各ディスプレイアップデートで実行されるコールバックを受けます。

00:13:15.000 --> 00:13:24.000
この例では、処理のためにCore Imageに送信するCVPixelBuffersを取得するために、すぐに探索するローカル関数を呼び出します。

00:13:24.000 --> 00:13:33.000
次に、ビデオプレーヤーアイテムオブザーバーを作成して、指定されたプレーヤーアイテムプロパティの変更を処理できるようにします。

00:13:33.000 --> 00:13:41.000
私たちの例では、プレイヤーアイテムのステータスが変更されるたびにこのコードを実行します。

00:13:41.000 --> 00:14:04.000
プレーヤーアイテムのステータスがreadyToPlayに変更されると、AVPlayerItemVideoOutputを返されたばかりの新しいAVPlayerItemに追加し、メイン実行ループを共通モードに設定してCADisplayLinkを登録し、ビデオプレーヤーで再生を呼び出してHDRビデオのリアルタイムデコードを開始します。

00:14:04.000 --> 00:14:15.000
最後に、先ほど「displayLinkCopyPixelBuffers」ローカル関数と呼んだCADisplayLinkコールバック実装の例を見ていきます。

00:14:15.000 --> 00:14:22.000
HDRビデオの再生が始まると、ディスプレイの更新ごとにCADisplayLinkコールバック機能が呼び出されます。

00:14:22.000 --> 00:14:27.000
たとえば、典型的なディスプレイでは1秒間に60回呼び出されるかもしれません。

00:14:27.000 --> 00:14:34.000
これは、新しいCVPixelBufferがある場合に表示されるフレームを更新するコードの機会です。

00:14:34.000 --> 00:14:43.000
各ディスプレイコールバックでは、現在のウォールクロック時間に表示されるデコードされたビデオフレームを含むCVPixelBufferをコピーしようとします。

00:14:43.000 --> 00:14:56.000
ただし、特に画面のリフレッシュレートが再生されているビデオのリフレッシュレートを超える場合、すべてのディスプレイの更新で常に新しいCVPixelBufferが利用できるわけではないため、「copyPixelBuffer」呼び出しは失敗する可能性があります。

00:14:56.000 --> 00:15:01.000
新しいCVPixelBufferがない場合は、呼び出しが失敗し、レンダリングをスキップします。

00:15:01.000 --> 00:15:06.000
これにより、前のフレームは別のディスプレイの更新のために画面に残ります。

00:15:06.000 --> 00:15:12.000
しかし、コピーが成功した場合、CVPixelBufferに新しいビデオフレームがあります。

00:15:12.000 --> 00:15:16.000
この新しいフレームを処理してレンダリングする方法はいくつかあります。

00:15:16.000 --> 00:15:21.000
1つの機会は、処理のためにCVPixelBufferをCore Imageに送信することです。

00:15:21.000 --> 00:15:29.000
Core Imageは、1つ以上のシチクターをストリングして、ビデオフレームにGPU加速画像処理を提供できます。

00:15:29.000 --> 00:15:38.000
すべてのシフィッターがEDRと互換性があるわけではないため、SDRへのクランプなど、HDRコンテンツに問題がある可能性があることに注意してください。

00:15:38.000 --> 00:15:42.000
コアイメージは、多くのEDR互換フィルターを提供します。

00:15:42.000 --> 00:15:49.000
CICategoryHighDynamicRangeでフィルター名を使用して、EDR互換のコアイメージフィルターを列挙します。

00:15:49.000 --> 00:15:53.000
この例では、シンプルなセピアトーン効果を追加します。

00:15:53.000 --> 00:15:58.000
では、例に戻り、Core Imageを統合しましょう。

00:15:58.000 --> 00:16:06.000
新鮮なCVPixelBufferを生成する各表示リンクコールバックで、そのピクセルバッファからCIImageを作成します。

00:16:06.000 --> 00:16:09.000
目的の効果を実装するためにCIFilterをインスタンスします。

00:16:09.000 --> 00:16:20.000
パラメータレスのシンプルさのためにセピアトーンフィルターを使用していますが、システムには多くのシフィッターが組み込まれており、自分で書くのも簡単です。

00:16:20.000 --> 00:16:26.000
CIFilterのinputImageを、先ほど作成したCIImageに設定します。

00:16:26.000 --> 00:16:32.000
そして、処理されたビデオの結果は、フィルターのoutputImageで利用可能になります。

00:16:32.000 --> 00:16:37.000
望ましい効果を達成するために、必要なだけ多くのCIFiltersをチェーンでつなぎます。

00:16:37.000 --> 00:16:44.000
次に、CIRenderDestinationを使用して、結果の画像をアプリケーションのビューコードにレンダリングします。

00:16:44.000 --> 00:16:51.000
このワークフローの詳細については、WWDC 2020トーク「ビデオアプリのコアイメージパイプラインの最適化」を参照してください。

00:16:51.000 --> 00:16:59.000
もう1つの機会は、メタルとカスタムメタルシェーダーを使用して、新鮮なCVPixelBufferを処理してレンダリングすることです。

00:16:59.000 --> 00:17:04.000
CVPixelBufferをMetalテクスチャに変換するプロセスを簡単に説明します。

00:17:04.000 --> 00:17:10.000
しかし、最高のパフォーマンスを維持しながらこの変換を実装することは、別の講演に残すのに最適な深いトピックです。

00:17:10.000 --> 00:17:19.000
代わりに、CoreVideo MetalテクスチャキャッシュからMetalテクスチャを導出することを推奨し、この講演の最後の例としてそのプロセスを説明します。

00:17:19.000 --> 00:17:33.000
一般的に言えば、プロセスはCVPixelBufferからIOSurfaceを取得し、MetalTextureDescriptorを作成し、「newTextureWithDescriptor」を使用してMetalDeviceからMetalTextureを作成することです。

00:17:33.000 --> 00:17:41.000
ただし、慎重なロックが適用されない場合、テクスチャが再利用され、オーバードローされる危険性があります。

00:17:41.000 --> 00:17:49.000
さらに、すべてのPixelBufferフォーマットがMetalTextureでネイティブにサポートされているわけではないため、この例ではハーフフロートを使用しています。

00:17:49.000 --> 00:17:56.000
これらの複雑さのため、代わりに、これから実演するように、Core VideoからMetalテクスチャに直接アクセスすることをお勧めします。

00:17:56.000 --> 00:18:00.000
コアビデオとメタルをさらに探求しましょう。

00:18:00.000 --> 00:18:07.000
前述のように、CVMetalTextureCacheは、MetalでCVPixelBuffersを使用するための簡単で効率的な方法です。

00:18:07.000 --> 00:18:14.000
CVMetalTextureCacheは、さらなる変換を必要とせずにキャッシュから直接Metalテクスチャを取得するので便利です。

00:18:14.000 --> 00:18:26.000
CVMetalTextureCacheは、CVPixelBufferとMetalTextureを自動的にブリッジし、コードを簡素化し、高速パスを維持します。

00:18:26.000 --> 00:18:37.000
CVPixelBufferPoolsと組み合わせて、CVMetalTextureCacheは、MTLTextureとIOSurfaceマッピングを存続させることで、パフォーマンス上の利点も提供します。

00:18:37.000 --> 00:18:43.000
最後に、CVMetalTextureCacheを使用すると、IOSurfacesを手動で追跡する必要がなくなります。

00:18:43.000 --> 00:18:52.000
さて、私たちの講演の最後の例は、CVMetalTextureCacheを使用してCore Videoから直接Metalテクスチャを抽出する方法です。

00:18:52.000 --> 00:18:55.000
ここでは、システムのデフォルトのMetalデバイスを取得することから始めます。

00:18:55.000 --> 00:19:04.000
それを使用してメタルテクスチャキャッシュを作成し、メタルテクスチャキャッシュに関連付けられたコアビデオメタルテクスチャキャッシュをインスタンス化します。

00:19:04.000 --> 00:19:13.000
その後、デコードされたビデオフレームにメタルテクスチャとしてアクセスすることができ、便利なメタルエンジンで直接使用できます。

00:19:13.000 --> 00:19:18.000
この例では、Metalシステムのデフォルトデバイスを作成して使用します。

00:19:18.000 --> 00:19:27.000
次に、CVMetalTextureCacheCreateを使用してCVMetalTextureCacheを作成し、作成したばかりのMetalデバイスを指定します。

00:19:27.000 --> 00:19:33.000
コアビデオメタルテクスチャを作成するために必要なCVPixelBufferの高さと幅を取得します。

00:19:33.000 --> 00:19:43.000
次に、「CVMetalTextureCacheCreateTextureFromImage」を呼び出すと、CVMetalTextureオブジェクトをインスタンス化し、それをCVPixelBufferに関連付けます。

00:19:43.000 --> 00:19:50.000
最後に、「CVMetalTextureGetTexture」を呼び出すと、目的のメタルテクスチャを取得します。

00:19:50.000 --> 00:20:01.000
Swiftアプリケーションは、CVMetalTextureの強力なリファレンスを使用する必要がありますが、Objective-Cを使用する場合は、CVMetalTextureRefをリリースする前に、Metalがテクスチャで完了していることを確認する必要があります。

00:20:01.000 --> 00:20:07.000
これは、金属コマンドバッファ補完ハンドラを使用して達成できます。

00:20:07.000 --> 00:20:09.000
そして、それだけです、皆さん!

00:20:09.000 --> 00:20:18.000
レビューするために、HDRビデオメディアをEDRにレンダリングし、再生、編集、または画像処理を行う多くのワークフローを調査しました。

00:20:18.000 --> 00:20:26.000
HDRメディアを再生するために、AVPlayerからAVKitのAVPlayerViewControllerに行く方法を学びました。

00:20:26.000 --> 00:20:34.000
また、AVPlayerLayerと一緒にAVPlayerを使用して、自分のビューでHDRメディアを表示する方法も学びました。

00:20:34.000 --> 00:20:38.000
そして最後に、再生中にリアルタイムエフェクトを追加する方法を探りました。

00:20:38.000 --> 00:20:44.000
AVFoundationのAVPlayerをCoreVideoに接続し、次にMetalに接続してレンダリングします。

00:20:44.000 --> 00:20:51.000
また、CoreImageフィルターとメタルシェーダーを使用してリアルタイムエフェクトを適用します。

00:20:51.000 --> 00:21:02.000
より深く掘り下げたい場合は、ビデオワークフローの作成と、HDRメディアとEDRの統合に関連するいくつかのWWDCセッションをお勧めします。

00:21:02.000 --> 00:21:08.000
特に「AVFoundationでHDRビデオを編集して再生する」というセッションを呼び出したい。

00:21:08.000 --> 00:21:17.000
このセッションでは、HDRメディアにエフェクトを適用するための「applyingCIFiltersWithHandler」を使用したAVVideoCompositionの使用を探ります。

00:21:17.000 --> 00:21:26.000
このセッションでは、各ビデオフレームが処理可能になったときに、CVPixelBufferで使用できるカスタムコンポジターの使用方法も学びます。

00:21:26.000 --> 00:21:49.000
冒頭で述べたように、今年はEDRに関する他の2つのセッションも発表します。EDR APIサポートがiOSを含むように拡大されたと発表した「iOSのEDR」と、「CoreImage、Metal、SwiftUIを使用したEDRを使用したHDRコンテンツ表示」で、EDRと他のメディアフレームワークとの統合をさらに検討します。

00:21:49.000 --> 00:21:56.000
macOSとiOSの両方で、HDRビデオをEDR対応アプリケーションに組み込むことを願っています。

00:21:56.000 --> 23:59:59.000
見てくれてありがとう。

