WEBVTT

00:00:00.000 -> 00:00:11.000
- こんにちは、ようこそ。

00:00:11.000 -> 00:00:17.000
私の名前はマルコ・ジョルダーノで、アップルのGPUソフトウェアエンジニアリングチームに所属しています。

00:00:17.000 -> 00:00:23.000
このセッションでは、Apple M1 GPU全体でワークロードを拡張する方法について説明します。

00:00:23.000 -> 00:00:33.000
複雑なコンピューティングワークロードに取り組み、Appleシリコンハードウェアを最大限に活用し、優れたスケーリングを実現する方法を知りたい場合は、この講演が最適です。

00:00:33.000 -> 00:00:42.000
まず、コンピューティングスケーラビリティの概念と、アプリケーションがM1 GPUファミリー全体で自然にパフォーマンスを拡張する方法について説明します。

00:00:42.000 -> 00:00:52.000
そして、ステップバイステップの「ハウツー」を共有し、ワークロードのコンピューティングスケーリングを最大化するために利用可能なツールについて話します。

00:00:52.000 -> 00:00:59.000
スケーラビリティとは何か、なぜそれがワークロードにとって重要なのかを理解することから始めましょう。

00:00:59.000 -> 00:01:08.000
Apple M1 GPUは、ゼロからスケールアップし、ワークロードがSOCファミリー全体で優れたパフォーマンスを達成できるように設計されています。

00:01:08.000 -> 00:01:17.000
すべてのMetal 3機能をサポートする同じGPUは、8コアのiPadから64コアのMac Studioまで拡張できます。

00:01:17.000 -> 00:01:24.000
高レベルのスケーリングを利用するために、M1用に最適化されたアプリを持つことは素晴らしい出発点です。

00:01:24.000 -> 00:01:34.000
多くの著名なプロアプリはすでにApple M1用に最適化されており、すべてのデバイスで優れたスケーリングを経験しています。

00:01:34.000 -> 00:01:44.000
たとえば、ここには、ポストプロダクション業界の写真とビデオ編集者であるAffinity PhotoとDaVinci Resolveがあります。

00:01:44.000 -> 00:01:47.000
これらのアプリは大きなスケーリングを達成しています。

00:01:47.000 -> 00:01:53.000
スケーラビリティの本当の意味と、「理想的な」スケーリングを達成する方法を定義しましょう。

00:01:53.000 -> 00:02:01.000
GPUワークロードのスケーラビリティは、GPUコアの数を増やしてパフォーマンスを向上させる能力です。

00:02:01.000 -> 00:02:07.000
右側のチャートは、GPUコア数の増加によるアプリケーションのスピードアップを示しています。

00:02:07.000 -> 00:02:12.000
線形比率の改善は理想的であると考えられています。

00:02:12.000 -> 00:02:25.000
しかし、アプリで作業している間、プラトーにヒットし、リターンが減少してスケールするか、GPUタイムラインのギャップのためにまったくスケールしないスケーリングの種類に気付くかもしれません。

00:02:25.000 -> 00:02:44.000
または、パフォーマンスが向上するが、ワークロードがいくつかのGPUリミッターに当たっているスタック全体で均一ではない別のタイプのスケーリングが見られるかもしれません。ここのように、24〜32または48〜64コアの間です。

00:02:44.000 -> 00:02:56.000
あなたの目標は、可能な限りリニアスケーリングに近づくことであり、ボトルネックを特定し、あなたが望む結果を達成するためのツールとテクニックを紹介します。

00:02:56.000 -> 00:03:01.000
次のセクションでは、GPUスケーリングを最大化するためのアプローチについて説明します。

00:03:01.000 -> 00:03:06.000
すべてのワークロードについて、まずボトルネックがどこにあるかを特定する必要があります。

00:03:06.000 -> 00:03:11.000
ワークロードは、計算または帯域幅のいずれかによって制限できます。

00:03:11.000 -> 00:03:16.000
最適化プロセス中に、一方と他方の間でバウンスしてしまう可能性があります。

00:03:16.000 -> 00:03:26.000
計算に縛られている場合は、負荷の一部をシフトしてメモリを活用して計算を減らすか、その逆を試みるかもしれません。

00:03:26.000 -> 00:03:29.000
ボトルネックはスケールアップすると変化する可能性があります。

00:03:29.000 -> 00:03:35.000
良い解決策の1つは、MPSやMPSGraphなどのAppleフレームワークを使用することです。

00:03:35.000 -> 00:03:41.000
プリミティブを活用できれば、すべてのコンピューティングカーネルがすべてのハードウェアで最適に動作するようにしました。

00:03:41.000 -> 00:03:50.000
ただし、すべてをMPSに置き換えることはできないため、ワークロードをプロファイリングして理解することが重要です。

00:03:50.000 -> 00:04:02.000
まず、GPUのギャップを最小限に抑えるのに役立つ3つの項目を取り上げます。作業分布の改善、GPUタイムラインのギャップの解消、およびアトミック操作の考慮事項です。

00:04:02.000 -> 00:04:18.000
次に、最初にワークロードの計算グリッド形状とメモリレイアウトの効果を調査し、最後にBlender Cyclesの特定の例を見て、GPUリミッターを最適化する方法を説明します。

00:04:18.000 -> 00:04:22.000
GPUのギャップを最小限に抑えることに焦点を当てることから始めます。

00:04:22.000 -> 00:04:32.000
この種のスケーリングは、ハードウェアがアイドル状態のGPUタイムラインにギャップがあり、GPUが完全に利用されていない結果である可能性があります。

00:04:32.000 -> 00:04:37.000
仕事の分布を調査することで、スケーリングを改善できるかどうか見てみましょう。

00:04:37.000 -> 00:04:47.000
小さなワークロードは通常、GPU全体を飽和させず、カーネル同期にはコストがかかるため、どちらも適切なスケーリングを防ぐことができます。

00:04:47.000 -> 00:04:55.000
ワークロードがハードウェアにどのようにマッピングされるかを理解することは非常に重要なので、それについて話しましょう。

00:04:55.000 -> 00:05:00.000
ワークロードは、スレッドグループの3Dグリッドの形でディスパッチされます。

00:05:00.000 -> 00:05:12.000
スレッドグループはGPUコアに均一に分散され、サイズが制限されているが、GPUコアにローカルで非常に高速であるスレッドグループメモリにアクセスできます。

00:05:12.000 -> 00:05:21.000
単一のスレッドグループはさらにSIMDグループに分解され、他の計算方言では波やワープとも呼ばれます。

00:05:21.000 -> 00:05:33.000
コンピューティングパイプライン状態オブジェクトの「threadExecutionWidth」をチェックすると、SIMD幅が返され、すべてのApple GPUでは32に等しくなります。

00:05:33.000 -> 00:05:42.000
スレッドグループは、スレッドグループごとに最大1024スレッドを持つことができ、スレッドは最大32Kのスレッドグループメモリを共有できます。

00:05:42.000 -> 00:05:48.000
GPUを忙しく保つために、すべてのGPUコアで十分な作業があるはずです。

00:05:48.000 -> 00:05:51.000
以下は、ディスパッチするグリッドの例です。

00:05:51.000 -> 00:05:59.000
スレッドグループはGPUクラスターにディスパッチされ、GPUコア間で分散されます。

00:05:59.000 -> 00:06:04.000
スレッドグループが少なすぎると、ワークロードはマシンを完全に飽和させません。

00:06:04.000 -> 00:06:08.000
これを修正する方法は次のとおりです。

00:06:08.000 -> 00:06:16.000
ワークロードが生成するスレッドの数を計算し、ディスパッチがマシン全体を飽和させるかどうかを大まかに確認します。

00:06:16.000 -> 00:06:30.000
比較的複雑なカーネルの場合、シェーダーコアあたり1Kから2Kの同時スレッドは非常に良い占有率と考えられているので、経験則としてGPUコアあたり1から2Kのスレッドを取ります。

00:06:30.000 -> 00:06:35.000
これで、ハードウェアを完全に飽和させるのに十分な作業があれば計算できます。

00:06:35.000 -> 00:06:43.000
ここの表は、異なるSOCを飽和させるための最小推奨スレッド数を示しています。

00:06:43.000 -> 00:06:48.000
考慮すべきもう1つのことは、不必要に大きなスレッドグループサイズの使用を避けることです。

00:06:48.000 -> 00:06:54.000
スレッドグループを小さくすると、ハードウェアへの負荷がより均一にマッピングされます。

00:06:54.000 -> 00:07:02.000
より大きなスレッドグループを使用すると、より均一な分布が妨げられ、GPUコアの不均衡につながる可能性があります。

00:07:02.000 -> 00:07:08.000
ワークロードにうまくマッピングされるSIMD幅の最小倍数を使用するのが最善です。

00:07:08.000 -> 00:07:16.000
より小さなスレッドグループを使用することで、GPUはワークロードのバランスをとる機会が増えます。

00:07:16.000 -> 00:07:23.000
XcodeまたはInstruments GPU Toolsで、常にカーネルランタイムのパフォーマンスを確認してください。

00:07:23.000 -> 00:07:28.000
たとえば、このGPUキャプチャでは、いくつかの計算を実行するカーネルがあります。

00:07:28.000 -> 00:07:32.000
占有率はかなり低く、これは予想外です。

00:07:32.000 -> 00:07:40.000
コンパイラの統計は、Xcode 14で新しい最大理論占有率が100%であることを示しています。

00:07:40.000 -> 00:07:51.000
これは、十分なスレッドがない可能性があることを示しており、実際、アルゴリズムがますます少ないスレッドをディスパッチし始め、マシンを飽和させなくなったことがわかります。

00:07:51.000 -> 00:07:55.000
占有率が低いと、他にもいくつかの原因があるかもしれません。

00:07:55.000 -> 00:08:01.000
すべての詳細を入手するには、MacBook Pro TechトークのMetal Computeを確認してください。

00:08:01.000 -> 00:08:09.000
さて、ワークロードが正しく分散されたので、GPUが常にビジーであることを確認する時が来ました。

00:08:09.000 -> 00:08:18.000
GPUの活用不足は決して理想的なスケーリングにつながることはなく、利用不足の最悪のケースは、それをアイドル状態に保つことです。

00:08:18.000 -> 00:08:23.000
GPUのタイムラインギャップにより、GPUはアイドル状態になる可能性があります。

00:08:23.000 -> 00:08:26.000
この例を考えてみましょう。

00:08:26.000 -> 00:08:34.000
これは、CPUとGPU間の作業シリアル化により、GPUの50%しか使用しないワークロードです。

00:08:34.000 -> 00:08:42.000
この場合、全体的なタスク期間は、重複のないCPUとGPUの作業の合計です。

00:08:42.000 -> 00:08:49.000
GPUコアを2倍にすると、GPUトラックの完了が速くなりますが、CPUトラックは影響を受けません。

00:08:49.000 -> 00:08:57.000
全体的なパフォーマンスは33%しか増加しませんが、理想的なスケーリングとは程遠いです。

00:08:57.000 -> 00:09:08.000
GPUコアが再び2倍にすると、GPUのワークロードはさらに速くなりますが、全体的なレイテンシは元の時間と比較してわずか60%削減されます!

00:09:08.000 -> 00:09:13.000
したがって、GPUコアのスケーリングは、そのような場合にリターンの減少をもたらします。

00:09:13.000 -> 00:09:17.000
これは理想とは程遠い。直そう！

00:09:17.000 -> 00:09:28.000
M1 proからのこのインストゥルメントトレースは、大きなGPUタイムラインギャップを示しており、これは明らかに適切なスケーリングを防ぎます。

00:09:28.000 -> 00:09:38.000
M1 Ultraでは、同じワークロードは確かに少し高速ですが、GPUのアイドル時間が高くなり、ワークロードはうまくスケーリングされていません。

00:09:38.000 -> 00:09:45.000
大きなギャップは、コマンドバッファのwaitUntilCompletedを使用したCPU同期によって引き起こされます。

00:09:45.000 -> 00:09:54.000
待機ロジックを変更し、シリアル化を削除した後、GPUは完全に利用され、これは素晴らしいことです。

00:09:54.000 -> 00:10:03.000
前後のワークロードスケーリングを比較すると、スケーリングが理想的なスケーリングにはるかに近づいたと述べることができます。

00:10:03.000 -> 00:10:15.000
前の例では、CPU/GPUの同期を完全に削除することができましたが、アプリケーションの性質上、必ずしもそうとは限りません。

00:10:15.000 -> 00:10:20.000
アイドル時間を短縮するために取ることができる他のアプローチがあります。

00:10:20.000 -> 00:10:30.000
MTLSharedEventsを使用して、CPUに信号を送り、より多くの作業をパイプライン化し、GPU駆動エンコーディングの使用を検討し、同時ディスパッチを使用します。

00:10:30.000 -> 00:10:35.000
それでは、GPUのタイムラインギャップを最小限に抑えるためのこれらのアプローチについて話し合いましょう。

00:10:35.000 -> 00:10:39.000
そのうちのいくつかはあなたのワークフローに合うかもしれません。

00:10:39.000 -> 00:10:44.000
CPUでGPUの完成を待つと、理想的なスケーリングにつながりません。

00:10:44.000 -> 00:10:51.000
アプリケーションがWaitUntilCompletedを使用している場合は、代わりにMTLSharedEventsを使用してみてください。

00:10:51.000 -> 00:10:57.000
MTLSharedEventsはオーバーヘッドが低く、タイムラインのギャップを減らすのに役立ちます。

00:10:57.000 -> 00:11:02.000
次に考慮すべきことは、ワークロードをパイプライン化することです。

00:11:02.000 -> 00:11:13.000
アルゴリズムに次のバッチの作業に必要なデータがある場合は、MTLSharedEventsを待つ前に、事前に1つ以上のバッチをエンコードすることができます。

00:11:13.000 -> 00:11:19.000
そうすることで、GPUは消耗することはなく、常に処理作業が必要です。

00:11:19.000 -> 00:11:26.000
同じキューで作業を事前にエンコードできない場合は、2番目のキューを使用して作業を重ねることを検討してください。

00:11:26.000 -> 00:11:35.000
複数のキューを使用すると、独立した作業を送信でき、イベントを待っているときに他の送信スレッドを停止することはありません。

00:11:35.000 -> 00:11:41.000
このようにして、GPUは作業を受信して処理し続ける機会があります。

00:11:41.000 -> 00:11:47.000
場合によっては、アルゴリズムはGPUから直接作業をエンコードすることができます。

00:11:47.000 -> 00:11:56.000
間接コマンドバッファを使用すると、次のバッチのエンコーディングをGPUに直接移動できるため、同期の必要がなくなります。

00:11:56.000 -> 00:12:02.000
間接コマンドバッファの詳細については、「Metalによるモダンレンダリング」をチェックしてください。

00:12:02.000 -> 00:12:09.000
ワークロードは、CPUとGPU間の高価な同期を可能な限り削除または最小限に抑えるようになりました。

00:12:09.000 -> 00:12:15.000
しかし、GPUのタイムラインが忙しくても、スケーリングの課題はまだ存在するかもしれません。

00:12:15.000 -> 00:12:17.000
調査しましょう。

00:12:17.000 -> 00:12:23.000
このグラフは、画像が一度に1フレームで処理される画像処理ワークロードからのものです。

00:12:23.000 -> 00:12:29.000
多くのバックツーバックコンピューティングシリアルディスパッチもスケーリングを制限することができます。

00:12:29.000 -> 00:12:41.000
GPUはビジーですが、カーネルの同期にはコストがかかり、さらに、すべてのディスパッチにはスレッドグループが分散され、コアがまだ飽和していない小さなランプアップがあります。

00:12:41.000 -> 00:12:49.000
同様に、スレッドグループが終了して引退すると、コアを完全に飽和させるのに十分な作業がない可能性があります。

00:12:49.000 -> 00:12:54.000
このような状況では、アドバイスは、可能な限り独立した作業を重ねることです。

00:12:54.000 -> 00:12:56.000
視覚的な例を見てみましょう。

00:12:56.000 -> 00:13:00.000
ここでは、2つの画像を次々に処理するワークロードがあります。

00:13:00.000 -> 00:13:04.000
通常、カーネルは互いに同期する必要があります。

00:13:04.000 -> 00:13:07.000
しかし、これは仕事をスケジュールする唯一の方法ではありません。

00:13:07.000 -> 00:13:13.000
同時ディスパッチを使用して、2つの画像の独立した作業をインターリーブできます。

00:13:13.000 -> 00:13:19.000
ここでは、同時派遣のおかげで、ドライバーは異なる作業をインターリーブすることができます。

00:13:19.000 -> 00:13:27.000
以前は背中合わせだった2つのカーネルが、いくつかの独立した作業で分離されていることがわかります。

00:13:27.000 -> 00:13:33.000
ただし、MTLDispatchTypeConcurrentを使用する場合は、手動でバリアを入れる必要があります。

00:13:33.000 -> 00:13:47.000
同時ディスパッチにより、ドライバーは作業をより緊密にパックし、依存するカーネル間の同期コストのほとんどを隠し、さまざまなカーネルのランプアップとテールエンドを埋めることができます。

00:13:47.000 -> 00:13:55.000
この最適化により、M1 MaxからM1 Ultraに移行する際のワークロードのパフォーマンスとスケーリングが大幅に改善されました。

00:13:55.000 -> 00:14:07.000
ワークロードは、以前のスケーリングと比較して、2つの画像がインターリーブされた状態で30%速く、3つの画像が並行して70%速くなります。

00:14:07.000 -> 00:14:11.000
カーネルが行っている原子操作を慎重に検討することが重要です。

00:14:11.000 -> 00:14:15.000
それが最も効率的な方法で作られていることを確認しましょう。

00:14:15.000 -> 00:14:22.000
アトミック操作により、複数のスレッドからデータを安全な方法で読み書きできます。

00:14:22.000 -> 00:14:26.000
グローバルアトミックは、GPU全体で一貫しています。

00:14:26.000 -> 00:14:32.000
多くのスレッドが同じグローバル値を読み書きしようとすると、これは競合につながります。

00:14:32.000 -> 00:14:38.000
GPUコアの数を増やすことは役に立たず、実際にはより多くの競合につながります。

00:14:38.000 -> 00:14:45.000
アルゴリズムのアトミック動作を改善する方法を例で調べてみましょう。

00:14:45.000 -> 00:14:51.000
これは、バッファ内のすべての値が合計される削減アルゴリズムです。

00:14:51.000 -> 00:14:57.000
最も簡単な方法は、メインメモリのスレッドごとにアトミック追加操作を実行することです。

00:14:57.000 -> 00:15:09.000
しかし、メインメモリの単一の値に大きな圧力をかけ、各メモリ書き込みを効果的にシリアル化するため、これは理想的ではありません。

00:15:09.000 -> 00:15:18.000
ハードウェアがアトミックメモリの競合を支援するために提供するものは2つあります。Simbdグループ命令とスレッドグループアトミックです。

00:15:18.000 -> 00:15:31.000
Prefix_exlusive sumやsimd_minなどのSIMD命令により、メモリへの往復なしでSIMDグループ内のレジスタ間で操作とメモリ交換を行うことができます。

00:15:31.000 -> 00:15:35.000
スレッドグループアトミックは、スレッドグループメモリによって満たされます。

00:15:35.000 -> 00:15:42.000
各GPUコアには独自のスレッドグループメモリがあり、GPUコアの数に合わせて拡張できます。

00:15:42.000 -> 00:15:48.000
これら2つの機能がワークロードの改善にどのように役立つか見てみましょう。

00:15:48.000 -> 00:15:56.000
ここでは同じ削減問題がありますが、今回はSIMDグループ命令、包括的なメモリ合計を使用し始めます。

00:15:56.000 -> 00:16:03.000
このような操作は、最後のスレッドのSIMDグループ内のすべての数字の合計を残します。

00:16:03.000 -> 00:16:14.000
各SIMDグループの最後のスレッドは、スレッドグループメモリで単一のアトミック追加を実行して、すべてのSIMDグループをスレッドグループメモリ内の単一の値に減らすことができます。

00:16:14.000 -> 00:16:24.000
このようにして、SIMDグループ命令とスレッドグループメモリを使用して、メインメモリにまったく触れることなくスレッドグループ全体が削減されました。

00:16:24.000 -> 00:16:29.000
各グループは、独立して並行して削減することができます。

00:16:29.000 -> 00:16:37.000
各スレッドグループが1つの値に縮小されたので、スレッドグループごとに1つのスレッドがメインメモリで1つのアトミックを実行できます。

00:16:37.000 -> 00:16:50.000
これは、スレッドグループごとに1つのアトミックのみを必要とするだけでなく、スレッドグループは異なる時間に完了するため、時間の経過とともにアトミックを散乱させ、メモリの競合をさらに減らします。

00:16:50.000 -> 00:17:02.000
要約すると、アトミックの有効性を最大化するために、メモリ局所性を活用し、SIMDグループ操作を使用し、スレッドグループメモリアトミックを活用してみてください。

00:17:02.000 -> 00:17:08.000
これらはすべて、スケーリングを防ぐ原子圧の低減に大きく役立つはずです。

00:17:08.000 -> 00:17:15.000
GPUのギャップが修正されたので、スケーリングが理想に近いかどうかを確認する時が来ました。

00:17:15.000 -> 00:17:25.000
XcodeとMetal System TraceのGPUリミッターは、GPUコア実行パイプラインのボトルネックや非効率性を最適化するのに役立ちます。

00:17:25.000 -> 00:17:36.000
たとえば、非効率的なメモリアクセスパターンは、常に高い最終レベルのキャッシュまたはメモリ管理ユニット、またはMMUリミッター、およびかなり低い使用率を引き起こします。

00:17:36.000 -> 00:17:43.000
最初に対処するのは、スレッドグループとメモリレイアウトを調整する方法です。

00:17:43.000 -> 00:17:54.000
メモリスパンと発散を減らすための鍵は、空間的にも時間的にもワークロードメモリアクセスパターンを明確に理解することです。

00:17:54.000 -> 00:18:09.000
それが理解されたら、2つの可能なチューニング方向があります。データアクセスの局所性を改善するためにデータレイアウトを再構成するか、アクセスパターンを調整してデータレイアウトによりよく一致し、メモリとキャッシュの局所性を改善します。

00:18:09.000 -> 00:18:12.000
例を見てみましょう。

00:18:12.000 -> 00:18:18.000
ここでは、データが次々に水平にレイアウトされるメモリバッファです。

00:18:18.000 -> 00:18:29.000
しかし、コンピューティングカーネルがディスパッチされると、正方形のスレッドグループが分散される2Dのようなパターンを持つことが一般的であり、これは非常に空間的にローカライズされています。

00:18:29.000 -> 00:18:36.000
このアクセスパターンとデータレイアウトは、データの局所性には最適ではありません。

00:18:36.000 -> 00:18:42.000
たとえば、最初のSIMDグループがデータにアクセスすると、リクエストはキャッシュ行にパックされます。

00:18:42.000 -> 00:18:50.000
ほとんどのキャッシュ行は使用されませんが、キャッシュ内のスペースを占有します。

00:18:50.000 -> 00:19:01.000
たとえば、行全体にまたがるのではなく、ストライプにローカライズされるなど、アクセスパターンに合わせてデータを再配置します。

00:19:01.000 -> 00:19:12.000
この新しいメモリレイアウトにより、スレッドグループはキャッシュラインで要求されるデータのほとんどを利用し、発散を減らし、キャッシュ効率を向上させることができます。

00:19:12.000 -> 00:19:19.000
もう1つのオプションは、現在のデータレイアウトによりよく合うように3Dグリッドのディスパッチ方法を変更することです。

00:19:19.000 -> 00:19:29.000
スレッドグループサイズで遊んで、たとえば、より長方形の形状など、メモリレイアウトによりよくマップされるグループを作成してみてください。

00:19:29.000 -> 00:19:37.000
この場合、アクセスパターンはメモリレイアウトと一致し、はるかに高いキャッシュ効率を提供します。

00:19:37.000 -> 00:19:41.000
ワークロードに最適なものを見つけるために実験する必要があるかもしれません。

00:19:41.000 -> 00:19:54.000
時には、トレードオフをしたり、メモリの局所性のためにスレッドの発散を犠牲にしたり、その逆に、データレイアウト、グリッドディスパッチ、またはそれらすべての組み合わせを変更する必要があります。

00:19:54.000 -> 00:20:00.000
すべてのワークロードとアクセスパターンは異なります。

00:20:00.000 -> 00:20:08.000
記憶の局所性を改善する方法を認識したので、Blender Cyclesでより具体的な例を見てみましょう。

00:20:08.000 -> 00:20:13.000
サイクルは、生産レンダリングのためのBlenderの物理ベースのパストレーサーです。

00:20:13.000 -> 00:20:24.000
これは、生産ニーズのための芸術的な制御と柔軟なシェーディングノードで、箱から出して物理的にに基づいた結果を提供するように設計されています。

00:20:24.000 -> 00:20:36.000
このインストゥルメントトレースは、低読み取り帯域幅、高トップGPUリミッター、高キャッシュリミッター、低ラストレベルキャッシュ使用率を明確に示しています。

00:20:36.000 -> 00:20:42.000
帯域幅とMMUリミッターをコントロールし続けることは、スケーリングにとって重要です。

00:20:42.000 -> 00:20:50.000
トップリミッターがラストレベルキャッシュまたはMMUの場合、メモリスパンを短縮し、データの局所性を最大化する必要があります。

00:20:50.000 -> 00:20:53.000
例を見てみましょう。

00:20:53.000 -> 00:20:57.000
サイクルは、発散を減らすためにデータのソートを使用します。

00:20:57.000 -> 00:21:01.000
それは、レイヒットを材料の種類で並べ替えることによってそれを行います。

00:21:01.000 -> 00:21:11.000
これはスレッドの発散を減らすのに最適ですが、空間メモリの発散を増加させ、高いMMUリミッターになります。

00:21:11.000 -> 00:21:17.000
これを助けるために、データの局所性を高めるためにソートする前にメモリ範囲を分割する実験をしました。

00:21:17.000 -> 00:21:18.000
それを視覚化しましょう。

00:21:18.000 -> 00:21:28.000
光の移動をシミュレートするために光線がシーンに撮影されると、それらはオブジェクトにヒットし、データはバッファに収集されます。

00:21:28.000 -> 00:21:39.000
交差点の点で、私たちは多くのことを知っています - ガラス、金属など、ヒットした材料の種類、交差点の位置、光線など。

00:21:39.000 -> 00:21:43.000
簡単にするために、材料の種類だけに焦点を当てましょう。

00:21:43.000 -> 00:21:47.000
これはメモリ内のバッファ内の材料です。

00:21:47.000 -> 00:21:53.000
レイヒットごとに多くのデータが収集されるため、メモリバッファはかなり大きくなる可能性があります。

00:21:53.000 -> 00:22:00.000
多くのメモリを移動しないように、インデックスのリストを入力し、代わりにそれらを並べ替えます。

00:22:00.000 -> 00:22:06.000
ソート後、同じ材料タイプのインデックスが一緒に閉じてパックされるようになりました。

00:22:06.000 -> 00:22:11.000
SIMDグループは、インデックスの読み込みを開始し、材料を処理できます。

00:22:11.000 -> 00:22:18.000
SIMDグループは、インデックスを使用して、対応するデータを元のバッファにロードします。

00:22:18.000 -> 00:22:25.000
しかし、SIMDグループはメモリスパン全体にわたって読み取り、MMUに圧力をかけます。

00:22:25.000 -> 00:22:28.000
新しいアプローチを調査しましょう。

00:22:28.000 -> 00:22:36.000
メモリ範囲は、単に異なるパーティションからのインデックスをミックスさせない理想的なパーティションで分割されます。

00:22:36.000 -> 00:22:46.000
ソートするとき、アクセスされたデータの範囲は、以前のように完全なメモリ範囲にまたがるのではなく、パーティション内に含まれていることは明らかです。

00:22:46.000 -> 00:22:52.000
これは、スレッド発散とメモリ発散の間のトレードオフとバランスです。

00:22:52.000 -> 00:22:58.000
理想的なパーティションの数とサイズは、ワークロードに大きく依存します。

00:22:58.000 -> 00:23:02.000
どれが一番うまくいくかを試す必要があるかもしれません。

00:23:02.000 -> 00:23:07.000
別のメタルシステムトレースを取り、ワークロードが改善されたかどうかを見てみましょう。

00:23:07.000 -> 00:23:11.000
ここでは、最適化されたバージョンのリミッターと使用率を確認します。

00:23:11.000 -> 00:23:17.000
トップパフォーマンスリミッターとラストレベルキャッシュリミッターがダウンしました。

00:23:17.000 -> 00:23:22.000
その結果、帯域幅とシェーダーのランタイムが大幅に改善されました。

00:23:22.000 -> 00:23:24.000
いくらか見てみましょう。

00:23:24.000 -> 00:23:29.000
トップリミッターとLLCリミッターは約20%減少しました。

00:23:29.000 -> 00:23:32.000
これは、データフローがより効率的であることを意味します。

00:23:32.000 -> 00:23:39.000
GPUの読み取り帯域幅が大幅に増加し、より多くのデータをGPUコアにプッシュできるようになりました。

00:23:39.000 -> 00:23:48.000
全体として、この実験でメモリの局所性を増やすと、シーンに応じてパフォーマンスが10〜30%向上しました。

00:23:48.000 -> 00:23:54.000
これは、メモリアクセスパターンを改善しようとする多くの方法のほんの一例でした。

00:23:54.000 -> 00:23:58.000
トップパフォーマンスリミッターの実験と最適化を続けてください。

00:23:58.000 -> 00:24:03.000
GPUツールには、調整するためのより便利なカウンターがあります。

00:24:03.000 -> 00:24:08.000
Xcodeは、コンパイラ統計ウィンドウに新しい理論的な占有率を持っています。

00:24:08.000 -> 00:24:24.000
XcodeとInstrumentsの両方に、いくつかのMMU関連のリミッターとカウンター、特に新しいMMUリミッター、MMU利用カウンター、およびMMU TLBミスレートカウンターがあります。

00:24:24.000 -> 00:24:27.000
今日は多くの分野をカバーしました。

00:24:27.000 -> 00:24:36.000
GPUのスケーラビリティと、スケールアップ時にボトルネックがどのように変化するか、そしてツールがスケーラビリティの問題を見つけて修正するのにどのように役立つかについて話し合いました。

00:24:36.000 -> 00:24:43.000
また、アプリケーションに最適な結果を得るために、どのように実験し、トレードオフを行う必要があるかについても議論しました。

00:24:43.000 -> 00:24:48.000
あなたのすべての素晴らしいアプリがAppleシリコンで驚くほどうまくスケールするのを見るのを楽しみにしています。

00:24:48.000 -> 23:59:59.000
ご覧いただきありがとうございます。

