WEBVTT

00:00:00.000 --> 00:00:03.000
♪まろやかなインストゥルメンタルヒップホップ音楽♪

00:00:03.000 --> 00:00:09.000
♪

00:00:09.000 --> 00:00:11.000
こんにちは！私の名前はアダム・ブラッドフォードです。

00:00:11.000 --> 00:00:18.000
私はVisionKitチームのエンジニアであり、あなたのアプリにライブテキストを追加しようとしているなら、あなたは正しい場所にいます。

00:00:18.000 --> 00:00:21.000
しかし、まず、ライブテキストとは何ですか?

00:00:21.000 --> 00:00:39.000
ライブテキストは、画像を分析し、テキストの選択とコピー、ルックアップや翻訳などのアクションの実行、アドレスのマッピング、番号のダイヤル、URLへのジャンプなどのデータ検出ワークフローの提供など、ユーザーがコンテンツと対話するための機能を提供します。

00:00:39.000 --> 00:00:43.000
ライブテキストは、QRコードのインタラクションも可能にします。

00:00:43.000 --> 00:00:45.000
これをアプリでどのように使用できるか想像してみてください。

00:00:45.000 --> 00:00:49.000
もっと知りたいですか？さて、あなたは正しい場所にいます。

00:00:49.000 --> 00:00:54.000
このセッションでは、Live Text APIの一般的な概要から始めます。

00:00:54.000 --> 00:01:00.000
次に、既存のアプリケーションでこのAPIを実装する方法を探ります。

00:01:00.000 --> 00:01:06.000
次に、アプリにライブテキストを追加する際に役立つヒントやコツをいくつか紹介します。

00:01:06.000 --> 00:01:10.000
では、Live Text APIの概要を説明します。

00:01:10.000 --> 00:01:14.000
高いレベルでは、Live Text APIはSwiftで利用できます。

00:01:14.000 --> 00:01:20.000
静止画像で美しく機能し、一時停止したビデオフレームに使用するように適応させることができます。

00:01:20.000 --> 00:01:28.000
テキストやQRコードなどのアイテムを検索するためにライブカメラストリームでビデオを分析する必要がある場合、VisionKitにはデータスキャナもあります。

00:01:28.000 --> 00:01:33.000
詳細については、同僚のロンからのこのセッションをチェックしてください。

00:01:33.000 --> 00:01:44.000
Live Text APIは、Apple Neural Engineを搭載したデバイス、およびmacOS 13をサポートするすべてのデバイスでiOS 16から利用できます。

00:01:44.000 --> 00:01:46.000
それは4つの主要なクラスで構成されています。

00:01:46.000 --> 00:01:49.000
それを使用するには、まず画像が必要です。

00:01:49.000 --> 00:01:54.000
その後、この画像はImageAnalyzerに入力され、非同期分析を実行します。

00:01:54.000 --> 00:02:05.000
分析が完了すると、プラットフォームに応じて、結果のImageAnalysisオブジェクトがImageAnalysisInteractionまたはImageAnalysisOverlayViewのいずれかに提供されます。

00:02:05.000 --> 00:02:08.000
今のところかなり簡単そうですよね?

00:02:08.000 --> 00:02:13.000
次に、既存のアプリケーションに追加する方法を実演します。

00:02:13.000 --> 00:02:16.000
そして、これが私たちのアプリケーションです。

00:02:16.000 --> 00:02:21.000
これはシンプルな画像ビューアで、スクロールビュー内に画像ビューがあります。

00:02:21.000 --> 00:02:24.000
注意してください、私はズームとパンの両方をすることができます。

00:02:24.000 --> 00:02:30.000
しかし、私は試してみてください、私はこのテキストのいずれかを選択するか、これらのデータ検出器のいずれかをアクティブにすることはできません。

00:02:30.000 --> 00:02:33.000
これは単純にはしません。

00:02:33.000 --> 00:02:35.000
これがXcodeのプロジェクトです。

00:02:35.000 --> 00:02:40.000
このアプリケーションにライブテキストを追加するには、ビューコントローラーのサブクラスを変更します。

00:02:40.000 --> 00:02:48.000
まず、ImageAnalyzerとImageAnalysisInteractionが必要です。

00:02:48.000 --> 00:02:54.000
ここでは、viewDidLoadをオーバーライドし、インタラクションをイメージビューに追加しているだけです。

00:02:54.000 --> 00:03:01.000
次に、いつ分析を実行するかを知る必要があります。

00:03:01.000 --> 00:03:10.000
新しい画像が設定されると、最初に古い画像の優先InteractionTypesと分析をリセットすることに注意してください。

00:03:10.000 --> 00:03:13.000
これで、すべてが新しい分析の準備が整いました。

00:03:13.000 --> 00:03:23.000
次に、使用する機能を作成し、画像が存在することを確認します。 画像が存在することを確認します。

00:03:23.000 --> 00:03:28.000
もしそうなら、タスクを作成してください。

00:03:28.000 --> 00:03:34.000
次に、アナライザに何を探すべきかを伝えるために設定を作成します。

00:03:34.000 --> 00:03:39.000
この場合、私はテキストと機械可読コードで行きます。

00:03:39.000 --> 00:03:43.000
分析を生成するとスローされる可能性があるので、必要に応じて処理してください。

00:03:43.000 --> 00:03:51.000
そして今最後に、分析プロセスを開始するメソッドanalysisImageWithConfigurationを呼び出す準備が整いました。

00:03:51.000 --> 00:04:05.000
分析が完了すると、不確定な時間が経過し、アプリケーションの状態が変更された可能性があるため、分析が成功し、表示された画像が変更されていないことを確認します。

00:04:05.000 --> 00:04:12.000
これらのチェックがすべて合格した場合、インタラクションの分析を設定し、優先インタラクションタイプを設定するだけです。

00:04:12.000 --> 00:04:17.000
ここでは.automaticを使用していますが、デフォルトのシステム動作が表示されます。

00:04:17.000 --> 00:04:20.000
これはテストの準備ができていると思います。

00:04:20.000 --> 00:04:22.000
ああ、それを見て！

00:04:22.000 --> 00:04:28.000
ライブテキストボタンが表示されたので、はい、テキストを選択できるようになりました。

00:04:28.000 --> 00:04:39.000
これらのインターフェイス要素が自動的に配置される方法に注目し、画像の境界と可視領域の両方の内部にそれらの位置を保ち、私の側での作業はありません。

00:04:39.000 --> 00:04:48.000
OK、ライブテキストボタンをタップすると、選択可能な項目を強調表示し、データ検出器に下線を引いて、クイックアクションが表示されることに注意してください。

00:04:48.000 --> 00:04:54.000
このクイックアクションを簡単にタップして電話をかけることができ、長押しでより多くのオプションを見ることができます。

00:04:54.000 --> 00:04:58.000
あなたは認めなければなりません、これはかなりクールです。

00:04:58.000 --> 00:05:04.000
この数行のコードだけで、私は普通の画像を撮影し、それを生き生きとさせました。

00:05:04.000 --> 00:05:14.000
このシンプルなアプリケーションは、画像上のテキストの選択、データ検出器、QRコードの有効化、検索、テキストの翻訳などできるようになりました。

00:05:14.000 --> 00:05:18.000
私に言ってくれれば、この数行のコードだけではあまりみすぼらしくありません。

00:05:18.000 --> 00:05:26.000
そして、ライブテキストを実装する方法を見たので、採用に役立つヒントやコツをいくつか紹介します。

00:05:26.000 --> 00:05:29.000
インタラクションの種類を探ることから始めます。

00:05:29.000 --> 00:05:36.000
ほとんどの開発者は、テキスト選択を提供する.automaticを望みますが、ライブテキストボタンがアクティブな場合はデータ検出器も強調表示します。

00:05:36.000 --> 00:05:43.000
これにより、該当する検出されたアイテムの下に線が引かれ、ワンタップアクセスでそれらをアクティブにすることができます。

00:05:43.000 --> 00:05:48.000
これは、組み込みアプリケーションから見られるのとまったく同じ動作です。

00:05:48.000 --> 00:05:59.000
アプリがデータ検出器なしでテキスト選択のみを持つことが理にかなっている場合は、タイプを.textSelectionに設定することができ、ライブテキストボタンの状態では変更されません。

00:05:59.000 --> 00:06:06.000
ただし、アプリがテキスト選択なしでデータ検出器のみを持つことが理にかなっている場合は、タイプを.dataDetectorsに設定します。

00:06:06.000 --> 00:06:17.000
このモードでは、選択が無効になっているため、ライブテキストボタンは表示されませんが、データ検出器には下線が引かれ、ワンタップアクセスの準備が整うことに注意してください。

00:06:17.000 --> 00:06:21.000
preferredInteractionTypesを空のセットに設定すると、インタラクションが無効になります。

00:06:21.000 --> 00:06:31.000
また、最後のメモは、テキスト選択または自動モードで、長押しでデータ検出器をアクティブにできることがわかります。

00:06:31.000 --> 00:06:39.000
これは、デフォルトであるtrueに設定するとアクティブになるallowLongPressForDataDetectorsIn TextModeプロパティによって制御されます。

00:06:39.000 --> 00:06:43.000
必要に応じて、これを無効にするには、falseに設定するだけです。

00:06:43.000 --> 00:06:49.000
少し時間を取って、補足インターフェースとして総称される下部にあるこれらのボタンについて話したいと思います。

00:06:49.000 --> 00:06:56.000
これは、通常は右下隅にあるライブテキストボタンと、左下に表示されるクイックアクションで構成されています。

00:06:56.000 --> 00:07:02.000
クイックアクションは、分析からのデータ検出器を表し、ライブテキストボタンがアクティブなときに表示されます。

00:07:02.000 --> 00:07:07.000
サイズ、位置、および可視性は、インタラクションによって制御されます。

00:07:07.000 --> 00:07:16.000
また、デフォルトの位置と外観はシステムと一致しますが、アプリにはカスタムインターフェイス要素があり、異なるフォントやシンボルの重みを妨害または利用する可能性があります。

00:07:16.000 --> 00:07:20.000
このインターフェースをカスタマイズする方法を見てみましょう。 

00:07:20.000 --> 00:07:23.000
まず、isSupplementary InterfaceHiddenプロパティ。

00:07:23.000 --> 00:07:37.000
アプリでテキストを選択できるようにしたいが、ライブテキストボタンを表示したくない場合、SupplementaryInterfaceHiddenをtrueに設定すると、ライブテキストボタンやクイックアクションは表示されません。

00:07:37.000 --> 00:07:40.000
また、コンテンツ挿入プロパティも利用できます。

00:07:40.000 --> 00:07:52.000
補足インターフェイスと重なるインターフェイス要素がある場合は、コンテンツインセットを調整して、ライブテキストボタンとクイックアクションが表示されたときに既存のアプリコンテンツにうまく適応させることができます。

00:07:52.000 --> 00:08:04.000
アプリがインターフェイスを採用したいカスタムフォントを使用している場合、補足InterfaceFontを設定すると、ライブテキストボタンとクイックアクションがテキストに指定されたフォントを使用し、シンボルにフォントの重みを使用します。

00:08:04.000 --> 00:08:10.000
ボタンのサイズの一貫性のために、ライブテキストはポイントサイズを無視することに注意してください。

00:08:10.000 --> 00:08:18.000
しばらくギアを切り替えると、UIImageviewを使用していない場合は、ハイライトが画像と一致しないことに気付くかもしれません。

00:08:18.000 --> 00:08:26.000
これは、UIImageViewを使用すると、VisionKitがContentModeプロパティを使用してContentRectを自動的に計算できるためです。

00:08:26.000 --> 00:08:36.000
ここでは、インタラクションのビューには、画像コンテンツよりも大きな境界がありますが、単位の長方形であるデフォルトのコンテンツ矩形を使用しています。

00:08:36.000 --> 00:08:49.000
これは、デリゲートメソッドcontentsRectForInteractionを実装し、これを修正するために画像コンテンツがインタラクションの境界にどのように関連しているかを説明する単位座標空間に長方形を返すことで簡単に解決できます。

00:08:49.000 --> 00:08:59.000
たとえば、これらの値で長方形を返すと問題が修正されますが、アプリの現在のコンテンツとレイアウトに基づいて正しい正規化された長方形を返してください。

00:08:59.000 --> 00:09:15.000
contentsRectForInteractionは、インタラクションの境界が変更されるたびに呼び出されますが、contentsRectが変更されたが、インタラクションの境界が変更されていない場合は、setContentsRectNeedsUpdate()を呼び出してインタラクションの更新を依頼できます。

00:09:15.000 --> 00:09:21.000
ライブテキストを採用する際のもう1つの質問は、このインタラクションを置くのに最適な場所はどこですか?

00:09:21.000 --> 00:09:27.000
理想的には、ライブテキストインタラクションは、画像コンテンツをホストするビューに直接配置されます。

00:09:27.000 --> 00:09:36.000
前述のように、UIImageViewはあなたのためにcontentsRect計算を処理し、必要ではありませんが、優先されます。

00:09:36.000 --> 00:09:43.000
UIImageviewを使用している場合は、imageViewでインタラクションを設定するだけで、VisionKitが残りを処理します。

00:09:43.000 --> 00:09:58.000
ただし、ImageViewがScrollView内にある場合は、ScrollViewにインタラクションを配置したくなる場合がありますが、これは推奨されず、contentRectが継続的に変更されるため、管理が難しい場合があります。

00:09:58.000 --> 00:10:08.000
ここでの解決策は同じで、倍率が適用されたScrollView内にある場合でも、画像コンテンツをホストするビューにインタラクションを配置します。

00:10:08.000 --> 00:10:16.000
私は少しの間ジェスチャーについて話すつもりです、ライブテキストは、控えめに言っても、ジェスチャーリコグナイザの非常に、非常に豊富なセットを持っています。

00:10:16.000 --> 00:10:25.000
アプリの構造によっては、アプリが実際に処理すべきジェスチャーやイベントに応答するインタラクションが見つかる可能性があり、その逆も同様です。

00:10:25.000 --> 00:10:26.000
パニックにならないでください。

00:10:26.000 --> 00:10:31.000
これらの問題が発生した場合に修正するために使用できるテクニックをいくつか紹介します。

00:10:31.000 --> 00:10:39.000
これを修正する一般的な方法の1つは、デリゲートメソッドのinteractionShouldBeginAtPointFor InteractionTypeを実装することです。

00:10:39.000 --> 00:10:42.000
Falseを返すと、アクションは実行されません。

00:10:42.000 --> 00:10:50.000
始めるのに良い場所は、インタラクションに指定されたポイントにインタラクティブなアイテムがあるかどうか、またはアクティブなテキスト選択があるかどうかを確認することです。

00:10:50.000 --> 00:10:58.000
テキスト選択チェックはここで使用されているため、テキストをタップして選択を解除することができます。

00:10:58.000 --> 00:11:08.000
一方、インタラクションがジェスチャーに反応しないように見える場合は、代わりにジェスチャーを処理しているジェスチャーリコグナイザがアプリにあるからかもしれません。

00:11:08.000 --> 00:11:17.000
この場合、 gestureRecognizer の gestureRecognizerShouldBegin デリゲートメソッドを使用して、同様のソリューションを作成できます。

00:11:17.000 --> 00:11:25.000
ここでは、同様のチェックを実行し、その場所にインタラクティブなアイテムがある場合、またはアクティブなテキスト選択がある場合はfalseを返します。

00:11:25.000 --> 00:11:26.000
余談ですが。

00:11:26.000 --> 00:11:36.000
この例では、まずnilを渡すことで、 gestureRecognizerの位置をウィンドウの座標空間に変換し、それをインタラクションのビューに変換します。

00:11:36.000 --> 00:11:42.000
これは、インタラクションが倍率が適用されたScrollView内にある場合に必要になる場合があります。

00:11:42.000 --> 00:11:46.000
ポイントが一致しない場合は、このテクニックを試してみてください。

00:11:46.000 --> 00:11:52.000
私が役に立つことがわかったもう1つの同様のオプションは、UIViewのhitTest:WithEventをオーバーライドすることです。

00:11:52.000 --> 00:12:00.000
ここで、もう一度、同様の話、私は以前と同じ種類のチェックを実行し、この場合、適切なビューを返します。

00:12:00.000 --> 00:12:12.000
いつものように、私たちはあなたのアプリができるだけレスポンシブであることを望んでおり、Neural Engineは分析を非常に効率的にしますが、最高のパフォーマンスを得るために共有したいImageAnalyzerのヒントがいくつかあります。

00:12:12.000 --> 00:12:16.000
理想的には、アプリで共有したいImageAnalyzerは1つだけです。

00:12:16.000 --> 00:12:19.000
また、いくつかの種類の画像もサポートしています。

00:12:19.000 --> 00:12:29.000
持っているネイティブタイプを渡すことで、常に画像変換を最小限に抑える必要があります。ただし、CVPixelBufferがある場合、それが最も効率的です。

00:12:29.000 --> 00:12:38.000
また、システムリソースを最大限に活用するには、画像が画面に表示される直前または直前に分析を開始する必要があります。

00:12:38.000 --> 00:12:47.000
アプリのコンテンツがスクロールする場合(たとえば、タイムラインがある場合)、スクロールが停止した後にのみ分析を開始します。

00:12:47.000 --> 00:12:56.000
今、このAPIはあなたがライブテキストを見る唯一の場所ではありません、サポートは、あなたのアプリがすでに使用しているかもしれないシステム全体のいくつかのフレームワークで自動的に提供されます。

00:12:56.000 --> 00:13:04.000
たとえば、UITextFieldまたはUITextViewは、キーボード入力にカメラを使用してライブテキストをサポートしています。

00:13:04.000 --> 00:13:08.000
また、ライブテキストはWebKitとクイックルックでもサポートされています。

00:13:08.000 --> 00:13:12.000
詳細については、これらのセッションをチェックしてください。

00:13:12.000 --> 00:13:16.000
今年のiOS 16の新機能は、AVKitにライブテキストサポートを追加しました。

00:13:16.000 --> 00:13:27.000
AVPlayerViewとViewControllerは、デフォルトで有効になっているallowsVideoFrameAnalysisプロパティを介して、一時停止したフレーム内のライブテキストを自動的にサポートします。

00:13:27.000 --> 00:13:32.000
これは、FairPlayで保護されていないコンテンツでのみ利用可能であることに注意してください。

00:13:32.000 --> 00:13:45.000
AVPlayerLayerを使用している場合は、分析とインタラクションを管理する責任がありますが、currentDisplayedPixelBufferプロパティを使用して現在のフレームを取得することは非常に重要です。

00:13:45.000 --> 00:13:49.000
これは、適切なフレームが分析されていることを保証する唯一の方法です。

00:13:49.000 --> 00:13:58.000
これは、ビデオ再生率がゼロの場合にのみ有効な値が返され、これは浅いコピーであり、書くのは絶対に安全ではありません。

00:13:58.000 --> 00:14:04.000
そしてもう一度、FairPlayで保護されていないコンテンツでのみ利用可能です。

00:14:04.000 --> 00:14:07.000
私たちはあなたのアプリにライブテキスト機能をもたらすのを手伝うことに興奮しています。

00:14:07.000 --> 00:14:12.000
ライブテキストチームの全員を代表して、このセッションにご参加いただきありがとうございます。

00:14:12.000 --> 00:14:16.000
アプリの画像にどのように使用するかを見て興奮しています。

00:14:16.000 --> 00:14:18.000
そして、いつものように、楽しんでください!

00:14:18.000 --> 23:59:59.000
♪

