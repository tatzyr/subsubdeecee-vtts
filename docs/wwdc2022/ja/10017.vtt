WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:15.000
チャオ、私の名前はGeppy Parzialeで、アップルの機械学習エンジニアです。

00:00:15.000 --> 00:00:27.000
今日は、機械学習を使用して、通常、非常に専門的な作業を実行するために専門家を必要とする問題を解決するアプリを構築する旅をご案内したいと思います。

00:00:27.000 --> 00:00:37.000
この旅は、あなたのアプリにオープンソースの機械学習モデルを追加し、素晴らしい新しい体験を作成する方法を示す機会を与えてくれます。

00:00:37.000 --> 00:00:49.000
旅の途中で、機械学習を使用してアプリを構築するために、Apple開発エコシステムで利用可能な多くのツール、フレームワーク、APIのいくつかも強調します。

00:00:49.000 --> 00:00:57.000
アプリを構築するとき、開発者であるあなたは、うまくいけばユーザーに最高の体験をもたらす一連の決定を下します。

00:00:57.000 --> 00:01:04.000
また、これはアプリケーションに機械学習機能を追加する場合にも当てはまります。

00:01:04.000 --> 00:01:10.000
開発中、あなたは尋ねることができます:この機能を構築するために機械学習を使用する必要がありますか?

00:01:10.000 --> 00:01:14.000
機械学習モデルを入手するにはどうすればよいですか?

00:01:14.000 --> 00:01:18.000
そのモデルをAppleのプラットフォームと互換性を持たせるにはどうすればよいですか?

00:01:18.000 --> 00:01:22.000
そのモデルは私の特定のユースケースで機能しますか?

00:01:22.000 --> 00:01:26.000
それはApple Neural Engineで動作しますか?

00:01:26.000 --> 00:01:29.000
だから、一緒にこの旅をしましょう。

00:01:29.000 --> 00:01:39.000
地下室の古い箱で見つけた家族の白黒写真にリアルな色を追加できるアプリを作りたいです。

00:01:39.000 --> 00:01:47.000
もちろん、プロの写真家は、写真編集ツールでいくつかの時間を費やす、いくつかの手作業でこれを行うことができます。

00:01:47.000 --> 00:01:54.000
代わりに、このプロセスを自動化し、ほんの数秒で着色を適用したい場合はどうなりますか?

00:01:54.000 --> 00:01:58.000
これは機械学習にとって完璧な作業のようです。

00:01:58.000 --> 00:02:06.000
Appleは、アプリでML機能を構築して統合するのに役立つ膨大な量のフレームワークとツールを提供しています。

00:02:06.000 --> 00:02:12.000
データ処理からモデルトレーニングや推論まで、あらゆるものを提供します。

00:02:12.000 --> 00:02:15.000
この旅のために、私はそれらのいくつかを使うつもりです。

00:02:15.000 --> 00:02:22.000
しかし、開発している特定の機械学習タスクに応じて、多くの選択肢があることを覚えておいてください。

00:02:22.000 --> 00:02:29.000
アプリで機械学習機能を開発するときに使用するプロセスは、一連の段階を経ます。

00:02:29.000 --> 00:02:38.000
まず、科学出版物または専門的なウェブサイトで適切な機械学習モデルを検索します。

00:02:38.000 --> 00:02:46.000
私は写真の着色を検索し、私が必要とするもののために働くかもしれないカラライザーと呼ばれるモデルを見つけました。

00:02:46.000 --> 00:02:53.000
以下は、このモデルを使用して取得できる着色の例です。

00:02:53.000 --> 00:02:56.000
ここに別のものがあります。

00:02:56.000 --> 00:03:00.000
そして、ここに別のものがあります。本当に素晴らしい。

00:03:00.000 --> 00:03:03.000
それがどのように機能するかをお見せしましょう。

00:03:03.000 --> 00:03:07.000
カラライザーモデルは、入力として白黒画像を期待しています。

00:03:07.000 --> 00:03:15.000
私が見つけたPythonのソースコードは、任意のRGB画像をLAB色空間画像に変換します。

00:03:15.000 --> 00:03:25.000
この色空間には3つのチャンネルがあります。1つは画像の明度またはLチャンネルを表し、他の2つは色の構成要素を表します。

00:03:25.000 --> 00:03:32.000
軽さがカラライザーモデルの入力になる間、カラーコンポーネントは破棄されます。

00:03:32.000 --> 00:03:41.000
次に、モデルは、入力Lチャネルと組み合わせて、結果の画像に色を提供する2つの新しいカラーコンポーネントを推定します。

00:03:41.000 --> 00:03:45.000
今、このモデルを私のアプリと互換性を持たせる時が来ました。

00:03:45.000 --> 00:03:53.000
これを達成するために、coremltoolsを使用して元のPyTorchモデルをCore ML形式に変換できます。

00:03:53.000 --> 00:03:59.000
これは、PyTorchモデルをCore MLに変換するために使用したシンプルなPythonスクリプトです。

00:03:59.000 --> 00:04:04.000
まず、PyTorchモデルのアーキテクチャと重みをインポートします。

00:04:04.000 --> 00:04:07.000
次に、インポートされたモデルを追跡します。

00:04:07.000 --> 00:04:12.000
最後に、PyTorchモデルをCore MLに変換して保存します。

00:04:12.000 --> 00:04:19.000
モデルがCore ML形式になったら、変換が正しく機能したことを確認する必要があります。

00:04:19.000 --> 00:04:23.000
私はcoremltoolsを使って再びPythonで直接それを行うことができます。

00:04:23.000 --> 00:04:25.000
そして、これは簡単です。

00:04:25.000 --> 00:04:32.000
画像をRGB色空間にインポートし、ラボ色空間に変換します。

00:04:32.000 --> 00:04:38.000
次に、カラーチャンネルから明度を分離し、それらを破棄します。

00:04:38.000 --> 00:04:42.000
Core MLモデルを使用して予測を実行します。

00:04:42.000 --> 00:04:49.000
そして最後に、推定されたカラーコンポーネントで入力の明度を構成し、RGBに変換します。

00:04:49.000 --> 00:04:57.000
これにより、変換されたモデルの機能が元のPyTorchモデルの機能と一致することを確認できます。

00:04:57.000 --> 00:05:01.000
私はこの段階をモデル検証と呼んでいます。

00:05:01.000 --> 00:05:05.000
しかし、もう1つの重要なチェックがあります。

00:05:05.000 --> 00:05:10.000
このモデルがターゲットデバイスで十分に速く実行できるかどうかを理解する必要があります。

00:05:10.000 --> 00:05:17.000
したがって、デバイス上のモデルを評価し、それが最高のユーザーエクスペリエンスを提供することを確認する必要があります。

00:05:17.000 --> 00:05:26.000
Xcode 14で現在利用可能な新しいCore MLパフォーマンスレポートは、Core MLモデルの時間ベースの分析を実行します。

00:05:26.000 --> 00:05:33.000
モデルをXcodeにドラッグアンドドロップして、数秒でパフォーマンスレポートを作成するだけです。

00:05:33.000 --> 00:05:44.000
このツールを使用すると、M1とiPadOS 16を搭載したiPad Proでは、推定予測時間がほぼ90ミリ秒であることがわかります。

00:05:44.000 --> 00:05:48.000
そして、これは私の写真の着色アプリに最適です。

00:05:48.000 --> 00:05:56.000
Xcodeのパフォーマンスレポートについてもっと知りたい場合は、今年のセッション「Core MLの使用を最適化する」を見ることをお勧めします。

00:05:56.000 --> 00:06:05.000
したがって、パフォーマンスレポートは、モデルを評価し、最高のデバイス上のユーザーエクスペリエンスを提供するのに役立ちます。

00:06:05.000 --> 00:06:13.000
モデルの機能とパフォーマンスについて確信が持てたので、アプリに統合させてください。

00:06:13.000 --> 00:06:26.000
統合プロセスは、私が今までPythonでやってきたことと同じですが、今回はXcodeとあなたがよく知っている他のすべてのツールを使用して、Swiftでシームレスに行うことができます。

00:06:26.000 --> 00:06:34.000
現在Core ML形式のモデルは、その軽さを表す単一チャンネル画像を期待していることを覚えておいてください。

00:06:34.000 --> 00:06:45.000
以前にPythonで行ったのと同様に、ラボの色空間を使用してRGB入力画像を画像に変換する必要があります。

00:06:45.000 --> 00:06:53.000
私はこの変換を複数の方法で書くことができます。vImageを使用してSwiftで直接、またはMetalを使用します。

00:06:53.000 --> 00:07:02.000
ドキュメントをより深く掘り下げてみると、Core Imageフレームワークがこれに役立つ何かを提供していることがわかりました。

00:07:02.000 --> 00:07:10.000
では、RGBからLABへの変換を実現し、Core MLモデルを使用して予測を実行する方法をお見せしましょう。

00:07:10.000 --> 00:07:17.000
これは、RGB画像から軽さを抽出し、Core MLモデルに渡すSwiftコードです。

00:07:17.000 --> 00:07:23.000
まず、RGB画像をLABに変換し、軽さを抽出します。

00:07:23.000 --> 00:07:31.000
次に、軽さをCGImageに変換し、Core MLモデルの入力を準備します。

00:07:31.000 --> 00:07:33.000
最後に、私は予測を実行します。

00:07:33.000 --> 00:07:45.000
入力RGB画像からLチャンネルを抽出するには、まず、新しいCIFilter convertRGBtoLabを使用して、RGB画像をLAB画像に変換します。

00:07:45.000 --> 00:07:51.000
軽さの値は0から100の間で設定されます。

00:07:51.000 --> 00:07:59.000
次に、ラボ画像にカラーマトリックスを乗算し、カラーチャンネルを破棄し、発信者に明るさを返します。

00:07:59.000 --> 00:08:04.000
それでは、モデルの出力で何が起こるかを分析しましょう。

00:08:04.000 --> 00:08:12.000
Core MLモデルは、推定カラーコンポーネントを含む2つのMLShapedArrayを返します。

00:08:12.000 --> 00:08:19.000
したがって、予測の後、2つのMLShapedArrayを2つのCIImageに変換します。

00:08:19.000 --> 00:08:23.000
最後に、私はそれらをモデル入力の軽さと組み合わせます。

00:08:23.000 --> 00:08:30.000
これにより、新しいLAB画像が生成され、RGBに変換して返します。

00:08:30.000 --> 00:08:38.000
2つのMLShapedArrayを2つのCIImageに変換するには、まず各形状の配列から値を抽出します。

00:08:38.000 --> 00:08:44.000
次に、2つのカラーチャンネルを表す2つのコア画像を作成し、それらを返します。

00:08:44.000 --> 00:08:56.000
軽さを推定カラーチャンネルと組み合わせるには、3つのチャンネルを入力として受け取り、CIImageを返すカスタムCIKernelを使用します。

00:08:56.000 --> 00:09:05.000
次に、新しいCIFilter convertLabToRGBを使用してラボ画像をRGBに変換し、発信者に返します。

00:09:05.000 --> 00:09:14.000
これは、単一のCIImageで2つの推定カラーチャンネルと明るさを組み合わせるために使用するカスタムCIKernelのソースコードです。

00:09:14.000 --> 00:09:29.000
RGB画像をLAB画像に変換する新しいCIフィルター、またはその逆については、セッション「Core Image、Metal、SwiftUIでEDRコンテンツを表示する」を参照してください。

00:09:29.000 --> 00:09:34.000
このML機能の統合をアプリに完了したので、実際に見てみましょう。

00:09:34.000 --> 00:09:36.000
でも待って。

00:09:36.000 --> 00:09:41.000
アプリケーションで古い家族写真をリアルタイムで色付けするにはどうすればよいですか?

00:09:41.000 --> 00:09:46.000
それぞれをデジタル化し、アプリにインポートするのに時間を費やすことができます。

00:09:46.000 --> 00:09:48.000
もっといい考えがあると思います。

00:09:48.000 --> 00:09:54.000
iPadのカメラを使ってこれらの写真をスキャンし、ライブで色付けするとどうなりますか?

00:09:54.000 --> 00:09:58.000
本当に楽しいと思いますし、これを達成するために必要なことはすべて揃っています。

00:09:58.000 --> 00:10:02.000
しかし、まず、私は問題を解決しなければなりません。

00:10:02.000 --> 00:10:06.000
私のモデルは画像を処理するのに90ミリ秒が必要です。

00:10:06.000 --> 00:10:11.000
ビデオを処理したい場合は、もっと速いものが必要です。

00:10:11.000 --> 00:10:17.000
スムーズなユーザーエクスペリエンスのために、デバイスカメラを少なくとも30fpsで実行したいと思います。

00:10:17.000 --> 00:10:24.000
つまり、カメラは約30ミリ秒ごとにフレームを生成します。

00:10:24.000 --> 00:10:35.000
しかし、モデルはビデオフレームを色付けするのに約90ミリ秒を必要とするので、各色付け中に2つまたは3つのフレームを失うことになります。

00:10:35.000 --> 00:10:44.000
モデルの合計予測時間は、そのアーキテクチャと、それがマッピングされる計算ユニット操作の両方の関数です。

00:10:44.000 --> 00:10:55.000
パフォーマンスレポートをもう一度見ると、私のモデルにはニューラルエンジンとCPUの組み合わせで合計61の操作が実行されていることがわかります。

00:10:55.000 --> 00:11:00.000
より速い予測時間が必要な場合は、モデルを変更する必要があります。

00:11:00.000 --> 00:11:07.000
私はモデルのアーキテクチャを試して、より速いかもしれないいくつかの選択肢を模索することにしました。

00:11:07.000 --> 00:11:13.000
しかし、アーキテクチャの変更は、ネットワークを再訓練する必要があることを意味します。

00:11:13.000 --> 00:11:20.000
Appleは、Macで機械学習モデルを直接トレーニングできるさまざまなソリューションを提供しています。

00:11:20.000 --> 00:11:35.000
私の場合、元のモデルはPyTorchで開発されたので、Apple Siliconが提供する驚異的なハードウェアアクセラレーションを活用できるように、Metalで新しいPyTorchを使用することにしました。

00:11:35.000 --> 00:11:50.000
メタルで加速されたPyTorchについてもっと知りたい場合は、「メタルで機械学習を加速する」というセッションを確認してください。この変更により、私たちの旅は一歩後退する必要があります。

00:11:50.000 --> 00:11:59.000
再トレーニング後、結果をCore ML形式に変換し、検証を再度実行する必要があります。

00:11:59.000 --> 00:12:04.000
今回、モデル統合は、単に古いモデルを新しいモデルに交換することで構成されています。

00:12:04.000 --> 00:12:11.000
いくつかの候補の代替モデルを再訓練した後、私の要件を満たすものを確認しました。

00:12:11.000 --> 00:12:16.000
これは、対応するパフォーマンスレポートです。

00:12:16.000 --> 00:12:27.000
それは完全にニューラルエンジンで実行され、予測時間は現在約16ミリ秒で、ビデオで機能します。

00:12:27.000 --> 00:12:33.000
しかし、パフォーマンスレポートは、私のアプリのパフォーマンスの1つの側面だけを教えてくれます。

00:12:33.000 --> 00:12:40.000
確かに、アプリを実行した後、着色が期待するほどスムーズではないことにすぐに気づきました。

00:12:40.000 --> 00:12:45.000
では、実行時に私のアプリで何が起こりますか?

00:12:45.000 --> 00:12:52.000
それを理解するために、Instrumentsで新しいCore MLテンプレートを使用できます。

00:12:52.000 --> 00:13:00.000
Core MLトレースの最初の部分を分析し、モデルをロードした後、アプリが予測を蓄積していることに気づきました。

00:13:00.000 --> 00:13:02.000
そして、これは予想外です。

00:13:02.000 --> 00:13:08.000
代わりに、フレームごとに1つの予測を期待します。

00:13:08.000 --> 00:13:19.000
トレースを拡大して最初の予測をチェックすると、最初の予測が終了する前にアプリが2番目のCore ML予測を要求することがわかります。

00:13:19.000 --> 00:13:27.000
ここでは、2番目のリクエストがCore MLに与えられたとき、ニューラルエンジンはまだ最初のリクエストに取り組んでいます。

00:13:27.000 --> 00:13:33.000
同様に、3番目の予測は、2番目の予測を処理しながら開始されます。

00:13:33.000 --> 00:13:42.000
4つの予測の後でも、要求と実行の間の遅れはすでに約20ミリ秒です。

00:13:42.000 --> 00:13:51.000
代わりに、これらのラグのカスケードを避けるために、前の予測が終了した場合にのみ、新しい予測が開始されることを確認する必要があります。

00:13:51.000 --> 00:14:03.000
この問題を解決している間に、誤ってカメラのフレームレートを目的の30fpsではなく60fpsに設定してしまったこともわかりました。

00:14:03.000 --> 00:14:22.000
以前の予測が完了した後、アプリが新しいフレームを処理することを確認し、カメラのフレームレートを30fpsに設定した後、Core MLが単一の予測をApple Neural Engineに正しくディスパッチし、アプリがスムーズに動作することがわかります。

00:14:22.000 --> 00:14:26.000
それで、私たちは旅の終わりに達しました。

00:14:26.000 --> 00:14:34.000
私の古い家族写真でアプリをテストしましょう。

00:14:34.000 --> 00:14:38.000
これが私の地下室で見つけた白黒写真です。

00:14:38.000 --> 00:14:49.000
彼らは私がずっと前に訪れたイタリアの場所のいくつかをキャプチャします。

00:14:49.000 --> 00:14:53.000
これはローマのコロッセオの素晴らしい写真です。

00:14:53.000 --> 00:14:59.000
壁と空の色はとても現実的です。

00:14:59.000 --> 00:15:03.000
これを確認しましょう。

00:15:03.000 --> 00:15:06.000
これはイタリア南部のカステル・デル・モンテです。

00:15:06.000 --> 00:15:09.000
本当にいいね。

00:15:09.000 --> 00:15:12.000
そして、これは私の故郷、グロッタリエです。

00:15:12.000 --> 00:15:17.000
これらの画像に色を追加すると、非常に多くの思い出が引き起こされました。

00:15:17.000 --> 00:15:26.000
残りのシーンを白黒に保ちながら、写真にのみ色付けを適用していることに注意してください。

00:15:26.000 --> 00:15:32.000
ここでは、ビジョンフレームワークで利用可能な長方形検出アルゴリズムを利用しています。

00:15:32.000 --> 00:15:41.000
VNDetectRectangleRequestを使用すると、シーン内の写真を分離し、Colorizerモデルへの入力として使用できます。

00:15:41.000 --> 00:15:44.000
そして今、要約させてください。

00:15:44.000 --> 00:15:56.000
私たちの旅の中で、私はあなたのアプリの機械学習機能を準備、統合、評価するためにAppleが提供する膨大な量のフレームワーク、API、ツールを探求しました。

00:15:56.000 --> 00:16:03.000
私は、それを解決するためにオープンソースの機械学習モデルを必要とする問題を特定するこの旅を始めました。

00:16:03.000 --> 00:16:10.000
必要な機能を備えたオープンソースモデルを見つけ、Appleプラットフォームと互換性を持たせました。

00:16:10.000 --> 00:16:16.000
新しいパフォーマンスレポートを使用して、デバイス上で直接モデルのパフォーマンスを評価しました。

00:16:16.000 --> 00:16:22.000
私はあなたがよく知っているツールとフレームワークを使って、私のアプリにモデルを統合しました。

00:16:22.000 --> 00:16:27.000
インストゥルメントの新しいCore MLテンプレートを使用してモデルを最適化しました。

00:16:27.000 --> 00:16:40.000
Appleのツールとフレームワークを使用すると、データの準備、トレーニング、統合、最適化から、Appleのデバイスとプラットフォームで直接開発プロセスの各段階を処理できます。

00:16:40.000 --> 00:16:49.000
今日、私たちは、開発者であるあなたがAppleが提供するフレームワークとツールで達成できることの表面を引っ掻きました。

00:16:49.000 --> 00:16:56.000
アプリに機械学習をもたらすための追加の刺激的なアイデアについては、これにリンクされている以前のセッションを参照してください。

00:16:56.000 --> 00:16:59.000
フレームワークとツールを探索して試してみてください。

00:16:59.000 --> 00:17:09.000
ソフトウェアとハードウェアの大きな相乗効果を活用して、機械学習機能を加速し、アプリのユーザーエクスペリエンスを豊かにします。

00:17:09.000 --> 23:59:59.000
素晴らしいWWDCを、そして到着してください。♪ ♪

