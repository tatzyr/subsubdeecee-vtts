WEBVTT

00:00:00.000 -> 00:00:09.000
♪ ♪

00:00:09.000 -> 00:00:11.000
クリスチャン：こんにちは、私の名前はクリスチャンです。

00:00:11.000 -> 00:00:18.000
私はARKitチームのエンジニアであり、私たちのセッション、Discover ARKit 6にあなたを歓迎したいと思います。

00:00:18.000 -> 00:00:24.000
拡張現実フレームワークの最新の進歩を活用する方法を学びます。

00:00:24.000 -> 00:00:30.000
私たちは、あなたがARKitで過去数年間に作成してきたものを見て喜んでいます。

00:00:30.000 -> 00:00:38.000
私たちは、インテリアデザイン、旅行、バーチャル展示会、ゲーム、その他多くの素晴らしいアプリを見ています。

00:00:38.000 -> 00:00:45.000
Appleのチームは、あなたのフィードバックに細心の注意を払い、その多くをARKit 6に組み込みました。

00:00:45.000 -> 00:00:46.000
見てみましょう。 

00:00:46.000 -> 00:00:54.000
これまでで最高の画像解像度でカメラストリームを実行できる新しい4Kビデオモードを導入しています。

00:00:54.000 -> 00:01:01.000
その後、ビデオの背景をより詳細に制御できる追加のカメラ機能強化について説明します。

00:01:01.000 -> 00:01:14.000
また、プレーンアンカーの動作に関する最新情報、モーションキャプチャAPIの追加、そして最後にロケーションアンカーがサポートされる新しい都市を共有します。

00:01:14.000 -> 00:01:17.000
4Kビデオから始めましょう。

00:01:17.000 -> 00:01:32.000
過去数年間で、高解像度コンテンツ、特に映画制作に拡張現実の力を活用するアプリに対する多くの需要が、より多くのピクセルに飢えています。

00:01:32.000 -> 00:01:36.000
ARKitで画像がどのようにキャプチャされ、処理されるかをお見せしましょう。

00:01:36.000 -> 00:01:39.000
これはiPhone 13 Proのカメラモジュールです。

00:01:39.000 -> 00:01:43.000
それを開くと、その設定を見ることができます。

00:01:43.000 -> 00:01:46.000
ワイドカメラとウルトラワイドカメラについて話しましょう。

00:01:46.000 -> 00:01:55.000
これらのカメラは両方とも、世界追跡、モーションキャプチャ、人物セグメンテーションなど、さまざまなコンピュータビジョンタスクに使用できます。

00:01:55.000 -> 00:02:02.000
ワイドカメラは、レンダリングの背景のための画像を提供するので、私たちの心の中で特別な場所を持っています。

00:02:02.000 -> 00:02:13.000
レンダリングのために画像がどのように処理されるかを理解することが重要なので、センサーレベルにズームインさせてください。

00:02:13.000 -> 00:02:17.000
ARKitで画像をキャプチャするときは、イメージセンサーの大部分を使用します。

00:02:17.000 -> 00:02:25.000
より正確には、この特定のモデルの3840x2880ピクセルの領域です。

00:02:25.000 -> 00:02:28.000
キャプチャ後、ビニングと呼ばれるプロセスを使用します。

00:02:28.000 -> 00:02:41.000
次のように動作します。Binningは2x2ピクセルの領域を取り、ピクセル値を平均化し、単一のピクセルを書き込みます。

00:02:41.000 -> 00:02:44.000
これには2つの大きな利点があります。

00:02:44.000 -> 00:02:53.000
まず、画像の寸法は2倍に縮小され、この場合は1920x1440ピクセルに縮小されます。

00:02:53.000 -> 00:02:58.000
その結果、各フレームはより少ないメモリと処理能力を消費します。

00:02:58.000 -> 00:03:05.000
これにより、デバイスは毎秒最大60フレームでカメラを実行でき、レンダリング用のリソースが解放されます。

00:03:05.000 -> 00:03:15.000
第二に、このプロセスは、ピクセル値の平均化がセンサーノイズの影響を低減する低照度環境で利点を提供します。

00:03:15.000 -> 00:03:25.000
私たちは、約17ミリ秒ごとにHD解像度で画像を提供するカメラストリームで終わります。

00:03:25.000 -> 00:03:33.000
さまざまなコンピュータビジョンタスクに現在のフレームを使用した後、ARKitはレンダリングのために現在のフレームを表面化します。

00:03:33.000 -> 00:03:42.000
独自のMetalレンダラーを作成している場合は、ARSessionのcurrentFrame.capturedImageからアクセスできます。

00:03:42.000 -> 00:03:49.000
RealityKitを使用している場合、画像は背景として使用するためにさらに自動的に処理されます。

00:03:49.000 -> 00:04:00.000
2532ピクセルの画面幅に合わせてデバイス上でスケーリングされ、ディスプレイのアスペクト比に合わせてトリミングされます。

00:04:00.000 -> 00:04:10.000
RealityKitは、この海賊船のような仮想コンテンツをレンダリングして合成するタスクをフレームの上に実行し、最終結果を画面に表示します。

00:04:10.000 -> 00:04:17.000
現在、最新のハードウェアのパワーにより、ARKitでフル4Kビデオモードを有効にします。

00:04:17.000 -> 00:04:27.000
アプリは、ビニングステップをスキップしてフル4K解像度で直接アクセスすることで、高解像度の画像を利用できるようになりました。

00:04:27.000 -> 00:04:37.000
4Kモードでは、3840x2160ピクセルの画像領域が使用され、毎秒30フレームでビデオをキャプチャできます。

00:04:37.000 -> 00:04:41.000
これらの変更とは別に、あなたのアプリは以前と同じように動作します。

00:04:41.000 -> 00:04:49.000
RealityKitを使用すると、スケーリング、トリミング、レンダリングが実行されます。

00:04:49.000 -> 00:04:54.000
いくつかの簡単な手順で4Kモードを有効にできます。

00:04:54.000 -> 00:04:58.000
それがコードでどのように見えるか見てみましょう。

00:04:58.000 -> 00:05:11.000
「ARConfiguration」には、そのモードがデバイスでサポートされている場合、4Kビデオフォーマットを返す新しい便利な機能「recommendedVideoFormatFor4KResolution」があります。

00:05:11.000 -> 00:05:17.000
デバイスまたは構成が4Kをサポートしていない場合、この機能はnilを返します。

00:05:17.000 -> 00:05:25.000
次に、このビデオ形式を設定に割り当て、その調整された設定でセッションを実行できます。

00:05:25.000 -> 00:05:34.000
4Kビデオモードは、iPhone 11以降、およびM1チップを搭載したiPad Proで利用できます。

00:05:34.000 -> 00:05:40.000
解像度は毎秒30フレームで3840x2160ピクセルです。

00:05:40.000 -> 00:05:53.000
iPadの場合、アスペクト比は16:9です。つまり、フルスクリーン表示のために画像を側面でトリミングする必要があり、最終的なレンダリングはズームインしているように見えるかもしれません。

00:05:53.000 -> 00:06:01.000
ARKitを使用する場合、特に新しい4K解像度では、最適な結果を得るためにいくつかのベストプラクティスに従うことが重要です。

00:06:01.000 -> 00:06:04.000
ARFrameをあまり長く保持しないでください。

00:06:04.000 -> 00:06:12.000
これにより、システムがメモリを解放する可能性があり、ARKitが新しいフレームを浮上させるのをさらに停止する可能性があります。

00:06:12.000 -> 00:06:15.000
これは、レンダリングのフレームドロップで表示されるようになります。

00:06:15.000 -> 00:06:20.000
最終的には、ARCameraの追跡状態は制限に戻る可能性があります。

00:06:20.000 -> 00:06:26.000
コンソールの警告をチェックして、いつでもあまりにも多くの画像を保持していないことを確認してください。

00:06:26.000 -> 00:06:32.000
また、新しい4Kビデオフォーマットが本当にあなたのアプリにとって正しい選択肢であるかどうかも検討してください。

00:06:32.000 -> 00:06:40.000
高解像度ビデオの恩恵を受けるアプリは、ビデオ、映画制作、バーチャルプロダクションアプリなど、優れた候補です。

00:06:40.000 -> 00:06:55.000
高解像度の画像を扱うことは追加のシステムリソースを占有するので、高いリフレッシュレートに依存するゲームやその他のアプリでは、毎秒60フレームでフルHDビデオを使用することをお勧めします。

00:06:55.000 -> 00:07:03.000
新しい4Kモードに加えて、カメラをより詳細に制御できるいくつかの追加機能強化があります。

00:07:03.000 -> 00:07:12.000
まず、高解像度の背景写真APIを導入し、新しいHDRモードを有効にする方法を紹介します。

00:07:12.000 -> 00:07:24.000
さらに、よりきめ細かい制御のために基礎となるAVCaptureDeviceにアクセスする方法を実演し、ARKitでEXIFタグを読み取る方法を紹介します。

00:07:24.000 -> 00:07:30.000
新しい高解像度の背景写真APIに飛び込みましょう。

00:07:30.000 -> 00:07:35.000
ARSessionを実行している間も、通常どおりビデオストリームにアクセスできます。

00:07:35.000 -> 00:07:46.000
さらに、ARKitを使用すると、ビデオストリームが途切れることなく実行されている間、バックグラウンドでオンデマンドで単一の写真のキャプチャをリクエストできます。

00:07:46.000 -> 00:07:50.000
これらの単一のフォトフレームは、カメラセンサーを最大限に活用します。

00:07:50.000 -> 00:07:55.000
私のiPhone 13では、それはワイドカメラの完全な12メガピクセルを意味します。

00:07:55.000 -> 00:08:06.000
WWDCの準備中、ARKitでは、この新しいAPIが作成に役立つものを強調する写真アプリの楽しいアイデアを思いつきました。

00:08:06.000 -> 00:08:16.000
私たちの例では、有名な海賊旗がアップルインフィニットループキャンパスの上に飛んでいた2016年4月1日にあなたを連れ戻します。

00:08:16.000 -> 00:08:25.000
私はオリジナルの写真家のトミーに、6年前にその写真をどこで撮ったのか尋ねました。

00:08:25.000 -> 00:08:38.000
この座標に基づいて、大きな青い点が示すように、トミーが2016年4月に立っていたのとまったく同じ場所に案内するロケーションアンカーを作成できます。

00:08:38.000 -> 00:08:45.000
その場所に到達すると、フォーカススクエアを表示することで、その完璧な画像をフレーミングするのに役立ちます。

00:08:45.000 -> 00:08:51.000
最後に、このアプリでは画面をタップして写真を撮ることができます。

00:08:51.000 -> 00:09:00.000
その写真は、別のAVCaptureセッションをスピンアップすることなく、現在のARKitセッションの実行中にネイティブカメラ解像度で撮影できます。

00:09:00.000 -> 00:09:06.000
ARと写真の力を組み合わせたアイデアを楽しみにしています。私たちは、あなたが持っているものを見て興奮しています。

00:09:06.000 -> 00:09:15.000
このAPIによって大きな恩恵を受けるもう1つのユースケースは、オブジェクトキャプチャを使用した3Dモデルの作成です。

00:09:15.000 -> 00:09:30.000
オブジェクトキャプチャは、このランニングシューズのような現実世界のオブジェクトの写真を撮り、最新のフォトグラメトリアルゴリズムを使用して、ARアプリに展開する準備ができている3Dモデルに変換します。

00:09:30.000 -> 00:09:37.000
ARKitを使用すると、物理的なオブジェクトの上に3D UIをオーバーレイし、より良いキャプチャガイダンスを提供できます。

00:09:37.000 -> 00:09:47.000
そして今、新しい高解像度の背景画像APIを使用すると、オブジェクトの高解像度の写真を撮り、さらにリアルな3Dモデルを作成できます。

00:09:47.000 -> 00:09:56.000
私はフォトグラメトリの大ファンなので、今年の「Bring your world to augmented reality」セッションをチェックすることを強くお勧めします。

00:09:56.000 -> 00:10:01.000
コードで高解像度の写真撮影を有効にする方法をお見せしましょう。

00:10:01.000 -> 00:10:07.000
まず、hiResCaptureをサポートするビデオフォーマットを確認します。

00:10:07.000 -> 00:10:15.000
そのために、便利な機能「recommendedVideoForForHighResolution FrameCapturing」を使用できます。

00:10:15.000 -> 00:10:22.000
フォーマットがサポートされていることを確認したら、新しいビデオフォーマットを設定してセッションを実行できます。

00:10:22.000 -> 00:10:27.000
さらに、高解像度画像をキャプチャするタイミングをARKitに伝える必要があります。

00:10:27.000 -> 00:10:32.000
以前の例では、写真のキャプチャは画面をタップするとトリガーされます。

00:10:32.000 -> 00:10:39.000
独自のアプリケーションでは、高解像度のフレームキャプチャをトリガーするさまざまなイベントに反応したいと思うかもしれません。

00:10:39.000 -> 00:10:42.000
それは本当にあなたのユースケースによります。

00:10:42.000 -> 00:10:47.000
ARSessionには、captureHighResolutionFrameという新しい関数があります。

00:10:47.000 -> 00:10:54.000
この関数を呼び出すと、高解像度画像のアウトオブバンドキャプチャがトリガーされます。

00:10:54.000 -> 00:11:03.000
完了ハンドラでは、高解像度画像と他のすべてのフレームプロパティを含むARFrameに非同期にアクセスできます。

00:11:03.000 -> 00:11:08.000
コンテンツにアクセスする前に、フレームキャプチャが成功したかどうかを確認する必要があります。

00:11:08.000 -> 00:11:11.000
この例では、フレームをディスクに保存します。

00:11:11.000 -> 00:11:22.000
また、特にこれらの画像はイメージセンサーのフル解像度を使用しているため、先に述べた画像保持に関するベストプラクティスを覚えておいてください。

00:11:22.000 -> 00:11:26.000
次に、HDRについて話しましょう。

00:11:26.000 -> 00:11:31.000
ハイダイナミックレンジは、より広い範囲の色をキャプチャし、それらをディスプレイにマッピングします。

00:11:31.000 -> 00:11:35.000
これは、コントラストの高い環境で最もよく見られます。

00:11:35.000 -> 00:11:38.000
これは私の裏庭の良い例です。

00:11:38.000 -> 00:11:47.000
このシーンは、木製のフェンスなど、非常に暗い領域と、空の雲のような非常に明るい領域の両方を特徴としています。

00:11:47.000 -> 00:11:58.000
HDRモードをオンにすると、右側のように、雲のふわふわのようなこれらの領域の詳細がHDRではるかによく保存されていることがわかります。

00:11:58.000 -> 00:12:01.000
コードでHDRがどのようにオンになっているか見てみましょう。

00:12:01.000 -> 00:12:09.000
「isVideoHDRSupported」プロパティを使用して、HDRをサポートしている場合は、任意のビデオフォーマットを照会できます。

00:12:09.000 -> 00:12:13.000
現在、バインドされていないビデオフォーマットのみがHDRをサポートしています。

00:12:13.000 -> 00:12:22.000
HDRがサポートされている場合は、設定でvideoHDRAllowedをtrueに設定し、その設定でセッションを実行します。

00:12:22.000 -> 00:12:29.000
HDRをオンにするとパフォーマンスに影響を与えるので、必要な場合にのみ使用してください。HDRをオンにすると、パフォーマンスに影響します。

00:12:29.000 -> 00:12:43.000
露出やホワイトバランスなどの設定を手動で制御することを好むユースケースでは、AVCaptureDeviceに直接アクセスし、その設定を変更する便利な方法があります。

00:12:43.000 -> 00:12:54.000
コード例では、設定の「configurableCaptureDevice ForPrimaryCamera」を呼び出すと、基盤となる「AVCaptureDevice」にアクセスできます。

00:12:54.000 -> 00:13:07.000
この機能を使用して、ARKitアプリのカスタムルックを作成しますが、画像はレンダリングの背景として使用されるだけでなく、シーンを分析するためにARKitによって積極的に使用されることに注意してください。

00:13:07.000 -> 00:13:14.000
したがって、強い露出過剰のような変化は、ARKitの出力品質に悪影響を及ぼす可能性があります。

00:13:14.000 -> 00:13:19.000
また、フォーカスイベントのトリガーなど、いくつかの高度な操作を実行することもできます。

00:13:19.000 -> 00:13:29.000
AVCaptureSessionsの設定方法の詳細については、developer.apple.comのAVCaptureドキュメントを参照してください。

00:13:29.000 -> 00:13:33.000
最後に、ARKitはEXIFタグをアプリに公開します。

00:13:33.000 -> 00:13:37.000
それらはすべてのARFrameで利用可能になりました。

00:13:37.000 -> 00:13:45.000
EXIFタグには、ホワイトバランス、露出、および後処理に価値のあるその他の設定に関する有用な情報が含まれています。

00:13:45.000 -> 00:13:49.000
これで、画像キャプチャパイプラインのすべての更新は終了です。

00:13:49.000 -> 00:13:53.000
平面アンカーにどのような変更があるか見てみましょう。

00:13:53.000 -> 00:13:58.000
飛行機のアンカーは、ARKitの最初のバージョン以来、人気のある機能です。

00:13:58.000 -> 00:14:05.000
あなた方の多くは、平面アンカーと基礎となる平面ジオメトリをよりきれいに分離する必要性を表明しました。

00:14:05.000 -> 00:14:12.000
そのため、平面アンカーの挙動と平面のジオメトリに関する最新情報を発表します。

00:14:12.000 -> 00:14:17.000
これは、iOS 15の典型的な飛行機アンカーの例です。

00:14:17.000 -> 00:14:24.000
ARセッションの開始時に、テーブルの上のこの質感の良いノートブックに平面をフィットさせます。

00:14:24.000 -> 00:14:31.000
セッションを実行すると、平面は徐々に更新され、表示されるテーブルの新しい部分が考慮されます。

00:14:31.000 -> 00:14:39.000
平面ジオメトリが更新されるたびに、アンカーの回転も更新され、平面の新しい向きが反映されます。

00:14:39.000 -> 00:14:47.000
iOS 16では、平面アンカーとその平面ジオメトリをよりクリーンに分離します。

00:14:47.000 -> 00:14:52.000
平面アンカーとジオメトリの更新が完全に分離されました。

00:14:52.000 -> 00:15:06.000
完全なテーブルが表示されるにつれて、平面がジオメトリを拡張および更新している間、アンカーの回転自体は一定のままです。

00:15:06.000 -> 00:15:21.000
左側の古い動作と対比すると、iOS 16の平面アンカーは、ARセッション全体を通して、ノートブックに合わせて同じ向きにとどまっていることがわかります。

00:15:21.000 -> 00:15:28.000
平面ジオメトリに関するすべての情報は、ARPlaneExtentというクラスに含まれるようになりました。

00:15:28.000 -> 00:15:33.000
回転の更新は、平面アンカー自体を回転させることによって表現されなくなりました。

00:15:33.000 -> 00:15:42.000
代わりに、ARPlaneExtentには、回転角度を表す新しいプロパティrotationOnYAxisが含まれています。

00:15:42.000 -> 00:15:52.000
この新しいプロパティに加えて、平面は幅と高さ、およびPlaneAnchorの中心座標によって完全に定義されています。

00:15:52.000 -> 00:15:58.000
この平面の視覚化をコードで作成する方法をお見せしましょう。

00:15:58.000 -> 00:16:06.000
まず、指定された幅と高さに従って平面メッシュに基づいてエンティティを生成します。

00:16:06.000 -> 00:16:16.000
次に、y軸の回転に従ってエンティティ変換を設定し、センタープロパティの値でオフセットします。

00:16:16.000 -> 00:16:26.000
飛行機が更新されるたびに、幅、高さ、中心座標と新しい回転OnYAxisが変わる可能性があるという事実を考慮する必要があります。

00:16:26.000 -> 00:16:34.000
この新しい動作を利用するには、Xcodeの設定で展開ターゲットをiOS 16に設定します。

00:16:34.000 -> 00:16:42.000
次のアップデートはMotionCaptureで、私たちの機械学習の首謀者はさらなる改善を行うために懸命に働きました。

00:16:42.000 -> 00:16:49.000
2Dスケルトンと3Dジョイントの両方に、一連のアップデートがあります。

00:16:49.000 -> 00:16:55.000
2Dスケルトンでは、左耳と右耳の2つの新しい関節を追跡しています。

00:16:55.000 -> 00:16:59.000
また、全体的なポーズ検出も改善しました。

00:16:59.000 -> 00:17:09.000
iPhone 12以降、およびM1チップを搭載した最新のiPad ProおよびiPad Airモデルでは、赤で示されているように3Dスケルトンが改善されました。

00:17:09.000 -> 00:17:14.000
ジッターが少なくなり、全体的に時間的な一貫性が高まるでしょう。

00:17:14.000 -> 00:17:21.000
また、人の一部が閉塞している場合や、カメラの近くを歩いているときに、追跡もより安定します。

00:17:21.000 -> 00:17:29.000
改良されたMotionCaptureを利用するには、Xcodeの設定で展開ターゲットをiOS 16に設定します。

00:17:29.000 -> 00:17:36.000
次に、LocationAnchorsがサポートされる新しい都市や国も発表したいと思います。

00:17:36.000 -> 00:17:42.000
小さなリマインダーとして、Apple MapsはLocationAnchor APIを使用して歩行者の歩行指示に電力を供給します。

00:17:42.000 -> 00:17:50.000
この例では、LocationAnchorsの力のおかげで、ロンドンの通りを案内できることがわかります。

00:17:50.000 -> 00:17:57.000
LocationAnchorsは、米国のますます多くの都市と英国のロンドンですでに利用可能です。

00:17:57.000 -> 00:18:03.000
今日から、カナダのバンクーバー、トロント、モントリオールの都市で利用可能になります。

00:18:03.000 -> 00:18:10.000
また、シンガポールや東京を含む日本の7つの大都市圏でもそれらを可能にしています。

00:18:10.000 -> 00:18:14.000
オーストラリアのメルボルンとシドニーも同様です。

00:18:14.000 -> 00:18:33.000
今年後半には、オークランド、ニュージーランド、テルアビブ、イスラエル、パリ、フランスで利用可能にします。LocationAnchorsが特定の座標でサポートされているかどうかを知りたい場合は、ARGeoTrackingConfigurationのcheckAvailabilityメソッドを使用してください。

00:18:33.000 -> 00:18:37.000
そして、これらはすべてARKit 6のアップデートでした。

00:18:37.000 -> 00:18:43.000
要約すると、私は新しい4KビデオフォーマットでARKitを実行する方法を紹介しました。

00:18:43.000 -> 00:18:51.000
高度なユースケースでは、HDRを有効にする方法、またはAVCaptureDeviceを手動で制御する方法を実演しました。

00:18:51.000 -> 00:18:59.000
さらに多くのピクセルに飢えたアプリケーションのために、私はARKitセッションから高解像度の写真を取得する方法を実演しました。

00:18:59.000 -> 00:19:07.000
プレーンアンカーの新しい動作について話し、新しい耳関節とMotionCaptureの他の改善を発表しました。

00:19:07.000 -> 00:19:14.000
また、LocationAnchorsが今年後半にどの国で利用可能になるかを知ることができました。

00:19:14.000 -> 00:19:15.000
チューニングしてくれてありがとう。

00:19:15.000 -> 23:59:59.000
素晴らしいWWDC 2022をお過ごしください!

