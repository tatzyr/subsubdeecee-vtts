WEBVTT

00:00:00.000 -> 00:00:09.000
♪インストゥルメンタルヒップホップ音楽♪

00:00:09.000 -> 00:00:14.000
こんにちは、「iOSカメラキャプチャの進歩を発見する」へようこそ。

00:00:14.000 -> 00:00:20.000
私はカメラソフトウェアチームのニコラス・ゲロで、iOSとiPadOSのエキサイティングな新しいカメラ機能を紹介します。

00:00:20.000 -> 00:00:25.000
AVFoundationを使用してLiDARスキャナから深度をストリーミングする方法から始めます。

00:00:25.000 -> 00:00:31.000
次に、顔駆動のオートフォーカスと自動露出で、アプリがどのように改善されたフェイスレンダリングを受けるかを見てみましょう。

00:00:31.000 -> 00:00:36.000
次に、高度なAVCaptureSessionストリーミング設定を案内します。

00:00:36.000 -> 00:00:41.000
そして最後に、あなたのアプリがマルチタスク中にカメラをどのように使用できるかをお見せします。

00:00:41.000 -> 00:00:45.000
AVFoundationを使用してLiDARスキャナから深度をストリーミングする方法から始めます。

00:00:45.000 -> 00:00:53.000
iPhone 12 Pro、iPhone 13 Pro、iPad Proには、高密度深度マップを出力できるLiDARスキャナーが装備されています。

00:00:53.000 -> 00:01:00.000
LiDARスキャナーは、周囲に光を撮影し、シーンの表面から反射した光を収集することで機能します。

00:01:00.000 -> 00:01:07.000
深さは、光がLiDARから環境に移動し、スキャナーに反射するのにかかった時間を測定することによって推定されます。

00:01:07.000 -> 00:01:11.000
このプロセス全体は毎秒数百万回実行されます。

00:01:11.000 -> 00:01:14.000
AVFoundationを使用して、LiDARスキャナーを実際にお見せします。

00:01:14.000 -> 00:01:20.000
ここiPhone 13 Pro Maxでは、新しいLiDAR深度カメラAVCaptureDeviceを使用するアプリを実行しています。

00:01:20.000 -> 00:01:24.000
このアプリは、ライブカメラフィードの上にストリーミング深度データをレンダリングします。

00:01:24.000 -> 00:01:29.000
近い物体には青、遠くにある物体には赤が表示されます。

00:01:29.000 -> 00:01:33.000
そして、スライダーを使って、深さの不透明度を調整できます。

00:01:33.000 -> 00:01:36.000
このアプリは、高解像度の深度マップでも写真を撮ります。

00:01:36.000 -> 00:01:42.000
写真を撮ると、同じ深度オーバーレイが適用されますが、静止画の解像度はさらに高くなります。

00:01:42.000 -> 00:01:45.000
このアプリには、もう1つのトリックがあります。

00:01:45.000 -> 00:01:52.000
トーチボタンを押すと、アプリはカラー画像付きの高解像度深度マップを使用して、RealityKitを使用してシーンにスポットライトを当てます。

00:01:52.000 -> 00:01:57.000
タップして、シーン内のさまざまなオブジェクトにスポットライトを当てることができます。

00:01:57.000 -> 00:01:59.000
スポットライトがギターをどのように強調しているかを見てください。

00:01:59.000 -> 00:02:04.000
または、壁の隅にある正しい場所をタップすると、スポットライトがハートの形を形成します。

00:02:04.000 -> 00:02:09.000
あのギターに戻りましょう。とてもかっこよく見えます。

00:02:09.000 -> 00:02:14.000
LiDARスキャナーのAPIは、iPadOS 13.4のARKitで初めて導入されました。

00:02:14.000 -> 00:02:21.000
WWDC 2020のプレゼンテーション「Explore ARKit 4」を見ていない場合は、ご覧になることをお勧めします。

00:02:21.000 -> 00:02:26.000
iOS 15.4の新機能で、アプリはAVFoundationでLiDARスキャナにアクセスできます。

00:02:26.000 -> 00:02:33.000
私たちは、ビデオと深度を提供する内蔵のLiDAR深度カメラである新しいAVCaptureデバイスタイプを導入しました。

00:02:33.000 -> 00:02:36.000
高品質で高精度な深度情報を生成します。

00:02:36.000 -> 00:02:43.000
この新しいAVCaptureDeviceは、背面の広角カメラを使用して、LiDARスキャナーでビデオを配信し、深度をキャプチャします。

00:02:43.000 -> 00:02:47.000
ビデオと奥行きの両方が広角カメラの視野でキャプチャされます。

00:02:47.000 -> 00:02:53.000
また、TrueDepth AVCaptureDeviceと同様に、すべてのフォーマットが深度データ配信をサポートしています。

00:02:53.000 -> 00:03:02.000
この新しいAVCaptureDeviceは、LiDARスキャナーからのまばらな出力と背面の広角カメラからのカラー画像を融合させることで、高品質の深度データを生成します。

00:03:02.000 -> 00:03:08.000
LiDARとカラー入力は、高密度深度マップを出力する機械学習モデルを使用して処理されます。

00:03:08.000 -> 00:03:17.000
LiDAR深度カメラは後ろ向きの広角カメラを使用しているため、望遠カメラと超広角カメラはAVCaptureMultiCamSessionに加えて使用できます。

00:03:17.000 -> 00:03:20.000
これは、複数のカメラを同時に使用したいアプリに便利です。

00:03:20.000 -> 00:03:32.000
LiDAR深度カメラは、640×480のビデオ解像度から4032×3024のフル12メガピクセル画像まで、多くのフォーマットを公開します。

00:03:32.000 -> 00:03:36.000
ストリーミング中は、最大320×240の深度マップを出力できます。

00:03:36.000 -> 00:03:43.000
また、写真撮影では、768×576の深度マップを受け取ることができます。

00:03:43.000 -> 00:03:47.000
深さの解像度は、16×9と4×3のフォーマットでわずかに異なることに注意してください。

00:03:47.000 -> 00:03:51.000
これは、ビデオのアスペクト比と一致するためです。

00:03:51.000 -> 00:03:58.000
LiDAR深度カメラAVCaptureDeviceは、iPhone 12 Pro、iPhone 13 Pro、iPad Pro第5世代で利用できます。

00:03:58.000 -> 00:04:03.000
iPhone 13 Proは、背面カメラの組み合わせを使用して深度データを配信できます。

00:04:03.000 -> 00:04:09.000
AVFoundation Capture APIは、これらを物理デバイスで構成される「仮想デバイス」と呼んでいます。

00:04:09.000 -> 00:04:20.000
iPhone 13 Proの背面には、使用できる4つの仮想AVCaptureDeviceがあります。新しいLiDAR深度カメラは、広角カメラでLiDARスキャナーを使用します。

00:04:20.000 -> 00:04:24.000
デュアルカメラは広角カメラと望遠カメラを使用します。

00:04:24.000 -> 00:04:28.000
ワイドカメラとウルトラワイドカメラを使用するデュアルワイドカメラ。

00:04:28.000 -> 00:04:33.000
そして、ワイド、超広角、望遠カメラを使用するトリプルカメラ。

00:04:33.000 -> 00:04:37.000
これらのデバイスが生み出す深さの種類には違いがあります。

00:04:37.000 -> 00:04:45.000
LiDAR深度カメラは「絶対深度」を生み出します。使用される飛行技術の時間により、現実世界のスケールを計算できます。

00:04:45.000 -> 00:04:49.000
たとえば、これは測定などのコンピュータビジョンタスクに最適です。

00:04:49.000 -> 00:04:56.000
TrueDepth、デュアル、デュアルワイド、トリプルカメラは、相対的な格差ベースの深さを生成します。

00:04:56.000 -> 00:05:00.000
これはより少ない電力を使用し、写真の効果をレンダリングするアプリに最適です。

00:05:00.000 -> 00:05:04.000
AVFoundationは、AVDepthDataクラスを使用して深さを表します。

00:05:04.000 -> 00:05:13.000
このクラスには、深度データ型、精度、フィルタリングされているかどうかなど、それを記述する他のプロパティを含む深度を含むピクセルバッファがあります。

00:05:13.000 -> 00:05:18.000
これは、新しいLiDAR深度カメラのように、深度対応のAVCaptureDeviceによって提供されます。

00:05:18.000 -> 00:05:25.000
AVCaptureDepthDataOutputから深度をストリーミングしたり、AVCapturePhotoOutputから写真に添付された深度を受信したりできます。

00:05:25.000 -> 00:05:27.000
深度データはデフォルトでフィルタリングされます。

00:05:27.000 -> 00:05:32.000
フィルタリングはノイズを低減し、深度マップの欠損値または穴を埋めます。

00:05:32.000 -> 00:05:39.000
これはビデオや写真アプリに最適なので、深度マップを使用してカラー画像に効果を適用すると、アーティファクトは表示されません。

00:05:39.000 -> 00:05:46.000
ただし、コンピュータビジョンアプリは、深度マップの元の値を維持するために、フィルタリングされていない深度データを好むべきです。

00:05:46.000 -> 00:05:51.000
フィルタリングが無効になっている場合、LiDAR深度カメラは低い信頼ポイントを除外します。

00:05:51.000 -> 00:06:03.000
深度データフィルタリングを無効にするには、AVCaptureDepthDataOutputのisFilteringEnabledプロパティをfalseに設定し、デリゲートコールバックからAVDepthDataオブジェクトを受信すると、フィルタリングされません。

00:06:03.000 -> 00:06:14.000
ARKitはすでにLiDARスキャナーへのアクセスを提供しているので、「AVFoundationはどのように比較されますか？」と尋ねるかもしれません。AVFoundationは、ビデオおよび写真アプリ用に設計されています。

00:06:14.000 -> 00:06:20.000
AVFoundationを使用すると、LiDARスキャナでキャプチャした深度データを高解像度の写真に埋め込むことができます。

00:06:20.000 -> 00:06:24.000
ARKitは、その名前が示すように、拡張現実アプリに最適です。

00:06:24.000 -> 00:06:31.000
LiDARスキャナーを使用すると、ARKitはシーンジオメトリやオブジェクトの配置などの機能を提供することができます。

00:06:31.000 -> 00:06:36.000
AVFoundationは、映画の録画や写真撮影に最適な高解像度ビデオを提供できます。

00:06:36.000 -> 00:06:42.000
AVFoundationのLiDAR深度カメラは、最大768×576の深度を出力できます。

00:06:42.000 -> 00:06:47.000
これは、ARKitの深度解像度256×192の2倍以上です。

00:06:47.000 -> 00:06:55.000
ARKitは低解像度の深度マップを使用しているため、その機能に拡張現実アルゴリズムを適用できます。

00:06:55.000 -> 00:07:05.000
AVFoundationを使用して深度データをキャプチャする方法に関するより「詳細な」情報については、WWDC 2017の以前のセッション「iPhone写真で深度をキャプチャする」をご覧ください。

00:07:05.000 -> 00:07:10.000
アプリでLiDAR深度カメラを使用できる興味深い方法を見て興奮しています。

00:07:10.000 -> 00:07:18.000
次に、オートフォーカスとオート露出システムの改善が、アプリのシーンでの顔の可視性を向上させるのにどのように役立つかについて説明します。

00:07:18.000 -> 00:07:23.000
オートフォーカスとオート露出システムは、シーンを分析して最高の画像をキャプチャします。

00:07:23.000 -> 00:07:33.000
オートフォーカスシステムは、被写体にピントを合わせるようにレンズを調整し、自動露光システムは、被写体を見えるようにシーンの最も明るい領域と暗い領域のバランスを取ります。

00:07:33.000 -> 00:07:38.000
ただし、自動調整が行われても、被写体の顔にピントが合わないことがあります。

00:07:38.000 -> 00:07:44.000
また、明るいバックライト付きのシーンでは、被写体の顔が見えにくい場合があります。

00:07:44.000 -> 00:07:52.000
デジタル一眼レフやその他のプロカメラの共通の特徴は、シーン内の顔を追跡して、フォーカスと露出を動的に調整して見えるようにすることです。

00:07:52.000 -> 00:07:58.000
iOS 15.4の新機能では、フォーカスシステムと露出システムが顔を優先します。

00:07:58.000 -> 00:08:04.000
私たちはこの利点がとても気に入ったので、iOS 15.4以降にリンクされているすべてのアプリでデフォルトで有効にしました。

00:08:04.000 -> 00:08:07.000
いくつかの例をお見せします。

00:08:07.000 -> 00:08:13.000
顔駆動のオートフォーカスがなければ、カメラは顔に再び焦点を合わせることなく背景に焦点を合わせたままになります。

00:08:13.000 -> 00:08:14.000
もう一度見てください。

00:08:14.000 -> 00:08:19.000
彼が振り向くとき、彼の顔が焦点から外れたままで、背景の木々が鋭く残っているのを見てください。

00:08:19.000 -> 00:08:23.000
顔駆動のオートフォーカスを有効にすると、彼の顔がはっきりと見えます。

00:08:23.000 -> 00:08:28.000
そして、彼が背を向けると、カメラは背景に焦点を変えます。

00:08:28.000 -> 00:08:32.000
ビデオを並べて比較すると、違いは明らかです。

00:08:32.000 -> 00:08:37.000
顔駆動のオートフォーカスが有効になっている右側では、彼のひげの細かい詳細を見ることができます。

00:08:37.000 -> 00:08:42.000
明るいバックライト付きのシーンでは、顔をよく露出したままにしておくのは難しいかもしれません。

00:08:42.000 -> 00:08:48.000
しかし、顔を優先する自動露出システムで、私たちは簡単に彼を見ることができます。

00:08:48.000 -> 00:08:52.000
並んで比較すると、ここでも違いがわかります。

00:08:52.000 -> 00:08:57.000
右の写真で彼の顔をよく露出しておくことで、背景の木々が明るく見えることに注目してください。

00:08:57.000 -> 00:08:59.000
そして、空もそうです。

00:08:59.000 -> 00:09:04.000
顔を優先すると、シーン全体の露出が調整されます。

00:09:04.000 -> 00:09:11.000
iOS 15.4では、AVCaptureDeviceには、顔駆動のオートフォーカスと自動露出が有効になっているときに制御するための新しいプロパティがあります。

00:09:11.000 -> 00:09:17.000
デバイスがこれらの設定を「自動的に調整」するかどうかを制御し、いつ有効にするかを決定できます。

00:09:17.000 -> 00:09:23.000
「isEnabled」プロパティを切り替える前に、まず自動調整を無効にする必要があります。

00:09:23.000 -> 00:09:26.000
この動作の自動有効化は、写真アプリに最適です。

00:09:26.000 -> 00:09:28.000
それはAppleのカメラアプリで使われています。

00:09:28.000 -> 00:09:32.000
また、ビデオ会議アプリが通話中に顔を見えるようにしておくのにも最適です。

00:09:32.000 -> 00:09:40.000
FaceTimeはこれを利用しますが、自動フォーカスと自動露出システムが顔によって駆動されるアプリには最適ではない場合があります。

00:09:40.000 -> 00:09:48.000
たとえば、アプリがキャプチャした画像をユーザーが手動で制御できるようにする場合は、これをオフにすることを検討してください。

00:09:48.000 -> 00:09:53.000
顔駆動のオートフォーカスまたは自動露出がアプリに適切ではないと判断した場合は、この動作をオプトアウトできます。

00:09:53.000 -> 00:09:56.000
まず、設定のためにAVCaptureDeviceをロックします。

00:09:56.000 -> 00:10:02.000
次に、顔駆動のオートフォーカスまたは自動露出の自動調整をオフにします。

00:10:02.000 -> 00:10:05.000
次に、顔駆動のオートフォーカスまたは自動露出を無効にします。

00:10:05.000 -> 00:10:10.000
そして最後に、設定のためにデバイスのロックを解除します。

00:10:10.000 -> 00:10:18.000
高度なストリーミング構成を使用して、アプリのニーズに合わせてカスタマイズされたオーディオおよびビデオデータを受信する方法について説明します。

00:10:18.000 -> 00:10:23.000
AVFoundation Capture APIを使用すると、開発者はカメラを使用して没入型アプリを構築できます。

00:10:23.000 -> 00:10:33.000
AVCaptureSessionは、AVCaptureOutputsに接続されているカメラやマイクなどの入力からのデータフローを管理し、ビデオ、オーディオ、写真などを配信できます。

00:10:33.000 -> 00:10:40.000
たとえば、一般的なカメラアプリのユースケースを考えてみましょう。録画されたビデオにフィルターやオーバーレイなどのカスタムエフェクトを適用します。

00:10:40.000 -> 00:10:51.000
このようなアプリには、カメラとマイクの2つの入力を備えたAVCaptureSessionがあり、1つはビデオデータ用、もう1つはオーディオデータ用の2つの出力に接続されています。

00:10:51.000 -> 00:11:00.000
その後、ビデオデータに効果が適用され、処理されたビデオはビデオプレビューと録画用のAVAssetWriterの2つの場所に送信されます。

00:11:00.000 -> 00:11:03.000
オーディオデータはAVAssetWriterにも送信されます。

00:11:03.000 -> 00:11:10.000
iOS 16とiPadOS 16の新機能で、アプリは複数のAVCaptureVideoDataOutputsを同時に使用できます。

00:11:10.000 -> 00:11:19.000
ビデオデータ出力ごとに、解像度、安定化、向き、ピクセルフォーマットをカスタマイズできます。

00:11:19.000 -> 00:11:21.000
カメラアプリの例に戻りましょう。

00:11:21.000 -> 00:11:25.000
このアプリがバランスをとっている競合するキャプチャ要件があります。

00:11:25.000 -> 00:11:31.000
このアプリは、キャプチャされたコンテンツのライブビデオプレビューを表示し、後で再生するために高品質のビデオを録画したいと考えています。

00:11:31.000 -> 00:11:36.000
プレビューのために、解像度はデバイスの画面に十分な大きさである必要があります。

00:11:36.000 -> 00:11:39.000
そして、処理は低遅延プレビューのために十分に高速である必要があります。

00:11:39.000 -> 00:11:44.000
しかし、録画するときは、高品質のエフェクトを適用して高解像度でキャプチャするのが最善です。

00:11:44.000 -> 00:11:50.000
2番目のAVCaptureVideoDataOutputを追加する機能により、キャプチャグラフを拡張できます。

00:11:50.000 -> 00:11:54.000
これで、ビデオデータ出力を最適化できます。

00:11:54.000 -> 00:12:01.000
1つの出力はプレビュー用に小さなバッファを提供でき、もう1つは録画用にフルサイズの4Kバッファを提供できます。

00:12:01.000 -> 00:12:11.000
また、このアプリは、より小さなプレビューバッファに、よりシンプルでパフォーマンスの高いバージョンのエフェクトをレンダリングし、録画時にフルサイズのバッファ用に高品質のエフェクトを予約できます。

00:12:11.000 -> 00:12:17.000
これで、アプリはプレビューや録画したビデオを危険にさらす必要がなくなりました。

00:12:17.000 -> 00:12:24.000
プレビューと録画に別々のビデオデータ出力を使用するもう1つの理由は、異なる安定化モードを適用することです。

00:12:24.000 -> 00:12:28.000
ビデオ安定化は、ビデオキャプチャパイプラインに追加のレイテンシをもたらします。

00:12:28.000 -> 00:12:34.000
プレビューでは、顕著な遅延によりコンテンツのキャプチャが困難になるため、レイテンシは望ましくありません。

00:12:34.000 -> 00:12:38.000
録画のために、後でビデオを見るときにより良い経験のために安定化を適用することができます。

00:12:38.000 -> 00:12:48.000
したがって、低遅延プレビューのために1つのビデオデータ出力に安定化を適用せず、後で再生するためにもう1つに安定化を適用することができます。

00:12:48.000 -> 00:12:52.000
ビデオデータ出力の解像度を設定する方法はたくさんあります。

00:12:52.000 -> 00:12:58.000
フルサイズの出力の場合、まず、出力バッファ寸法の自動設定を無効にします。

00:12:58.000 -> 00:13:02.000
次に、プレビューサイズの出力バッファの配信を無効にします。

00:13:02.000 -> 00:13:08.000
ただし、ほとんどの場合、ビデオデータ出力はすでにフルサイズの出力用に設定されています。

00:13:08.000 -> 00:13:16.000
プレビューサイズの出力の場合、再び自動設定を無効にしますが、代わりにプレビューサイズの出力バッファの配信を有効にします。

00:13:16.000 -> 00:13:21.000
これは、写真AVCaptureSessionPresetを使用する場合、デフォルトで有効になります。

00:13:21.000 -> 00:13:27.000
カスタム解像度を要求するには、出力のビデオ設定辞書で幅と高さを指定します。

00:13:27.000 -> 00:13:32.000
幅と高さのアスペクト比は、ソースデバイスのactiveFormatのアスペクト比と一致する必要があります。

00:13:32.000 -> 00:13:35.000
ビデオデータ出力を設定する方法は他にもあります。

00:13:35.000 -> 00:13:43.000
安定化を適用するには、好みの安定化をシネマティック拡張のようなモードに設定し、見るのに最適なビデオを生成します。

00:13:43.000 -> 00:13:47.000
縦向きのバッファを受信するように方向を変更できます。

00:13:47.000 -> 00:13:53.000
また、10ビットのロスレスYUVバッファを受信するピクセル形式を指定できます。

00:13:53.000 -> 00:14:01.000
AVCaptureVideoDataOutputのピクセル形式の選択の詳細については、Technote 3121を参照してください。

00:14:01.000 -> 00:14:14.000
iOS 16とiPadOS 16以降、複数のビデオデータ出力を使用することに加えて、アプリはAVCaptureVideoDataOutputとAVCaptureAudioDataOutputからデータを受信しながら、AVCaptureMovieFileOutputで録画できます。

00:14:14.000 -> 00:14:25.000
セッションに何を追加できるかを判断するには、出力を追加できるかどうかを確認し、セッションのハードウェアコストプロパティを照会して、システムが設定をサポートできるかどうかを判断できます。

00:14:25.000 -> 00:14:33.000
ムービーファイル出力でビデオデータを受信することで、録画中にビデオを検査し、シーンを分析することができます。

00:14:33.000 -> 00:14:40.000
また、ムービーファイルの出力でオーディオデータを受信すると、録音中にオーディオをサンプリングし、録音されているものを聴くことができます。

00:14:40.000 -> 00:14:50.000
このようなキャプチャグラフを使用すると、非圧縮ビデオとオーディオのサンプルを受信しながら、AVCaptureMovieFileOutputに録音の仕組みをオフロードできます。

00:14:50.000 -> 00:14:55.000
これらの高度なストリーミング構成を実装するには、新しいAPIを使用する必要がありません。

00:14:55.000 -> 00:15:01.000
既存のAPIでより多くのことをできるようにすることで、これを可能にしました。

00:15:01.000 -> 00:15:06.000
そして最後に、ユーザーがマルチタスクをしている間、あなたのアプリがどのようにカメラを使用できるかについて話し合います。

00:15:06.000 -> 00:15:09.000
iPadでは、ユーザーはさまざまな方法でマルチタスクを実行できます。

00:15:09.000 -> 00:15:19.000
たとえば、Split ViewまたはSlide Overでメモを読みながらボイスメモを録音し、Safariの上のフローティングウィンドウでフルスクリーンでメモを書きます。

00:15:19.000 -> 00:15:26.000
ピクチャー・イン・ピクチャーを使用すると、より多くのWWDCビデオを見るためのリマインダーを追加しながら、ビデオの再生を続けることができます。

00:15:26.000 -> 00:15:33.000
また、iPadOS 16の新しいStage Managerを使用すると、ユーザーはサイズ変更可能なフローティングウィンドウで複数のアプリを開くことができます。

00:15:33.000 -> 00:15:38.000
iOS 16以降、AVCaptureSessionsはマルチタスク中にカメラを使用できるようになります。

00:15:38.000 -> 00:15:46.000
マルチタスク中にカメラシステムが提供できるサービスの質が懸念されるため、以前はマルチタスク中にカメラへのアクセスを阻止しました。

00:15:46.000 -> 00:15:54.000
カメラを使用してアプリと一緒に実行されるゲームのようなリソースを大量に消費するアプリは、フレームドロップやその他のレイテンシを誘発し、カメラのフィードが悪くなる可能性があります。

00:15:54.000 -> 00:16:00.000
数ヶ月または数年後に品質の悪いビデオを見ているユーザーは、マルチタスク中に録画したことを覚えていないかもしれません。

00:16:00.000 -> 00:16:05.000
良いカメラ体験を提供することが私たちの優先事項です。

00:16:05.000 -> 00:16:13.000
システムがマルチタスク中にカメラから録画されたビデオを検出すると、低品質のビデオの可能性についてユーザーに知らせるダイアログが表示されます。

00:16:13.000 -> 00:16:20.000
このダイアログは、AVCaptureMovieFileOutputまたはAVAssetWriterで録画が終了した後に表示されます。

00:16:20.000 -> 00:16:26.000
すべてのアプリのシステムによって一度だけ表示され、閉じるためのOKボタンがあります。

00:16:26.000 -> 00:16:33.000
AVCaptureSessionには、マルチタスクカメラアクセスがサポートされ、有効になっていることを示す2つの新しいプロパティが追加されています。

00:16:33.000 -> 00:16:45.000
これを有効にしたキャプチャセッションは、「ビデオデバイスが複数のフォアグラウンドアプリで利用できない」という理由で中断されなくなります。一部のアプリでは、カメラを使用するためにフルスクリーン体験が必要な場合があります。

00:16:45.000 -> 00:16:50.000
これは、アプリがシステムリソースのために他のフォアグラウンドアプリと競合しないようにしたい場合に便利です。

00:16:50.000 -> 00:16:56.000
たとえば、ARKitはマルチタスク中のカメラの使用をサポートしていません。

00:16:56.000 -> 00:16:59.000
他のアプリと一緒に実行するときは、アプリがうまく機能することを確認する必要があります。

00:16:59.000 -> 00:17:08.000
通知を監視することで、アプリをシステムプレッシャーの増加に回復力のあるものにし、フレームレートを下げるなど、影響を軽減するための措置を講じます。

00:17:08.000 -> 00:17:15.000
低解像度、ビニング、または非HDR形式を要求することで、システム上のアプリのフットプリントを減らすことができます。

00:17:15.000 -> 00:17:23.000
パフォーマンスを維持するためのベストプラクティスの詳細については、記事「マルチタスク中にカメラにアクセスする」をお読みください。

00:17:23.000 -> 00:17:30.000
また、ビデオ通話やビデオ会議アプリは、システムが提供するピクチャ・イン・ピクチャウィンドウにリモート参加者を表示できます。

00:17:30.000 -> 00:17:36.000
これで、アプリのユーザーはiPadでマルチタスクをしながらシームレスにビデオ通話を続けることができます。

00:17:36.000 -> 00:17:43.000
AVKitは、アプリがリモートコール参加者を表示するためのビューコントローラーを指定するためのAPIをiOS 15に導入しました。

00:17:43.000 -> 00:17:48.000
ビデオ通話ビューコントローラーを使用すると、ウィンドウの内容をカスタマイズできます。

00:17:48.000 -> 00:17:55.000
養子縁組の詳細については、「ビデオ通話のためのピクチャー・イン・ピクチャーの採用」の記事を参照してください。

00:17:55.000 -> 00:17:58.000
そして、これでiOSカメラキャプチャの進歩が終わります。

00:17:58.000 -> 00:18:12.000
AVFoundationを使用してLiDARスキャナーから深度をストリーミングする方法、アプリが改善されたフェイスレンダリングを受け取る方法、アプリに合わせた高度なAVCaptureSessionストリーミング構成、そして最後に、アプリがマルチタスク中にカメラを使用する方法を示しました。

00:18:12.000 -> 00:18:14.000
あなたのWWDCがうまくいくことを願っています。

00:18:14.000 -> 23:59:59.000
♪ ♪

