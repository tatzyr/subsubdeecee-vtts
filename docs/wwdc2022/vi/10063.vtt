WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:12.000
Dhruva: Chào mừng đến với WWDC 2022.

00:00:12.000 --> 00:00:16.000
Tên tôi là Dhruva, và tôi là một Kỹ sư GPUSW.

00:00:16.000 --> 00:00:25.000
Hôm nay, Matteo và tôi sẽ khám phá tất cả các tính năng và cải tiến mới được giới thiệu cho việc học máy trong năm nay trong Metal.

00:00:25.000 --> 00:00:31.000
Đào tạo học máy là quá trình tính toán chuyên sâu nhất của đường ống ML.

00:00:31.000 --> 00:00:37.000
Do tính chất song song của chúng, GPU vượt trội trong khối lượng công việc ML.

00:00:37.000 --> 00:00:44.000
Các API học máy Metal được hiển thị thông qua một khuôn khổ gọi là Metal Performance Shaders, hoặc MPS.

00:00:44.000 --> 00:00:56.000
MPS là một bộ sưu tập các GPU nguyên thủy hiệu suất cao cho các lĩnh vực khác nhau như Xử lý hình ảnh, Đại số tuyến tính, Truy tìm tia và học máy.

00:00:56.000 --> 00:01:02.000
Những hạt nhân Kim loại này được tối ưu hóa để cung cấp hiệu suất tốt nhất trên tất cả các nền tảng của chúng tôi.

00:01:02.000 --> 00:01:08.000
Ví dụ bộ lọc MPSImageCanny trả về một bản đồ cạnh cho một hình ảnh đầu vào.

00:01:08.000 --> 00:01:12.000
Đây là một thao tác phổ biến trong các ứng dụng phân đoạn hình ảnh.

00:01:12.000 --> 00:01:21.000
Năm nay, bộ lọc Canny có thể xử lý hình ảnh 4K, độ phân giải cao nhanh hơn tới tám lần.

00:01:21.000 --> 00:01:32.000
Biểu đồ MPS, là một biểu đồ tính toán mục đích chung cho GPU nằm trên khung MPS và mở rộng hỗ trợ cho các tenxơ đa chiều.

00:01:32.000 --> 00:01:38.000
Tôi khuyên bạn nên xem phiên trước để biết thêm chi tiết về cách sử dụng MPS Graph.

00:01:38.000 --> 00:01:45.000
Các khung ML cấp cao như CoreML và Tensorflow nằm trên đầu Biểu đồ MPS.

00:01:45.000 --> 00:01:51.000
Bạn có thể tăng tốc mạng TensorFlow của mình trên GPU với trình cắm TensorFlow Metal.

00:01:51.000 --> 00:01:57.000
Để biết thêm về cách tận dụng tối đa TensorFlow, hãy xem phiên năm ngoái.

00:01:57.000 --> 00:02:01.000
Matteo và tôi có ba chủ đề cần đề cập trong phiên này.

00:02:01.000 --> 00:02:07.000
Đầu tiên, tôi sẽ giới thiệu khung ML mới nhất đến với Apple GPUs, PyTorch.

00:02:07.000 --> 00:02:14.000
Tiếp theo, tôi sẽ đi sâu vào những cải tiến được thực hiện cho TensorFlow trong năm nay.

00:02:14.000 --> 00:02:20.000
Cuối cùng, Matteo sẽ nói về những gì mới trong khuôn khổ MPS Graph.

00:02:20.000 --> 00:02:27.000
Chúng tôi thực sự vui mừng vì bây giờ bạn sẽ có thể tăng tốc mạng PyTorch của mình trên GPU Mac của mình.

00:02:27.000 --> 00:02:32.000
PyTorch, là một khung học máy mã nguồn mở phổ biến.

00:02:32.000 --> 00:02:41.000
Tính năng được yêu cầu nhiều nhất, trong cộng đồng PyTorch là hỗ trợ tăng tốc GPU trên Apple silicon.

00:02:41.000 --> 00:02:50.000
Chúng tôi đang mang sức mạnh của Metal đến PyTorch bằng cách giới thiệu một phụ trợ MPS mới cho hệ sinh thái PyTorch.

00:02:50.000 --> 00:02:56.000
Phần phụ trợ này sẽ là một phần của bản phát hành PyTorch 1.12 chính thức.

00:02:56.000 --> 00:03:02.000
Phần phụ trợ MPS triển khai các hạt nhân hoạt động PyTorch, cũng như khung Thời gian chạy.

00:03:02.000 --> 00:03:10.000
Các hoạt động gọi vào MPS Graph và MPS và thành phần Runtime sử dụng Metal.

00:03:10.000 --> 00:03:22.000
Điều này cho phép PyTorch sử dụng các hạt nhân hiệu quả cao từ MPS cùng với hàng đợi Lệnh của Metal, bộ đệm Lệnh và nguyên thủy đồng bộ hóa.

00:03:22.000 --> 00:03:32.000
Các hạt nhân hoạt động và các thành phần PyTorch MPS Runtime là một phần của mã nguồn mở và được hợp nhất vào kho lưu trữ PyTorch GitHub chính thức.

00:03:32.000 --> 00:03:37.000
Sử dụng phụ trợ MPS PyTorch là một quy trình ba bước đơn giản.

00:03:37.000 --> 00:03:44.000
Đầu tiên, bắt đầu với PyTorch 1.12, bạn có thể cài đặt gói cơ sở bằng cách sử dụng 'pip install torch'.

00:03:44.000 --> 00:03:49.000
Gói này có sẵn trong kho lưu trữ gói python chính thức.

00:03:49.000 --> 00:03:57.000
Để biết thêm chi tiết về thiết lập và cài đặt môi trường, vui lòng tham khảo trang web Tài nguyên Nhà phát triển Kim loại.

00:03:57.000 --> 00:04:01.000
Thứ hai, nhập PyTorch và tạo thiết bị MPS.

00:04:01.000 --> 00:04:09.000
Đoạn mã này sử dụng phụ trợ thiết bị MPS nếu nó có sẵn, nếu không nó sẽ quay trở lại CPU.

00:04:09.000 --> 00:04:14.000
Bước cuối cùng là chuyển đổi các mô hình và đầu vào của bạn để sử dụng thiết bị MPS.

00:04:14.000 --> 00:04:23.000
Để chứng minh cách thực hiện điều này, tôi sẽ sử dụng một ví dụ chạy suy luận trên mô hình ResNet50 được đào tạo trước từ torchvision.

00:04:23.000 --> 00:04:27.000
Theo mặc định, mô hình sẽ chạy trên CPU.

00:04:27.000 --> 00:04:32.000
Bạn có thể sử dụng phương pháp "đến" để chuyển đổi mô hình thành sử dụng thiết bị MPS.

00:04:32.000 --> 00:04:39.000
Điều này đảm bảo rằng các tenxơ trung gian bên trong mô hình cũng sẽ sử dụng phụ trợ MPS tăng tốc.

00:04:39.000 --> 00:04:42.000
Cuối cùng, bạn có thể chạy mô hình.

00:04:42.000 --> 00:04:47.000
Ví dụ này chuyển một tenxơ đầu vào ngẫu nhiên đến mô hình MPS.

00:04:47.000 --> 00:04:51.000
Theo mặc định, tất cả các tenxơ được phân bổ trên CPU.

00:04:51.000 --> 00:04:58.000
Để sử dụng phụ trợ MPS, bạn cũng sẽ cần cung cấp mpsDevice tại đây.

00:04:58.000 --> 00:05:03.000
Tất cả các thao tác tiếp theo trên tenxơ này sẽ được tăng tốc trên GPU.

00:05:03.000 --> 00:05:09.000
Cuối cùng, chuyển đầu vào mẫu cho mô hình MPS để có được dự đoán.

00:05:09.000 --> 00:05:15.000
Bây giờ bạn đã biết cách sử dụng thiết bị MPS, tôi sẽ chỉ cho bạn một ví dụ về PyTorch đang hoạt động.

00:05:15.000 --> 00:05:17.000
Tôi luôn muốn trở thành một nghệ sĩ nổi tiếng.

00:05:17.000 --> 00:05:25.000
Vì vậy, tôi quyết định sử dụng máy học và GPU của mình để giúp tạo ra tác phẩm nghệ thuật của mình bằng cách sử dụng mạng StyleTransfer.

00:05:25.000 --> 00:05:30.000
Mạng lưới này cho phép bạn áp dụng một phong cách nghệ thuật khác cho một hình ảnh.

00:05:30.000 --> 00:05:37.000
Trong trường hợp này, mục tiêu là học cách áp dụng phong cách của Van Gogh trong Đêm đầy sao vào bức tranh về một con mèo này.

00:05:37.000 --> 00:05:45.000
Với thiết bị MPS mới, bạn sẽ có thể sử dụng GPU để đào tạo mạng PyTorch của mình nhanh hơn đáng kể.

00:05:45.000 --> 00:05:53.000
Để chứng minh điều này, tôi sẽ bắt đầu đào tạo mạng này trên cả CPU và GPU đồng thời trên M1 Max.

00:05:53.000 --> 00:06:03.000
Phải mất hàng nghìn lần lặp lại để học phong cách này, nhưng GPU có thể hội tụ thành một mô hình hợp lý trong thời gian ngắn hơn nhiều.

00:06:03.000 --> 00:06:10.000
Ngoài StyleTransfer, chúng tôi đã thấy tốc độ đáng kinh ngạc trên tất cả các điểm chuẩn PyTorch này.

00:06:10.000 --> 00:06:18.000
Trên M1 Ultra, chúng tôi đã thấy tốc độ nhanh hơn tới 20 lần với tốc độ trung bình nhanh hơn 8,3 lần.

00:06:18.000 --> 00:06:27.000
PyTorch giúp bạn dễ dàng phát triển các mô hình học máy và bạn sẽ có thể tiết kiệm rất nhiều thời gian bằng cách sử dụng GPU của Apple để đào tạo chúng.

00:06:27.000 --> 00:06:33.000
Tiếp theo, tôi sẽ đi sâu vào tất cả các cải tiến mà chúng tôi đã thực hiện trong năm nay cho TensorFlow.

00:06:33.000 --> 00:06:43.000
Tăng tốc TensorFlow Metal đã có sẵn kể từ TensorFlow phiên bản 2.5 thông qua trình cắm TensorFlow Metal.

00:06:43.000 --> 00:06:47.000
Kể từ đó, một số tính năng và cải tiến bổ sung đã được thêm vào.

00:06:47.000 --> 00:06:58.000
Chúng bao gồm đào tạo được cải thiện với các lô lớn hơn, các hoạt động mới và hỗ trợ hoạt động tùy chỉnh, cải tiến RNN và đào tạo phân tán.

00:06:58.000 --> 00:07:10.000
Các bản phát hành trình cắm TensorFlow Metal phù hợp với các bản phát hành TensorFlow chính, vì vậy hãy đảm bảo bạn cập nhật các gói TensorFlow của mình để có được các tính năng và cải tiến mới nhất.

00:07:10.000 --> 00:07:13.000
Hãy bắt đầu với kích thước lô lớn hơn.

00:07:13.000 --> 00:07:21.000
Những cải tiến phần mềm năm nay trong TensorFlow Metal cho phép bạn tận dụng những lợi ích độc đáo của kiến trúc silicon Apple.

00:07:21.000 --> 00:07:28.000
Biểu đồ này cho thấy tốc độ đào tạo mô hình ResNet50 với nhiều kích thước lô khác nhau.

00:07:28.000 --> 00:07:38.000
Dữ liệu cho thấy hiệu suất được cải thiện với kích thước lô lớn hơn vì mỗi bản cập nhật gradient tương ứng chặt chẽ hơn với gradient thực sự.

00:07:38.000 --> 00:07:46.000
Kiến trúc bộ nhớ thống nhất của Apple silicon cho phép bạn chạy các mạng lớn hơn hoặc kích thước lô lớn hơn.

00:07:46.000 --> 00:07:53.000
Bây giờ bạn có thể chạy khối lượng công việc của mình trên một Mac Studio duy nhất thay vì chia nó thành một cụm đám mây, điều đó thật tuyệt vời!

00:07:53.000 --> 00:08:01.000
Kiến trúc Apple Silicon cũng có hiệu suất cao trên mỗi watt, có nghĩa là mạng của bạn chạy hiệu quả hơn bao giờ hết.

00:08:01.000 --> 00:08:06.000
Tiếp theo tôi sẽ nói về các hoạt động mới và các hoạt động tùy chỉnh.

00:08:06.000 --> 00:08:18.000
Trình cắm Tensorflow Metal hiện có khả năng tăng tốc GPU cho nhiều hoạt động mới, bao gồm argMin, all, pack, adaDelta, và nhiều hơn nữa.

00:08:18.000 --> 00:08:26.000
Nhưng điều gì sẽ xảy ra nếu bạn muốn tăng tốc GPU cho một hoạt động hiện không được hỗ trợ trong TensorFlow API?

00:08:26.000 --> 00:08:30.000
Để làm điều này, bạn sẽ cần tạo một thao tác tùy chỉnh.

00:08:30.000 --> 00:08:36.000
Đây là một ví dụ về một mạng tích chập đơn giản chạy cho hai lần lặp lại.

00:08:36.000 --> 00:08:42.000
Dòng thời gian đại diện cho công việc được thực hiện trên GPU và CPU, tương ứng ở trên và dưới.

00:08:42.000 --> 00:08:50.000
Mạng thực hiện tích chập theo sau là maxpool-ing và sau đó là tổn thất entropy chéo softmax.

00:08:50.000 --> 00:09:00.000
Tất cả các hoạt động này đều được tăng tốc GPU trong trình cắm TensorFlow Metal thông qua MPS Graph Nhưng bạn có thể muốn sử dụng chức năng mất tùy chỉnh.

00:09:00.000 --> 00:09:13.000
Nếu không có khả năng tăng tốc GPU MPS cho sự mất mát tùy chỉnh này, công việc đó sẽ cần được thực hiện trên dòng thời gian CPU, điều này giới thiệu chi phí đồng bộ hóa và bỏ đói GPU.

00:09:13.000 --> 00:09:18.000
Bạn có thể đạt được hiệu suất tốt hơn nhiều bằng cách thực hiện tổn thất tùy chỉnh này trên GPU.

00:09:18.000 --> 00:09:26.000
Để thực hiện một thao tác tùy chỉnh, bạn sẽ cần hiểu giao thức TensorFlow Metal Stream.

00:09:26.000 --> 00:09:31.000
Đây là một giao thức mà bạn sử dụng để mã hóa các hoạt động của GPU.

00:09:31.000 --> 00:09:37.000
Luồng Metal giữ một tham chiếu đến MTLCommandBuffer mà bạn sử dụng để mã hóa hạt nhân GPU của mình.

00:09:37.000 --> 00:09:45.000
Nó cũng hiển thị dispatch_queue để sử dụng cho đồng bộ hóa phía CPU trong khi mã hóa vì có thể có nhiều luồng gửi công việc.

00:09:45.000 --> 00:09:51.000
Sử dụng các phương thức cam kết hoặc commitAndWait để gửi công việc đến GPU.

00:09:51.000 --> 00:09:59.000
CommitAndWait là một công cụ gỡ lỗi sẽ đợi cho đến khi bộ đệm lệnh hiện tại được thực hiện để bạn có thể quan sát các bài gửi được tuần tự hóa.

00:09:59.000 --> 00:10:04.000
Bây giờ hãy xem những khái niệm này có thể được sử dụng như thế nào để triển khai một hoạt động tùy chỉnh.

00:10:04.000 --> 00:10:07.000
Có ba bước để viết một thao tác tùy chỉnh.

00:10:07.000 --> 00:10:09.000
Đầu tiên, đăng ký hoạt động.

00:10:09.000 --> 00:10:13.000
Tiếp theo, thực hiện thao tác bằng MetalStream.

00:10:13.000 --> 00:10:19.000
Và cuối cùng, nhập thao tác vào tập lệnh đào tạo của bạn và bắt đầu sử dụng nó.

00:10:19.000 --> 00:10:22.000
Tôi sẽ bắt đầu với việc đăng ký hoạt động.

00:10:22.000 --> 00:10:32.000
Sử dụng macro REGISTER_OP được hiển thị bởi lõi TensorFlow để chỉ định ngữ nghĩa của op và cách nó nên được xác định trong trình cắm TensorFlow Metal.

00:10:32.000 --> 00:10:36.000
Tiếp theo, triển khai op bằng cách sử dụng TensorFlow_MetalStream.

00:10:36.000 --> 00:10:40.000
Bắt đầu bằng cách xác định chức năng "tính toán".

00:10:40.000 --> 00:10:50.000
Bây giờ, bên trong hàm này, lấy các đối tượng TensorFlow_Tensor cho đầu vào và xác định đầu ra, có thể yêu cầu phân bổ.

00:10:50.000 --> 00:10:55.000
Sau đó tạo một bộ mã hóa bằng cách sử dụng bộ đệm lệnh của luồng Metal.

00:10:55.000 --> 00:10:58.000
Tiếp theo, xác định hạt nhân GPU tùy chỉnh.

00:10:58.000 --> 00:11:02.000
Hoạt động của bạn nên được mã hóa bên trong dispatch_queue được cung cấp bởi luồng Metal.

00:11:02.000 --> 00:11:08.000
Điều này đảm bảo các bài gửi từ nhiều luồng được tuần tự hóa.

00:11:08.000 --> 00:11:16.000
Sau đó cam kết hạt nhân bằng cách sử dụng phương thức được cung cấp trong giao thức TensorFlow_MetalStream.

00:11:16.000 --> 00:11:21.000
Cuối cùng, xóa các tham chiếu đến các tenxơ được phân bổ.

00:11:21.000 --> 00:11:27.000
Cuối cùng, nhập thao tác vào tập lệnh đào tạo của bạn để bắt đầu sử dụng nó.

00:11:27.000 --> 00:11:35.000
Trong bước này, hãy xây dựng tệp thư viện động được chia sẻ của op tùy chỉnh được gọi là zero_out.so.

00:11:35.000 --> 00:11:41.000
Tham khảo Tài nguyên Nhà phát triển Kim loại để biết thông tin về cách xây dựng và nhập các tệp .so.

00:11:41.000 --> 00:11:50.000
Ví dụ này nhập thao tác vào tập lệnh đào tạo bằng cách sử dụng TensorFlow load_op_library, đây là một bước tùy chọn.

00:11:50.000 --> 00:11:56.000
Bây giờ, điều này hoạt động giống như một trình bao bọc python và op tùy chỉnh của chúng tôi có thể được gọi trong tập lệnh đào tạo.

00:11:56.000 --> 00:12:04.000
Tiếp theo, tôi muốn chỉ cho bạn một ví dụ về một ứng dụng thú vị được gọi là Neural Radiance Fields, hoặc NeRF.

00:12:04.000 --> 00:12:13.000
Chúng tôi đã viết một thao tác tùy chỉnh giúp nâng cao hiệu suất của mạng bằng cách cho phép tăng tốc GPU để có thuật toán tốt hơn.

00:12:13.000 --> 00:12:18.000
NeRF là một mạng được sử dụng để tổng hợp các chế độ xem 3D của một mô hình.

00:12:18.000 --> 00:12:23.000
Để đào tạo, NeRF lấy đầu vào, hình ảnh của một đối tượng từ các góc độ khác nhau.

00:12:23.000 --> 00:12:32.000
Mạng NeRF bao gồm hai perceptron nhiều lớp xếp chồng lên nhau và đầu ra là biểu diễn thể tích của mô hình.

00:12:32.000 --> 00:12:38.000
Tối ưu hóa hiệu suất chính cho đào tạo thời gian thực sử dụng triển khai bảng băm.

00:12:38.000 --> 00:12:43.000
Mạng được cập nhật này cho phép một perceptron nhiều lớp nhỏ hơn nhiều.

00:12:43.000 --> 00:12:52.000
TensorFlow không hỗ trợ các bảng băm nguyên bản nên chúng tôi sử dụng tính năng op tùy chỉnh để triển khai chúng trong trình cắm Metal.

00:12:52.000 --> 00:12:58.000
Tăng tốc GPU cho các bảng băm giúp có thể đào tạo NeRF nhanh hơn nhiều.

00:12:58.000 --> 00:13:06.000
Tôi sẽ bắt đầu trên chiếc MacBook này và chạy triển khai perceptron nhiều lớp ban đầu.

00:13:06.000 --> 00:13:13.000
Để làm cho bất cứ điều gì hợp lý, chúng ta cần ít nhất 20 kỷ nguyên nhưng mỗi kỷ nguyên mất khoảng 100 giây.

00:13:13.000 --> 00:13:17.000
Điều đó có nghĩa là sẽ mất khoảng 30 phút trước khi bất cứ thứ gì được nhìn thấy.

00:13:17.000 --> 00:13:26.000
Vì vậy, bây giờ tôi sẽ khởi động lại khóa đào tạo từ một tệp trạm kiểm soát được đào tạo trước, được để lại để đào tạo trước 30 phút.

00:13:26.000 --> 00:13:28.000
Điều này bắt đầu từ kỷ nguyên 20.

00:13:28.000 --> 00:13:33.000
Mô hình 3D bị mờ và không rõ ràng ngay cả sau 30 phút đào tạo.

00:13:33.000 --> 00:13:38.000
Nó sẽ đòi hỏi thời gian đào tạo lâu hơn nhiều để mạng lưới học một mô hình rõ ràng hơn.

00:13:38.000 --> 00:13:45.000
Cách tiếp cận perceptron nhiều lớp hai lớp được xếp chồng lên nhau ban đầu mà không có bảng băm tùy chỉnh là quá chậm.

00:13:45.000 --> 00:13:52.000
Bây giờ trên MacBook này, tôi sẽ khởi động phiên bản được tối ưu hóa sử dụng các bảng băm tùy chỉnh.

00:13:52.000 --> 00:14:00.000
Việc triển khai này đã có thể hiển thị một mô hình rõ ràng hơn nhiều và mỗi kỷ nguyên chỉ mất 10 giây để học.

00:14:00.000 --> 00:14:09.000
Để biết thêm thông tin về dự án này, hãy xem mã mẫu mà chúng tôi đã tải lên Metal Developer Resources.

00:14:09.000 --> 00:14:20.000
NeRF chỉ là một trong nhiều mạng thể hiện cách bạn có thể triển khai tăng tốc GPU cho các hoạt động tùy chỉnh của riêng mình để làm cho mạng của bạn chạy cực nhanh.

00:14:20.000 --> 00:14:26.000
Tôi mong muốn được tìm hiểu về tất cả các tùy chỉnh sáng tạo mà bạn thực hiện, trong tương lai.

00:14:26.000 --> 00:14:33.000
Bây giờ tôi muốn chỉ cho bạn cách sử dụng GPU của Apple để phân phối đào tạo khối lượng công việc ML.

00:14:33.000 --> 00:14:46.000
Để phân phối đào tạo khối lượng công việc, bạn có thể chạy nhiều phiên bản của tập lệnh đào tạo trong các quy trình riêng biệt trong đó mỗi quy trình đánh giá một lần lặp duy nhất của mô hình.

00:14:46.000 --> 00:14:50.000
Mỗi quy trình sẽ đọc dữ liệu từ một kho dữ liệu trung tâm.

00:14:50.000 --> 00:14:55.000
Sau đó, nó sẽ chạy qua mô hình và tính toán độ dốc của mô hình.

00:14:55.000 --> 00:15:06.000
Tiếp theo, các quy trình sẽ tính trung bình các độ dốc và truyền đạt điều này với nhau để mỗi quá trình có cùng độ dốc trước lần lặp tiếp theo.

00:15:06.000 --> 00:15:13.000
Cuối cùng, mô hình được cập nhật và bạn có thể lặp lại quy trình này cho đến khi tất cả các lần lặp lại cạn kiệt.

00:15:13.000 --> 00:15:23.000
Để chứng minh điều này trên TensorFlow, tôi sẽ sử dụng một ví dụ về đào tạo phân tán bằng cách sử dụng một khung mã nguồn mở phổ biến được gọi là Horovod.

00:15:23.000 --> 00:15:27.000
Horovod sử dụng phương pháp giảm tất cả vòng.

00:15:27.000 --> 00:15:34.000
Trong thuật toán này, mỗi N nút giao tiếp với hai trong số các đồng nghiệp của nó nhiều lần.

00:15:34.000 --> 00:15:40.000
Sử dụng giao tiếp này, công nhân xử lý đồng bộ hóa độ dốc trước mỗi lần lặp.

00:15:40.000 --> 00:15:47.000
Tôi sẽ thể hiện điều này trong hành động bằng cách sử dụng bốn Mac Studios được kết nối với nhau bằng cáp Thunderbolt.

00:15:47.000 --> 00:15:53.000
Đối với ví dụ này, tôi sẽ đào tạo ResNet, một trình phân loại cho hình ảnh.

00:15:53.000 --> 00:15:59.000
Thanh ở bên cạnh mỗi Mac Studio hiển thị việc sử dụng GPU trong khi đào tạo mạng này.

00:15:59.000 --> 00:16:04.000
Đối với một Mac Studio duy nhất, hiệu suất là khoảng 200 hình ảnh mỗi giây.

00:16:04.000 --> 00:16:16.000
Khi tôi thêm một Mac Studio khác được kết nối qua Thunderbolt, hiệu suất gần như tăng gấp đôi lên 400 hình ảnh mỗi giây vì cả hai GPU đều được sử dụng tối đa.

00:16:16.000 --> 00:16:24.000
Cuối cùng, khi tôi kết nối thêm hai Mac Studios, hiệu suất được nâng lên 800 hình ảnh mỗi giây.

00:16:24.000 --> 00:16:30.000
Đây gần như là tỷ lệ tuyến tính trên khối lượng công việc đào tạo ràng buộc tính toán của bạn.

00:16:30.000 --> 00:16:35.000
Bây giờ đây là một cái nhìn về hiệu suất đào tạo Phân tán của TensorFlow.

00:16:35.000 --> 00:16:41.000
Biểu đồ này cho thấy tốc độ tương đối cho một, hai và bốn Mac Studios.

00:16:41.000 --> 00:16:52.000
Chúng được kết nối trong một cấu trúc liên kết vòng và chạy các mạng TensorFlow liên kết tính toán như resNet và DistilBERT với trình cắm TensorFlow Metal mới nhất và Horovod.

00:16:52.000 --> 00:16:57.000
Cơ sở là màn trình diễn trên một Mac Studio duy nhất.

00:16:57.000 --> 00:17:12.000
Biểu đồ cho thấy hiệu suất mạng có quy mô với việc bổ sung từng GPU để giờ đây bạn có thể tận dụng GPU trên nhiều thiết bị, để tăng tốc thời gian đào tạo và tận dụng tối đa tất cả các thiết bị Apple của mình.

00:17:12.000 --> 00:17:24.000
Tất cả các cải tiến và tính năng được mở khóa cho TensorFlow năm nay lên đến đỉnh điểm trong biểu đồ này cho thấy hiệu suất tương đối so với việc triển khai CPU với nhiều cải tiến hơn trong tương lai.

00:17:24.000 --> 00:17:30.000
Bây giờ Matteo sẽ chia sẻ những gì mới trong khuôn khổ MPSGraph.

00:17:30.000 --> 00:17:31.000
Matteo: Cảm ơn, Dhruva.

00:17:31.000 --> 00:17:35.000
Xin chào, tên tôi là Matteo, và tôi là một kỹ sư phần mềm GPU.

00:17:35.000 --> 00:17:41.000
PyTorch và TensorFlow nằm trên đỉnh của khung MPSGraph.

00:17:41.000 --> 00:17:50.000
Đổi lại, MPSGraph sử dụng các nguyên thủy song song được hiển thị bởi khung MPS để tăng tốc công việc trên GPU.

00:17:50.000 --> 00:17:59.000
Hôm nay tôi sẽ nói về hai tính năng mà bạn có thể sử dụng để tăng tốc khối lượng công việc tính toán của mình hơn nữa với MPSGraph.

00:17:59.000 --> 00:18:05.000
Đầu tiên, tôi sẽ hiển thị API sự kiện được chia sẻ mới cho phép bạn đồng bộ hóa công việc giữa hai biểu đồ.

00:18:05.000 --> 00:18:13.000
Thứ hai, tôi sẽ xem xét các thao tác mới, mà bạn có thể sử dụng để làm nhiều hơn nữa với MPSGraph.

00:18:13.000 --> 00:18:16.000
Tôi sẽ bắt đầu với API Sự kiện được Chia sẻ.

00:18:16.000 --> 00:18:22.000
Chạy các ứng dụng trên cùng một hàng đợi lệnh đảm bảo đồng bộ hóa giữa các khối lượng công việc.

00:18:22.000 --> 00:18:33.000
Trong ví dụ này, khối lượng công việc tính toán được đảm bảo luôn chấm dứt trước khi các khối lượng công việc khác, chẳng hạn như xử lý hậu kỳ và hiển thị, được gửi đi.

00:18:33.000 --> 00:18:39.000
Trong những trường hợp như thế này, bạn sẽ tận dụng tính song song của GPU trong mỗi công văn.

00:18:39.000 --> 00:18:52.000
Tuy nhiên, một số ứng dụng có thể được hưởng lợi từ tính song song hơn, trong đó phần đầu tiên của GPU được sử dụng để tính toán và phần thứ hai được sử dụng để xử lý hậu kỳ và hiển thị.

00:18:52.000 --> 00:18:58.000
Điều này có thể đạt được bằng cách gửi công việc đến GPU trên nhiều hàng đợi lệnh.

00:18:58.000 --> 00:19:09.000
Thật không may, trong trường hợp này, đường ống xử lý hậu kỳ có thể được gửi đi trước khi máy tính tạo ra kết quả, giới thiệu một cuộc đua dữ liệu.

00:19:09.000 --> 00:19:21.000
API Sự kiện được Chia sẻ có thể được sử dụng để giải quyết vấn đề này và giới thiệu đồng bộ hóa trên các hàng đợi lệnh để đảm bảo rằng các phụ thuộc quy trình làm việc có thể được thỏa mãn.

00:19:21.000 --> 00:19:25.000
Sử dụng các sự kiện được chia sẻ trong mã của bạn rất đơn giản.

00:19:25.000 --> 00:19:29.000
Giả sử bạn đang làm việc với hai biểu đồ.

00:19:29.000 --> 00:19:32.000
Đầu tiên chịu trách nhiệm về khối lượng công việc tính toán.

00:19:32.000 --> 00:19:37.000
Thứ hai, chịu trách nhiệm về khối lượng công việc xử lý bài đăng.

00:19:37.000 --> 00:19:47.000
Chúng ta cũng giả sử rằng kết quả của biểu đồ tính toán được sử dụng làm đầu vào cho biểu đồ xử lý hậu kỳ và chúng chạy trên các hàng đợi lệnh khác nhau.

00:19:47.000 --> 00:19:55.000
Bản nhạc MPSGraph mới trong Metal System Trace chỉ ra rằng các hàng đợi lệnh đang chồng chéo với nhau.

00:19:55.000 --> 00:19:58.000
Điều này tạo ra một cuộc đua dữ liệu.

00:19:58.000 --> 00:20:01.000
Bạn có thể giải quyết vấn đề này bằng cách sử dụng một sự kiện được chia sẻ.

00:20:01.000 --> 00:20:05.000
Đầu tiên, tạo sự kiện bằng thiết bị Metal.

00:20:05.000 --> 00:20:14.000
Tiếp theo, gọi phương thức tín hiệu trong bộ mô tả thực thi, cung cấp sự kiện, hành động và giá trị.

00:20:14.000 --> 00:20:24.000
Sau đó, tất cả những gì bạn phải làm là gọi phương thức chờ trên bộ mô tả thứ hai, cung cấp biến sự kiện và giá trị.

00:20:24.000 --> 00:20:37.000
Bây giờ, dấu vết hệ thống Metal chỉ ra rằng hai hàng đợi lệnh được chạy tuần tự và sự phụ thuộc giữa biểu đồ tính toán và xử lý hậu kỳ đã được giải quyết.

00:20:37.000 --> 00:20:43.000
Đó là cách bạn có thể sử dụng các sự kiện được chia sẻ để giải quyết các vấn đề đồng bộ hóa trong các ứng dụng của mình.

00:20:43.000 --> 00:20:48.000
Thứ hai, tôi sẽ nói về các hoạt động mới được hỗ trợ bởi MPSGraph.

00:20:48.000 --> 00:20:52.000
Những thao tác này cho phép bạn làm được nhiều việc hơn với khuôn khổ.

00:20:52.000 --> 00:20:59.000
Tôi sẽ xem xét một số chi tiết của từng hoạt động mới này, bắt đầu với RNN.

00:20:59.000 --> 00:21:07.000
MPSGraph hiện hiển thị ba hoạt động thường được sử dụng trong các ứng dụng Mạng thần kinh tái phát.

00:21:07.000 --> 00:21:12.000
Đây là các lớp RNN, LSTM và GRU.

00:21:12.000 --> 00:21:19.000
Tất cả các hoạt động này đều hoạt động tương tự nhau, vì vậy tôi sẽ chỉ tập trung vào LSTM hôm nay.

00:21:19.000 --> 00:21:25.000
Hoạt động LSTM thường được sử dụng để xử lý ngôn ngữ tự nhiên và các ứng dụng khác.

00:21:25.000 --> 00:21:29.000
Sơ đồ này cho thấy cách hoạt động của LSTM.

00:21:29.000 --> 00:21:35.000
Để tìm hiểu thêm về nó, hãy xem phiên WWDC trước đây của chúng tôi.

00:21:35.000 --> 00:21:43.000
Bạn có thể tự triển khai đơn vị LSTM, nhưng để làm như vậy, bạn sẽ phải xây dựng biểu đồ con tùy chỉnh khá phức tạp này.

00:21:43.000 --> 00:21:53.000
Thay vào đó, bạn có thể sử dụng thao tác LSTM mới, mã hóa hiệu quả tất cả các công việc GPU theo yêu cầu của thiết bị lặp lại.

00:21:53.000 --> 00:22:01.000
Hoạt động mới này làm cho các mô hình suy luận CoreML dựa trên LSTM nhanh hơn đáng kể.

00:22:01.000 --> 00:22:08.000
Để sử dụng thao tác LSTM mới, trước tiên hãy tạo Mô tả MPSGraphLSTM.

00:22:08.000 --> 00:22:15.000
Bạn có thể sửa đổi các thuộc tính mô tả khi cần thiết, ví dụ như chọn các chức năng kích hoạt.

00:22:15.000 --> 00:22:21.000
Tiếp theo, thêm đơn vị LSTM vào biểu đồ, cung cấp các tenxơ đầu vào.

00:22:21.000 --> 00:22:27.000
Bạn cũng có thể cung cấp một vectơ thiên vị, cũng như trạng thái ban đầu và ô cho hoạt động.

00:22:27.000 --> 00:22:30.000
Cuối cùng, cung cấp mô tả.

00:22:30.000 --> 00:22:34.000
Đó là tất cả những gì bạn cần làm để thiết lập LSTM.

00:22:34.000 --> 00:22:38.000
Các hoạt động RNN khác hoạt động tương tự.

00:22:38.000 --> 00:22:44.000
Tôi khuyến khích bạn thử các thao tác này và xem bạn có thể tăng tốc loại nào trong ứng dụng của mình.

00:22:44.000 --> 00:22:48.000
Tiếp theo, tôi sẽ chỉ cho bạn sự hỗ trợ được cải thiện cho Max Pooling.

00:22:48.000 --> 00:23:00.000
Thao tác Max Pooling lấy một tenxơ đầu vào và kích thước cửa sổ và tính toán, đối với mỗi ứng dụng của cửa sổ, giá trị tối đa của đầu vào trong cửa sổ.

00:23:00.000 --> 00:23:05.000
Nó thường được sử dụng trong thị giác máy tính để giảm kích thước của hình ảnh.

00:23:05.000 --> 00:23:13.000
API đã được mở rộng để trả về các chỉ số của vị trí giá trị tối đa được trích xuất bởi toán tử gộp.

00:23:13.000 --> 00:23:23.000
Bạn có thể sử dụng các chỉ số trong gradient pass, trong đó các gradient phải được truyền qua các vị trí nơi các giá trị tối đa được trích xuất.

00:23:23.000 --> 00:23:26.000
API mới cũng hoạt động để đào tạo.

00:23:26.000 --> 00:23:34.000
Tái sử dụng các chỉ số trong quá trình đào tạo có thể nhanh hơn tới sáu lần đối với PyTorch và TensorFlow.

00:23:34.000 --> 00:23:40.000
Để thiết lập điều này trong mã, trước tiên, hãy tạo mô tả GraphPooling.

00:23:40.000 --> 00:23:46.000
Bạn có thể chỉ định returnIndicesMode, ví dụ, globalFlatten4D.

00:23:46.000 --> 00:23:53.000
Sau đó, bạn có thể gọi hoạt động tổng hợp trên biểu đồ với API Chỉ số Trả về.

00:23:53.000 --> 00:23:56.000
Kết quả của ca phẫu thuật có hai mặt.

00:23:56.000 --> 00:24:01.000
Đầu tiên, Tổng hợpTensor, và thứ hai, indicesTensor.

00:24:01.000 --> 00:24:09.000
Bạn có thể lưu trữ indicesTensor để sử dụng sau này, ví dụ, trên một đường ống đào tạo.

00:24:09.000 --> 00:24:19.000
Biểu đồ MPS hiện hiển thị một trình tạo số ngẫu nhiên song song mới, có thể được sử dụng, ví dụ, để khởi tạo trọng số của biểu đồ đào tạo.

00:24:19.000 --> 00:24:28.000
Thao tác ngẫu nhiên mới sử dụng thuật toán Philox và trả về kết quả tương tự như TensorFlow cho một hạt giống nhất định.

00:24:28.000 --> 00:24:41.000
Thao tác mới lấy, làm đầu vào, một tenxơ trạng thái; nó trả về dưới dạng đầu ra một tenxơ ngẫu nhiên và một tenxơ trạng thái mới có thể được sử dụng, ví dụ, làm đầu vào cho một thao tác ngẫu nhiên thứ hai.

00:24:41.000 --> 00:24:46.000
Để sử dụng thao tác ngẫu nhiên mới, hãy gọi phương thức randomPhiloxStateTensor.

00:24:46.000 --> 00:24:52.000
Phương pháp này khởi tạo một stateTensor đầu vào với hạt giống đã cho.

00:24:52.000 --> 00:24:59.000
Sau đó khai báo bộ mô tả RandomOp, lấy đầu vào phân phối và kiểu dữ liệu.

00:24:59.000 --> 00:25:07.000
Trong ví dụ, bộ mô tả chỉ định một phân phối Bình thường bị cắt ngắn của các giá trị dấu phẩy động 32 bit.

00:25:07.000 --> 00:25:12.000
Bạn cũng có thể sử dụng phân phối Bình thường và Đồng nhất.

00:25:12.000 --> 00:25:21.000
Bạn có thể xác định thêm các đặc điểm phân phối bằng cách chỉ định giá trị trung bình, độ lệch chuẩn, giá trị tối thiểu và tối đa.

00:25:21.000 --> 00:25:32.000
Cuối cùng, bạn có thể tạo thao tác ngẫu nhiên, cung cấp shapeTensor, descriptor và stateTensor vừa tạo.

00:25:32.000 --> 00:25:42.000
Ngoài Ngẫu nhiên, MPSGraph hiện hỗ trợ hoạt động tăng tốc GPU mới để tính toán khoảng cách Hamming giữa hai vectơ bit.

00:25:42.000 --> 00:25:58.000
Khoảng cách hamming, được định nghĩa là số bit khác nhau giữa hai đầu vào có cùng độ dài, là thước đo khoảng cách chỉnh sửa giữa hai chuỗi và nó được sử dụng trên một số ứng dụng, từ tin sinh học đến mật mã học.

00:25:58.000 --> 00:26:06.000
Để sử dụng HammingDistance, hãy gọi API trên biểu đồ, cung cấp primaryTensor, secondaryTensor và resultDataType.

00:26:06.000 --> 00:26:13.000
Lưu ý rằng hạt nhân mới hỗ trợ phát sóng qua kích thước hàng loạt trên GPU.

00:26:13.000 --> 00:26:20.000
Bây giờ, tôi sẽ chỉ cho bạn tất cả về các thao tác tensor mới, rất dễ sử dụng.

00:26:20.000 --> 00:26:26.000
Bây giờ bạn có thể mở rộng kích thước của tenxơ, ví dụ, từ hai đến ba chiều.

00:26:26.000 --> 00:26:30.000
Và bạn có thể ép kích thước trở lại.

00:26:30.000 --> 00:26:36.000
Bạn cũng có thể chia đều một tenxơ cung cấp một số lát cắt và một trục.

00:26:36.000 --> 00:26:40.000
Hoặc xếp các tenxơ dọc theo một trục nhất định.

00:26:40.000 --> 00:26:46.000
Bạn cũng có thể tạo các giá trị tọa độ dọc theo kích thước tensor cho một hình dạng đầu vào nhất định.

00:26:46.000 --> 00:26:54.000
Ví dụ, bạn có thể điền một tenxơ hình dạng hai x bốn với tọa độ dọc theo trục 0.

00:26:54.000 --> 00:26:59.000
Điều này cũng có thể được sử dụng để thực hiện hoạt động range1D.

00:26:59.000 --> 00:27:07.000
Ví dụ, giả sử bạn muốn tạo phạm vi số từ 3 đến 27 với gia số là 4.

00:27:07.000 --> 00:27:15.000
Bạn có thể làm như vậy bằng cách trước tiên tạo tọa độ dọc theo kích thước 0 của một tenxơ hình 6.

00:27:15.000 --> 00:27:21.000
Sau đó, tất cả những gì bạn phải làm là nhân với gia số và cộng phần bù.

00:27:21.000 --> 00:27:25.000
Đó là tất cả các hoạt động mới được bổ sung trong năm nay.

00:27:25.000 --> 00:27:34.000
Với tất cả các hoạt động mới này, bạn sẽ có thể làm được nhiều hơn nữa và đạt được hiệu suất cao hơn trên hệ sinh thái Apple bằng MPSGraph.

00:27:34.000 --> 00:27:41.000
Bây giờ, tôi sẽ chỉ cho bạn những cải tiến hiệu suất mà bạn có thể nhận được trên Apple silicon từ MPSGraph.

00:27:41.000 --> 00:27:50.000
Blackmagic vừa phát hành DaVinci Resolve phiên bản 18, sử dụng MPS Graph để tăng tốc khối lượng công việc học máy.

00:27:50.000 --> 00:28:00.000
Magic Mask là một tính năng của Resolve sử dụng máy học để xác định một đối tượng chuyển động trên màn hình và áp dụng có chọn lọc lên trên nó.

00:28:00.000 --> 00:28:08.000
Đầu tiên tôi sẽ trình bày cách thức hoạt động của Resolve trong phiên bản trước của Resolve, và sau đó tôi sẽ so sánh nó với phiên bản hiện tại.

00:28:08.000 --> 00:28:13.000
Để tạo mặt nạ, bạn chỉ cần chọn đối tượng mục tiêu.

00:28:13.000 --> 00:28:16.000
Bạn có thể xem mặt nạ bằng cách chuyển đổi lớp phủ.

00:28:16.000 --> 00:28:22.000
Mặt nạ được xác định bởi vùng màu đỏ, đánh dấu chính xác hình dạng của đối tượng.

00:28:22.000 --> 00:28:28.000
Bây giờ, nếu tôi phát video, mặt nạ sẽ theo dõi đối tượng khi nó di chuyển trên màn hình.

00:28:28.000 --> 00:28:35.000
Điều này trông rất tuyệt, nhưng nó đang chạy ở tốc độ khung hình khá thấp, vì đường ống học máy chạy dưới mui xe.

00:28:35.000 --> 00:28:44.000
Bây giờ tôi sẽ chuyển sang phiên bản Resolve mới nhất, sử dụng MPSGraph để tăng tốc mạng Magic Mask.

00:28:44.000 --> 00:28:49.000
Chạy lại cùng một dòng thời gian, tốc độ khung hình nhanh hơn trước.

00:28:49.000 --> 00:28:55.000
Điều này dẫn đến trải nghiệm chỉnh sửa tốt hơn nhiều trên Apple silicon.

00:28:55.000 --> 00:29:00.000
Đây là loại tăng tốc bạn có thể nhận được chỉ bằng cách áp dụng Biểu đồ MPS.

00:29:00.000 --> 00:29:04.000
Tôi khuyến khích bạn khám phá loại hiệu suất mà nó có thể mang lại cho ứng dụng của bạn.

00:29:04.000 --> 00:29:13.000
Để kết thúc, bây giờ bạn sẽ có thể tận dụng khả năng tăng tốc GPU cho PyTorch và dự án hiện là mã nguồn mở.

00:29:13.000 --> 00:29:23.000
Bạn sẽ tìm thấy những cách mới để tăng tốc khối lượng công việc đào tạo bằng cách sử dụng trình cắm TensorFlow Metal, ví dụ, sử dụng các hoạt động tùy chỉnh và đào tạo phân tán.

00:29:23.000 --> 00:29:34.000
Cuối cùng, bạn sẽ có thể tối ưu hóa các tác vụ học máy đòi hỏi khắt khe nhất với khung MPSGraph để tận dụng tối đa silicon của Apple, sử dụng các sự kiện được chia sẻ và các hoạt động mới.

00:29:34.000 --> 00:29:39.000
Dhruva và tôi nóng lòng muốn xem bạn sẽ sử dụng những tính năng mới này trong các ứng dụng của mình như thế nào.

00:29:39.000 --> 23:59:59.000
Cảm ơn bạn đã xem phiên họp, và chúc bạn có một WWDC tuyệt vời.

