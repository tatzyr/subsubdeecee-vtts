WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:13.000
Ken Greenebaum: Chào mọi người! Chào mừng đến với WWDC 2022.

00:00:13.000 --> 00:00:18.000
Tên tôi là Ken Greenebaum, và tôi làm việc trong nhóm Công nghệ Màu sắc và Hiển thị tại Apple.

00:00:18.000 --> 00:00:21.000
Chúng tôi rất vui mừng khi có ba cuộc nói chuyện EDR trong năm nay.

00:00:21.000 --> 00:00:33.000
Hy vọng bạn đã có cơ hội xem "Khám phá EDR trên iOS", nơi chúng tôi đã công bố hỗ trợ API EDR cho iOS, cũng như "Hiển thị nội dung EDR với Core Image, Metal và SwiftUI."

00:00:33.000 --> 00:00:44.000
Một số bạn cũng có thể đã xem bài nói chuyện EDR của tôi vào năm ngoái, nơi chúng tôi đã trình bày cách sử dụng AVPlayer để phát lại video HDR, sử dụng EDR.

00:00:44.000 --> 00:00:59.000
Trong buổi nói chuyện này, chúng ta sẽ đi sâu hơn và khám phá cách sử dụng giao diện Core Media để cung cấp, không chỉ phát lại EDR mà còn cả cách giải mã và phát lại video HDR, vào các lớp hoặc chế độ xem EDR của riêng bạn.

00:00:59.000 --> 00:01:20.000
Sau đó, chúng tôi sẽ tiếp tục không chỉ đơn giản là phát lại nội dung, để chỉ ra cách truy cập các khung hình video được giải mã trong thời gian thực, thông qua liên kết hiển thị của Core Video, gửi các khung hình đó đến Bộ lọc CoreImage hoặc Metal Shader, để thêm quản lý màu sắc, hiệu ứng hình ảnh hoặc áp dụng xử lý tín hiệu khác

00:01:20.000 --> 00:01:30.000
Chúng tôi sẽ bắt đầu bằng cách xem xét các khung phương tiện video tương thích EDR, để giúp bạn quyết định cái nào phù hợp nhất với yêu cầu của ứng dụng của bạn.

00:01:30.000 --> 00:01:42.000
Tiếp theo, chúng ta sẽ thảo luận ngắn gọn về các khung AVKit và AVFoundation cấp cao, có thể thực hiện tất cả công việc phát video HDR, nếu ứng dụng của bạn yêu cầu phát lại thẳng về phía trước.

00:01:42.000 --> 00:01:54.000
Và cuối cùng, chúng ta sẽ thảo luận về các phương pháp hay nhất để sử dụng các khung hình video được giải mã, với Core Video và Metal, trong công cụ phát lại, chỉnh sửa hoặc xử lý hình ảnh EDR của bạn.

00:01:54.000 --> 00:02:10.000
Hãy bắt đầu bằng cách thực hiện một cuộc khảo sát nhanh về các khung video của Apple; Bắt đầu với các giao diện cấp cao nhất; dễ sử dụng nhất; và tiếp tục đến các khung cấp thấp hơn cung cấp nhiều cơ hội hơn, với chi phí thêm độ phức tạp vào mã của bạn.

00:02:10.000 --> 00:02:17.000
Tốt nhất là sử dụng khung cấp cao nhất có thể để tận dụng các tối ưu hóa được cung cấp tự động cho bạn.

00:02:17.000 --> 00:02:31.000
Điều này sẽ giúp chúng ta sẵn sàng đi sâu vào phần nội dung của cuộc nói chuyện, nơi chúng ta sẽ khám phá một số tình huống, từ phát lại EDR đơn giản đến hệ thống ống nước tinh vi hơn của các khung hình video được giải mã đến CoreImage hoặc Metal để xử lý thời gian thực.

00:02:31.000 --> 00:02:34.000
Ở cấp độ cao nhất, có AVKit.

00:02:34.000 --> 00:02:45.000
Với AVKit, bạn có thể tạo giao diện người dùng để phát lại phương tiện; hoàn chỉnh với các điều khiển vận chuyển, điều hướng chương, hỗ trợ Hình ảnh trong Hình ảnh và hiển thị phụ đề và phụ đề chi tiết.

00:02:45.000 --> 00:02:52.000
AVKit có thể phát lại nội dung HDR dưới dạng EDR, như chúng tôi sẽ trình diễn bằng cách sử dụng AVPlayerViewController.

00:02:52.000 --> 00:03:01.000
Tuy nhiên, nếu ứng dụng của bạn yêu cầu xử lý thêm các khung hình video, bạn sẽ phải sử dụng khung phương tiện có thể giúp bạn kiểm soát nhiều hơn đối với đường ống của mình.

00:03:01.000 --> 00:03:04.000
Tiếp theo là AVFoundation.

00:03:04.000 --> 00:03:12.000
AVFoundation là khuôn khổ đầy đủ tính năng để làm việc với phương tiện âm thanh hình ảnh dựa trên thời gian trên Nền tảng Apple.

00:03:12.000 --> 00:03:24.000
Sử dụng AVFoundation, bạn có thể dễ dàng phát, tạo và chỉnh sửa phim QuickTime và tệp MPEG 4, phát các luồng HLS và xây dựng chức năng đa phương tiện mạnh mẽ vào ứng dụng của mình.

00:03:24.000 --> 00:03:30.000
Chúng ta sẽ khám phá việc sử dụng AVPlayer và giao diện AVPlayerLayer liên quan trong buổi nói chuyện này.

00:03:30.000 --> 00:03:35.000
Core Video là một khuôn khổ cung cấp một mô hình đường ống cho video kỹ thuật số.

00:03:35.000 --> 00:03:40.000
Nó đơn giản hóa cách bạn làm việc với video bằng cách phân chia quy trình thành các bước riêng biệt.

00:03:40.000 --> 00:03:51.000
Core Video cũng giúp bạn dễ dàng truy cập và thao tác các khung hình riêng lẻ mà không phải lo lắng về việc dịch giữa các loại dữ liệu hoặc lo lắng về việc đồng bộ hóa màn hình.

00:03:51.000 --> 00:03:56.000
Chúng tôi sẽ trình diễn việc sử dụng DisplayLink và CVPixelBuffer's với Core Image.

00:03:56.000 --> 00:03:59.000
Và CVMetalTextureCache, với Metal.

00:03:59.000 --> 00:04:01.000
Tiếp theo là Hộp công cụ Video.

00:04:01.000 --> 00:04:06.000
Đây là một khung cấp thấp cung cấp quyền truy cập trực tiếp vào bộ mã hóa và bộ giải mã phần cứng.

00:04:06.000 --> 00:04:15.000
Hộp công cụ Video cung cấp dịch vụ nén và giải nén video, và để chuyển đổi giữa các định dạng hình ảnh raster được lưu trữ trong bộ đệm pixel Core Video.

00:04:15.000 --> 00:04:24.000
VTDecompressionSession là một giao diện cấp thấp mạnh mẽ nằm ngoài phạm vi của cuộc nói chuyện này, nhưng các nhà phát triển nâng cao có thể muốn điều tra thêm.

00:04:24.000 --> 00:04:26.000
Và cuối cùng, có Core Media.

00:04:26.000 --> 00:04:33.000
Khung này xác định đường ống truyền thông được sử dụng bởi AVFoundation và các khuôn khổ truyền thông cấp cao khác.

00:04:33.000 --> 00:04:41.000
Bạn luôn có thể sử dụng các loại dữ liệu và giao diện cấp thấp của Core Media để xử lý hiệu quả các mẫu phương tiện và quản lý hàng đợi dữ liệu phương tiện.

00:04:41.000 --> 00:04:47.000
Trong phần còn lại của bài nói chuyện này, chúng tôi sẽ trình bày cách thức và thời điểm sử dụng các khuôn khổ này trong ứng dụng của bạn.

00:04:47.000 --> 00:04:55.000
Đầu tiên, cách sử dụng AVKit và AVFoundation để dễ dàng phát lại video HDR được hiển thị dưới dạng EDR.

00:04:55.000 --> 00:05:17.000
Sau đó, một loạt các ứng dụng phức tạp hơn của AVPlayer: để hiển thị cho lớp của riêng bạn, truy cập các khung được giải mã riêng lẻ thông qua CADisplayLink và gửi CVPixelBuffers kết quả đến Core Image để xử lý, và cuối cùng, truy cập các khung được giải mã dưới dạng kết cấu Kim loại thông qua CVMetalTex

00:05:17.000 --> 00:05:26.000
Bây giờ chúng ta đã có cái nhìn tổng quan về lớp phương tiện video trên nền tảng Apple, chúng ta sẽ tập trung vào các khung AVKit và AVFoundation.

00:05:26.000 --> 00:05:33.000
Hãy bắt đầu mọi thứ bằng cách thảo luận trước về việc phát lại nội dung video HDR của bạn bằng giao diện AVPlayer của AVFoundation.

00:05:33.000 --> 00:05:39.000
AVPlayer là một đối tượng điều khiển, được sử dụng để quản lý việc phát lại và thời gian của một nội dung phương tiện.

00:05:39.000 --> 00:05:48.000
Giao diện AVPlayer có thể được sử dụng để phát lại video HDR hiệu suất cao, tự động hiển thị kết quả dưới dạng EDR khi có thể.

00:05:48.000 --> 00:05:58.000
Với AVPlayer, bạn có thể phát phương tiện dựa trên tệp cục bộ và từ xa, chẳng hạn như phim QuickTime; cũng như phương tiện phát trực tuyến, được phục vụ bằng HLS.

00:05:58.000 --> 00:06:02.000
Về cơ bản, AVPlayer được sử dụng để phát một nội dung đa phương tiện tại một thời điểm.

00:06:02.000 --> 00:06:17.000
Bạn có thể sử dụng lại phiên bản trình phát để phát nối tiếp các tài sản phương tiện bổ sung hoặc thậm chí tạo nhiều phiên bản để phát nhiều tài sản cùng một lúc, nhưng AVPlayer chỉ quản lý việc phát lại một tài sản phương tiện duy nhất tại một thời điểm.

00:06:17.000 --> 00:06:29.000
Khung AVFoundation cũng cung cấp một lớp con của AVPlayer được gọi là AVQueuePlayer mà bạn có thể sử dụng để tạo và quản lý việc xếp hàng và phát các tài sản phương tiện HDR tuần tự.

00:06:29.000 --> 00:06:39.000
Nếu ứng dụng của bạn yêu cầu phát lại phương tiện video HDR đơn giản được hiển thị thành EDR, thì AVPlayer với AVPlayerViewController, có thể là cách tiếp cận tốt nhất.

00:06:39.000 --> 00:06:46.000
Sử dụng AVPlayer với AVPlayerLayer để phát lại chế độ xem của riêng bạn trên iOS hoặc macOS.

00:06:46.000 --> 00:06:49.000
Đây là những cách đơn giản nhất để sử dụng AVPlayer.

00:06:49.000 --> 00:06:51.000
Hãy xem xét các ví dụ của cả hai.

00:06:51.000 --> 00:06:59.000
Đầu tiên chúng ta sẽ xem xét cách bạn có thể sử dụng giao diện AVPlayer của AVFoundation, kết hợp với Bộ điều khiển chế độ xem AVPlayer của AVKit.

00:06:59.000 --> 00:07:06.000
Ở đây, chúng tôi bắt đầu bằng cách khởi tạo AVPlayer từ URL của phương tiện truyền thông.

00:07:06.000 --> 00:07:18.000
Tiếp theo, chúng tôi tạo một AVPlayerViewController, sau đó đặt thuộc tính trình phát của bộ điều khiển người xem của chúng tôi thành trình phát mà chúng tôi vừa tạo từ URL của phương tiện.

00:07:18.000 --> 00:07:23.000
Và trình bày bộ điều khiển chế độ xem theo phương thức để bắt đầu phát lại video.

00:07:23.000 --> 00:07:31.000
AVKit quản lý tất cả các chi tiết cho bạn và sẽ tự động phát lại Video HDR dưới dạng EDR trên các màn hình hỗ trợ EDR.

00:07:31.000 --> 00:07:37.000
Như tôi đã đề cập, một số ứng dụng sẽ cần phát lại phương tiện video HDR vào chế độ xem của riêng chúng.

00:07:37.000 --> 00:07:42.000
Hãy xem cách thực hiện điều này bằng cách sử dụng AVPlayer với AVPlayerLayer.

00:07:42.000 --> 00:07:51.000
Để phát phương tiện video HDR dưới dạng EDR theo chế độ xem của riêng bạn, chúng tôi lại bắt đầu bằng cách tạo AVPlayer với URL của phương tiện.

00:07:51.000 --> 00:07:57.000
Tuy nhiên lần này chúng tôi khởi tạo một AVPlayerLayer với trình phát mà chúng tôi vừa tạo.

00:07:57.000 --> 00:08:02.000
Tiếp theo chúng ta cần đặt giới hạn trên lớp trình phát, mà chúng ta nhận được từ chế độ xem.

00:08:02.000 --> 00:08:10.000
Bây giờ lớp người chơi có giới hạn từ chế độ xem, chúng ta có thể thêm lớp người chơi dưới dạng lớp con vào chế độ xem.

00:08:10.000 --> 00:08:15.000
Cuối cùng, để phát lại phương tiện video HDR, chúng tôi gọi phương thức phát của AVPlayer.

00:08:15.000 --> 00:08:24.000
Đó là tất cả những gì cần thiết để phát lại phương tiện video HDR dưới dạng EDR trong lớp của riêng bạn bằng cách sử dụng AVPlayer và AVPlayerLayer.

00:08:24.000 --> 00:08:29.000
Chúng tôi vừa khám phá hai quy trình phát lại video HDR đơn giản nhất bằng cách sử dụng AVPlayer.

00:08:29.000 --> 00:08:35.000
Tuy nhiên, nhiều ứng dụng yêu cầu nhiều hơn là phát lại phương tiện đơn giản.

00:08:35.000 --> 00:08:43.000
Ví dụ, một ứng dụng có thể yêu cầu xử lý hình ảnh, chẳng hạn như phân loại màu sắc hoặc khóa sắc độ được áp dụng cho video.

00:08:43.000 --> 00:08:55.000
Hãy cùng khám phá quy trình làm việc nhận các khung hình video được giải mã từ AVPlayer, áp dụng các bộ lọc Core Image hoặc bộ đổ bóng kim loại trong thời gian thực và hiển thị kết quả dưới dạng EDR.

00:08:55.000 --> 00:09:17.000
Chúng tôi sẽ trình bày cách sử dụng AVPlayer và AVPlayerItem để giải mã các khung EDR khỏi phương tiện video HDR của bạn, truy cập các khung hình được giải mã từ liên kết hiển thị Core Video, gửi bộ đệm pixel kết quả đến Core Image hoặc Metal để xử lý, sau đó hiển thị kết quả trong CAMetalLayer dưới dạng EDR trên màn

00:09:17.000 --> 00:09:28.000
Với suy nghĩ này, trước tiên chúng ta hãy chứng minh việc thiết lập một vài thuộc tính chính trên CAMetalLayer, được yêu cầu để đảm bảo phương tiện HDR sẽ hiển thị chính xác dưới dạng EDR.

00:09:28.000 --> 00:09:34.000
Trước tiên, chúng ta cần lấy CAMetalLayer mà chúng ta sẽ hiển thị nội dung video HDR.

00:09:34.000 --> 00:09:42.000
Trên lớp đó, chúng tôi chọn tham gia EDR bằng cách đặt cờ wantsExtendedDynamicRangeContent thành true.

00:09:42.000 --> 00:09:48.000
Vui lòng đảm bảo sử dụng định dạng pixel hỗ trợ nội dung Dải động mở rộng.

00:09:48.000 --> 00:10:01.000
Đối với ví dụ AVPlayer tiếp theo, chúng tôi sẽ đặt CAMetalLayer sử dụng định dạng nửa pixel float, tuy nhiên định dạng mười bit được sử dụng kết hợp với chức năng truyền PQ hoặc HLG cũng sẽ hoạt động.

00:10:01.000 --> 00:10:10.000
Để tránh giới hạn kết quả ở SDR, chúng ta cũng cần đặt lớp thành không gian màu phạm vi mở rộng tương thích EDR.

00:10:10.000 --> 00:10:18.000
Trong các ví dụ của chúng tôi, chúng tôi sẽ đặt kết cấu kim loại nửa nổi thành không gian màu P3 hiển thị tuyến tính mở rộng.

00:10:18.000 --> 00:10:23.000
Chúng tôi vừa làm xước bề mặt liên quan đến EDR, không gian màu và định dạng bộ đệm điểm ảnh.

00:10:23.000 --> 00:10:33.000
Bạn có thể muốn xem phiên của tôi từ năm ngoái, "Kết xuất HDR với EDR", cũng như "EDR trên iOS" năm nay, để biết thêm chi tiết.

00:10:33.000 --> 00:10:42.000
Bây giờ chúng ta đã đặt các thuộc tính cơ bản trên CAMetalLayer, hãy tiếp tục trình diễn bằng cách thêm xử lý hình ảnh theo thời gian thực bằng Core Image hoặc Metal shader.

00:10:42.000 --> 00:10:49.000
Chúng tôi sẽ sử dụng liên kết hiển thị kết hợp với AVPlayer để truy cập các khung hình video được giải mã trong thời gian thực.

00:10:49.000 --> 00:10:53.000
Đối với quy trình làm việc này, bạn bắt đầu bằng cách tạo một AVPlayer từ AVPlayerItem.

00:10:53.000 --> 00:11:02.000
Tiếp theo, bạn khởi tạo AVPlayerItemVideoOutput, được định cấu hình với định dạng bộ đệm pixel thích hợp và không gian màu cho EDR.

00:11:02.000 --> 00:11:05.000
Sau đó bạn tạo và cấu hình một liên kết Hiển thị.

00:11:05.000 --> 00:11:11.000
Và cuối cùng, bạn chạy liên kết Hiển thị để đưa bộ đệm pixel đến Core Image hoặc Metal để xử lý.

00:11:11.000 --> 00:11:16.000
Chúng tôi sẽ trình diễn CADisplayLink như được sử dụng trên iOS.

00:11:16.000 --> 00:11:21.000
Vui lòng sử dụng giao diện CVDisplayLink tương đương khi phát triển cho macOS.

00:11:21.000 --> 00:11:32.000
Lần này chúng tôi chọn tạo AVPlayerItem từ URL của phương tiện và khởi tạo AVPlayer với AVPlayerItem mà chúng tôi vừa tạo.

00:11:32.000 --> 00:11:39.000
Bây giờ chúng tôi tạo một cặp từ điển để chỉ định không gian màu và định dạng bộ đệm pixel của các khung được giải mã.

00:11:39.000 --> 00:11:45.000
Từ điển đầu tiên, videoColorProperties, là nơi chỉ định không gian màu và chức năng truyền.

00:11:45.000 --> 00:12:00.000
Trong ví dụ này, chúng tôi yêu cầu không gian màu Display P3, tương ứng với không gian màu của hầu hết các màn hình Apple và chức năng truyền tuyến tính cho phép AVFoundation duy trì các giá trị phạm vi mở rộng cần thiết cho EDR.

00:12:00.000 --> 00:12:11.000
Từ điển thứ hai, outputVideoSettings, chỉ định các đặc điểm của định dạng bộ đệm pixel và cũng cung cấp tham chiếu đến từ điển videoColorProperties mà chúng tôi vừa tạo.

00:12:11.000 --> 00:12:17.000
Trong ví dụ này, chúng tôi yêu cầu màu rộng và định dạng bộ đệm nửa điểm ảnh nổi.

00:12:17.000 --> 00:12:34.000
Điều rất hữu ích là AVPlayerItemVideoOutput, không chỉ giải mã video thành định dạng bộ đệm pixel mà chúng tôi chỉ định trong từ điển cài đặt đầu ra, mà còn tự động thực hiện bất kỳ chuyển đổi màu nào được yêu cầu thông qua phiên truyền pixel.

00:12:34.000 --> 00:12:39.000
Nhớ lại, một video có thể chứa nhiều clip, có khả năng với các không gian màu khác nhau.

00:12:39.000 --> 00:12:57.000
AVFoundation tự động quản lý những thứ này cho chúng tôi và như chúng tôi sẽ sớm chứng minh, hành vi này cũng cho phép các khung hình video được giải mã kết quả được gửi đến các khung cấp thấp như Metal mà bản thân chúng không cung cấp chuyển đổi không gian màu tự động sang không gian màu của màn hình.

00:12:57.000 --> 00:13:03.000
Bây giờ chúng tôi tạo AVPlayerItemVideoOutput với từ điển outputVideoSettings.

00:13:03.000 --> 00:13:10.000
Bước thứ ba, chúng tôi thiết lập liên kết Hiển thị, liên kết này sẽ được sử dụng để truy cập các khung được giải mã trong thời gian thực.

00:13:10.000 --> 00:13:15.000
CADisplayLink nhận một cuộc gọi lại được chạy trên mỗi bản cập nhật hiển thị.

00:13:15.000 --> 00:13:24.000
Trong ví dụ của chúng tôi, chúng tôi gọi một hàm cục bộ mà chúng tôi sẽ khám phá trong giây lát để lấy CVPixelBuffers mà chúng tôi sẽ gửi đến Core Image để xử lý.

00:13:24.000 --> 00:13:33.000
Tiếp theo, chúng tôi tạo một người quan sát mục trình phát video để cho phép chúng tôi xử lý các thay đổi đối với các thuộc tính Mục trình phát được chỉ định.

00:13:33.000 --> 00:13:41.000
Ví dụ của chúng tôi sẽ thực thi mã này mỗi khi thay đổi trạng thái của vật phẩm của người chơi.

00:13:41.000 --> 00:14:04.000
Khi trạng thái của vật phẩm trình phát thay đổi thành readyToPlay, chúng tôi thêm AVPlayerItemVideoOutput của mình vào AVPlayerItem mới vừa được trả về, đăng ký CADisplayLink với vòng lặp chạy chính được đặt ở chế độ chung và bắt đầu giải mã video HDR theo thời gian thực bằng cách gọi phát trên trình phát video.

00:14:04.000 --> 00:14:15.000
Cuối cùng, chúng ta sẽ xem xét một ví dụ về việc triển khai gọi lại CADisplayLink, mà chúng ta đã gọi trước đó là hàm cục bộ `displayLinkCopyPixelBuffers`.

00:14:15.000 --> 00:14:22.000
Khi video HDR bắt đầu phát, chức năng gọi lại CADisplayLink được gọi trên mỗi lần làm mới màn hình.

00:14:22.000 --> 00:14:27.000
Ví dụ, nó có thể được gọi 60 lần một giây cho một màn hình điển hình.

00:14:27.000 --> 00:14:34.000
Đây là cơ hội của mã của chúng tôi để cập nhật khung được hiển thị nếu có CVPixelBuffer mới.

00:14:34.000 --> 00:14:43.000
Trên mỗi cuộc gọi lại hiển thị, chúng tôi cố gắng sao chép CVPixelBuffer chứa khung video được giải mã để hiển thị tại thời điểm đồng hồ treo tường hiện tại.

00:14:43.000 --> 00:14:56.000
Tuy nhiên, cuộc gọi `copyPixelBuffer` có thể thất bại, vì không phải lúc nào cũng có CVPixelBuffer mới có sẵn ở mỗi lần làm mới màn hình, đặc biệt là khi tốc độ làm mới màn hình vượt quá tốc độ làm mới video đang phát.

00:14:56.000 --> 00:15:01.000
Nếu không có CVPixelBuffer mới, thì cuộc gọi không thành công và chúng tôi bỏ qua kết xuất.

00:15:01.000 --> 00:15:06.000
Điều này làm cho khung hình trước đó vẫn ở trên màn hình để làm mới màn hình khác.

00:15:06.000 --> 00:15:12.000
Nhưng nếu bản sao thành công, thì chúng tôi có một khung video mới trong CVPixelBuffer.

00:15:12.000 --> 00:15:16.000
Có một số cách mà chúng tôi có thể xử lý và hiển thị khung mới này.

00:15:16.000 --> 00:15:21.000
Một cơ hội là gửi CVPixelBuffer đến Core Image để xử lý.

00:15:21.000 --> 00:15:29.000
Core Image có thể xâu chuỗi một hoặc nhiều CIFilters lại với nhau để cung cấp khả năng xử lý hình ảnh tăng tốc GPU cho khung hình video.

00:15:29.000 --> 00:15:38.000
Xin lưu ý rằng không phải tất cả CIFilters đều tương thích với EDR và có thể gặp sự cố với nội dung HDR, bao gồm cả việc kẹp vào SDR hoặc tệ hơn.

00:15:38.000 --> 00:15:42.000
Core Image cung cấp nhiều Bộ lọc tương thích EDR.

00:15:42.000 --> 00:15:49.000
Sử dụng tên bộ lọc với CICategoryHighDynamicRange, để liệt kê các bộ lọc Core Image tương thích EDR.

00:15:49.000 --> 00:15:53.000
Trong ví dụ của chúng tôi, chúng tôi sẽ thêm một hiệu ứng tông màu nâu đỏ đơn giản.

00:15:53.000 --> 00:15:58.000
Bây giờ hãy quay lại ví dụ của chúng tôi và tích hợp Core Image.

00:15:58.000 --> 00:16:06.000
Trên mỗi lần gọi lại liên kết hiển thị mang lại CVPixelBuffer mới, hãy tạo CIImage từ bộ đệm pixel đó.

00:16:06.000 --> 00:16:09.000
Ví dụ CIFilter để thực hiện hiệu ứng mong muốn.

00:16:09.000 --> 00:16:20.000
Tôi đang sử dụng bộ lọc tông màu nâu đỏ vì sự đơn giản ít tham số của nó, tuy nhiên có rất nhiều CIFilters được tích hợp trong hệ thống và việc viết của riêng bạn cũng rất đơn giản.

00:16:20.000 --> 00:16:26.000
Đặt inputImage của CIFilter thành CIImage mà chúng tôi vừa tạo.

00:16:26.000 --> 00:16:32.000
Và kết quả video đã xử lý sẽ có sẵn trong Hình ảnh đầu ra của bộ lọc.

00:16:32.000 --> 00:16:37.000
Xích nhiều CIFilters lại với nhau theo yêu cầu để đạt được hiệu quả mong muốn của bạn.

00:16:37.000 --> 00:16:44.000
Sau đó sử dụng CIRenderDestination để hiển thị hình ảnh kết quả vào mã xem ứng dụng của bạn.

00:16:44.000 --> 00:16:51.000
Vui lòng tham khảo bài nói chuyện WWDC 2020 "Tối ưu hóa đường ống Core Image cho ứng dụng video của bạn" để tìm hiểu thêm về quy trình làm việc này.

00:16:51.000 --> 00:16:59.000
Một cơ hội khác, là xử lý và hiển thị CVPixelBuffer mới bằng cách sử dụng Metal và các bộ đổ bóng Metal tùy chỉnh.

00:16:59.000 --> 00:17:04.000
Chúng tôi sẽ mô tả ngắn gọn quá trình chuyển đổi CVPixelBuffer thành kết cấu Kim loại.

00:17:04.000 --> 00:17:10.000
Tuy nhiên, việc thực hiện chuyển đổi này để duy trì hiệu suất tốt nhất là một chủ đề sâu sắc tốt nhất nên để lại cho một cuộc nói chuyện khác.

00:17:10.000 --> 00:17:19.000
Thay vào đó, chúng tôi khuyên bạn nên lấy kết cấu Kim loại từ bộ nhớ đệm kết cấu CoreVideo Metal và sẽ đi qua quy trình đó làm ví dụ cuối cùng trong bài nói chuyện này.

00:17:19.000 --> 00:17:33.000
Nói chung, quy trình là lấy bề mặt IOS từ CVPixelBuffer, tạo MetalTextureDescriptor, và sau đó tạo MetalTexture từ MetalDevice, sử dụng `newTextureWithDescriptor`.

00:17:33.000 --> 00:17:41.000
Tuy nhiên, có một mối nguy hiểm là các kết cấu có thể được sử dụng lại và vẽ quá mức, nếu không áp dụng khóa cẩn thận.

00:17:41.000 --> 00:17:49.000
Hơn nữa, không phải tất cả các định dạng PixelBuffer đều được MetalTexture hỗ trợ nguyên bản, đó là lý do tại sao chúng tôi sử dụng half float trong ví dụ này.

00:17:49.000 --> 00:17:56.000
Vì những phức tạp này, thay vào đó chúng tôi khuyên bạn nên truy cập trực tiếp kết cấu Kim loại từ Core Video, như bây giờ chúng tôi sẽ chứng minh.

00:17:56.000 --> 00:18:00.000
Hãy cùng khám phá thêm Core Video và Metal.

00:18:00.000 --> 00:18:07.000
Như đã đề cập, CVMetalTextureCache vừa là một cách đơn giản vừa hiệu quả để sử dụng CVPixelBuffers với Metal.

00:18:07.000 --> 00:18:14.000
CVMetalTextureCache rất tiện dụng vì bạn nhận được kết cấu Kim loại trực tiếp từ bộ nhớ cache mà không cần chuyển đổi thêm.

00:18:14.000 --> 00:18:26.000
CVMetalTextureCache tự động kết nối giữa CVPixelBuffer's và MetalTexture's, do đó vừa đơn giản hóa mã của bạn vừa giúp bạn đi nhanh.

00:18:26.000 --> 00:18:37.000
Kết hợp với CVPixelBufferPools, CVMetalTextureCache cũng cung cấp các lợi ích về hiệu suất, bằng cách giữ cho MTLTexture cho bản đồ bề mặt IOS tồn tại.

00:18:37.000 --> 00:18:43.000
Cuối cùng, sử dụng CVMetalTextureCache loại bỏ nhu cầu theo dõi thủ công các bề mặt IOS.

00:18:43.000 --> 00:18:52.000
Bây giờ là ví dụ cuối cùng trong bài nói chuyện của chúng tôi: cách trích xuất kết cấu Kim loại trực tiếp từ Core Video bằng CVMetalTextureCache.

00:18:52.000 --> 00:18:55.000
Ở đây, chúng tôi bắt đầu bằng cách lấy thiết bị Metal mặc định của hệ thống.

00:18:55.000 --> 00:19:04.000
Chúng tôi sử dụng điều đó để tạo Bộ nhớ đệm kết cấu kim loại, và sau đó khởi tạo Bộ nhớ đệm kết cấu kim loại video cốt lõi được liên kết với Bộ nhớ đệm kết cấu kim loại.

00:19:04.000 --> 00:19:13.000
Sau đó, điều đó có thể được sử dụng để truy cập các khung hình video được giải mã dưới dạng Kết cấu kim loại, thuận tiện, có thể được sử dụng trực tiếp trong công cụ Kim loại của chúng tôi.

00:19:13.000 --> 00:19:18.000
Trong ví dụ này, chúng tôi tạo và sử dụng thiết bị mặc định của hệ thống Metal.

00:19:18.000 --> 00:19:27.000
Tiếp theo, chúng tôi tạo CVMetalTextureCache với CVMetalTextureCacheCreate, chỉ định thiết bị Metal mà chúng tôi vừa tạo.

00:19:27.000 --> 00:19:33.000
Chúng tôi nhận được chiều cao và chiều rộng của CVPixelBuffer cần thiết để tạo ra kết cấu Core Video Metal.

00:19:33.000 --> 00:19:43.000
Sau đó, chúng tôi gọi `CVMetalTextureCacheCreateTextureFromImage`, để khởi tạo một đối tượng CVMetalTexture và liên kết nó với CVPixelBuffer.

00:19:43.000 --> 00:19:50.000
Cuối cùng chúng tôi gọi `CVMetalTextureGetTexture`, để có được kết cấu Kim loại mong muốn.

00:19:50.000 --> 00:20:01.000
Các ứng dụng Swift nên sử dụng tham chiếu mạnh mẽ cho CVMetalTexture, tuy nhiên, khi sử dụng Objective-C, bạn phải đảm bảo rằng Metal được hoàn thành với kết cấu của bạn trước khi bạn phát hành CVMetalTextureRef.

00:20:01.000 --> 00:20:07.000
Điều này có thể được thực hiện bằng cách sử dụng trình xử lý hoàn thành bộ đệm lệnh kim loại.

00:20:07.000 --> 00:20:09.000
Và đó là tất cả, các bạn!

00:20:09.000 --> 00:20:18.000
Để xem xét, chúng tôi đã khám phá một số quy trình làm việc sẽ hiển thị phương tiện video HDR của bạn thành EDR, để phát lại, chỉnh sửa hoặc xử lý hình ảnh.

00:20:18.000 --> 00:20:26.000
Bạn đã học cách chuyển từ AVPlayer sang AVPlayerViewController của AVKit, để phát lại phương tiện HDR.

00:20:26.000 --> 00:20:34.000
Bạn cũng đã học cách sử dụng AVPlayer, cùng với AVPlayerLayer, để hiển thị phương tiện HDR theo chế độ xem của riêng bạn.

00:20:34.000 --> 00:20:38.000
Và cuối cùng, chúng tôi đã khám phá cách thêm hiệu ứng thời gian thực trong khi phát lại.

00:20:38.000 --> 00:20:44.000
Kết nối AVPlayer của AVFoundation với CoreVideo và sau đó với Metal để kết xuất.

00:20:44.000 --> 00:20:51.000
Và áp dụng các hiệu ứng thời gian thực bằng cách sử dụng các bộ lọc CoreImage, cũng như các bộ đổ bóng kim loại.

00:20:51.000 --> 00:21:02.000
Nếu bạn muốn tìm hiểu sâu hơn, tôi đề xuất một vài phiên WWDC liên quan đến việc tạo quy trình làm việc video, cũng như tích hợp phương tiện HDR với EDR.

00:21:02.000 --> 00:21:08.000
Tôi đặc biệt muốn gọi phiên "Chỉnh sửa và phát lại video HDR với AVFoundation".

00:21:08.000 --> 00:21:17.000
Phiên này khám phá việc sử dụng AVVideoComposition với `applyingCIFiltersWithHandler` để áp dụng các hiệu ứng cho phương tiện HDR của bạn.

00:21:17.000 --> 00:21:26.000
Trong phiên này, bạn cũng sẽ học cách sử dụng bộ tổng hợp tùy chỉnh, sau đó có thể được sử dụng với CVPixelBuffer, khi mỗi khung hình video có sẵn để xử lý.

00:21:26.000 --> 00:21:49.000
Như tôi đã đề cập ở phần đầu, năm nay chúng tôi cũng trình bày hai phiên khác trên EDR: "EDR trên iOS", nơi chúng tôi thông báo hỗ trợ API EDR đã mở rộng để bao gồm iOS và "hiển thị nội dung HDR với EDR bằng CoreImage, Metal và SwiftUI", nơi chúng tôi khám phá thêm về việc tích hợp EDR với các

00:21:49.000 --> 00:21:56.000
Hy vọng bạn kết hợp video HDR vào các ứng dụng hỗ trợ EDR của mình trên cả macOS và bây giờ là iOS.

00:21:56.000 --> 23:59:59.000
Cảm ơn vì đã xem.

