WEBVTT

00:00:00.000 --> 00:00:03.000
♪ Nhạc hip-hop nhạc cụ êm dịu ♪

00:00:03.000 --> 00:00:09.000
♪

00:00:09.000 --> 00:00:13.000
Xin chào, tên tôi là Ben, và tôi là một kỹ sư trong đội Core ML.

00:00:13.000 --> 00:00:18.000
Hôm nay tôi sẽ giới thiệu một số tính năng mới thú vị được thêm vào Core ML.

00:00:18.000 --> 00:00:23.000
Trọng tâm của các tính năng này là giúp bạn tối ưu hóa việc sử dụng Core ML của mình.

00:00:23.000 --> 00:00:34.000
Trong phiên này, tôi sẽ xem xét các công cụ hiệu suất hiện có sẵn để cung cấp cho bạn thông tin bạn cần để hiểu và tối ưu hóa hiệu suất của mô hình khi sử dụng Core ML.

00:00:34.000 --> 00:00:40.000
Sau đó, tôi sẽ xem xét một số API nâng cao sẽ cho phép bạn thực hiện các tối ưu hóa đó.

00:00:40.000 --> 00:00:47.000
Và cuối cùng, tôi sẽ đưa ra một cái nhìn tổng quan về một số khả năng Core ML bổ sung và các tùy chọn tích hợp.

00:00:47.000 --> 00:00:50.000
Hãy để tôi bắt đầu với các công cụ hiệu suất.

00:00:50.000 --> 00:00:56.000
Để cung cấp một số thông tin cơ bản, tôi sẽ bắt đầu bằng cách tóm tắt quy trình làm việc tiêu chuẩn khi sử dụng Core ML trong ứng dụng của bạn.

00:00:56.000 --> 00:00:59.000
Bước đầu tiên là chọn mô hình của bạn.

00:00:59.000 --> 00:01:13.000
Điều này có thể được thực hiện theo nhiều cách khác nhau, chẳng hạn như sử dụng các công cụ Core ML để chuyển đổi mô hình PyTorch hoặc TensorFlow sang định dạng Core ML, sử dụng mô hình Core ML đã tồn tại hoặc sử dụng Create ML để đào tạo và xuất mô hình của bạn.

00:01:13.000 --> 00:01:20.000
Để biết thêm chi tiết về chuyển đổi mô hình hoặc để tìm hiểu về Tạo ML, tôi khuyên bạn nên xem các phiên này.

00:01:20.000 --> 00:01:23.000
Bước tiếp theo là tích hợp mô hình đó vào ứng dụng của bạn.

00:01:23.000 --> 00:01:33.000
Điều này liên quan đến việc kết hợp mô hình với ứng dụng của bạn và sử dụng Core ML APIs để tải và chạy suy luận trên mô hình đó trong quá trình thực thi ứng dụng của bạn.

00:01:33.000 --> 00:01:39.000
Bước cuối cùng là tối ưu hóa cách bạn sử dụng Core ML.

00:01:39.000 --> 00:01:41.000
Đầu tiên, tôi sẽ xem xét việc chọn một mô hình.

00:01:41.000 --> 00:01:47.000
Có nhiều khía cạnh của một mô hình mà bạn có thể muốn xem xét khi quyết định xem bạn có nên sử dụng mô hình đó trong ứng dụng của mình hay không.

00:01:47.000 --> 00:01:53.000
Bạn cũng có thể có nhiều ứng cử viên của các mô hình mà bạn muốn chọn, nhưng làm thế nào để bạn quyết định sử dụng mô hình nào?

00:01:53.000 --> 00:01:58.000
Bạn cần có một mô hình có chức năng phù hợp với yêu cầu của tính năng mà bạn muốn kích hoạt.

00:01:58.000 --> 00:02:03.000
Điều này bao gồm việc hiểu độ chính xác của mô hình cũng như hiệu suất của nó.

00:02:03.000 --> 00:02:07.000
Một cách tuyệt vời để tìm hiểu về mô hình Core ML là mở nó trong Xcode.

00:02:07.000 --> 00:02:12.000
Chỉ cần nhấp đúp vào bất kỳ mô hình nào, và nó sẽ hiển thị những điều sau đây.

00:02:12.000 --> 00:02:19.000
Ở trên cùng, bạn sẽ tìm thấy loại mô hình, kích thước của nó và các yêu cầu hệ điều hành.

00:02:19.000 --> 00:02:30.000
Trong tab Chung, nó hiển thị các chi tiết bổ sung được ghi lại trong siêu dữ liệu của mô hình, độ chính xác tính toán và lưu trữ của nó và thông tin, chẳng hạn như nhãn lớp mà nó có thể dự đoán.

00:02:30.000 --> 00:02:37.000
Tab Xem trước là để kiểm tra mô hình của bạn bằng cách cung cấp đầu vào ví dụ và xem những gì nó dự đoán.

00:02:37.000 --> 00:02:45.000
Tab Dự đoán hiển thị đầu vào và đầu ra của mô hình, cũng như các loại và kích thước mà Core ML sẽ mong đợi trong thời gian chạy.

00:02:45.000 --> 00:02:52.000
Và cuối cùng, tab Tiện ích có thể giúp thực hiện các tác vụ mã hóa và triển khai mô hình.

00:02:52.000 --> 00:02:58.000
Nhìn chung, những quan điểm này cung cấp cho bạn cái nhìn tổng quan nhanh về chức năng của mô hình và xem trước độ chính xác của nó.

00:02:58.000 --> 00:03:02.000
Nhưng còn hiệu suất của người mẫu của bạn thì sao?

00:03:02.000 --> 00:03:12.000
Chi phí tải một mô hình, lượng thời gian mà một dự đoán duy nhất cần hoặc phần cứng mà nó sử dụng, có thể là những yếu tố quan trọng cho trường hợp sử dụng của bạn.

00:03:12.000 --> 00:03:22.000
Bạn có thể có các mục tiêu cứng liên quan đến các ràng buộc dữ liệu phát trực tuyến theo thời gian thực hoặc cần đưa ra các quyết định thiết kế quan trọng xung quanh giao diện người dùng tùy thuộc vào độ trễ nhận thức.

00:03:22.000 --> 00:03:32.000
Một cách để có được cái nhìn sâu sắc về hiệu suất của mô hình là thực hiện tích hợp ban đầu vào ứng dụng của bạn hoặc bằng cách tạo một nguyên mẫu nhỏ mà bạn có thể đo lường và đo lường.

00:03:32.000 --> 00:03:39.000
Và vì hiệu suất phụ thuộc vào phần cứng, bạn có thể muốn thực hiện các phép đo này trên nhiều phần cứng được hỗ trợ.

00:03:39.000 --> 00:03:45.000
Xcode và Core ML hiện có thể giúp bạn thực hiện nhiệm vụ này ngay cả trước khi viết một dòng mã duy nhất.

00:03:45.000 --> 00:03:47.000
Core ML hiện cho phép bạn tạo báo cáo hiệu suất.

00:03:47.000 --> 00:03:52.000
Để tôi chỉ cho bạn.

00:03:52.000 --> 00:03:59.000
Bây giờ tôi đã mở trình xem mô hình Xcode cho mô hình phát hiện đối tượng YOLOv3.

00:03:59.000 --> 00:04:04.000
Giữa các tab Dự đoán và Tiện ích, hiện có một tab Hiệu suất.

00:04:04.000 --> 00:04:20.000
Để tạo báo cáo hiệu suất, tôi sẽ chọn biểu tượng dấu cộng ở phía dưới bên trái, chọn thiết bị tôi muốn chạy - đó là iPhone của tôi - nhấp vào tiếp theo, sau đó chọn đơn vị tính toán nào tôi muốn Core ML sử dụng.

00:04:20.000 --> 00:04:26.000
Tôi sẽ để nó trên All, để cho phép Core ML tối ưu hóa độ trễ với tất cả các đơn vị tính toán có sẵn.

00:04:26.000 --> 00:04:31.000
Bây giờ tôi sẽ hoàn thành bằng cách nhấn Run Test.

00:04:31.000 --> 00:04:36.000
Để đảm bảo bài kiểm tra có thể chạy, hãy đảm bảo rằng thiết bị đã chọn đã được mở khóa.

00:04:36.000 --> 00:04:40.000
Nó hiển thị một biểu tượng quay trong khi báo cáo hiệu suất đang được tạo.

00:04:40.000 --> 00:04:50.000
Để tạo báo cáo, mô hình được gửi đến thiết bị, sau đó có một số lần lặp lại biên dịch, tải và dự đoán được chạy với mô hình.

00:04:50.000 --> 00:04:55.000
Khi những điều đó hoàn tất, các số liệu trong báo cáo hiệu suất sẽ được tính toán.

00:04:55.000 --> 00:05:00.000
Bây giờ nó đang chạy mô hình trên iPhone của tôi, và nó hiển thị báo cáo hiệu suất.

00:05:00.000 --> 00:05:09.000
Ở trên cùng, nó hiển thị một số chi tiết về thiết bị nơi thử nghiệm được chạy cũng như đơn vị tính toán nào đã được chọn.

00:05:09.000 --> 00:05:12.000
Tiếp theo nó hiển thị số liệu thống kê về việc chạy.

00:05:12.000 --> 00:05:20.000
Thời gian dự đoán trung bình là 22,19 mili giây và thời gian tải trung bình là khoảng 400 ms.

00:05:20.000 --> 00:05:28.000
Ngoài ra, nếu bạn dự định biên dịch mô hình của mình trên thiết bị, điều này cho thấy thời gian biên dịch là khoảng 940 ms.

00:05:28.000 --> 00:05:39.000
Thời gian dự đoán khoảng 22 ms cho tôi biết rằng mô hình này có thể hỗ trợ khoảng 45 khung hình mỗi giây nếu tôi muốn chạy nó trong thời gian thực.

00:05:39.000 --> 00:05:45.000
Vì mô hình này chứa một mạng lưới thần kinh, nên có một chế độ xem lớp được hiển thị ở cuối báo cáo hiệu suất.

00:05:45.000 --> 00:05:53.000
Điều này hiển thị tên và loại của tất cả các lớp, cũng như đơn vị tính toán mà mỗi lớp chạy.

00:05:53.000 --> 00:05:59.000
Dấu kiểm đã điền có nghĩa là lớp đã được thực thi trên đơn vị tính toán đó.

00:05:59.000 --> 00:06:06.000
Dấu kiểm chưa điền có nghĩa là lớp được hỗ trợ trên đơn vị tính toán đó, nhưng Core ML đã không chọn chạy nó ở đó.

00:06:06.000 --> 00:06:12.000
Và một viên kim cương rỗng có nghĩa là lớp không được hỗ trợ trên đơn vị tính toán đó.

00:06:12.000 --> 00:06:19.000
Trong trường hợp này, 54 lớp đã được chạy trên GPU và 32 lớp đã được chạy trên Neural Engine.

00:06:19.000 --> 00:06:29.000
Bạn cũng có thể lọc các lớp theo một đơn vị tính toán bằng cách nhấp vào nó.

00:06:29.000 --> 00:06:35.000
Đó là cách bạn có thể sử dụng Xcode 14 để tạo báo cáo hiệu suất cho các mô hình Core ML của mình.

00:06:35.000 --> 00:06:45.000
Điều này đã được hiển thị để chạy trên iPhone, nhưng nó sẽ cho phép bạn kiểm tra trên nhiều kết hợp hệ điều hành và phần cứng mà không cần phải viết một dòng mã nào.

00:06:45.000 --> 00:06:50.000
Bây giờ bạn đã chọn mô hình của mình, bước tiếp theo là tích hợp mô hình này vào ứng dụng của bạn.

00:06:50.000 --> 00:06:59.000
Điều này liên quan đến việc kết hợp mô hình với ứng dụng của bạn và sử dụng Core ML APIs để tải mô hình và đưa ra dự đoán với nó.

00:06:59.000 --> 00:07:08.000
Trong trường hợp này, tôi đã xây dựng một ứng dụng sử dụng các mô hình chuyển kiểu Core ML để thực hiện chuyển kiểu trên khung hình từ phiên máy ảnh trực tiếp.

00:07:08.000 --> 00:07:15.000
Nó hoạt động bình thường; tuy nhiên, tốc độ khung hình chậm hơn tôi mong đợi và tôi muốn hiểu tại sao.

00:07:15.000 --> 00:07:20.000
Đây là nơi bạn sẽ chuyển sang bước ba, đó là để tối ưu hóa việc sử dụng Core ML của bạn.

00:07:20.000 --> 00:07:32.000
Tạo báo cáo hiệu suất có thể hiển thị hiệu suất mà một mô hình có khả năng đạt được trong một môi trường độc lập; tuy nhiên, bạn cũng cần một cách để lập hồ sơ hiệu suất của một mô hình đang chạy trực tiếp trong ứng dụng của bạn.

00:07:32.000 --> 00:07:38.000
Đối với điều này, bây giờ bạn có thể sử dụng Công cụ Core ML được tìm thấy trong ứng dụng Dụng cụ trong Xcode 14.

00:07:38.000 --> 00:07:46.000
Công cụ này cho phép bạn hình dung hiệu suất của mô hình của mình khi nó chạy trực tiếp trong ứng dụng của bạn và giúp bạn xác định các vấn đề hiệu suất tiềm ẩn.

00:07:46.000 --> 00:07:50.000
Hãy để tôi chỉ ra cách nó có thể được sử dụng.

00:07:50.000 --> 00:07:56.000
Vì vậy, tôi đang ở trong Xcode với không gian làm việc ứng dụng chuyển phong cách của mình đang mở và tôi đã sẵn sàng lập hồ sơ ứng dụng.

00:07:56.000 --> 00:08:02.000
Tôi sẽ buộc nhấp vào nút Run và chọn Profile.

00:08:02.000 --> 00:08:10.000
Điều này sẽ cài đặt phiên bản mã mới nhất trên thiết bị của tôi và mở Công cụ cho tôi với thiết bị và ứng dụng được nhắm mục tiêu của tôi được chọn.

00:08:10.000 --> 00:08:17.000
Vì tôi muốn lập hồ sơ sử dụng Core ML của mình, tôi sẽ chọn mẫu Core ML.

00:08:17.000 --> 00:08:24.000
Mẫu này bao gồm Công cụ Core ML, cũng như một số Công cụ hữu ích khác sẽ giúp bạn lập hồ sơ sử dụng Core ML của mình.

00:08:24.000 --> 00:08:32.000
Để ghi lại dấu vết, tôi sẽ chỉ cần nhấn Ghi lại.

00:08:32.000 --> 00:08:35.000
Ứng dụng hiện đang chạy trên iPhone của tôi.

00:08:35.000 --> 00:08:39.000
Tôi sẽ để nó chạy trong vài giây và sử dụng một vài kiểu khác nhau.

00:08:39.000 --> 00:08:42.000
Và bây giờ tôi sẽ kết thúc dấu vết bằng cách nhấn nút Dừng.

00:08:42.000 --> 00:08:44.000
Bây giờ tôi có dấu vết Nhạc cụ của mình.

00:08:44.000 --> 00:08:48.000
Tôi sẽ tập trung vào Core ML Instrument.

00:08:48.000 --> 00:08:53.000
Công cụ Core ML hiển thị tất cả các sự kiện Core ML đã được ghi lại trong dấu vết.

00:08:53.000 --> 00:09:01.000
Chế độ xem ban đầu nhóm tất cả các sự kiện thành ba làn đường: Hoạt động, Dữ liệu và Tính toán.

00:09:01.000 --> 00:09:14.000
Làn hoạt động hiển thị các sự kiện Core ML cấp cao nhất có mối quan hệ 1-1 với các API Core ML thực tế mà bạn sẽ gọi trực tiếp, chẳng hạn như tải và dự đoán.

00:09:14.000 --> 00:09:25.000
Làn dữ liệu hiển thị các sự kiện trong đó Core ML đang thực hiện kiểm tra dữ liệu hoặc chuyển đổi dữ liệu để đảm bảo rằng nó có thể hoạt động an toàn với đầu vào và đầu ra của mô hình.

00:09:25.000 --> 00:09:33.000
Làn tính toán hiển thị khi Core ML gửi yêu cầu tính toán đến các đơn vị tính toán cụ thể, chẳng hạn như Neural Engine hoặc GPU.

00:09:33.000 --> 00:09:42.000
Bạn cũng có thể chọn chế độ xem Ungrouped nơi có một làn đường riêng cho từng loại sự kiện.

00:09:42.000 --> 00:09:46.000
Ở phía dưới, có chế độ xem Tổng hợp Hoạt động Mô hình.

00:09:46.000 --> 00:09:51.000
Chế độ xem này cung cấp số liệu thống kê tổng hợp cho tất cả các sự kiện được hiển thị trong dấu vết.

00:09:51.000 --> 00:10:02.000
Ví dụ, trong dấu vết này, tải trọng mô hình trung bình mất 17,17 ms và dự đoán trung bình mất 7,2 ms.

00:10:02.000 --> 00:10:05.000
Một lưu ý khác là nó có thể sắp xếp các sự kiện theo thời lượng.

00:10:05.000 --> 00:10:19.000
Ở đây, danh sách cho tôi biết rằng nhiều thời gian đang được dành để tải mô hình hơn là thực sự đưa ra dự đoán với nó, với tổng số 6,41 giây tải, so với chỉ 2,69 giây dự đoán.

00:10:19.000 --> 00:10:22.000
Có lẽ điều này có điều gì đó với tốc độ khung hình thấp.

00:10:22.000 --> 00:10:28.000
Hãy để tôi thử tìm xem tất cả những tải trọng này đến từ đâu.

00:10:28.000 --> 00:10:33.000
Tôi nhận thấy rằng tôi đang tải lại mô hình Core ML của mình trước khi gọi từng dự đoán.

00:10:33.000 --> 00:10:38.000
Đây thường không phải là cách thực hành tốt vì tôi chỉ có thể tải mô hình một lần và giữ nó trong bộ nhớ.

00:10:38.000 --> 00:10:47.000
Tôi sẽ quay lại mã của mình và cố gắng sửa lỗi này.

00:10:47.000 --> 00:10:50.000
Tôi đã tìm thấy vùng mã nơi tôi tải mô hình của mình.

00:10:50.000 --> 00:11:01.000
Vấn đề ở đây là đây là một tính toán đúng cách, có nghĩa là mỗi lần tôi tham chiếu biến styleTransferModel, nó sẽ tính toán lại thuộc tính, có nghĩa là tải lại mô hình, trong trường hợp này.

00:11:01.000 --> 00:11:14.000
Tôi có thể nhanh chóng khắc phục điều này bằng cách thay đổi điều này thành một biến lười biếng.

00:11:14.000 --> 00:11:27.000
Bây giờ tôi sẽ cấu hình lại ứng dụng để kiểm tra xem điều này có khắc phục được sự cố tải lặp đi lặp lại hay không.

00:11:27.000 --> 00:11:34.000
Tôi sẽ một lần nữa chọn mẫu Core ML và ghi lại dấu vết.

00:11:34.000 --> 00:11:36.000
Điều này phù hợp hơn nhiều với những gì tôi mong đợi.

00:11:36.000 --> 00:11:49.000
Cột đếm cho tôi biết rằng có tổng cộng năm sự kiện tải, khớp với số kiểu tôi đã sử dụng trong ứng dụng và tổng thời lượng tải nhỏ hơn nhiều so với tổng thời lượng dự đoán.

00:11:49.000 --> 00:11:54.000
Ngoài ra, khi tôi cuộn qua...

00:11:54.000 --> 00:12:02.000
...Nó hiển thị chính xác các sự kiện dự đoán lặp đi lặp lại mà không có tải ở giữa mỗi sự kiện.

00:12:02.000 --> 00:12:07.000
Một lưu ý khác là cho đến nay, tôi chỉ xem các chế độ xem hiển thị tất cả hoạt động của mô hình Core ML.

00:12:07.000 --> 00:12:14.000
Trong ứng dụng này, có một mô hình Core ML cho mỗi kiểu, vì vậy tôi có thể muốn phân tích hoạt động Core ML theo mô hình.

00:12:14.000 --> 00:12:17.000
Nhạc cụ giúp việc này trở nên dễ dàng.

00:12:17.000 --> 00:12:27.000
Trong biểu đồ chính, bạn có thể nhấp vào mũi tên ở trên cùng bên trái và nó sẽ tạo một bản nhạc phụ cho mỗi mô hình được sử dụng trong dấu vết.

00:12:27.000 --> 00:12:32.000
Ở đây nó hiển thị tất cả các mô hình chuyển đổi phong cách khác nhau đã được sử dụng.

00:12:32.000 --> 00:12:43.000
Chế độ xem Tổng hợp cũng cung cấp chức năng tương tự bằng cách cho phép bạn chia nhỏ số liệu thống kê theo mô hình.

00:12:43.000 --> 00:12:50.000
Tiếp theo, tôi muốn đi sâu vào dự đoán về một trong những mô hình của mình để hiểu rõ hơn về cách nó đang được vận hành.

00:12:50.000 --> 00:12:54.000
Tôi sẽ tìm hiểu sâu hơn về mô hình Màu nước.

00:12:54.000 --> 00:13:03.000
Trong dự đoán này, làn đường Tính toán cho tôi biết rằng mô hình của tôi được chạy trên sự kết hợp giữa Neural Engine và GPU.

00:13:03.000 --> 00:13:16.000
Core ML đang gửi các yêu cầu tính toán này không đồng bộ, vì vậy nếu tôi muốn xem khi nào các đơn vị tính toán này đang tích cực chạy mô hình, tôi có thể kết hợp Core ML Instrument với GPU Instrument và Neural Engine Instrument mới.

00:13:16.000 --> 00:13:23.000
Để làm điều này, tôi đã ghim ba Dụng cụ ở đây.

00:13:23.000 --> 00:13:33.000
Công cụ Core ML cho tôi thấy toàn bộ khu vực nơi mô hình chạy.

00:13:33.000 --> 00:13:47.000
Và trong khu vực này, Neural Engine Instrument hiển thị máy tính chạy đầu tiên trên Neural Engine, sau đó GPU Instrument cho thấy mô hình đã được chuyển từ Neural Engine để hoàn thành chạy trên GPU.

00:13:47.000 --> 00:13:54.000
Điều này cho tôi ý tưởng tốt hơn về cách mô hình của tôi thực sự được thực thi trên phần cứng.

00:13:54.000 --> 00:14:02.000
Tóm lại, tôi đã sử dụng Core ML Instrument trong Xcode 14 để tìm hiểu về hiệu suất của mô hình khi chạy trực tiếp trong ứng dụng của mình.

00:14:02.000 --> 00:14:07.000
Sau đó tôi đã xác định được một vấn đề trong đó tôi quá thường xuyên tải lại mô hình của mình.

00:14:07.000 --> 00:14:14.000
Tôi đã khắc phục sự cố trong mã của mình, định hình lại ứng dụng và xác minh rằng sự cố đã được khắc phục.

00:14:14.000 --> 00:14:24.000
Tôi cũng có thể kết hợp Core ML, GPU và Neural Engine Instrument mới để biết thêm chi tiết về cách mô hình của tôi thực sự chạy trên các đơn vị tính toán khác nhau.

00:14:24.000 --> 00:14:29.000
Đó là tổng quan về các công cụ mới để giúp bạn hiểu hiệu suất.

00:14:29.000 --> 00:14:34.000
Tiếp theo, tôi sẽ xem xét một số API nâng cao có thể giúp tối ưu hóa hiệu suất đó.

00:14:34.000 --> 00:14:39.000
Hãy để tôi bắt đầu bằng cách xem xét cách Core ML xử lý đầu vào và đầu ra của mô hình.

00:14:39.000 --> 00:14:47.000
Khi bạn tạo một mô hình Core ML, mô hình đó có một tập hợp các tính năng đầu vào và đầu ra, mỗi tính năng có một loại và kích thước.

00:14:47.000 --> 00:14:55.000
Trong thời gian chạy, bạn sử dụng Core ML APIs để cung cấp đầu vào phù hợp với giao diện của mô hình và nhận đầu ra sau khi chạy suy luận.

00:14:55.000 --> 00:15:00.000
Hãy để tôi tập trung vào hình ảnh và MultiArrays chi tiết hơn một chút.

00:15:00.000 --> 00:15:08.000
Đối với hình ảnh, Core ML hỗ trợ hình ảnh thang độ xám 8 bit và hình ảnh màu 32 bit với 8 bit cho mỗi thành phần.

00:15:08.000 --> 00:15:16.000
Và đối với các mảng đa chiều, Core ML hỗ trợ Int32, Double và Float32 làm các loại vô hướng.

00:15:16.000 --> 00:15:21.000
Nếu ứng dụng của bạn đã hoạt động với các loại này, vấn đề chỉ đơn giản là kết nối chúng với mô hình.

00:15:21.000 --> 00:15:24.000
Tuy nhiên, đôi khi các loại của bạn có thể khác nhau.

00:15:24.000 --> 00:15:26.000
Hãy để tôi đưa ra một ví dụ.

00:15:26.000 --> 00:15:30.000
Tôi muốn thêm một bộ lọc mới vào ứng dụng xử lý hình ảnh và phong cách của mình.

00:15:30.000 --> 00:15:35.000
Bộ lọc này hoạt động để làm sắc nét hình ảnh bằng cách hoạt động trên hình ảnh một kênh.

00:15:35.000 --> 00:15:43.000
Ứng dụng của tôi có một số thao tác tiền và sau xử lý trên GPU và đại diện cho kênh duy nhất này ở độ chính xác Float16.

00:15:43.000 --> 00:15:51.000
Để làm điều này, tôi đã sử dụng coremltools để chuyển đổi mô hình đèn pin làm sắc nét hình ảnh sang định dạng Core ML như được hiển thị ở đây.

00:15:51.000 --> 00:15:54.000
Mô hình được thiết lập để sử dụng tính toán chính xác Float16.

00:15:54.000 --> 00:15:59.000
Ngoài ra, nó lấy đầu vào hình ảnh và tạo ra đầu ra hình ảnh.

00:15:59.000 --> 00:16:02.000
Tôi có một mô hình trông như thế này.

00:16:02.000 --> 00:16:07.000
Lưu ý rằng nó chụp ảnh thang độ xám là 8-bit cho Core ML.

00:16:07.000 --> 00:16:19.000
Để thực hiện công việc này, tôi đã phải viết một số mã để giảm đầu vào của mình từ OneComponent16Half xuống OneComponent8 và sau đó tăng đầu ra từ OneComponent8 lên OneComponent16Half.

00:16:19.000 --> 00:16:22.000
Tuy nhiên, đây không phải là toàn bộ câu chuyện.

00:16:22.000 --> 00:16:33.000
Vì mô hình được thiết lập để thực hiện tính toán ở độ chính xác Float16, tại một số điểm, Core ML cần chuyển đổi các đầu vào 8-bit này thành Float16.

00:16:33.000 --> 00:16:39.000
Nó thực hiện chuyển đổi một cách hiệu quả, nhưng khi nhìn vào dấu vết của Công cụ với ứng dụng đang chạy, nó sẽ hiển thị điều này.

00:16:39.000 --> 00:16:47.000
Lưu ý các bước dữ liệu Core ML đang thực hiện trước và sau khi tính toán Neural Engine.

00:16:47.000 --> 00:16:57.000
Khi phóng to làn dữ liệu, nó cho thấy Core ML đang sao chép dữ liệu để chuẩn bị tính toán trên Neural Engine, có nghĩa là chuyển đổi nó thành Float16, trong trường hợp này.

00:16:57.000 --> 00:17:02.000
Điều này có vẻ đáng tiếc vì dữ liệu ban đầu đã là Float16.

00:17:02.000 --> 00:17:12.000
Lý tưởng nhất, các chuyển đổi dữ liệu này có thể tránh được cả trong ứng dụng và bên trong Core ML bằng cách làm cho mô hình hoạt động trực tiếp với đầu vào và đầu ra Float16.

00:17:12.000 --> 00:17:24.000
Bắt đầu từ iOS 16 và macOS Ventura, Core ML sẽ có hỗ trợ gốc cho một hình ảnh thang độ xám OneComponent16Half và Float16 MultiArrays.

00:17:24.000 --> 00:17:36.000
Bạn có thể tạo một mô hình chấp nhận đầu vào và đầu ra Float16 bằng cách chỉ định bố cục màu mới cho hình ảnh hoặc kiểu dữ liệu mới cho MultiArrays, đồng thời gọi phương thức chuyển đổi coremltools.

00:17:36.000 --> 00:17:43.000
Trong trường hợp này, tôi đang chỉ định đầu vào và đầu ra của mô hình của mình là hình ảnh Float16 thang độ xám.

00:17:43.000 --> 00:17:56.000
Vì hỗ trợ Float16 có sẵn bắt đầu từ iOS 16 và macOS Ventura, các tính năng này chỉ khả dụng khi mục tiêu triển khai tối thiểu được chỉ định là iOS 16.

00:17:56.000 --> 00:17:59.000
Đây là cách phiên bản được chuyển đổi lại của mô hình trông như thế nào.

00:17:59.000 --> 00:18:04.000
Lưu ý rằng các đầu vào và đầu ra được đánh dấu là Grayscale16Half.

00:18:04.000 --> 00:18:16.000
Với sự hỗ trợ Float16 này, ứng dụng của tôi có thể trực tiếp cung cấp hình ảnh Float16 cho Core ML, điều này sẽ tránh được nhu cầu hạ thấp đầu vào và nâng cấp đầu ra trong ứng dụng.

00:18:16.000 --> 00:18:18.000
Đây là cách nó trông như thế nào trong mã.

00:18:18.000 --> 00:18:27.000
Vì tôi có dữ liệu đầu vào của mình dưới dạng OneComponent16Half CVPixelBuffer, tôi có thể chỉ cần gửi bộ đệm pixel trực tiếp đến Core ML.

00:18:27.000 --> 00:18:31.000
Điều này không phát sinh bất kỳ bản sao hoặc chuyển đổi dữ liệu nào.

00:18:31.000 --> 00:18:36.000
Sau đó tôi nhận được một OneComponent16Half CVPixelBuffer làm đầu ra.

00:18:36.000 --> 00:18:42.000
Điều này dẫn đến mã đơn giản hơn và không yêu cầu chuyển đổi dữ liệu.

00:18:42.000 --> 00:18:53.000
Ngoài ra còn có một điều thú vị khác mà bạn có thể làm, và đó là yêu cầu Core ML điền vào bộ đệm được phân bổ trước của bạn cho đầu ra thay vì để Core ML phân bổ bộ đệm mới cho mỗi dự đoán.

00:18:53.000 --> 00:18:59.000
Bạn có thể làm điều này bằng cách phân bổ bộ đệm sao lưu đầu ra và đặt nó trên các tùy chọn dự đoán.

00:18:59.000 --> 00:19:06.000
Đối với ứng dụng của mình, tôi đã viết một hàm gọi là outputBackingBuffer trả về OneComponent1 HalfCVPixelBuffer.

00:19:06.000 --> 00:19:14.000
Sau đó, tôi đặt điều này trên các tùy chọn dự đoán và cuối cùng gọi phương thức dự đoán trên mô hình của mình với các tùy chọn dự đoán đó.

00:19:14.000 --> 00:19:21.000
Bằng cách chỉ định các bản sao lưu đầu ra, bạn có thể kiểm soát tốt hơn việc quản lý bộ đệm cho các đầu ra của mô hình.

00:19:21.000 --> 00:19:31.000
Vì vậy, với những thay đổi được thực hiện, tóm lại, đây là những gì được hiển thị trong dấu vết Dụng cụ khi sử dụng phiên bản gốc của mô hình có đầu vào và đầu ra 8 bit.

00:19:31.000 --> 00:19:42.000
Và đây là cách dấu vết Instruments cuối cùng trông như thế nào sau khi sửa đổi mã để cung cấp bộ đệm Float16 được hỗ trợ bởi IOSurface cho phiên bản Float16 mới của mô hình.

00:19:42.000 --> 00:19:49.000
Các phép biến đổi dữ liệu trước đây được hiển thị trong làn dữ liệu hiện đã biến mất, vì Core ML không còn cần thực hiện chúng nữa.

00:19:49.000 --> 00:19:55.000
Tóm lại, Core ML hiện có hỗ trợ gốc đầu cuối cho dữ liệu Float16.

00:19:55.000 --> 00:20:03.000
Điều này có nghĩa là bạn có thể cung cấp đầu vào Float16 cho Core ML và Core ML trả lại cho bạn đầu ra Float16.

00:20:03.000 --> 00:20:11.000
Bạn cũng có thể sử dụng API sao lưu đầu ra mới để Core ML lấp đầy bộ đệm đầu ra được phân bổ trước của bạn thay vì tạo bộ đệm mới.

00:20:11.000 --> 00:20:25.000
Và cuối cùng, chúng tôi khuyên bạn nên sử dụng bộ đệm được hỗ trợ bề mặt IOS bất cứ khi nào có thể, vì điều này cho phép Core ML chuyển dữ liệu giữa các đơn vị tính toán khác nhau mà không cần sao chép dữ liệu bằng cách tận dụng bộ nhớ thống nhất.

00:20:25.000 --> 00:20:31.000
Tiếp theo, tôi sẽ xem qua một chuyến tham quan nhanh về một số khả năng bổ sung được thêm vào Core ML.

00:20:31.000 --> 00:20:33.000
Đầu tiên là nén trọng lượng.

00:20:33.000 --> 00:20:39.000
Nén trọng lượng của mô hình của bạn có thể cho phép bạn đạt được độ chính xác tương tự trong khi có một mô hình nhỏ hơn.

00:20:39.000 --> 00:20:48.000
Trong iOS 12, Core ML đã giới thiệu tính năng nén trọng lượng sau đào tạo cho phép bạn giảm kích thước của các mô hình mạng thần kinh Core ML.

00:20:48.000 --> 00:20:59.000
Chúng tôi hiện đang mở rộng hỗ trợ 16 và 8 bit cho loại mô hình Chương trình ML và ngoài ra, giới thiệu một tùy chọn mới để lưu trữ trọng lượng trong một biểu diễn thưa thớt.

00:20:59.000 --> 00:21:09.000
Với các tiện ích coremltools, giờ đây bạn sẽ có thể định lượng, phân loại và tạo trọng lượng cho các mô hình Chương trình ML của mình.

00:21:09.000 --> 00:21:12.000
Tiếp theo là một tùy chọn đơn vị tính toán mới.

00:21:12.000 --> 00:21:18.000
Core ML luôn nhằm mục đích giảm thiểu độ trễ suy luận cho sở thích đơn vị tính toán đã cho.

00:21:18.000 --> 00:21:24.000
Các ứng dụng có thể chỉ định tùy chọn này bằng cách đặt thuộc tính MLModelConfiguration computeUnits.

00:21:24.000 --> 00:21:31.000
Ngoài ba tùy chọn đơn vị tính toán hiện có, hiện có một tùy chọn mới gọi là cpuAndNeuralEngine.

00:21:31.000 --> 00:21:44.000
Điều này yêu cầu Core ML không gửi tính toán trên GPU, điều này có thể hữu ích khi ứng dụng sử dụng GPU cho các tính toán khác và do đó, thích Core ML giới hạn trọng tâm của nó đối với CPU và Neural Engine.

00:21:44.000 --> 00:21:54.000
Tiếp theo, chúng tôi đang thêm một cách mới để khởi tạo phiên bản mô hình Core ML của bạn cung cấp thêm tính linh hoạt về mặt tuần tự hóa mô hình.

00:21:54.000 --> 00:22:01.000
Điều này cho phép bạn mã hóa dữ liệu mô hình của mình bằng các sơ đồ mã hóa tùy chỉnh và giải mã nó ngay trước khi tải.

00:22:01.000 --> 00:22:11.000
Với các API mới này, bạn có thể biên dịch và tải thông số kỹ thuật mô hình Core ML trong bộ nhớ mà không yêu cầu mô hình được biên dịch phải nằm trên đĩa.

00:22:11.000 --> 00:22:16.000
Bản cập nhật cuối cùng là về các gói Swift và cách chúng hoạt động với Core ML.

00:22:16.000 --> 00:22:20.000
Các gói hàng là một cách tuyệt vời để đóng gói và phân phối mã có thể tái sử dụng.

00:22:20.000 --> 00:22:28.000
Với Xcode 14, bạn có thể bao gồm các mô hình Core ML trong các gói Swift của mình và bây giờ khi ai đó nhập gói của bạn, mô hình của bạn sẽ hoạt động.

00:22:28.000 --> 00:22:36.000
Xcode sẽ tự động biên dịch và đóng gói mô hình Core ML của bạn và tạo cùng một giao diện tạo mã mà bạn đã quen làm việc.

00:22:36.000 --> 00:22:43.000
Chúng tôi rất vui mừng về sự thay đổi này, vì nó sẽ giúp việc phân phối các mô hình của bạn trong hệ sinh thái Swift dễ dàng hơn rất nhiều.

00:22:43.000 --> 00:22:46.000
Điều đó đưa chúng ta đến cuối phiên này.

00:22:46.000 --> 00:22:56.000
Các báo cáo hiệu suất ML cốt lõi và Công cụ trong Xcode 14 ở đây để giúp bạn phân tích và tối ưu hóa hiệu suất của các tính năng được hỗ trợ ML trong ứng dụng của mình.

00:22:56.000 --> 00:23:03.000
Hỗ trợ Float16 mới và API sao lưu đầu ra cho phép bạn kiểm soát nhiều hơn cách dữ liệu chảy vào và ra khỏi Core ML.

00:23:03.000 --> 00:23:09.000
Hỗ trợ mở rộng cho việc nén trọng lượng có thể giúp bạn giảm thiểu kích thước của các mô hình của mình.

00:23:09.000 --> 00:23:18.000
Và với các mô hình trong bộ nhớ và hỗ trợ gói Swift, bạn thậm chí còn có nhiều lựa chọn hơn khi nói đến cách bạn thể hiện, tích hợp và chia sẻ các mô hình Core ML.

00:23:18.000 --> 00:23:22.000
Đây là Ben từ đội Core ML, và có một phần còn lại tuyệt vời của WWDC.

00:23:22.000 --> 23:59:59.000
♪

