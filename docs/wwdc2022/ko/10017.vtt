WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:15.000
안녕, 내 이름은 Geppy Parziale이고, 나는 여기 애플의 기계 학습 엔지니어야.

00:00:15.000 --> 00:00:27.000
오늘, 저는 일반적으로 매우 전문적인 작업을 수행하는 데 전문가가 필요한 문제를 해결하기 위해 기계 학습을 사용하는 앱을 만드는 여정을 안내하고 싶습니다.

00:00:27.000 --> 00:00:37.000
이 여정은 오픈 소스 기계 학습 모델을 앱에 추가하고 환상적인 새로운 경험을 만드는 방법을 보여줄 수 있는 기회를 제공합니다.

00:00:37.000 --> 00:00:49.000
여행 중에, 저는 또한 기계 학습을 사용하여 앱을 구축하기 위해 Apple 개발 생태계에서 사용할 수 있는 많은 도구, 프레임워크 및 API 중 몇 가지를 강조할 것입니다.

00:00:49.000 --> 00:00:57.000
앱을 만들 때, 개발자인 당신은 사용자에게 최고의 경험을 가져다 줄 일련의 결정을 거칩니다.

00:00:57.000 --> 00:01:04.000
그리고 이것은 응용 프로그램에 기계 학습 기능을 추가할 때도 마찬가지입니다.

00:01:04.000 --> 00:01:10.000
개발 중에, 당신은 물어볼 수 있습니다: 이 기능을 구축하기 위해 기계 학습을 사용해야 하나요?

00:01:10.000 --> 00:01:14.000
기계 학습 모델을 어떻게 얻을 수 있나요?

00:01:14.000 --> 00:01:18.000
그 모델을 Apple 플랫폼과 호환되도록 하려면 어떻게 해야 하나요?

00:01:18.000 --> 00:01:22.000
그 모델이 내 특정 사용 사례에 효과가 있을까?

00:01:22.000 --> 00:01:26.000
애플 뉴럴 엔진에서 실행되나요?

00:01:26.000 --> 00:01:29.000
그러니 이 여행을 함께 해보자.

00:01:29.000 --> 00:01:39.000
지하실에 있는 오래된 상자에서 찾은 가족 흑백 사진에 사실적인 색상을 추가할 수 있는 앱을 만들고 싶습니다.

00:01:39.000 --> 00:01:47.000
물론, 전문 사진작가는 사진 편집 도구에서 시간을 보내면서 수작업으로 이것을 할 수 있다.

00:01:47.000 --> 00:01:54.000
대신, 이 과정을 자동화하고 몇 초 안에 채색을 적용하려면 어떻게 해야 하나요?

00:01:54.000 --> 00:01:58.000
이것은 기계 학습을 위한 완벽한 작업인 것 같다.

00:01:58.000 --> 00:02:06.000
Apple은 앱에서 ML 기능을 구축하고 통합하는 데 도움이 되는 엄청난 양의 프레임워크와 도구를 제공합니다.

00:02:06.000 --> 00:02:12.000
그들은 데이터 처리부터 모델 훈련과 추론에 이르기까지 모든 것을 제공한다.

00:02:12.000 --> 00:02:15.000
이 여행을 위해, 나는 그것들 중 몇 개를 사용할 것이다.

00:02:15.000 --> 00:02:22.000
하지만 당신이 개발하고 있는 특정 기계 학습 작업에 따라 선택할 수 있는 것이 많다는 것을 기억하세요.

00:02:22.000 --> 00:02:29.000
내 앱에서 기계 학습 기능을 개발할 때 사용하는 과정은 일련의 단계를 거친다.

00:02:29.000 --> 00:02:38.000
먼저, 나는 과학 출판물이나 전문 웹사이트에서 올바른 기계 학습 모델을 검색한다.

00:02:38.000 --> 00:02:46.000
나는 사진 채색을 검색했고, 내가 필요한 것에 사용할 수 있는 컬러라이저라는 모델을 찾았다.

00:02:46.000 --> 00:02:53.000
여기 이 모델을 사용하여 얻을 수 있는 채색의 예가 있습니다.

00:02:53.000 --> 00:02:56.000
여기 또 하나 있어.

00:02:56.000 --> 00:03:00.000
그리고 여기 또 하나 있어. 정말 좋아.

00:03:00.000 --> 00:03:03.000
그게 어떻게 작동하는지 보여줄게.

00:03:03.000 --> 00:03:07.000
컬러라이저 모델은 입력으로 흑백 이미지를 기대한다.

00:03:07.000 --> 00:03:15.000
내가 찾은 파이썬 소스 코드는 모든 RGB 이미지를 LAB 색 공간 이미지로 변환한다.

00:03:15.000 --> 00:03:25.000
이 색 공간에는 3개의 채널이 있습니다: 하나는 이미지 밝기 또는 L 채널을 나타내고, 다른 두 개는 색상 구성 요소를 나타냅니다.

00:03:25.000 --> 00:03:32.000
밝기가 컬러라이저 모델의 입력이 되는 동안 색상 구성 요소는 폐기됩니다.

00:03:32.000 --> 00:03:41.000
그런 다음 모델은 입력 L 채널과 결합하여 결과 이미지에 색상을 제공하는 두 가지 새로운 색상 구성 요소를 추정합니다.

00:03:41.000 --> 00:03:45.000
이제 이 모델을 내 앱과 호환되도록 만들 때입니다.

00:03:45.000 --> 00:03:53.000
이를 달성하기 위해, 저는 coremltools를 사용하여 원래 PyTorch 모델을 Core ML 형식으로 변환할 수 있습니다.

00:03:53.000 --> 00:03:59.000
다음은 PyTorch 모델을 Core ML로 변환하는 데 사용한 간단한 파이썬 스크립트입니다.

00:03:59.000 --> 00:04:04.000
먼저, 나는 PyTorch 모델 아키텍처와 가중치를 가져온다.

00:04:04.000 --> 00:04:07.000
그리고 나서 나는 수입된 모델을 추적한다.

00:04:07.000 --> 00:04:12.000
마지막으로, 나는 PyTorch 모델을 Core ML로 변환하고 저장한다.

00:04:12.000 --> 00:04:19.000
모델이 Core ML 형식이 되면, 변환이 올바르게 작동하는지 확인해야 합니다.

00:04:19.000 --> 00:04:23.000
나는 coremltools를 사용하여 파이썬에서 직접 그것을 다시 할 수 있다.

00:04:23.000 --> 00:04:25.000
그리고 이건 쉬워.

00:04:25.000 --> 00:04:32.000
RGB 색 공간으로 이미지를 가져와서 실험실 색 공간으로 변환합니다.

00:04:32.000 --> 00:04:38.000
그런 다음 나는 색상 채널에서 가벼움을 분리하고 그것들을 버린다.

00:04:38.000 --> 00:04:42.000
나는 Core ML 모델을 사용하여 예측을 실행한다.

00:04:42.000 --> 00:04:49.000
그리고 마지막으로, 예상 색상 구성 요소로 입력 밝기를 구성하고 RGB로 변환하세요.

00:04:49.000 --> 00:04:57.000
이를 통해 변환된 모델의 기능이 원래 PyTorch 모델의 기능과 일치하는지 확인할 수 있습니다.

00:04:57.000 --> 00:05:01.000
나는 이 단계를 모델 검증이라고 부른다.

00:05:01.000 --> 00:05:05.000
하지만, 해야 할 또 다른 중요한 점검이 있다.

00:05:05.000 --> 00:05:10.000
나는 이 모델이 내 목표 장치에서 충분히 빠르게 실행될 수 있는지 이해해야 한다.

00:05:10.000 --> 00:05:17.000
그래서 나는 장치에서 모델을 평가하고 그것이 최고의 사용자 경험을 제공할 수 있는지 확인해야 할 것이다.

00:05:17.000 --> 00:05:26.000
현재 Xcode 14에서 사용할 수 있는 새로운 Core ML 성능 보고서는 Core ML 모델의 시간 기반 분석을 수행합니다.

00:05:26.000 --> 00:05:33.000
모델을 Xcode로 끌어다 놓고 몇 초 안에 성능 보고서를 만들기만 하면 됩니다.

00:05:33.000 --> 00:05:44.000
이 도구를 사용하면 M1과 iPadOS 16이 설치된 iPad Pro에서 예상 예측 시간이 거의 90밀리초라는 것을 알 수 있습니다.

00:05:44.000 --> 00:05:48.000
그리고 이것은 내 사진 채색 앱에 완벽하다.

00:05:48.000 --> 00:05:56.000
Xcode의 성능 보고서에 대해 더 알고 싶다면, 올해의 세션 "핵심 ML 사용 최적화"를 보는 것이 좋습니다.

00:05:56.000 --> 00:06:05.000
따라서 성능 보고서는 모델을 평가하고 최고의 온디바이스 사용자 경험을 제공하는 데 도움이 될 수 있습니다.

00:06:05.000 --> 00:06:13.000
이제 제 모델의 기능과 성능에 대해 확신했으니, 제 앱에 통합하겠습니다.

00:06:13.000 --> 00:06:26.000
통합 과정은 내가 지금까지 파이썬에서 했던 것과 동일하지만, 이번에는 Xcode와 당신이 익숙한 다른 모든 도구를 사용하여 Swift에서 원활하게 할 수 있습니다.

00:06:26.000 --> 00:06:34.000
이제 코어 ML 형식의 모델은 가벼움을 나타내는 단일 채널 이미지를 기대한다는 것을 기억하세요.

00:06:34.000 --> 00:06:45.000
그래서 내가 이전에 파이썬에서 했던 것과 비슷하게, 나는 실험실 색 공간을 사용하여 RGB 입력 이미지를 이미지로 변환해야 한다.

00:06:45.000 --> 00:06:53.000
나는 이 변환을 여러 가지 방법으로 쓸 수 있다: vImage와 함께 Swift에서 직접, 또는 Metal을 사용하여.

00:06:53.000 --> 00:07:02.000
문서를 더 깊이 탐구하면서, 나는 코어 이미지 프레임워크가 이것에 도움이 될 수 있는 무언가를 제공한다는 것을 발견했다.

00:07:02.000 --> 00:07:10.000
그래서 RGB를 LAB으로 변환하고 Core ML 모델을 사용하여 예측을 실행하는 방법을 보여드리겠습니다.

00:07:10.000 --> 00:07:17.000
다음은 RGB 이미지에서 밝기를 추출하여 Core ML 모델에 전달하는 Swift 코드입니다.

00:07:17.000 --> 00:07:23.000
먼저, 나는 RGB 이미지를 LAB으로 변환하고 가벼움을 추출한다.

00:07:23.000 --> 00:07:31.000
그런 다음, 나는 가벼움을 CGImage로 변환하고 Core ML 모델에 대한 입력을 준비한다.

00:07:31.000 --> 00:07:33.000
마지막으로, 나는 예측을 실행한다.

00:07:33.000 --> 00:07:45.000
입력 RGB 이미지에서 L 채널을 추출하려면 먼저 새로운 CIFilter convertRGBtoLab을 사용하여 RGB 이미지를 LAB 이미지로 변환합니다.

00:07:45.000 --> 00:07:51.000
가벼움의 값은 0에서 100 사이로 설정됩니다.

00:07:51.000 --> 00:07:59.000
그런 다음, 나는 실험실 이미지에 컬러 매트릭스를 곱하고 컬러 채널을 버리고 발신자에게 가벼움을 돌려줍니다.

00:07:59.000 --> 00:08:04.000
이제 모델의 출력에서 무슨 일이 일어나는지 분석해 봅시다.

00:08:04.000 --> 00:08:12.000
코어 ML 모델은 예상 색상 구성 요소를 포함하는 두 개의 MLShapedArray를 반환합니다.

00:08:12.000 --> 00:08:19.000
그래서 예측 후, 나는 두 개의 MLShapedArray를 두 개의 CIImages로 변환한다.

00:08:19.000 --> 00:08:23.000
마지막으로, 나는 그것들을 모델 입력 가벼움과 결합한다.

00:08:23.000 --> 00:08:30.000
이것은 내가 RGB로 변환하고 반환하는 새로운 LAB 이미지를 생성한다.

00:08:30.000 --> 00:08:38.000
두 개의 MLShapedArray를 두 개의 CIImages로 변환하기 위해, 먼저 각 모양의 배열에서 값을 추출합니다.

00:08:38.000 --> 00:08:44.000
그런 다음, 두 개의 컬러 채널을 나타내는 두 개의 핵심 이미지를 만들고 반환합니다.

00:08:44.000 --> 00:08:56.000
밝기와 예상 색상 채널을 결합하기 위해, 저는 세 개의 채널을 입력으로 사용하고 CIImage를 반환하는 사용자 지정 CIKernel을 사용합니다.

00:08:56.000 --> 00:09:05.000
그런 다음, 새로운 CIFilter convertLabToRGB를 사용하여 실험실 이미지를 RGB로 변환하고 발신자에게 반환합니다.

00:09:05.000 --> 00:09:14.000
이것은 단일 CIImage에서 두 개의 예상 색상 채널과 밝기를 결합하는 데 사용하는 사용자 지정 CIKernel의 소스 코드입니다.

00:09:14.000 --> 00:09:29.000
RGB 이미지를 LAB 이미지로 변환하는 새로운 CI 필터에 대한 자세한 내용은 "Core Image, Metal 및 SwiftUI로 EDR 콘텐츠 표시" 세션을 참조하십시오.

00:09:29.000 --> 00:09:34.000
이제 내 앱에서 이 ML 기능의 통합을 완료했으니, 실제로 작동하는 것을 봅시다.

00:09:34.000 --> 00:09:36.000
하지만 기다려.

00:09:36.000 --> 00:09:41.000
응용 프로그램으로 오래된 가족 사진을 실시간으로 색칠하려면 어떻게 해야 하나요?

00:09:41.000 --> 00:09:46.000
나는 그들 각각을 디지털화하고 내 앱에서 가져오는 데 시간을 할애할 수 있다.

00:09:46.000 --> 00:09:48.000
더 좋은 생각이 있는 것 같아.

00:09:48.000 --> 00:09:54.000
iPad 카메라를 사용하여 이 사진을 스캔하고 실시간으로 색칠하면 어떨까요?

00:09:54.000 --> 00:09:58.000
나는 그것이 정말 재미있을 것이라고 생각하며, 나는 이것을 성취하는 데 필요한 모든 것을 가지고 있다.

00:09:58.000 --> 00:10:02.000
하지만 먼저, 나는 문제를 해결해야 해.

00:10:02.000 --> 00:10:06.000
내 모델은 이미지를 처리하는 데 90밀리초가 필요하다.

00:10:06.000 --> 00:10:11.000
내가 비디오를 처리하고 싶다면, 나는 더 빠른 무언가가 필요할 것이다.

00:10:11.000 --> 00:10:17.000
원활한 사용자 경험을 위해, 장치 카메라를 최소 30fps로 실행하고 싶습니다.

00:10:17.000 --> 00:10:24.000
그것은 카메라가 약 30밀리초마다 프레임을 생산할 것이라는 것을 의미한다.

00:10:24.000 --> 00:10:35.000
하지만 모델이 비디오 프레임을 채색하는 데 약 90밀리초가 필요하기 때문에, 나는 각 채색 중에 2개 또는 3개의 프레임을 잃을 것이다.

00:10:35.000 --> 00:10:44.000
모델의 총 예측 시간은 아키텍처와 매핑되는 컴퓨팅 단위 작업의 함수이다.

00:10:44.000 --> 00:10:55.000
성능 보고서를 다시 보면, 내 모델이 신경 엔진과 CPU의 조합으로 실행되는 총 61개의 작업이 있다는 것을 알 수 있다.

00:10:55.000 --> 00:11:00.000
만약 내가 더 빠른 예측 시간을 원한다면, 나는 모델을 바꿔야 할 것이다.

00:11:00.000 --> 00:11:07.000
나는 모델의 아키텍처를 실험하고 더 빠를 수 있는 몇 가지 대안을 탐구하기로 결정했다.

00:11:07.000 --> 00:11:13.000
그러나, 아키텍처의 변화는 내가 네트워크를 재교육해야 한다는 것을 의미한다.

00:11:13.000 --> 00:11:20.000
Apple은 Mac에서 직접 기계 학습 모델을 훈련시킬 수 있는 다양한 솔루션을 제공합니다.

00:11:20.000 --> 00:11:35.000
제 경우, 원래 모델이 PyTorch에서 개발되었기 때문에, 저는 Metal에 새로운 PyTorch를 사용하기로 결정했기 때문에 Apple Silicon이 제공하는 엄청난 하드웨어 가속을 활용할 수 있습니다.

00:11:35.000 --> 00:11:50.000
Metal으로 가속화된 PyTorch에 대해 더 알고 싶다면, "Accelerate machine learning with Metal" 세션을 확인하세요. 이 변화 때문에 우리의 여정은 한 걸음 물러서야 합니다.

00:11:50.000 --> 00:11:59.000
재교육 후, 결과를 Core ML 형식으로 변환하고 검증을 다시 실행해야 합니다.

00:11:59.000 --> 00:12:04.000
이번에, 모델 통합은 단순히 이전 모델을 새 모델로 바꾸는 것으로 구성되어 있다.

00:12:04.000 --> 00:12:11.000
몇 가지 후보 대체 모델을 재교육한 후, 나는 내 요구 사항을 충족할 모델을 확인했다.

00:12:11.000 --> 00:12:16.000
여기 해당 성과 보고서가 있습니다.

00:12:16.000 --> 00:12:27.000
그것은 전적으로 신경 엔진에서 실행되며 예측 시간은 이제 비디오에서 작동하는 약 16밀리초이다.

00:12:27.000 --> 00:12:33.000
하지만 성과 보고서는 내 앱의 성능의 한 측면만 알려준다.

00:12:33.000 --> 00:12:40.000
실제로, 내 앱을 실행한 후, 나는 채색이 내가 예상했던 것만큼 매끄럽지 않다는 것을 즉시 알아차렸다.

00:12:40.000 --> 00:12:45.000
그래서 런타임에 내 앱에서 무슨 일이 일어나나요?

00:12:45.000 --> 00:12:52.000
그것을 이해하기 위해, 나는 Instruments에서 새로운 Core ML 템플릿을 사용할 수 있다.

00:12:52.000 --> 00:13:00.000
코어 ML 추적의 초기 부분을 분석하면서, 모델을 로드한 후, 앱이 예측을 축적한다는 것을 알 수 있다.

00:13:00.000 --> 00:13:02.000
그리고 이건 예상치 못한 일이야.

00:13:02.000 --> 00:13:08.000
대신, 나는 프레임당 하나의 예측을 할 것으로 예상한다.

00:13:08.000 --> 00:13:19.000
추적을 확대하고 첫 번째 예측을 확인하면서, 나는 앱이 첫 번째 예측이 완료되기 전에 두 번째 코어 ML 예측을 요청하는 것을 관찰한다.

00:13:19.000 --> 00:13:27.000
여기서, 신경 엔진은 두 번째 요청이 Core ML에 주어질 때 여전히 첫 번째 요청에서 작동하고 있다.

00:13:27.000 --> 00:13:33.000
마찬가지로, 세 번째 예측은 여전히 두 번째 예측을 처리하는 동안 시작된다.

00:13:33.000 --> 00:13:42.000
네 번의 예측 후에도, 요청과 실행 사이의 지연은 이미 약 20밀리초이다.

00:13:42.000 --> 00:13:51.000
대신, 나는 이러한 지연을 피하기 위해 이전 예측이 완료된 경우에만 새로운 예측이 시작되도록 해야 한다.

00:13:51.000 --> 00:14:03.000
이 문제를 해결하는 동안, 나는 또한 실수로 카메라 프레임 속도를 원하는 30fps 대신 60fps로 설정했다는 것을 알게 되었다.

00:14:03.000 --> 00:14:22.000
이전 예측이 완료된 후 앱이 새 프레임을 처리하는지 확인하고 카메라 프레임 속도를 30fps로 설정한 후, Core ML이 Apple Neural Engine에 단일 예측을 올바르게 발송하고 이제 앱이 원활하게 실행된다는 것을 알 수 있습니다.

00:14:22.000 --> 00:14:26.000
그래서 우리는 여행의 끝에 도달했다.

00:14:26.000 --> 00:14:34.000
내 오래된 가족 사진에서 앱을 테스트해 보자.

00:14:34.000 --> 00:14:38.000
여기 제가 지하실에서 찾은 흑백 사진이 있습니다.

00:14:38.000 --> 00:14:49.000
그들은 내가 오래 전에 방문한 이탈리아의 장소 중 일부를 포착한다.

00:14:49.000 --> 00:14:53.000
여기 로마 콜로세움의 멋진 사진이 있습니다.

00:14:53.000 --> 00:14:59.000
벽과 하늘의 색깔은 매우 현실적이다.

00:14:59.000 --> 00:15:03.000
이거 확인해 보자.

00:15:03.000 --> 00:15:06.000
여기는 이탈리아 남부의 카스텔 델 몬테이다.

00:15:06.000 --> 00:15:09.000
정말 좋아.

00:15:09.000 --> 00:15:12.000
그리고 여기는 내 고향인 그로타글리에야.

00:15:12.000 --> 00:15:17.000
이 이미지에 색상을 추가하는 것은 많은 추억을 불러일으켰다.

00:15:17.000 --> 00:15:26.000
나머지 장면을 흑백으로 유지하면서 사진에만 채색을 적용하고 있습니다.

00:15:26.000 --> 00:15:32.000
여기서, 저는 비전 프레임워크에서 사용할 수 있는 직사각형 감지 알고리즘을 활용하고 있습니다.

00:15:32.000 --> 00:15:41.000
VNDetectRectangleRequest를 사용하면 장면에서 사진을 분리하고 Colorizer 모델의 입력으로 사용할 수 있습니다.

00:15:41.000 --> 00:15:44.000
그리고 이제 내가 요약할게.

00:15:44.000 --> 00:15:56.000
우리의 여정 동안, 저는 Apple이 앱의 기계 학습 기능을 준비, 통합 및 평가하기 위해 제공하는 엄청난 양의 프레임워크, API 및 도구를 탐구했습니다.

00:15:56.000 --> 00:16:03.000
나는 그것을 해결하기 위해 오픈 소스 기계 학습 모델이 필요한 문제를 식별하는 이 여정을 시작했다.

00:16:03.000 --> 00:16:10.000
나는 원하는 기능을 갖춘 오픈 소스 모델을 찾았고 Apple 플랫폼과 호환되도록 만들었다.

00:16:10.000 --> 00:16:16.000
나는 새로운 성능 보고서를 사용하여 장치에서 직접 모델 성능을 평가했다.

00:16:16.000 --> 00:16:22.000
나는 당신이 익숙한 도구와 프레임워크를 사용하여 내 앱에 모델을 통합했습니다.

00:16:22.000 --> 00:16:27.000
악기의 새로운 코어 ML 템플릿을 사용하여 모델을 최적화했습니다.

00:16:27.000 --> 00:16:40.000
Apple 도구와 프레임워크를 사용하면 데이터 준비, 교육, 통합 및 최적화에서 Apple 장치 및 플랫폼에서 직접 개발 프로세스의 각 단계를 처리할 수 있습니다.

00:16:40.000 --> 00:16:49.000
오늘, 우리는 개발자인 당신이 Apple이 제공하는 프레임워크와 도구로 달성할 수 있는 것의 표면을 긁어 냈습니다.

00:16:49.000 --> 00:16:56.000
앱에 기계 학습을 도입하기 위한 추가적인 영감을 주는 아이디어는 이와 연결된 이전 세션을 참조하십시오.

00:16:56.000 --> 00:16:59.000
프레임워크와 도구를 탐색하고 시도해 보세요.

00:16:59.000 --> 00:17:09.000
소프트웨어와 하드웨어 간의 큰 시너지 효과를 활용하여 기계 학습 기능을 가속화하고 앱의 사용자 경험을 풍부하게 하세요.

00:17:09.000 --> 23:59:59.000
좋은 WWDC 되세요, 그리고 도착하세요. ♪ ♪

