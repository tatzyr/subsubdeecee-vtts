WEBVTT

00:00:00.000 --> 00:00:03.000
♪ 부드러운 기악 힙합 음악 ♪

00:00:03.000 --> 00:00:09.000
♪

00:00:09.000 --> 00:00:13.000
안녕하세요, 제 이름은 벤이고, 저는 코어 ML 팀의 엔지니어입니다.

00:00:13.000 --> 00:00:18.000
오늘 저는 Core ML에 추가되는 흥미진진한 새로운 기능 중 일부를 보여줄 것입니다.

00:00:18.000 --> 00:00:23.000
이러한 기능의 초점은 Core ML 사용을 최적화하는 데 도움이 되는 것입니다.

00:00:23.000 --> 00:00:34.000
이 세션에서는 Core ML을 사용할 때 모델의 성능을 이해하고 최적화하는 데 필요한 정보를 제공하기 위해 사용할 수 있는 성능 도구를 살펴볼 것입니다.

00:00:34.000 --> 00:00:40.000
그런 다음 최적화를 할 수 있는 몇 가지 향상된 API를 검토하겠습니다.

00:00:40.000 --> 00:00:47.000
그리고 마지막으로, 몇 가지 추가 코어 ML 기능과 통합 옵션에 대한 개요를 제공하겠습니다.

00:00:47.000 --> 00:00:50.000
성능 도구부터 시작하겠습니다.

00:00:50.000 --> 00:00:56.000
배경을 제공하기 위해, 앱 내에서 Core ML을 사용할 때 표준 워크플로우를 요약하는 것으로 시작하겠습니다.

00:00:56.000 --> 00:00:59.000
첫 번째 단계는 모델을 선택하는 것입니다.

00:00:59.000 --> 00:01:13.000
이것은 Core ML 도구를 사용하여 PyTorch 또는 TensorFlow 모델을 Core ML 형식으로 변환하거나, 기존 Core ML 모델을 사용하거나, Create ML을 사용하여 모델을 훈련하고 내보내는 것과 같은 다양한 방법으로 수행될 수 있습니다.

00:01:13.000 --> 00:01:20.000
모델 변환에 대한 자세한 내용이나 Create ML에 대해 알아보려면, 이 세션을 확인하는 것이 좋습니다.

00:01:20.000 --> 00:01:23.000
다음 단계는 그 모델을 앱에 통합하는 것입니다.

00:01:23.000 --> 00:01:33.000
여기에는 모델을 애플리케이션과 번들링하고 Core ML API를 사용하여 앱 실행 중에 해당 모델에서 추론을 로드하고 실행하는 것이 포함됩니다.

00:01:33.000 --> 00:01:39.000
마지막 단계는 Core ML을 사용하는 방식을 최적화하는 것입니다.

00:01:39.000 --> 00:01:41.000
먼저, 나는 모델을 선택하는 것을 검토할 것이다.

00:01:41.000 --> 00:01:47.000
앱 내에서 해당 모델을 사용해야 하는지 결정할 때 고려해야 할 모델의 많은 측면이 있습니다.

00:01:47.000 --> 00:01:53.000
또한 선택하고 싶은 모델의 여러 후보가 있을 수 있지만, 어느 것을 사용할지 어떻게 결정합니까?

00:01:53.000 --> 00:01:58.000
활성화하려는 기능의 요구 사항과 일치하는 기능이 있는 모델이 있어야 합니다.

00:01:58.000 --> 00:02:03.000
여기에는 모델의 정확성과 성능을 이해하는 것이 포함된다.

00:02:03.000 --> 00:02:07.000
코어 ML 모델에 대해 배울 수 있는 좋은 방법은 Xcode에서 여는 것입니다.

00:02:07.000 --> 00:02:12.000
아무 모델이나 두 번 클릭하면 다음과 같은 것이 올라옵니다.

00:02:12.000 --> 00:02:19.000
상단에서 모델 유형, 크기 및 운영 체제 요구 사항을 찾을 수 있습니다.

00:02:19.000 --> 00:02:30.000
일반 탭에서 모델의 메타데이터, 컴퓨팅 및 스토리지 정밀도, 예측할 수 있는 클래스 라벨과 같은 정보에 캡처된 추가 세부 사항을 보여줍니다.

00:02:30.000 --> 00:02:37.000
미리보기 탭은 예시 입력을 제공하고 예측하는 것을 보고 모델을 테스트하기 위한 것입니다.

00:02:37.000 --> 00:02:45.000
예측 탭은 모델의 입력과 출력뿐만 아니라 Core ML이 런타임에 기대하는 유형과 크기를 표시합니다.

00:02:45.000 --> 00:02:52.000
그리고 마지막으로, 유틸리티 탭은 모델 암호화 및 배포 작업에 도움이 될 수 있습니다.

00:02:52.000 --> 00:02:58.000
전반적으로, 이러한 견해는 모델의 기능에 대한 간략한 개요와 정확성에 대한 미리보기를 제공합니다.

00:02:58.000 --> 00:03:02.000
하지만 네 모델의 성능은 어때?

00:03:02.000 --> 00:03:12.000
모델을 로드하는 비용, 단일 예측에 걸리는 시간 또는 사용하는 하드웨어는 사용 사례에 중요한 요소가 될 수 있습니다.

00:03:12.000 --> 00:03:22.000
실시간 스트리밍 데이터 제약과 관련된 어려운 목표가 있거나 인지된 대기 시간에 따라 사용자 인터페이스에 대한 주요 설계 결정을 내려야 할 수도 있습니다.

00:03:22.000 --> 00:03:32.000
모델의 성능에 대한 통찰력을 얻는 한 가지 방법은 앱에 초기 통합을 하거나 계측하고 측정할 수 있는 작은 프로토타입을 만드는 것입니다.

00:03:32.000 --> 00:03:39.000
그리고 성능은 하드웨어에 의존하기 때문에, 지원되는 다양한 하드웨어에서 이러한 측정을 하고 싶을 것입니다.

00:03:39.000 --> 00:03:45.000
Xcode와 Core ML은 이제 한 줄의 코드를 작성하기 전에도 이 작업을 도울 수 있습니다.

00:03:45.000 --> 00:03:47.000
Core ML을 사용하면 이제 성과 보고서를 만들 수 있습니다.

00:03:47.000 --> 00:03:52.000
내가 보여줄게.

00:03:52.000 --> 00:03:59.000
이제 YOLOv3 객체 감지 모델을 위한 Xcode 모델 뷰어가 열려 있습니다.

00:03:59.000 --> 00:04:04.000
예측과 유틸리티 탭 사이에는 이제 성능 탭이 있습니다.

00:04:04.000 --> 00:04:20.000
성능 보고서를 생성하려면, 왼쪽 하단의 더하기 아이콘을 선택하고, 내 iPhone에서 실행하고 싶은 장치를 선택하고, 다음을 클릭한 다음, Core ML을 사용할 컴퓨팅 단위를 선택합니다.

00:04:20.000 --> 00:04:26.000
Core ML이 사용 가능한 모든 컴퓨팅 단위로 대기 시간을 최적화할 수 있도록 All에 둘 것입니다.

00:04:26.000 --> 00:04:31.000
이제 나는 Run Test를 눌러 끝낼 것이다.

00:04:31.000 --> 00:04:36.000
테스트를 실행할 수 있도록, 선택한 장치가 잠금 해제되었는지 확인하세요.

00:04:36.000 --> 00:04:40.000
성능 보고서가 생성되는 동안 회전하는 아이콘을 보여줍니다.

00:04:40.000 --> 00:04:50.000
보고서를 만들기 위해, 모델은 장치로 전송되며, 모델과 함께 실행되는 컴파일, 로드 및 예측의 몇 가지 반복이 있습니다.

00:04:50.000 --> 00:04:55.000
그것들이 완료되면, 성과 보고서의 지표가 계산된다.

00:04:55.000 --> 00:05:00.000
이제 내 iPhone에서 모델을 실행하고, 성능 보고서를 표시합니다.

00:05:00.000 --> 00:05:09.000
상단에는 테스트가 실행된 장치와 어떤 컴퓨팅 단위가 선택되었는지에 대한 몇 가지 세부 사항을 보여줍니다.

00:05:09.000 --> 00:05:12.000
다음으로 그것은 달리기에 대한 통계를 보여준다.

00:05:12.000 --> 00:05:20.000
중간 예측 시간은 22.19밀리초였고 중간 로드 시간은 약 400ms였다.

00:05:20.000 --> 00:05:28.000
또한, 장치에서 모델을 컴파일할 계획이라면, 이것은 컴파일 시간이 약 940ms였다는 것을 보여줍니다.

00:05:28.000 --> 00:05:39.000
약 22ms의 예측 시간은 내가 실시간으로 실행하고 싶다면 이 모델이 초당 약 45프레임을 지원할 수 있다는 것을 말해준다.

00:05:39.000 --> 00:05:45.000
이 모델에는 신경망이 포함되어 있기 때문에, 성능 보고서 하단에 레이어 뷰가 표시됩니다.

00:05:45.000 --> 00:05:53.000
이것은 모든 레이어의 이름과 유형뿐만 아니라 각 레이어가 실행된 계산 단위를 보여줍니다.

00:05:53.000 --> 00:05:59.000
채워진 체크 표시는 레이어가 해당 컴퓨팅 유닛에서 실행되었음을 의미합니다.

00:05:59.000 --> 00:06:06.000
채워지지 않은 체크 표시는 레이어가 해당 컴퓨팅 장치에서 지원된다는 것을 의미하지만, Core ML은 그곳에서 실행하기로 선택하지 않았다.

00:06:06.000 --> 00:06:12.000
그리고 빈 다이아몬드는 그 레이어가 그 컴퓨팅 유닛에서 지원되지 않는다는 것을 의미한다.

00:06:12.000 --> 00:06:19.000
이 경우, 54개의 레이어가 GPU에서 실행되었고, 32개의 레이어가 신경 엔진에서 실행되었다.

00:06:19.000 --> 00:06:29.000
또한 클릭하여 컴퓨팅 단위로 레이어를 필터링할 수 있습니다.

00:06:29.000 --> 00:06:35.000
그것이 Xcode 14를 사용하여 Core ML 모델에 대한 성능 보고서를 생성할 수 있는 방법입니다.

00:06:35.000 --> 00:06:45.000
이것은 iPhone에서 실행하기 위해 표시되었지만, 한 줄의 코드를 작성하지 않고도 여러 운영 체제와 하드웨어 조합에서 테스트할 수 있습니다.

00:06:45.000 --> 00:06:50.000
이제 모델을 선택했으므로, 다음 단계는 이 모델을 앱에 통합하는 것입니다.

00:06:50.000 --> 00:06:59.000
여기에는 모델을 앱과 번들로 묶고 Core ML API를 사용하여 모델을 로드하고 예측하는 것이 포함됩니다.

00:06:59.000 --> 00:07:08.000
이 경우, 저는 Core ML 스타일 전송 모델을 사용하여 라이브 카메라 세션의 프레임에서 스타일 전송을 수행하는 앱을 만들었습니다.

00:07:08.000 --> 00:07:15.000
그것은 제대로 작동하고 있지만, 프레임 속도는 내가 예상했던 것보다 느리고, 그 이유를 이해하고 싶다.

00:07:15.000 --> 00:07:20.000
이것은 당신이 Core ML 사용을 최적화하는 3단계로 넘어갈 곳입니다.

00:07:20.000 --> 00:07:32.000
성능 보고서를 생성하면 모델이 독립형 환경에서 달성할 수 있는 성능을 보여줄 수 있지만, 앱에서 실시간으로 실행되는 모델의 성능을 프로파일링하는 방법도 필요합니다.

00:07:32.000 --> 00:07:38.000
이를 위해, 이제 Xcode 14의 Instruments 앱에 있는 Core ML Instrument를 사용할 수 있습니다.

00:07:38.000 --> 00:07:46.000
이 도구를 사용하면 앱에서 실시간으로 실행될 때 모델의 성능을 시각화할 수 있으며, 잠재적인 성능 문제를 식별하는 데 도움이 됩니다.

00:07:46.000 --> 00:07:50.000
그게 어떻게 사용될 수 있는지 보여줄게.

00:07:50.000 --> 00:07:56.000
그래서 저는 스타일 전송 앱 작업 공간이 열려 있는 Xcode에 있으며, 앱을 프로파일링할 준비가 되었습니다.

00:07:56.000 --> 00:08:02.000
실행 버튼을 강제로 클릭하고 프로필을 선택하겠습니다.

00:08:02.000 --> 00:08:10.000
이것은 내 장치에 최신 버전의 코드를 설치하고 대상 장치와 앱을 선택한 상태에서 Instruments를 엽니다.

00:08:10.000 --> 00:08:17.000
Core ML 사용을 프로파일링하고 싶기 때문에, Core ML 템플릿을 선택할 것입니다.

00:08:17.000 --> 00:08:24.000
이 템플릿에는 Core ML 악기뿐만 아니라 Core ML 사용을 프로파일링하는 데 도움이 되는 몇 가지 다른 유용한 악기가 포함되어 있습니다.

00:08:24.000 --> 00:08:32.000
흔적을 포착하기 위해, 나는 단순히 기록을 누를 것이다.

00:08:32.000 --> 00:08:35.000
그 앱은 지금 내 아이폰에서 실행되고 있어.

00:08:35.000 --> 00:08:39.000
나는 그것을 몇 초 동안 실행하고 몇 가지 다른 스타일을 사용할 것이다.

00:08:39.000 --> 00:08:42.000
그리고 이제 나는 정지 버튼을 눌러 흔적을 끝낼 것이다.

00:08:42.000 --> 00:08:44.000
이제 나는 내 악기 흔적을 가지고 있다.

00:08:44.000 --> 00:08:48.000
나는 코어 ML 악기에 집중할 것이다.

00:08:48.000 --> 00:08:53.000
코어 ML 악기는 추적에서 캡처된 모든 코어 ML 이벤트를 보여줍니다.

00:08:53.000 --> 00:09:01.000
초기 보기는 모든 이벤트를 세 개의 차선으로 그룹화합니다: 활동, 데이터 및 계산.

00:09:01.000 --> 00:09:14.000
활동 레인은 로드 및 예측과 같이 직접 호출할 실제 Core ML API와 일대일 관계를 맺고 있는 최상위 Core ML 이벤트를 보여줍니다.

00:09:14.000 --> 00:09:25.000
데이터 레인은 Core ML이 모델의 입력 및 출력으로 안전하게 작동할 수 있도록 데이터 검사 또는 데이터 변환을 수행하는 이벤트를 보여줍니다.

00:09:25.000 --> 00:09:33.000
컴퓨팅 레인은 코어 ML이 신경 엔진이나 GPU와 같은 특정 컴퓨팅 장치에 컴퓨팅 요청을 보낼 때를 보여줍니다.

00:09:33.000 --> 00:09:42.000
각 이벤트 유형에 대한 개별 차선이 있는 그룹화되지 않은 보기를 선택할 수도 있습니다.

00:09:42.000 --> 00:09:46.000
하단에는 모델 활동 집계 보기가 있습니다.

00:09:46.000 --> 00:09:51.000
이 보기는 추적에 표시된 모든 이벤트에 대한 집계 통계를 제공합니다.

00:09:51.000 --> 00:10:02.000
예를 들어, 이 추적에서, 평균 모델 부하에는 17.17ms가 걸렸고, 평균 예측에는 7.2ms가 걸렸다.

00:10:02.000 --> 00:10:05.000
또 다른 점은 기간별로 이벤트를 정렬할 수 있다는 것이다.

00:10:05.000 --> 00:10:19.000
여기서, 그 목록은 단지 2.69초의 예측에 비해 총 6.41초의 부하로 실제로 예측하는 것보다 모델을 로딩하는 데 더 많은 시간이 소비되고 있다는 것을 말해주고 있다.

00:10:19.000 --> 00:10:22.000
아마도 이것은 낮은 프레임 속도와 관련이 있을 것이다.

00:10:22.000 --> 00:10:28.000
이 모든 짐들이 어디서 오는지 찾아볼게.

00:10:28.000 --> 00:10:33.000
각 예측을 호출하기 전에 Core ML 모델을 다시 로드하고 있다는 것을 알아차리고 있습니다.

00:10:33.000 --> 00:10:38.000
모델을 한 번 로드하고 메모리에 보관할 수 있기 때문에 이것은 일반적으로 좋은 관행이 아니다.

00:10:38.000 --> 00:10:47.000
나는 내 코드로 돌아가서 이것을 고치려고 노력할 것이다.

00:10:47.000 --> 00:10:50.000
나는 내 모델을 로드하는 코드 영역을 찾았다.

00:10:50.000 --> 00:11:01.000
여기서 문제는 이것이 제대로 계산되었다는 것입니다. 즉, styleTransferModel 변수를 참조할 때마다 속성을 다시 계산할 것이며, 이는 이 경우 모델을 다시 로드하는 것을 의미합니다.

00:11:01.000 --> 00:11:14.000
나는 이것을 게으른 변수로 바꿔서 이것을 빠르게 고칠 수 있다.

00:11:14.000 --> 00:11:27.000
이제 반복되는 로드 문제가 해결되었는지 확인하기 위해 앱을 다시 프로파일할 것입니다.

00:11:27.000 --> 00:11:34.000
다시 한 번 Core ML 템플릿을 선택하고 추적을 캡처하겠습니다.

00:11:34.000 --> 00:11:36.000
이것은 내가 기대했던 것과 훨씬 더 일치한다.

00:11:36.000 --> 00:11:49.000
카운트 열은 내가 앱에서 사용한 스타일 수와 일치하는 총 5개의 로드 이벤트가 있으며, 총 로드 기간은 총 예측 기간보다 훨씬 작다는 것을 알려줍니다.

00:11:49.000 --> 00:11:54.000
또한, 내가 스크롤할 때...

00:11:54.000 --> 00:12:02.000
...그것은 각각 사이에 부하 없이 반복되는 예측 이벤트를 정확하게 보여준다.

00:12:02.000 --> 00:12:07.000
또 다른 메모는 지금까지 모든 핵심 ML 모델 활동을 보여주는 뷰만 보았다는 것입니다.

00:12:07.000 --> 00:12:14.000
이 앱에는 스타일당 하나의 코어 ML 모델이 있으므로, 모델별로 코어 ML 활동을 분류하고 싶을 수도 있습니다.

00:12:14.000 --> 00:12:17.000
그 도구는 이것을 쉽게 할 수 있게 해준다.

00:12:17.000 --> 00:12:27.000
메인 그래프에서 왼쪽 상단의 화살표를 클릭하면 추적에 사용되는 각 모델에 대해 하나의 하위 트랙을 만들 수 있습니다.

00:12:27.000 --> 00:12:32.000
여기에 사용된 모든 다른 스타일 전송 모델을 표시합니다.

00:12:32.000 --> 00:12:43.000
집계 보기는 또한 모델별로 통계를 분류할 수 있도록 함으로써 유사한 기능을 제공합니다.

00:12:43.000 --> 00:12:50.000
다음으로 저는 그것이 어떻게 실행되고 있는지에 대한 더 나은 아이디어를 얻기 위해 제 모델 중 하나에 대한 예측에 뛰어들고 싶습니다.

00:12:50.000 --> 00:12:54.000
나는 수채화 모델을 더 자세히 살펴볼 것이다.

00:12:54.000 --> 00:13:03.000
이 예측에서, 컴퓨팅 레인은 내 모델이 신경 엔진과 GPU의 조합으로 실행되었다고 말하고 있다.

00:13:03.000 --> 00:13:16.000
Core ML은 이러한 컴퓨팅 요청을 비동기적으로 보내고 있으므로, 이러한 컴퓨팅 장치가 언제 모델을 적극적으로 실행하고 있는지 알고 싶다면, Core ML Instrument를 GPU Instrument 및 새로운 Neural Engine Instrument와 결합할 수 있습니다.

00:13:16.000 --> 00:13:23.000
이것을 하기 위해, 나는 여기에 고정된 세 개의 악기를 가지고 있다.

00:13:23.000 --> 00:13:33.000
코어 ML 악기는 모델이 실행된 전체 지역을 보여준다.

00:13:33.000 --> 00:13:47.000
그리고 이 영역 내에서, 신경 엔진 기기는 먼저 신경 엔진에서 실행되는 컴퓨팅을 보여주고, GPU 기기는 GPU에서 실행을 완료하기 위해 모델이 신경 엔진에서 전달되었음을 보여줍니다.

00:13:47.000 --> 00:13:54.000
이것은 내 모델이 실제로 하드웨어에서 어떻게 실행되고 있는지에 대한 더 나은 아이디어를 준다.

00:13:54.000 --> 00:14:02.000
요약하자면, 저는 Xcode 14의 Core ML Instrument를 사용하여 앱에서 라이브로 실행할 때 모델의 성능에 대해 배웠습니다.

00:14:02.000 --> 00:14:07.000
그리고 나서 나는 내 모델을 너무 자주 다시 로드하는 문제를 확인했다.

00:14:07.000 --> 00:14:14.000
나는 내 코드에서 문제를 수정하고, 애플리케이션을 다시 프로파일하고, 문제가 해결되었음을 확인했다.

00:14:14.000 --> 00:14:24.000
나는 또한 Core ML, GPU 및 새로운 Neural Engine Instrument를 결합하여 내 모델이 실제로 다른 컴퓨팅 장치에서 어떻게 실행되었는지에 대한 자세한 내용을 얻을 수 있었다.

00:14:24.000 --> 00:14:29.000
그것은 당신이 성과를 이해하는 데 도움이 되는 새로운 도구에 대한 개요였습니다.

00:14:29.000 --> 00:14:34.000
다음으로, 그 성능을 최적화하는 데 도움이 될 수 있는 몇 가지 향상된 API를 살펴볼 것입니다.

00:14:34.000 --> 00:14:39.000
Core ML이 모델 입력과 출력을 어떻게 처리하는지 살펴봅시다.

00:14:39.000 --> 00:14:47.000
코어 ML 모델을 만들 때, 그 모델에는 각각 유형과 크기가 있는 입력 및 출력 기능 세트가 있습니다.

00:14:47.000 --> 00:14:55.000
런타임에 Core ML API를 사용하여 모델의 인터페이스를 준수하는 입력을 제공하고 추론을 실행한 후 출력을 얻을 수 있습니다.

00:14:55.000 --> 00:15:00.000
이미지와 멀티어레이에 좀 더 자세히 집중하겠습니다.

00:15:00.000 --> 00:15:08.000
이미지의 경우, Core ML은 구성 요소당 8비트의 8비트 그레이스케일 및 32비트 컬러 이미지를 지원합니다.

00:15:08.000 --> 00:15:16.000
그리고 다차원 배열의 경우, Core ML은 Int32, Double 및 Float32를 스칼라 유형으로 지원합니다.

00:15:16.000 --> 00:15:21.000
앱이 이미 이러한 유형으로 작동하고 있다면, 단순히 모델에 연결하는 문제입니다.

00:15:21.000 --> 00:15:24.000
하지만, 때때로 당신의 유형은 다를 수 있습니다.

00:15:24.000 --> 00:15:26.000
예를 하나 보여드릴게요.

00:15:26.000 --> 00:15:30.000
이미지 처리 및 스타일 앱에 새 필터를 추가하고 싶습니다.

00:15:30.000 --> 00:15:35.000
이 필터는 단일 채널 이미지에서 작동하여 이미지를 선명하게 합니다.

00:15:35.000 --> 00:15:43.000
내 앱에는 GPU에서 사전 및 사후 처리 작업이 있으며 이 단일 채널을 Float16 정밀도로 나타냅니다.

00:15:43.000 --> 00:15:51.000
이를 위해, 저는 여기에 표시된 것처럼 이미지 샤프닝 토치 모델을 Core ML 형식으로 변환하기 위해 coremltools를 사용했습니다.

00:15:51.000 --> 00:15:54.000
그 모델은 Float16 정밀 계산을 사용하도록 설정되었다.

00:15:54.000 --> 00:15:59.000
또한, 그것은 이미지 입력을 받고 이미지 출력을 생성합니다.

00:15:59.000 --> 00:16:02.000
나는 이렇게 생긴 모델을 가지고 있어.

00:16:02.000 --> 00:16:07.000
Core ML의 경우 8비트인 그레이스케일 이미지를 찍는다는 점에 유의하십시오.

00:16:07.000 --> 00:16:19.000
이 작업을 하기 위해, 나는 내 입력을 OneComponent16Half에서 OneComponent8로 다운캐스트한 다음 OneComponent8에서 OneComponent16Half로 출력을 업캐스트하는 코드를 작성해야 했다.

00:16:19.000 --> 00:16:22.000
하지만, 이것이 전체 이야기가 아니다.

00:16:22.000 --> 00:16:33.000
모델이 Float16 정밀도로 계산을 수행하도록 설정되었기 때문에, 어느 시점에서 Core ML은 이러한 8비트 입력을 Float16으로 변환해야 합니다.

00:16:33.000 --> 00:16:39.000
그것은 변환을 효율적으로 수행하지만, 앱이 실행 중인 Instruments 추적을 볼 때, 이것을 보여준다.

00:16:39.000 --> 00:16:47.000
코어 ML이 신경 엔진 계산 전후에 수행하는 데이터 단계에 주목하세요.

00:16:47.000 --> 00:16:57.000
데이터 레인을 확대할 때, Core ML이 신경 엔진에서 계산을 준비하기 위해 데이터를 복사하고 있음을 보여 주며, 이는 이 경우 Float16으로 변환하는 것을 의미합니다.

00:16:57.000 --> 00:17:02.000
원본 데이터가 이미 Float16이었기 때문에 이것은 불행한 것 같다.

00:17:02.000 --> 00:17:12.000
이상적으로, 이러한 데이터 변환은 Float16 입력 및 출력과 함께 모델이 직접 작동하도록 함으로써 인앱과 코어 ML 내부 모두에서 피할 수 있습니다.

00:17:12.000 --> 00:17:24.000
iOS 16과 macOS Ventura부터 Core ML은 OneComponent16Half 그레이스케일 이미지와 Float16 MultiArrays를 기본적으로 지원합니다.

00:17:24.000 --> 00:17:36.000
Coremltools 변환 방법을 호출하면서 이미지의 새로운 색상 레이아웃이나 MultiArrays의 새로운 데이터 유형을 지정하여 Float16 입력 및 출력을 허용하는 모델을 만들 수 있습니다.

00:17:36.000 --> 00:17:43.000
이 경우, 저는 제 모델의 입력과 출력을 그레이스케일 Float16 이미지로 지정하고 있습니다.

00:17:43.000 --> 00:17:56.000
Float16 지원은 iOS 16과 macOS Ventura에서 사용할 수 있기 때문에, 이러한 기능은 최소 배포 대상이 iOS 16으로 지정된 경우에만 사용할 수 있습니다.

00:17:56.000 --> 00:17:59.000
이것이 모델의 개조된 버전이 보이는 방식이다.

00:17:59.000 --> 00:18:04.000
입력과 출력은 Grayscale16Half로 표시되어 있습니다.

00:18:04.000 --> 00:18:16.000
이 Float16 지원으로, 내 앱은 Float16 이미지를 Core ML에 직접 공급할 수 있어, 입력을 다운캐스팅하고 앱의 출력을 업캐스팅할 필요가 없습니다.

00:18:16.000 --> 00:18:18.000
이것이 코드에서 보이는 방식이다.

00:18:18.000 --> 00:18:27.000
OneComponent16Half CVPixelBuffer의 형태로 입력 데이터를 가지고 있기 때문에, 픽셀 버퍼를 Core ML로 직접 보낼 수 있습니다.

00:18:27.000 --> 00:18:31.000
이것은 어떠한 데이터 복사나 변환도 초래하지 않는다.

00:18:31.000 --> 00:18:36.000
그런 다음 출력으로 OneComponent16Half CVPixelBuffer를 얻는다.

00:18:36.000 --> 00:18:42.000
이로 인해 코드가 더 간단해지며, 데이터 변환이 필요하지 않습니다.

00:18:42.000 --> 00:18:53.000
당신이 할 수 있는 또 다른 멋진 일이 있는데, 그것은 Core ML이 각 예측에 새로운 버퍼를 할당하는 대신 출력에 대해 미리 할당된 버퍼를 채우도록 Core ML에 요청하는 것입니다.

00:18:53.000 --> 00:18:59.000
출력 백업 버퍼를 할당하고 예측 옵션에 설정하여 이를 수행할 수 있습니다.

00:18:59.000 --> 00:19:06.000
내 앱을 위해, 나는 OneComponent1 HalfCVPixelBuffer를 반환하는 outputBackingBuffer라는 함수를 작성했다.

00:19:06.000 --> 00:19:14.000
그런 다음 나는 이것을 예측 옵션에 설정하고, 마침내 그 예측 옵션으로 내 모델의 예측 방법을 호출한다.

00:19:14.000 --> 00:19:21.000
출력 백킹을 지정하면 모델 출력에 대한 버퍼 관리를 더 잘 제어할 수 있습니다.

00:19:21.000 --> 00:19:31.000
그래서 이러한 변경 사항과 함께, 요약하자면, 8비트 입력과 출력이 있는 모델의 원래 버전을 사용할 때 Instruments 추적에 표시된 것은 다음과 같습니다.

00:19:31.000 --> 00:19:42.000
그리고 새로운 Float16 버전의 모델에 IOSurface 백업 Float16 버퍼를 제공하기 위해 코드를 수정한 후 최종 인스트루먼트 추적이 어떻게 보이는지 다음과 같습니다.

00:19:42.000 --> 00:19:49.000
Core ML이 더 이상 수행할 필요가 없기 때문에 이전에 데이터 레인에 표시된 데이터 변환은 이제 사라졌습니다.

00:19:49.000 --> 00:19:55.000
요약하자면, Core ML은 이제 Float16 데이터에 대한 엔드 투 엔드 네이티브 지원을 제공합니다.

00:19:55.000 --> 00:20:03.000
즉, Core ML에 Float16 입력을 제공하고 Core ML이 Float16 출력을 다시 제공하도록 할 수 있습니다.

00:20:03.000 --> 00:20:11.000
또한 새로운 출력 백업 API를 사용하여 Core ML이 새 출력 버퍼를 만드는 대신 미리 할당된 출력 버퍼를 채우도록 할 수 있습니다.

00:20:11.000 --> 00:20:25.000
그리고 마지막으로, Core ML이 통합 메모리를 활용하여 데이터 복사본 없이 다른 컴퓨팅 단위 간에 데이터를 전송할 수 있기 때문에 가능할 때마다 IOSurface 백업 버퍼를 사용하는 것이 좋습니다.

00:20:25.000 --> 00:20:31.000
다음으로, Core ML에 추가되는 몇 가지 추가 기능에 대한 빠른 둘러보기를 살펴보겠습니다.

00:20:31.000 --> 00:20:33.000
첫 번째는 무게 압축이다.

00:20:33.000 --> 00:20:39.000
모델의 가중치를 압축하면 더 작은 모델을 가지고 있는 동안 비슷한 정확도를 얻을 수 있습니다.

00:20:39.000 --> 00:20:48.000
iOS 12에서 Core ML은 Core ML 신경망 모델의 크기를 줄일 수 있는 훈련 후 무게 압축을 도입했습니다.

00:20:48.000 --> 00:20:59.000
우리는 이제 ML 프로그램 모델 유형으로 16비트 및 8비트 지원을 확장하고 있으며, 또한 희소 표현으로 가중치를 저장하는 새로운 옵션을 도입하고 있습니다.

00:20:59.000 --> 00:21:09.000
Coremltools 유틸리티를 사용하면 이제 ML 프로그램 모델의 가중치를 정량화, palettize 및 sparsify할 수 있습니다.

00:21:09.000 --> 00:21:12.000
다음은 새로운 컴퓨팅 유닛 옵션입니다.

00:21:12.000 --> 00:21:18.000
Core ML은 항상 주어진 컴퓨팅 단위 선호도에 대한 추론 대기 시간을 최소화하는 것을 목표로 한다.

00:21:18.000 --> 00:21:24.000
앱은 MLModelConfiguration computeUnits 속성을 설정하여 이 기본 설정을 지정할 수 있습니다.

00:21:24.000 --> 00:21:31.000
기존의 세 가지 컴퓨팅 유닛 옵션 외에도, 이제 cpuAndNeuralEngine이라는 새로운 옵션이 있습니다.

00:21:31.000 --> 00:21:44.000
이것은 Core ML에게 GPU에서 계산을 발송하지 말라고 말하며, 이는 앱이 다른 계산을 위해 GPU를 사용할 때 도움이 될 수 있으며, 따라서 Core ML이 CPU와 신경 엔진에 초점을 제한하는 것을 선호합니다.

00:21:44.000 --> 00:21:54.000
다음으로, 모델 직렬화 측면에서 추가적인 유연성을 제공하는 Core ML 모델 인스턴스를 초기화하는 새로운 방법을 추가하고 있습니다.

00:21:54.000 --> 00:22:01.000
이를 통해 사용자 지정 암호화 체계로 모델 데이터를 암호화하고 로드하기 직전에 해독할 수 있습니다.

00:22:01.000 --> 00:22:11.000
이러한 새로운 API를 사용하면 컴파일된 모델이 디스크에 있을 필요 없이 인메모리 코어 ML 모델 사양을 컴파일하고 로드할 수 있습니다.

00:22:11.000 --> 00:22:16.000
마지막 업데이트는 Swift 패키지와 Core ML에서 어떻게 작동하는지에 관한 것입니다.

00:22:16.000 --> 00:22:20.000
패키지는 재사용 가능한 코드를 묶고 배포하는 좋은 방법입니다.

00:22:20.000 --> 00:22:28.000
Xcode 14를 사용하면 Swift 패키지에 Core ML 모델을 포함할 수 있으며, 이제 누군가가 패키지를 가져오면 모델이 작동합니다.

00:22:28.000 --> 00:22:36.000
Xcode는 Core ML 모델을 자동으로 컴파일하고 번들로 묶고 작업하는 데 익숙한 것과 동일한 코드 생성 인터페이스를 만듭니다.

00:22:36.000 --> 00:22:43.000
우리는 스위프트 생태계에서 모델을 배포하는 것을 훨씬 쉽게 만들 수 있기 때문에 이 변화에 대해 흥분하고 있습니다.

00:22:43.000 --> 00:22:46.000
그것은 우리를 이 세션의 끝으로 이끈다.

00:22:46.000 --> 00:22:56.000
Xcode 14의 Core ML 성능 보고서와 Instrument는 앱에서 ML 기반 기능의 성능을 분석하고 최적화하는 데 도움을 줍니다.

00:22:56.000 --> 00:23:03.000
새로운 Float16 지원 및 출력 백업 API를 사용하면 Core ML 안팎으로 데이터가 어떻게 흐르는지 더 잘 제어할 수 있습니다.

00:23:03.000 --> 00:23:09.000
무게 압축에 대한 확장된 지원은 모델의 크기를 최소화하는 데 도움이 될 수 있습니다.

00:23:09.000 --> 00:23:18.000
그리고 인메모리 모델과 스위프트 패키지 지원을 통해 코어 ML 모델을 표현, 통합 및 공유하는 방법과 관련하여 더 많은 옵션이 있습니다.

00:23:18.000 --> 00:23:22.000
이것은 코어 ML 팀의 벤이었고, 남은 WWDC를 잘 보내고 있다.

00:23:22.000 --> 23:59:59.000
♪

