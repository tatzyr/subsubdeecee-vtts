WEBVTT

00:00:00.000 --> 00:00:05.000
♪ベース音楽の演奏♪

00:00:05.000 --> 00:00:09.000
♪

00:00:09.000 --> 00:00:12.000
セルゲイ・カメンスキー：皆さん、こんにちは、WWDCへようこそ。

00:00:12.000 --> 00:00:15.000
私の名前はセルゲイ・カメンスキーで、ビジョンフレームワークチームのソフトウェアエンジニアです。

00:00:15.000 --> 00:00:20.000
今日のセッションのトピックは、ビジョンフレームワークが人々の分析にどのように役立つかを示すことです。

00:00:20.000 --> 00:00:23.000
今日の議題は2つの主要な項目で構成されています。

00:00:23.000 --> 00:00:27.000
まず、ビジョンフレームワークの人材分析技術の概要を説明します。

00:00:27.000 --> 00:00:31.000
その間、私たちは特に新しい追加に焦点を当てます。

00:00:31.000 --> 00:00:37.000
第二に、新しい人物セグメンテーション機能の詳細なレビューを行います。

00:00:37.000 --> 00:00:40.000
まず、人材分析技術から始めましょう。

00:00:40.000 --> 00:00:44.000
ビジョンにおける人物分析の礎石は、人物の顔分析です。

00:00:44.000 --> 00:00:49.000
ビジョンフレームワークの開始以来、私たちは人間の顔分析機能を追加し、強化してきました。

00:00:49.000 --> 00:00:56.000
現在、顔検出、顔ランドマーク検出、顔キャプチャ品質検出を提供しています。

00:00:56.000 --> 00:01:02.000
ビジョンフレームワークの顔検出機能は、DetectFaceRectanglesRequestによって公開されます。

00:01:02.000 --> 00:01:11.000
私たちの顔検出器は、高精度とリコールメトリックを提供し、任意の向き、異なるサイズ、そして部分的に閉塞された顔を見つけることができます。

00:01:11.000 --> 00:01:15.000
これまでのところ、私たちはメガネや帽子のような閉塞をサポートしてきました。

00:01:15.000 --> 00:01:26.000
現在、顔検出器をリビジョン3号にアップグレードしており、既存のすべての優れた品質を向上させることに加えて、マスクで覆われた顔も検出できるようになりました。

00:01:26.000 --> 00:01:33.000
私たちの顔検出器の主な機能は、もちろん顔のバウンディングボックスを見つけることですが、顔のポーズメトリックを検出することもできます。

00:01:33.000 --> 00:01:36.000
以前は、ロールとヨーメトリックのみを提供していました。

00:01:36.000 --> 00:01:41.000
ほとんどのメトリックはラジアンで報告され、その値は離散ビンで返されます。

00:01:41.000 --> 00:01:47.000
新しいリビジョンの導入により、ピッチメトリックも追加し、全体像を完成させています。

00:01:47.000 --> 00:01:49.000
しかし、私たちはそこで止まりませんでした。

00:01:49.000 --> 00:01:54.000
また、3つのメトリクスすべてを連続空間で報告しています。

00:01:54.000 --> 00:02:03.000
すべてのフェイスポーズメトリクスは、DetectFaceRectanglesRequestを実行した結果であるFaceObservationオブジェクトのプロパティとして返されます。

00:02:03.000 --> 00:02:08.000
顔のポーズ検出機能を示すように設計されたデモアプリを見てみましょう。

00:02:08.000 --> 00:02:16.000
このアプリは、ビジョンフェイスディテクタを実行してカメラフィードを処理し、結果をラジアンから度に変換した後、フェイスポーズメトリックをユーザーに提示します。

00:02:16.000 --> 00:02:28.000
メトリクスの変化をよりよく追跡するために、アプリは赤い色のグラデーションを使用して、顔のポーズメトリクスが正の方向に増加したときに表示し、青色の色のグラデーションを使用して、メトリクスが負の方向に増加したときに表示します。

00:02:28.000 --> 00:02:33.000
どちらの場合も、色が明るいほど、メトリックはゼロの位置に近くなります。

00:02:33.000 --> 00:02:41.000
各メトリックのゼロ位置は、人がまっすぐ見ているときに、人間の頭の中立的な位置と呼ぶものです。

00:02:41.000 --> 00:02:46.000
すでに説明したように、ロール、ヨー、ピッチの3つのフェイスポーズメトリックがあります。

00:02:46.000 --> 00:02:53.000
これらの用語は飛行力学に由来し、航空機の重心に関する航空機の原理軸を記述します。

00:02:53.000 --> 00:02:57.000
人間の頭のポーズを記述するためにも同じ用語が採用されています。

00:02:57.000 --> 00:03:03.000
頭のポーズ、または顔のポーズと呼ばれるものに適用すると、人間の頭の動きを次のように追跡します。

00:03:03.000 --> 00:03:09.000
ロールは、この方向に頭の動きを追跡しています。

00:03:09.000 --> 00:03:25.000
ロールの最も否定的な値から最も肯定的な値に行くと、背景色が濃い青から水色、ニュートラル、そして明るい赤、そして最後に濃い赤に変わることがわかります。

00:03:25.000 --> 00:03:33.000
同様の色の変化は、頭が右または左に曲がっているときに角度を追跡するヨーメトリックで起こっています。

00:03:33.000 --> 00:03:40.000
そして最後に、ピッチメトリックは、私の頭が上下にうなずいているときに私の頭の動きを追跡しています。

00:03:40.000 --> 00:03:50.000
ここでは、私がスペクトルの最も否定的な端から最も肯定的な端に行くとき、同様の色の変化を再び見ることができます。

00:03:50.000 --> 00:03:55.000
顔のランドマーク検出は、私たちの顔分析スイートのもう一つの重要な機能です。

00:03:55.000 --> 00:04:02.000
顔のランドマークの検出は、DetectFaceLandmarksRequestによって提供され、最新のリビジョンはリビジョン番号3です。

00:04:02.000 --> 00:04:12.000
この改訂は、主要な顔領域をよりよく表し、正確な瞳孔検出を提供するために76点の星座を提供します。

00:04:12.000 --> 00:04:16.000
顔分析スイートには、顔キャプチャ品質検出も含まれています。

00:04:16.000 --> 00:04:25.000
この全体的な測定は、人間の顔の表情、照明、閉塞、ぼかし、フォーカスなどの属性を考慮に入れます。

00:04:25.000 --> 00:04:33.000
これはDetectFaceCaptureQualityRequest APIを介して公開され、このリクエストの最新のリビジョンはリビジョン番号2です。

00:04:33.000 --> 00:04:38.000
顔のキャプチャ品質は、同じ主題の比較尺度であることを覚えておくことが重要です。

00:04:38.000 --> 00:04:46.000
この機能は、例えば、フォトバーストシリーズから最高の写真を選んだり、フォトライブラリの人を表すのに最適な写真を選んだりするのに最適です。

00:04:46.000 --> 00:04:52.000
この機能は、異なる人々の顔を比較するようには設計されていません。

00:04:52.000 --> 00:04:57.000
人体分析は、ビジョンフレームワークによって提供される人材分析技術のもう一つの大きなセクションです。

00:04:57.000 --> 00:05:07.000
ビジョンは、人体検出、人間のポーズ検出、そして最後に、人間の手のポーズ検出を含む、この分野でいくつかの機能を提供します。

00:05:07.000 --> 00:05:10.000
まず、人体の検出を見てみましょう。

00:05:10.000 --> 00:05:17.000
この機能はDetectHumanRectanglesRequestを介して提供され、現在は人間の上半身のみを検出します。

00:05:17.000 --> 00:05:25.000
このリクエストに新しい機能を追加しているため、このリビジョンをリビジョン番号2にアップグレードします。

00:05:25.000 --> 00:05:31.000
新しい改訂では、上半身検出に加えて、全身検出も提供します。

00:05:31.000 --> 00:05:39.000
上半身と全身検出の選択は、DetectHumanRectanglesRequestの新しいupperBodyOnlyプロパティを介して制御されます。

00:05:39.000 --> 00:05:45.000
このプロパティのデフォルト値は、下位互換性を維持するためにtrueに設定されています。

00:05:45.000 --> 00:05:50.000
人体のポーズ検出は、DetectHumanBodyPoseRequestを介してビジョンフレームワークで提供されています。

00:05:50.000 --> 00:05:55.000
このリクエストを処理すると、人体の関節の場所のコレクションが提供されます。

00:05:55.000 --> 00:06:02.000
改訂番号1は、このリクエストの最新かつ唯一の利用可能な改訂版です。

00:06:02.000 --> 00:06:07.000
ビジョンフレームワークは、DetectHumanHandPoseRequestとして人間の手のポーズ検出も提供します。

00:06:07.000 --> 00:06:16.000
人体のポーズ検出と同様に、手のポーズ要求の処理は、人間の手の関節の位置のコレクションを返します。

00:06:16.000 --> 00:06:23.000
結果の観察に重要なプロパティであるハンドキラリティを追加することで、このリクエストの機能をアップグレードしています。

00:06:23.000 --> 00:06:30.000
HumanHandPoseObservationの新しいキラリティプロパティには、検出された手が左か右かの情報が含まれます。

00:06:30.000 --> 00:06:41.000
ハンドポーズ検出の詳細を知りたい場合は、「Create MLでハンドポーズとアクションを分類する」セッションを見ることをお勧めします。

00:06:41.000 --> 00:06:46.000
これで、人材分析技術スイートの新しいアップグレードの概要が終わります。

00:06:46.000 --> 00:06:53.000
今、私たちのセッションの2番目のトピック、つまり人のセグメンテーションに移る時が来ました。

00:06:53.000 --> 00:06:55.000
人のセグメンテーションとは何ですか?

00:06:55.000 --> 00:07:00.000
非常に簡単に言えば、それは人々を現場から切り離す能力です。

00:07:00.000 --> 00:07:04.000
今日、人のセグメンテーション技術には多くのアプリケーションがあります。

00:07:04.000 --> 00:07:09.000
たとえば、ビデオ会議アプリの仮想バックグラウンド機能に精通しています。

00:07:09.000 --> 00:07:13.000
また、ライブスポーツ分析、自動運転、その他多くの場所で使用されています。

00:07:13.000 --> 00:07:19.000
人物セグメンテーションは、私たちの有名なポートレートモードにも力を与えます。

00:07:19.000 --> 00:07:23.000
ビジョンフレームワークの個人セグメンテーションは、単一のフレームで動作するように設計された機能です。

00:07:23.000 --> 00:07:28.000
ストリーミングパイプラインで使用でき、オフライン処理にも適しています。

00:07:28.000 --> 00:07:38.000
この機能は、macOS、iOS、iPadOS、tvOSなどの複数のプラットフォームでサポートされています。

00:07:38.000 --> 00:07:48.000
ビジョンフレームワークは、セマンティックパーソンセグメンテーションを実装しています。これは、フレーム内のすべての人に単一のマスクを返すことを意味します。

00:07:48.000 --> 00:07:56.000
人セグメンテーションのためのビジョンAPIは、GeneratePersonSegmentationRequestによって実装されています。これはステートフルなリクエストです。

00:07:56.000 --> 00:08:03.000
ビジョンフレームワークの従来のリクエストとは対照的に、ステートフルリクエストオブジェクトはフレームのシーケンス全体で再利用されます。

00:08:03.000 --> 00:08:10.000
私たちの特定のケースでは、リクエストオブジェクトを使用すると、高速品質レベルモデルのフレーム間の時間的変化を滑らかにするのに役立ちます。

00:08:10.000 --> 00:08:15.000
ビジョンフレームワークが提供するPerson Segmentation APIを見てみましょう。

00:08:15.000 --> 00:08:18.000
このAPIは、すでに身近で確立されたパターンに従います。

00:08:18.000 --> 00:08:27.000
リクエストを作成し、リクエストハンドラーを作成し、リクエストハンドラーでリクエストを処理し、最後に結果を確認します。

00:08:27.000 --> 00:08:37.000
GeneratePersonSegmentationRequestオブジェクトのデフォルトの初期化は、リビジョン、qualityLevel、およびoutputPixelFormatプロパティをデフォルト値に設定するのと同じです。

00:08:37.000 --> 00:08:41.000
すべてのプロパティを1つずつ見直しましょう。

00:08:41.000 --> 00:08:43.000
まず、リビジョンプロパティです。

00:08:43.000 --> 00:08:46.000
ここでは、リビジョンをリビジョン番号1に設定します。

00:08:46.000 --> 00:08:51.000
新しいリクエストタイプを扱っているため、これはデフォルトであり、利用可能な唯一のリビジョンです。

00:08:51.000 --> 00:08:58.000
今日は技術的には選択の余地はありませんが、将来的に決定論的な行動を保証するように明示的に設定することを常にお勧めします。

00:08:58.000 --> 00:09:06.000
これは、新しいリビジョンが導入された場合、デフォルトも利用可能な最新のリビジョンを表すように変更されるためです。

00:09:06.000 --> 00:09:09.000
2つ目はqualityLevelプロパティです。

00:09:09.000 --> 00:09:16.000
ビジョンAPIは3つの異なるレベルを提供します。正確で、これもデフォルトレベルです。バランスが取れています。そして高速です。

00:09:16.000 --> 00:09:22.000
ユースケースに関しては、計算写真に正確なレベルを使用することをお勧めします。

00:09:22.000 --> 00:09:29.000
これは、可能な限り最高の品質を達成したいユースケースであり、通常は時間に制限されません。

00:09:29.000 --> 00:09:37.000
同様のロジックを使用して、ビデオフレームごとのセグメンテーションにはバランスの取れたレベルが推奨され、ストリーミング処理には高速です。

00:09:37.000 --> 00:09:40.000
3番目のプロパティは、出力マスク形式です。

00:09:40.000 --> 00:09:50.000
結果のマスクを詳細に確認しますが、ここでは、クライアントとして、結果のマスクが返される形式を指定できることに言及したいと思います。

00:09:50.000 --> 00:10:02.000
ここには3つの選択肢があります。典型的な0〜255の量子化範囲を持つ符号なし8ビット整数マスクと、2つの浮動小数点マスク形式です。1つは32ビットの完全精度で、もう1つは16ビットの半精度です。

00:10:02.000 --> 00:10:14.000
16ビットの半精度は、Metalを使用したさらなるGPUベースの処理に直接挿入できる縮小メモリ浮動小数点フォーマットを提供することを目的としています。

00:10:14.000 --> 00:10:19.000
これまでのところ、個人セグメンテーション要求を作成、設定、実行する方法を学びました。

00:10:19.000 --> 00:10:22.000
今、結果を見る時が来ました。

00:10:22.000 --> 00:10:28.000
人のセグメンテーション要求を処理した結果は、PixelBufferObservationオブジェクトの形式になります。

00:10:28.000 --> 00:10:35.000
PixelBufferObservationは観測から派生し、重要なpixelBufferプロパティを追加します。

00:10:35.000 --> 00:10:45.000
このプロパティに格納されている実際のCVPixelBufferオブジェクトは、個人セグメンテーション要求が設定されたのと同じピクセル形式です。

00:10:45.000 --> 00:10:49.000
人セグメンテーション要求の処理は、セグメンテーションマスクを生成します。

00:10:49.000 --> 00:10:57.000
元の画像と、個人セグメンテーション要求の実行によって生成された3つの異なる品質レベルマスクを見てみましょう。

00:10:57.000 --> 00:11:02.000
速く、バランスが取れていて、正確です。

00:11:02.000 --> 00:11:05.000
ズームインして、各マスクの詳細を見てみましょう。

00:11:05.000 --> 00:11:15.000
予想通り、高速からバランスのとれた、そして最終的には正確になると、マスクの品質が向上し、より多くの詳細が見え始めます。

00:11:15.000 --> 00:11:21.000
それでは、品質とパフォーマンスの関数として、さまざまなマスクレベルを調べてみましょう。

00:11:21.000 --> 00:11:30.000
高速からバランスのとれたもの、そして最終的には正確に移行すると、マスクの品質は向上しますが、リソースの使用量も向上します。

00:11:30.000 --> 00:11:36.000
ダイナミックレンジ、マスクの解像度、メモリ消費、処理時間はすべて、マスクの品質が上がると上がります。

00:11:36.000 --> 00:11:46.000
これは、セグメンテーションマスクの品質とマスクの計算に必要なリソース消費の間のトレードオフを表しています。

00:11:46.000 --> 00:11:50.000
だから、あなたはすでにマスクの生成とその特性についてすべてを知っています。

00:11:50.000 --> 00:11:54.000
マスクで実際に何ができますか?

00:11:54.000 --> 00:11:56.000
3つの画像から始めましょう。

00:11:56.000 --> 00:12:03.000
入力画像、入力画像を処理して得られたセグメンテーションマスク、および背景画像。

00:12:03.000 --> 00:12:12.000
私たちがしたいのは、マスク領域の外側の領域の元の画像の背景を別の画像の背景に置き換えることです。

00:12:12.000 --> 00:12:22.000
そのようなブレンド操作を実行すると、元の画像の若者がビーチの遊歩道から森に運ばれます。

00:12:22.000 --> 00:12:26.000
このブレンドシーケンスはコード内でどのように見えますか?

00:12:26.000 --> 00:12:35.000
まず、関連するすべての処理を行い、入力画像、マスク、背景の3つの画像がすでにあると仮定しましょう。

00:12:35.000 --> 00:12:40.000
次に、マスクと背景の両方を元の画像のサイズに拡大縮小する必要があります。

00:12:40.000 --> 00:12:45.000
次に、Core Imageブレンドフィルターを作成して初期化します。

00:12:45.000 --> 00:12:48.000
あなたはおそらく、私が赤いマスクで私のブレンドフィルターを作成したことに気づいたでしょう。

00:12:48.000 --> 00:12:58.000
これは、すべてのマスクがそうであるように、CIImageが1つのコンポーネントPixelBufferで初期化されると、デフォルトで赤いチャンネルを持つオブジェクトが作成されるためです。

00:12:58.000 --> 00:13:04.000
最後に、結果を得るためにブレンド操作を実行します。

00:13:04.000 --> 00:13:09.000
ビジョンフレームワークで個人セグメンテーション機能を使用する方法を見てみましょう。

00:13:09.000 --> 00:13:16.000
ダウンロード可能な2番目のデモアプリは、顔のポーズメトリック検出と新しい人物セグメンテーション機能を兼ね備えています。

00:13:16.000 --> 00:13:20.000
このアプリは、顔検出と人物セグメンテーションを実行してカメラフィードを処理します。

00:13:20.000 --> 00:13:28.000
次に、エンドセグメンテーションマスクを取り、それを使用してマスクピクセルの外側の領域の背景を別の色に置き換えます。

00:13:28.000 --> 00:13:36.000
使用する背景色の決定は、任意の時点でのロール、ヨー、ピッチの値の組み合わせから行われます。

00:13:36.000 --> 00:13:46.000
私は現在、テーブルと椅子のある部屋にいて、デモアプリは、私の頭の位置に対応するカラーミックスである新しい背景の上に私のセグメント化されたシルエットを示しています。

00:13:46.000 --> 00:13:49.000
ロール、ヨー、ピッチの変化を追跡するかどうか見てみましょう。

00:13:49.000 --> 00:13:58.000
私がこのように頭を向けるとき、ロールは背景色ミックスの決定に大きく貢献します。

00:13:58.000 --> 00:14:04.000
頭を左右に回すと、ヨーが主な貢献者になります。

00:14:04.000 --> 00:14:12.000
そして最後に、頭を上下にうなずくと、ピッチが主要な貢献者になります。

00:14:12.000 --> 00:14:15.000
ビジョンフレームワークは、個人セグメンテーションAPIを提供する唯一の場所ではありません。

00:14:15.000 --> 00:14:20.000
同じ技術で駆動される同様の機能を提供する他のいくつかのフレームワークがあります。

00:14:20.000 --> 00:14:23.000
それぞれを簡単に見てみましょう。 それぞれを見てみましょう。

00:14:23.000 --> 00:14:26.000
まずはAVFoundationです。

00:14:26.000 --> 00:14:33.000
AVFoundationは、フォトキャプチャセッション中に、一部の新世代のデバイスで個人セグメンテーションマスクを返すことができます。

00:14:33.000 --> 00:14:39.000
セグメンテーションマスクは、AVCapturePhotoのPortraitEffectsMatteプロパティを介して返されます。

00:14:39.000 --> 00:14:45.000
それを取得するには、まずそれがサポートされているかどうかを確認する必要があります。もしそうなら、それの配信を有効にしてください。

00:14:45.000 --> 00:14:50.000
人物セグメンテーションAPIを提供する2番目のフレームワークはARKitです。

00:14:50.000 --> 00:14:56.000
この機能は、A12 Bionic以降のデバイスでサポートされており、カメラフィードを処理するときに生成されます。

00:14:56.000 --> 00:15:01.000
セグメンテーションマスクは、ARFrameのsegmentationBufferプロパティを介して返されます。

00:15:01.000 --> 00:15:12.000
取得しようとする前に、ARWorldTrackingConfigurationクラスのsupportsFrameSemanticsプロパティを調べて、サポートされているかどうかを確認する必要があります。

00:15:12.000 --> 00:15:14.000
3番目のフレームワークはコアイメージです。

00:15:14.000 --> 00:15:22.000
Core Imageは、Vision personセグメンテーションAPIの上に薄いラッパーを提供しているため、Core Imageドメイン内でユースケース全体を実行できます。

00:15:22.000 --> 00:15:28.000
Core Image APIを使用して個人セグメンテーションを実装する方法を見てみましょう。

00:15:28.000 --> 00:15:32.000
セグメンテーションを実行するために画像をロギングすることから始めます。

00:15:32.000 --> 00:15:43.000
次に、person segmentation CIFilterを作成し、inputImageを割り当て、フィルタを実行してセグメンテーションマスクを取得します。

00:15:43.000 --> 00:15:47.000
人物セグメンテーションAPIとApple SDKの複数のバージョンをレビューしたところです。

00:15:47.000 --> 00:15:51.000
それぞれがどこで使用できるかをまとめてみましょう。

00:15:51.000 --> 00:15:56.000
AVFoundationは、AVCaptureSessionを搭載した一部のiOSデバイスで利用できます。

00:15:56.000 --> 00:16:00.000
キャプチャセッションが実行されている場合は、これが選択できます。

00:16:00.000 --> 00:16:06.000
ARKitアプリを開発している場合は、セグメンテーションマスクを取得できるARセッションがすでにあるはずです。

00:16:06.000 --> 00:16:10.000
この場合、ARKit APIが推奨されます。

00:16:10.000 --> 00:16:16.000
Vision APIは、オンラインとオフラインのシングルフレーム処理のために複数のプラットフォームで利用できます。

00:16:16.000 --> 00:16:24.000
そして最後に、Core ImageはVision APIの薄いラッパーを提供します。これは、Core Imageドメイン内にとどまりたい場合に便利なオプションです。

00:16:24.000 --> 00:16:31.000
他のアルゴリズムと同様に、人のセグメンテーションにはベストプラクティスがあります。言い換えれば、それが最もよく機能する一連の条件です。

00:16:31.000 --> 00:16:37.000
人セグメンテーション機能の使用を計画している場合、これらのルールに従おうとすると、アプリはより良いパフォーマンスを発揮します。

00:16:37.000 --> 00:16:45.000
まず、おそらく自然な閉塞で、すべての人々がほとんど見えるシーンで最大4人をセグメント化しようとする必要があります。

00:16:45.000 --> 00:16:53.000
第二に、各人の高さは画像の高さの少なくとも半分で、理想的には背景と比較して良いコントラストでなければなりません。

00:16:53.000 --> 00:17:02.000
そして第三に、彫像、人の写真、遠くにいる人などの曖昧さを避けることをお勧めします。

00:17:02.000 --> 00:17:03.000
これで私たちのセッションは終わりです。

00:17:03.000 --> 00:17:07.000
今日学んだことを簡単に見てみましょう。そして、私たちが学んだことを簡単に見てみましょう

00:17:07.000 --> 00:17:19.000
まず、マスクされた顔検出、フェイスピッチメトリックの追加、すべてのフェイスポーズメトリックを連続空間で報告するなどのアップグレードに焦点を当てながら、ビジョンフレームワークの人物分析技術の概要を把握しました。

00:17:19.000 --> 00:17:25.000
また、人間の手のポーズ検出に新しい手のキラリティメトリックを導入しました。

00:17:25.000 --> 00:17:30.000
第2部では、Visionフレームワークに追加された新しい人物セグメンテーションAPIを深く掘り下げました。

00:17:30.000 --> 00:17:37.000
また、同様の機能を提供する他のAPIについても検討し、それぞれが使用できるガイダンスを提供しました。

00:17:37.000 --> 00:17:44.000
このセッションを見ることで、アプリを開発するための新しいツールを学び、すぐに試してみることを本当に熱望していることを願っています。

00:17:44.000 --> 00:17:50.000
今日を終える前に、見てくれてありがとう、幸運を祈ります、そしてWWDCの素晴らしい残りをお過ごしください。

00:17:50.000 --> 23:59:59.000
♪

