WEBVTT

00:00:02.000 --> 00:00:10.000
こんにちは。

00:00:10.000 --> 00:00:13.000
ARKitチームのエンジニア、デビッドです。

00:00:13.000 --> 00:00:17.000
今日、クリストファーと私はARKit 5の幅広い改善を共有します。

00:00:17.000 --> 00:00:21.000
iOS 15の変更点について議論できることを嬉しく思います。

00:00:21.000 --> 00:00:27.000
今年は、全面的に多くのアップグレードを行い、複数の機能について話し合います。

00:00:27.000 --> 00:00:32.000
その前に、皆さんがLiDARで築いてきた経験を紹介したいと思います。

00:00:32.000 --> 00:00:43.000
生産性、フォトフィルターエフェクト、エンターテイメント、さらにはリビングルームでプレイできるゲームなど、シーンの再構築と深度APIを使用して、さまざまなLiDAR対応アプリを見てきました。

00:00:43.000 --> 00:00:48.000
ARKitコミュニティが示した創造性と機知を見て、本当にうれしいです。

00:00:48.000 --> 00:00:56.000
これらのアプリを作成している間、私たちはあなたに世界最高のARフレームワークを提供し、可能なことの限界を押し広げることに懸命に取り組んでいます。

00:00:56.000 --> 00:01:00.000
ARKit 5での変更点を見てみましょう。

00:01:00.000 --> 00:01:10.000
まず、現実世界の屋外でのAR体験を可能にするロケーションアンカーのいくつかのアップデートとベストプラクティスを共有します。

00:01:10.000 --> 00:01:18.000
次に、アプリクリップコードについて説明します。これは、アプリクリップを発見し、ARでコンテンツを配置するための素晴らしい方法です。

00:01:18.000 --> 00:01:26.000
新しいiPad Proの超ワイドフロントカメラを使用したフェイストラッキングのいくつかの改善を強調します。

00:01:26.000 --> 00:01:31.000
そして、ARKitモーションキャプチャのいくつかの機能強化で締めくくります。

00:01:31.000 --> 00:01:38.000
私たちは、地域のサポートを拡大し、生活の質の向上を提供するために取り組んできたロケーションアンカーから始めます。

00:01:38.000 --> 00:01:42.000
また、アプリケーションを作成するためのベストプラクティスもいくつかお勧めします。

00:01:42.000 --> 00:01:50.000
ロケーションアンカーは昨年、特定の緯度と経度にARコンテンツを配置できるように導入されました。

00:01:50.000 --> 00:01:56.000
彼らの目的は、地理的な場所に結びついたAR体験の作成を可能にすることです。

00:01:56.000 --> 00:01:58.000
例を見てみましょう。 例を見てみましょう。

00:01:58.000 --> 00:02:05.000
これは、ロケーションアンカーAPIを使用して構築されたScavengARアプリケーションのNew Nature体験です。

00:02:05.000 --> 00:02:13.000
ScavengARは、現実世界の場所でARコンテンツをホストし、仮想パブリックアートのインスタレーションやアクティビティの作成を可能にします。

00:02:13.000 --> 00:02:19.000
これは、世界が再開するにつれて、ロケーションアンカーが屋外体験にどのように電力を供給できるかの良い例です。

00:02:19.000 --> 00:02:24.000
マップアプリは、iOS 15でAPIを使用する新しいAR機能も導入しています。

00:02:24.000 --> 00:02:26.000
見てみましょう。 

00:02:26.000 --> 00:02:32.000
今年、マップは、ロケーションアンカーAPIを使用して、ARに表示されるターンバイターンの歩行指示を追加します。

00:02:32.000 --> 00:02:35.000
彼らは私たちが推奨するいくつかの慣行を取り入れています。

00:02:35.000 --> 00:02:39.000
これらについては、後で説明して、優れたアプリケーションを構築する方法を紹介します。

00:02:39.000 --> 00:02:49.000
いくつかのサンプルを見たので、GeoTrackingConfigurationを設定するために必要な手順から始めて、ロケーションアンカーを使用してそれらを作成する方法を要約しましょう。

00:02:49.000 --> 00:02:53.000
まず、その機能がデバイスでサポートされていることを確認します。

00:02:53.000 --> 00:02:59.000
ロケーションアンカーには、A12チップ以降、セルラーとGPSのサポートが必要です。

00:02:59.000 --> 00:03:04.000
次に、起動する前に、その機能がその場所で利用可能であることを確認してください。

00:03:04.000 --> 00:03:08.000
カメラと位置情報の許可は、デバイスの所有者によって承認されなければなりません。

00:03:08.000 --> 00:03:11.000
ARKitは、必要に応じて許可を求めます。

00:03:11.000 --> 00:03:23.000
ARKit 4とサンプルプロジェクト「ARでの地理的位置の追跡」を紹介する昨年のプレゼンテーションでは、これらすべてのトピックとAPIの使用をより深くカバーしています。

00:03:23.000 --> 00:03:28.000
これらのソースの両方に精通していることを強くお勧めします。

00:03:28.000 --> 00:03:32.000
このコードサンプルは、前のスライドのチェックを実行する方法を示しています。

00:03:32.000 --> 00:03:42.000
デバイスサポートを照会し、GeoTrackingConfigurationを実行しようとする前に、現在の場所で機能が利用可能かどうかを検証します。

00:03:42.000 --> 00:03:47.000
GeoAnchorsは、他のタイプのアンカーと同様にARSessionに追加できます。

00:03:47.000 --> 00:03:52.000
それらは緯度-経度座標と、オプションで高度で指定されています。

00:03:52.000 --> 00:04:01.000
GeoTrackingConfigurationのステータスを監視して、機能がローカライズされているかどうか、および解決すべき問題が残っているかどうかを確認することが重要です。

00:04:01.000 --> 00:04:07.000
開発者サンプルには、ステータスの更新を受信するメソッドを実装する方法の例が含まれています。

00:04:07.000 --> 00:04:13.000
デバイスの場所の近くで可用性を確認することは、ジオトラッキングでアプリケーションを開始するために重要です。

00:04:13.000 --> 00:04:16.000
私たちは、より多くの地域をサポートするために常に働いています。

00:04:16.000 --> 00:04:27.000
ロケーションアンカーは、最初のリリースのために5つのメトロエリアに制限され、それ以来、サポートは米国全土の25以上の都市に拡大しました。

00:04:27.000 --> 00:04:31.000
私たちはまた、世界中の都市にロケーションアンカーをもたらすために懸命に働いています。

00:04:31.000 --> 00:04:36.000
初めて、米国外の市場を発表できることを嬉しく思います。

00:04:36.000 --> 00:04:39.000
ロケーションアンカーがロンドンにやってくる。

00:04:39.000 --> 00:04:43.000
私たちは、時間の経過とともに新しい地域を追加する作業を継続します。

00:04:43.000 --> 00:04:52.000
サポートされているメトロエリアに住んでいない場合は、録画とリプレイを使用してロケーションアンカーの実験を開始することもできます。このセッションの後半で説明します。

00:04:52.000 --> 00:04:59.000
サポートされている地域のリストについては、いつでもARGeoTrackingConfigurationのオンラインドキュメントを参照してください。

00:04:59.000 --> 00:05:06.000
ロケーションアンカーがより多くの地域で利用可能になるにつれて、私たちは人々を導くために共通の視覚言語を持つ必要性を認識しています。

00:05:06.000 --> 00:05:14.000
一貫したオンボーディングプロセスを支援するために、ARCoachingOverlayViewで使用する新しい.geoTracking目標を追加しています。

00:05:14.000 --> 00:05:21.000
世界追跡のための既存のオーバーレイと同様に、人々が良い経験を達成するのを助けるためにアニメーションを表示します。

00:05:21.000 --> 00:05:30.000
コーチングオーバーレイは、マップを含む多くの異なるARアプリで使用されているため、人々はすでにそれらに精通しており、対応方法を知っているでしょう。

00:05:30.000 --> 00:05:35.000
この機能の学習曲線を容易にするために、コーチングオーバーレイを含めることをお勧めします。

00:05:35.000 --> 00:05:45.000
コーチングオーバーレイを使用している間も、追跡状態に関するより詳細な情報を含む.geoTrackingステータスの更新を監視することをお勧めします。

00:05:45.000 --> 00:05:48.000
.geoTrackingコーチングオーバーレイは次のようになります。

00:05:48.000 --> 00:05:55.000
UIは、デバイスを地面から遠ざけて、建物のファサードに向ける指示を示しています。

00:05:55.000 --> 00:06:01.000
数秒後、追跡が成功し、アプリはジオトラッキングされたコンテンツを配置できます。

00:06:01.000 --> 00:06:06.000
このアニメーションを表示するためのコードは、他のコーチングオーバーレイに使用されるコードと非常によく似ています。

00:06:06.000 --> 00:06:10.000
ユニークなのは、オーバーレイの.geoTracking目標の導入です。

00:06:10.000 --> 00:06:14.000
正しいガイドを表示するには、必ずこの目標を設定してください。

00:06:14.000 --> 00:06:18.000
コーチングオーバーレイが統一されたオンボーディングプロセスを作成する方法を見てきました。

00:06:18.000 --> 00:06:24.000
次に、ジオトラッキングされたARエクスペリエンスを作成するのに役立つ他のベストプラクティスについて説明します。

00:06:24.000 --> 00:06:29.000
私たちの最初の推奨事項は、より迅速な開発のために録音とリプレイを使用することです。

00:06:29.000 --> 00:06:36.000
ARKitセッションは、App Storeで入手可能なReality Composerを使用してデバイスで録画できます。

00:06:36.000 --> 00:06:42.000
これはロケーションアンカーに特に便利なので、テストのために頻繁に外に出る必要はありません。

00:06:42.000 --> 00:06:46.000
また、遠隔地のクリエイターとのコラボレーションも可能です。

00:06:46.000 --> 00:06:49.000
録画は、Xcodeを使用してデバイスで再生できます。

00:06:49.000 --> 00:06:56.000
非互換性の問題を回避するには、同じデバイスとiOSバージョンを使用することをお勧めします。

00:06:56.000 --> 00:06:59.000
これは、他のタイプのARKitアプリケーションでも機能します。

00:06:59.000 --> 00:07:03.000
リプレイはロケーションアンカーに固有のものではありません。

00:07:03.000 --> 00:07:07.000
録音をキャプチャするプロセスを見てみましょう。

00:07:07.000 --> 00:07:13.000
録画するには、Reality Composerを開き、右上のその他のオプションをタップします。

00:07:13.000 --> 00:07:17.000
次に、開発者ペインを開き、ARセッションを記録するを選択します。

00:07:17.000 --> 00:07:20.000
位置情報サービスが有効になっていることを確認してください。

00:07:20.000 --> 00:07:25.000
赤いボタンをタップして、録音を開始および停止します。

00:07:25.000 --> 00:07:29.000
録画を再生するには、Xcodeを実行しているコンピュータにデバイスを接続します。

00:07:29.000 --> 00:07:35.000
[スキームの編集]をクリックし、実行設定のARKitリプレイデータオプションを設定します。

00:07:35.000 --> 00:07:37.000
次に、アプリケーションを実行します。

00:07:37.000 --> 00:07:43.000
録画と再生は開発のスピードアップに役立ちますが、コンテンツの配置には他のプラクティスをお勧めします。

00:07:43.000 --> 00:07:46.000
これはこれらを実演するビデオです。

00:07:46.000 --> 00:07:55.000
ARコンテンツが大きくてはっきりと見え、環境の構造と重なすことなく情報が伝えられることに注目してください。

00:07:55.000 --> 00:08:05.000
開発時間と配置精度のトレードオフとして、現実世界のオブジェクトと密接に重なろうとするのではなく、空中に浮かぶコンテンツを作成することを検討してください。

00:08:05.000 --> 00:08:09.000
コンテンツを配置するための推奨事項は他にもいくつかあります。

00:08:09.000 --> 00:08:18.000
オブジェクトを配置する緯度と経度の座標を取得するには、Appleマップアプリを使用し、少なくとも6桁の精度で座標をコピーします。

00:08:18.000 --> 00:08:25.000
これの手順は、ARKit 4を紹介するビデオで示されているので、詳細については、そこを参照してください。

00:08:25.000 --> 00:08:35.000
アプリケーションを作成するときは、良い経験を生み出すために、必要に応じてロケーションアンカーに対するコンテンツの高度を調整することも重要です。

00:08:35.000 --> 00:08:44.000
アプリがより正確なコンテンツの配置が必要な場合は、デバイスがその場所から50メートル以内にあるときにジオアンカーを追加します。

00:08:44.000 --> 00:08:52.000
ARKitが正確な高度でアンカーを配置すると、アンカーの高度ソースフィールドを更新してこれを示します。

00:08:52.000 --> 00:08:58.000
CLLocationクラスには、2点間の距離をメートル単位で計算するために使用できるメソッドがあります。

00:08:58.000 --> 00:09:03.000
これは、アンカーを追加する前に、誰かが場所の近くにいることを確認するために使用できます。

00:09:03.000 --> 00:09:06.000
これで、ロケーションアンカーに関するセッションは終了です。

00:09:06.000 --> 00:09:10.000
ARKit 5を使用して、アプリにARコンテンツを配置する方法は他にもあります。

00:09:10.000 --> 00:09:13.000
だから、クリストファーに渡しましょう。クリストファーはもっと教えてくれます。

00:09:13.000 --> 00:09:14.000
ありがとう、デビッド。

00:09:14.000 --> 00:09:17.000
こんにちは、私の名前はクリストファーで、ARKitチームのエンジニアです。

00:09:17.000 --> 00:09:20.000
ARKit 5の他の素晴らしい新機能についてもっとお話しできることを嬉しく思います。

00:09:20.000 --> 00:09:23.000
ARKitのアプリクリップコードから始めましょう。

00:09:23.000 --> 00:09:26.000
おそらく、昨年WWDCでApp Clipsを導入したことを覚えているでしょう。

00:09:26.000 --> 00:09:33.000
アプリクリップは、アプリ全体をインストールすることなく、アプリの1つのコンテキストワークフローを通して人々を連れて行くアプリの小さなスライスです。

00:09:33.000 --> 00:09:42.000
ファイルサイズが小さいため、アプリクリップはダウンロード時間を節約し、現在のコンテキストに非常に関連性の高いアプリの特定の部分に人々を直接連れて行きます。

00:09:42.000 --> 00:09:47.000
また、アプリクリップコードを視覚的に発見して起動するための素晴らしい方法であるアプリクリップコードも導入しました。

00:09:47.000 --> 00:09:50.000
App Storeへの旅行は必要ありません。

00:09:50.000 --> 00:09:51.000
これがアプリクリップコードです。

00:09:51.000 --> 00:09:54.000
彼らは様々な形や色で来ることができます。

00:09:54.000 --> 00:09:58.000
開発者として、シナリオに最適な外観を作成できます。

00:09:58.000 --> 00:10:05.000
また、アプリクリップコードでエンコードするデータと、どのアプリクリップがどのコードに関連付けられているかを決定します。

00:10:05.000 --> 00:10:15.000
すべてのアプリクリップコードには、視覚的にスキャン可能なパターンが含まれており、ここに示されている赤、青、オレンジのコードのように、ユーザーの便宜のためにNFCタグも含まれています。

00:10:15.000 --> 00:10:21.000
人々はカメラでコードをスキャンしたり、携帯電話を埋め込まれたNFCタグにかざして、関連するアプリクリップを起動することができます。

00:10:21.000 --> 00:10:26.000
そして今、AR体験でアプリクリップコードを認識して追跡することもできます。

00:10:26.000 --> 00:10:29.000
このセッションの後半で、それがどのように行われるかを見ていきます。

00:10:29.000 --> 00:10:36.000
しかし、まず、アプリクリップコードを使用してAR体験を起動するプライマーによって開発されたこのアプリクリップを見てみましょう。

00:10:36.000 --> 00:10:43.000
PrimerはCle Tileと提携し、App Clip Codesの助けを借りて、サンプルがARでどのように見えるかを人々に示しました。

00:10:43.000 --> 00:10:47.000
iPhoneとiPadをApp Clipコードの上に置くだけで、AR体験を呼び出すことができます。

00:10:47.000 --> 00:10:53.000
今、人々はアプリをダウンロードすることなく、自分の壁にタイル見本をプレビューすることができます。

00:10:53.000 --> 00:10:54.000
それはかなりかっこいいですよね?

00:10:54.000 --> 00:11:00.000
したがって、iOSとiPad 14.3から、AR体験でアプリクリップコードを検出して追跡することができます。

00:11:00.000 --> 00:11:07.000
App Clip Codeの追跡には、iPhone XSのようなA12 Bionicプロセッサ以降を搭載したデバイスが必要です。

00:11:07.000 --> 00:11:10.000
ARKitでアプリクリップコードを使用する方法を詳しく見てみましょう。

00:11:10.000 --> 00:11:16.000
iOS 14.3では、新しいタイプのARAnchor、ARAppClipCodeAnchorを導入しました。

00:11:16.000 --> 00:11:26.000
このアンカーには、アプリクリップコードに埋め込まれたURL、URLデコード状態、アプリクリップコードの半径（メートル単位）の3つの新しいプロパティがあります。

00:11:26.000 --> 00:11:29.000
説明させてください。

00:11:29.000 --> 00:11:33.000
各アプリクリップコードには、正しいコンテンツを表示するためにデコードされたURLが含まれています。

00:11:33.000 --> 00:11:36.000
URLのデコードは即時ではありません。

00:11:36.000 --> 00:11:39.000
ARKitは、アプリクリップコードの存在をすばやく検出できます。

00:11:39.000 --> 00:11:48.000
しかし、ユーザーのコードまでの距離や照明などの他の要因に応じて、ARKitがURLをデコードするのに少し時間がかかる場合があります。

00:11:48.000 --> 00:11:56.000
このため、App Clip Codeアンカーには.decoding状態プロパティが含まれており、3つの状態のいずれかにすることができます。

00:11:56.000 --> 00:12:00.000
初期状態の.decodingは、ARKitがまだURLをデコードしていることを示します。

00:12:00.000 --> 00:12:05.000
ARKitがURLのデコードに成功するとすぐに、状態は.decodedに切り替わります。

00:12:05.000 --> 00:12:10.000
URLのデコードが不可能な場合、状態は代わりに.failedに切り替わります。

00:12:10.000 --> 00:12:16.000
これは、たとえば、誰かがアプリクリップに関連付けられていないアプリクリップコードをスキャンしたときに発生する可能性があります。

00:12:16.000 --> 00:12:21.000
アプリクリップコードトラッキングを使用するには、まずデバイスでサポートされているかどうかを確認する必要があります。

00:12:21.000 --> 00:12:26.000
アプリクリップコードの追跡は、A12 Bionicプロセッサ以降を搭載したデバイスでのみサポートされていることを忘れないでください。

00:12:26.000 --> 00:12:34.000
次に、設定のappClipCodeTrackingEnabledプロパティをtrueに設定し、セッションを実行します。

00:12:34.000 --> 00:12:44.000
アプリクリップコードのURLを読み取るには、ARセッションを監視し、アンカーコールバックを更新し、検出されたアプリクリップコードアンカーのデコード状態を確認します。

00:12:44.000 --> 00:12:57.000
ARKitがApp Clipコードをデコードしている間、App Clipコードの上にプレースホルダの視覚化を表示して、App Clipコードが検出されたが、まだデコードする必要があるという即時フィードバックをユーザーに提供したい場合があります。

00:12:57.000 --> 00:13:06.000
前述のように、アプリクリップコードのデコードも失敗する可能性があります。例えば、誰かがあなたのアプリクリップに属していないアプリクリップコードに電話を向けたとき。

00:13:06.000 --> 00:13:10.000
その場合もフィードバックを提供することをお勧めします。

00:13:10.000 --> 00:13:17.000
アプリクリップコードがデコードされると、最終的にそのURLにアクセスし、このアプリクリップコードに適したコンテンツの表示を開始できます。

00:13:17.000 --> 00:13:25.000
たとえば、先ほど見たプライマーアプリクリップの場合、URLにはどのタイルスウォッチを表示するかに関する情報が含まれています。

00:13:25.000 --> 00:13:31.000
アプリクリップコードがデコードされたら、問題は、このコードに関連付けられているコンテンツをどこに表示すべきかということです。

00:13:31.000 --> 00:13:34.000
1つのオプションは、App Clip Codeアンカーの上に直接表示することです。

00:13:34.000 --> 00:13:40.000
ただし、ユースケースによっては、App Clip Code自体がコンテンツを表示するのに最適な場所ではない場合があります。

00:13:40.000 --> 00:13:46.000
したがって、たとえば、App Clip Codeの近くにコンテンツを固定された相対位置で配置できます。

00:13:46.000 --> 00:13:57.000
これは、アプリクリップコードがオブジェクト、例えばコーヒーメーカーに印刷され、機械のボタンの上にそれを操作する方法に関する仮想指示を表示したい場合にうまく機能します。

00:13:57.000 --> 00:14:02.000
または、App Clip CodeのトラッキングをARKitでサポートされている他のトラッキング技術と組み合わせることもできます。

00:14:02.000 --> 00:14:04.000
例えば、画像追跡。

00:14:04.000 --> 00:14:07.000
その実装を見てみましょう。 では、その実装を見てみましょう

00:14:07.000 --> 00:14:16.000
次を見るビデオとコードは、developer.apple.comでダウンロードできる「ARでアプリクリップコードと対話する」サンプルコードに基づいています。

00:14:16.000 --> 00:14:20.000
あなたが今見ているのは、サンプルのAR体験の記録です。

00:14:20.000 --> 00:14:24.000
まず、カメラアプリから始めて、ヒマワリの種パッケージをスキャンします。

00:14:24.000 --> 00:14:28.000
たぶん、私は園芸店で買い物をしていて、どの植物の種を買うかを決めようとしています。

00:14:28.000 --> 00:14:34.000
iOSはパッケージのアプリクリップコードを認識し、関連するシードショップアプリクリップを起動します。

00:14:34.000 --> 00:14:40.000
ここでは、アプリクリップコードを2回目にスキャンすると、成長したヒマワリがシードパッケージに表示されます。

00:14:40.000 --> 00:14:45.000
アプリクリップは、シードパッケージ全体の画像追跡を使用し、その上にヒマワリを置くことに注意してください。

00:14:45.000 --> 00:14:54.000
このアプローチは、右上の小さなアプリクリップコードではなく、シードパッケージ全体に注目される可能性が高いため、このユースケースでは理にかなっています。

00:14:54.000 --> 00:14:57.000
しかし、誰かが自分の庭で植物が育つのを見たいと思ったらどうなりますか?

00:14:57.000 --> 00:14:59.000
これがどのように見えるかです。

00:14:59.000 --> 00:15:04.000
ここでは、コードが初めてスキャンされると、アプリクリップのダウンロードが呼び出されることがわかります。

00:15:04.000 --> 00:15:13.000
次に、アプリクリップ内から同じコードを再度スキャンすると、コードをヒマワリの種箱に関連付け、芝生をタップするとヒマワリが表示されます。

00:15:13.000 --> 00:15:18.000
代わりに、アプリクリップがバラの種箱のコードを見たら、芝生にバラの植物を産んだだろう。

00:15:18.000 --> 00:15:22.000
アプリクリップには1つのワークフローしか含まれていないことに注意してください。

00:15:22.000 --> 00:15:28.000
しかし、アプリクリップは、彼らのスペースでプレビューできる他の植物を体験するために、完全なシードショップアプリをダウンロードするためのボタンを提供することができます。

00:15:28.000 --> 00:15:32.000
App Clipコードのトラッキングは、App Clipの親アプリでも機能することを忘れないでください。

00:15:32.000 --> 00:15:37.000
芝生にヒマワリを置くために必要なコードを見てみましょう。

00:15:37.000 --> 00:15:42.000
まず、tapGestureRecognizerをビューに追加して、画面上のタップを検出します。

00:15:42.000 --> 00:15:51.000
人が画面をタップすると、世界に光線をキャストし、デバイスの前の水平平面上の結果の位置を取り戻すことができます。

00:15:51.000 --> 00:15:55.000
私たちのシナリオでは、これはその人の芝生になります。

00:15:55.000 --> 00:16:04.000
次に、デコードされた最後のアプリクリップコードURLをつかみ、芝生に新しいARAnchorを追加します。

00:16:04.000 --> 00:16:09.000
最後に、ヒマワリの3Dモデルをダウンロードし、芝生に表示します。

00:16:09.000 --> 00:16:14.000
では、ARKitのアプリクリップコードのベストプラクティスについて話しましょう。

00:16:14.000 --> 00:16:18.000
アプリクリップは、さまざまな環境やさまざまなユースケースで使用できます。

00:16:18.000 --> 00:16:22.000
NFCアプリクリップコードを作成するオプションであるかどうかを検討してください。

00:16:22.000 --> 00:16:27.000
人々が物理的にコードにアクセスできる環境には、NFCアプリクリップコードをお勧めします。

00:16:27.000 --> 00:16:39.000
NFCアプリクリップコードを使用する場合は、タグをタップするように人々を導く適切な行動を促すテキストを使用するか、代わりに、コードをスキャンするための明示的なアフォーダンスを提供します。

00:16:39.000 --> 00:16:46.000
最後になりましたが、アプリクリップコードがユーザーの環境に適したサイズで印刷されていることを確認する必要があります。

00:16:46.000 --> 00:16:57.000
たとえば、レストランのメニューはA4用紙に印刷され、人々は最大50センチメートルの距離からそのメニューの2.5センチメートルのアプリクリップコードを快適にスキャンすることができます。

00:16:57.000 --> 00:17:07.000
しかし、映画のポスターは通常はるかに大きく、最大2.5メートル離れた場所から携帯電話でスキャンできる12センチメートルのアプリクリップコードのための十分なスペースがあるかもしれません。

00:17:07.000 --> 00:17:14.000
推奨コードサイズの詳細については、アプリクリップコードに関するヒューマンインターフェースガイドラインをご覧ください。

00:17:14.000 --> 00:17:17.000
それがARKitでアプリクリップコードを使用する方法です。

00:17:17.000 --> 00:17:26.000
アプリクリップとアプリクリップコードを深く掘り下げたい場合は、「アプリクリップの新機能」と「軽くて高速なアプリクリップを構築する」セッションを必ずチェックしてください。

00:17:26.000 --> 00:17:29.000
では、フェイストラッキングにジャンプしましょう。

00:17:29.000 --> 00:17:36.000
フェイストラッキングを使用すると、フロントカメラで顔を検出し、仮想コンテンツをオーバーレイし、顔の表情をリアルタイムでアニメーション化できます。

00:17:36.000 --> 00:17:41.000
iPhone Xの発売以来、ARKitはフェイストラッキングを利用する多くの素晴らしいアプリを見てきました。

00:17:41.000 --> 00:17:50.000
複数の顔の追跡から、フロントカメラとバックカメラのユースケースでのフェイストラッキングの実行まで、このAPIは長年にわたって多くの進歩を遂げてきました。

00:17:50.000 --> 00:17:57.000
昨年は、A12 Bionicプロセッサ以降を搭載している限り、TrueDepthセンサーのないデバイスにフェイストラッキングを導入しました。

00:17:57.000 --> 00:18:05.000
そして今年初め、AR顔追跡体験のための超広視野フロントカメラを提供する新しいiPad Proを発売しました。

00:18:05.000 --> 00:18:07.000
見てみましょう。 

00:18:07.000 --> 00:18:12.000
ここでは、通常のフロントカメラの視野が見えます。

00:18:12.000 --> 00:18:15.000
そして、これは新しいiPad Proの新しい超広視野です。

00:18:15.000 --> 00:18:18.000
それは本当に違いを生みますね?

00:18:18.000 --> 00:18:22.000
既存のアプリは、フェイストラッキングに通常のカメラを使用し続けることに注意してください。

00:18:22.000 --> 00:18:32.000
新しいiPad Proでユーザーエクスペリエンスを超広視野にアップグレードしたい場合は、利用可能なビデオフォーマットを確認し、新しい超広幅フォーマットをオプトインする必要があります。

00:18:32.000 --> 00:18:39.000
サポートされているすべてのビデオフォーマットを反復し、builtInUltraWideCameraオプションをチェックすることで、これを行うことができます。

00:18:39.000 --> 00:18:45.000
次に、AR設定でこの形式を設定し、セッションを実行します。

00:18:45.000 --> 00:18:51.000
注意すべき点の1つは、新しいiPad Proの超広角カメラは、TrueDepthセンサーよりもはるかに広い視野を持っているということです。

00:18:51.000 --> 00:18:57.000
したがって、超ワイドビデオフォーマットを使用する場合、ARFrameにキャプチャされたDepthDataバッファは取得されません。

00:18:57.000 --> 00:18:59.000
最後になりましたが、モーションキャプチャーについて話しましょう。

00:18:59.000 --> 00:19:10.000
2019年の発売以来、モーションキャプチャは、2Dおよび3Dシミュレーションで使用されるとともに、仮想キャラクターをアニメーション化するなど、ARシーンでの実際の人々の堅牢な統合を可能にしました。

00:19:10.000 --> 00:19:13.000
iOS 15では、モーションキャプチャがさらに良くなっています。

00:19:13.000 --> 00:19:20.000
iPhone 12のようなApple A14 Bionicプロセッサを搭載したデバイスでは、モーションキャプチャがより広い範囲のボディポーズをサポートするようになりました。

00:19:20.000 --> 00:19:23.000
そして、これはコードの変更をまったく必要としません。

00:19:23.000 --> 00:19:27.000
iOS 15のすべてのモーションキャプチャアプリは、この恩恵を受けるでしょう。

00:19:27.000 --> 00:19:33.000
最も注目すべきは、ローテーションがこれまで以上に正確で、スポーツアクションをはるかに正確に追跡するのに役立ちます。

00:19:33.000 --> 00:19:38.000
もう1つの大きな改善点は、デバイスカメラがはるかに遠くから体の関節を追跡できるようになったことです。

00:19:38.000 --> 00:19:43.000
また、手足の動きの範囲を追跡することも大幅に増加しています。

00:19:43.000 --> 00:19:45.000
例を見てみましょう。 例を見てみましょう。

00:19:45.000 --> 00:19:50.000
これは私の同僚の1人であるEjlerが、Driven2winアプリで彼のトレーニングを追跡しています。

00:19:50.000 --> 00:19:54.000
iOS 15の結果はこれまで以上に正確です。

00:19:54.000 --> 00:19:57.000
要約すると、ARKit 5は多くの新機能と改善をもたらします。

00:19:57.000 --> 00:20:01.000
ロケーションアンカーは新しい都市で利用可能で、新しいコーチングオーバーレイを備えています。

00:20:01.000 --> 00:20:08.000
アプリクリップコードの追跡は、アプリクリップでのARの簡単な発見と使用、および仮想コンテンツの正確な配置を支援します。

00:20:08.000 --> 00:20:16.000
顔追跡は、新しいiPad Proの新しい超広視野で動作し、モーションキャプチャはより良い精度とより広い可動域を追加します。

00:20:16.000 --> 00:20:21.000
私はあなたがARKit 5で作成するすべての素晴らしい体験を見てとても興奮しています。

00:20:21.000 --> 23:59:59.000
[音楽]。

