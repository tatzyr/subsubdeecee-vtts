WEBVTT

00:00:02.000 --> 00:00:11.000
こんにちは、WWDCへようこそ。

00:00:11.000 --> 00:00:16.000
私の名前はジョンで、Appleの機械学習フレームワークであるCore MLに取り組んでいます。

00:00:16.000 --> 00:00:22.000
同僚のブライアンと一緒に、機械学習の魔法をアプリにもたらしながら、モデルを調整する方法をお見せできることを嬉しく思います。

00:00:22.000 --> 00:00:27.000
まず始めに、機械学習APIの機能強化をいくつか紹介します。

00:00:27.000 --> 00:00:32.000
その後、さまざまな新しい可能性を開くファイル形式の改善に飛び込みます。

00:00:32.000 --> 00:00:42.000
後で、ブライアンは私たちにMLプログラムを見せ、私たちをボンネットの下に連れて行き、タイプされた実行と、それを使用してモデルの精度とパフォーマンスを微調整する方法を説明します。

00:00:42.000 --> 00:00:48.000
これらの改善を使用して、ワークフローを合理化し、MLを搭載したエクスペリエンスをさらに押し上げることができます。

00:00:48.000 --> 00:00:52.000
APIの改善から始めましょう。

00:00:52.000 --> 00:00:56.000
Core MLは、ユーザーのデバイス上のモデルを操作するためのシンプルなAPIを提供します。

00:00:56.000 --> 00:01:05.000
これらのモデルは、文字列やプリミティブ値、または画像やマルチアレイなどのより複雑な入力など、さまざまな入力と出力で動作するように設計できます。

00:01:05.000 --> 00:01:09.000
この最後のタイプ、MultiArrayについてもっと話しましょう。

00:01:09.000 --> 00:01:14.000
Core MLを使用すると、MLMultiArrayを使用して多次元データを簡単に操作できます。

00:01:14.000 --> 00:01:20.000
シンプルなAPIですが、データを操作するために書かなければならないコードは、Swiftでは必ずしも自然に感じるとは限りません。

00:01:20.000 --> 00:01:26.000
たとえば、多数の整数でMultiArrayを初期化するには、実行時に型を渡す必要があります。

00:01:26.000 --> 00:01:34.000
さらに、通常の整数の代わりにNSNumberを使用する必要がありますが、それはタイプセーフではなく、エレガントなSwiftのようには見えません。

00:01:34.000 --> 00:01:39.000
Core MLは、多次元データでの作業を容易にするために、MLShapedArrayを導入しています。

00:01:39.000 --> 00:01:45.000
MLShapedArrayは、通常の配列に似ていますが、複数の次元をサポートする純粋なSwiftタイプです。

00:01:45.000 --> 00:01:55.000
配列と同様に、コピーオンライトセマンティクスと、既存のMLMultiArrayコードで簡単に機能する豊富なスライス構文を備えた値タイプです。

00:01:55.000 --> 00:02:02.000
2次元MLMultiArrayを初期化するには、通常、2つのネストされた「for」ループを使用します。

00:02:02.000 --> 00:02:08.000
MLShapedArrayを使用すると、同じ2D配列を1行で初期化できます。

00:02:08.000 --> 00:02:14.000
MLShapedArrayはSwiftに自然にフィットし、コードの作成とレビューがはるかに簡単になります。

00:02:14.000 --> 00:02:15.000
これは別の例です。

00:02:15.000 --> 00:02:20.000
2行目をスライスとしてアクセスするには、このようにインデックスを作成するだけです。

00:02:20.000 --> 00:02:27.000
複数の行と列をスライスとしてアクセスするには、各次元の範囲を使用できます。

00:02:27.000 --> 00:02:31.000
MLShapedArrayとMLMultiArrayは互いに完全に互換性があります。

00:02:31.000 --> 00:02:37.000
他のタイプのインスタンスを取る初期化子を使用して、あるタイプを別のタイプに簡単に変換できます。

00:02:37.000 --> 00:02:41.000
変換初期化子を使用してデータ型を変換することもできます。

00:02:41.000 --> 00:02:47.000
たとえば、このコードは、doublesのMultiArrayをFloatsのShaedArrayに変換します。

00:02:47.000 --> 00:02:51.000
形状配列は、多次元データを操作する必要があるときはいつでも便利です。

00:02:51.000 --> 00:02:58.000
たとえば、YOLOオブジェクト検出モデルは画像内のオブジェクトを見つけ、2次元配列を出力します。

00:02:58.000 --> 00:03:00.000
この表は、1つの予測からのデータを示しています。

00:03:00.000 --> 00:03:05.000
各行は境界ボックスを表し、各列の値は0から1の範囲です。

00:03:05.000 --> 00:03:12.000
各値は、バウンディングボックスに人、自転車、車などが含まれているというモデルの自信を示しています。

00:03:12.000 --> 00:03:17.000
バウンディングボックスごとに最も可能性の高いラベルを選択するためのコードを書きたいです。

00:03:17.000 --> 00:03:19.000
これを行う方法の例を次に示します。

00:03:19.000 --> 00:03:24.000
コードは、2次元マルチアレイである出力の信頼プロパティから始まります。

00:03:24.000 --> 00:03:29.000
この関数は各行をループして、その行で最も高い信頼スコアを見つけます。

00:03:29.000 --> 00:03:33.000
整数をNSNumberに頻繁にキャストしなければならないことに注意してください。

00:03:33.000 --> 00:03:39.000
このコードは代わりにMLShapedArrayを使用し、読みやすい少ない行で同じ作業を行います。

00:03:39.000 --> 00:03:44.000
モデルの予測結果が、信頼値を含むShaedArrayプロパティを与えることに注意してください。

00:03:44.000 --> 00:03:50.000
MLShapedArrayとそのスカラーは標準のSwiftコレクションプロトコルに準拠しているため、このコードはより簡単です。

00:03:50.000 --> 00:03:56.000
これは、より読みやすく、Swiftで作業する喜びである素敵な強くタイプされた経験を提供します。

00:03:56.000 --> 00:04:01.000
次に、Core MLモデルと、それらがファイルシステムでどのように表現されているかについて話しましょう。

00:04:01.000 --> 00:04:06.000
Core MLを使用すると、ユーザー向けに豊富な機械学習によるエクスペリエンスを簡単に構築できます。

00:04:06.000 --> 00:04:09.000
MLモデルは、これらの経験に命を吹き込むエンジンです。

00:04:09.000 --> 00:04:15.000
.Mlmodelファイル形式は、モデルの機能をエンコードおよび抽象化するので、心配する必要はありません。

00:04:15.000 --> 00:04:19.000
このフォーマットは、モデルのすべての実装の詳細と複雑さを格納します。

00:04:19.000 --> 00:04:24.000
開発者として、それがツリーアンサンブルであろうと、何百万ものパラメータを持つニューラルネットワークであろうと気にする必要はありません。

00:04:24.000 --> 00:04:32.000
MLモデルは、他のAPIと同様に、Xcodeプロジェクトに追加し、それで動作するコードを書くだけの単一のファイルです。

00:04:32.000 --> 00:04:36.000
各Core MLモデルファイルは、いくつかのコンポーネントで構成されています。

00:04:36.000 --> 00:04:42.000
メタデータは、著者、ライセンス、バージョン、簡単な説明などの情報を保存します。

00:04:42.000 --> 00:04:46.000
インターフェイスは、モデルの入力と出力を定義します。

00:04:46.000 --> 00:04:49.000
アーキテクチャは、モデルの内部構造を定義します。

00:04:49.000 --> 00:04:56.000
たとえば、ニューラルネットワークでは、アーキテクチャセクションでは、モデルのレイヤーとそれらの間のすべての接続について説明します。

00:04:56.000 --> 00:05:03.000
最後に、最後のセクションでは、モデルがトレーニング段階で学習した膨大な値の配列を保存します。

00:05:03.000 --> 00:05:12.000
MLモデルファイルは、これらすべてのセクションをプロトブフバイナリ形式にエンコードし、ファイルシステムとソース管理ソフトウェアは単一のバイナリファイルと見なします。

00:05:12.000 --> 00:05:18.000
ソース制御ソフトウェアは、バイナリモデルファイルが実際にはいくつかの異なるコンポーネントの組み合わせであることを伝えることができません。

00:05:18.000 --> 00:05:27.000
それを解決するために、Core MLは、macOSの組み込みパッケージ機能を使用して、これらのコンポーネントを別々のファイルに分割する新しいモデルフォーマットを追加しています。

00:05:27.000 --> 00:05:30.000
これにより、新しいCore MLモデルパッケージに登場します。

00:05:30.000 --> 00:05:37.000
これは、モデルの各コンポーネントを独自のファイルに格納し、アーキテクチャ、重み、メタデータを分離するコンテナです。

00:05:37.000 --> 00:05:44.000
これらのコンポーネントを分離することで、モデルパッケージを使用すると、メタデータを簡単に編集し、ソース制御で変更を追跡できます。

00:05:44.000 --> 00:05:50.000
また、より効率的にコンパイルし、モデルを読み書きするツールに柔軟性を提供します。

00:05:50.000 --> 00:05:55.000
Core MLとXcodeは、依然として元のMLモデルフォーマットを完全にサポートしています。

00:05:55.000 --> 00:06:00.000
しかし、モデルパッケージに更新することで、より拡張可能な形式に移行し、より効率的にコンパイルすることができます。

00:06:00.000 --> 00:06:02.000
Xcodeでこれを試してみましょう。

00:06:02.000 --> 00:06:07.000
これは、物体検出モデルを使用して画像内の動物を識別するシンプルなアプリです。

00:06:07.000 --> 00:06:10.000
メタデータフィールドの一部が空であることに注意してください。

00:06:10.000 --> 00:06:14.000
メタデータが入力されていないモデルに出くわすのはかなり一般的です。

00:06:14.000 --> 00:06:17.000
以前は、Xcodeでこれらのフィールドを編集できませんでした。

00:06:17.000 --> 00:06:21.000
しかし、Xcodeがモデルパッケージをサポートした今、あなたはできます。

00:06:21.000 --> 00:06:30.000
現在、モデルのファイルタイプはMLモデルですが、編集ボタンをクリックすると、XcodeはMLモデルファイルをMLパッケージに更新するように促します。

00:06:30.000 --> 00:06:37.000
Xcodeは、私のワークスペースの参照を元のモデルファイルに更新して、new.mlpackageを指すようにしようとしていることを教えてくれます。

00:06:37.000 --> 00:06:42.000
先に進んで、[更新と編集]をクリックします。

00:06:42.000 --> 00:06:48.000
XcodeのUIは、モデルがMLパッケージ形式であることを示すようになりました。

00:06:48.000 --> 00:06:51.000
これで、不足している値をXcodeで直接入力できます。

00:06:51.000 --> 00:07:01.000
先に進んで、説明を「動物」という言葉で更新します。このモデルは私の同僚のジョセフから来たので、著者フィールドに彼の名前を入れます。

00:07:01.000 --> 00:07:07.000
MITライセンスとバージョン2.0と言います。

00:07:07.000 --> 00:07:11.000
また、追加のメタデータフィールドを追加、変更、削除することもできます。

00:07:11.000 --> 00:07:17.000
WWDCでこのモデルを使用した年を示す新しいメタデータ項目を追加します。

00:07:17.000 --> 00:07:19.000
だから、私たちは2021年と言います。

00:07:19.000 --> 00:07:27.000
現在、UIのサポートに加えて、これらの情報はすべて、実行時にCore MLのMLModelDescription APIを使用してアクセスすることもできます。

00:07:27.000 --> 00:07:32.000
「予測」タブでモデルの入力と出力の説明を変更することもできます。

00:07:32.000 --> 00:07:35.000
ここでは、この入力の説明を変更します。

00:07:35.000 --> 00:07:38.000
「動物の」を追加します。

00:07:38.000 --> 00:07:43.000
そしてここで、不足しているハイフンを追加してタイプミスを修正します。

00:07:43.000 --> 00:07:46.000
さて、良いメタデータを持つモデルは、良いコメントを持つコードによく似ています。

00:07:46.000 --> 00:07:54.000
あなたとあなたのチームがモデルの意図を理解するのに役立つので、モデルの入力と出力について良い説明を書くことが特に重要です。

00:07:54.000 --> 00:07:57.000
[完了]をクリックして変更を保存します。

00:07:57.000 --> 00:08:07.000
ソースコントロールをクリックしてからコミットすると、Xcodeは差分ビューで変更を表示します。

00:08:07.000 --> 00:08:12.000
メタデータは独自の.jsonファイルにあるため、変更を簡単に確認できます。

00:08:12.000 --> 00:08:17.000
同様に、機能の説明には独自の別々の.jsonファイルがあります。

00:08:17.000 --> 00:08:24.000
62メガバイトのバイナリMLモデルファイルの数バイトを変更していたら、62メガバイトのバイナリ差分があったでしょう。

00:08:24.000 --> 00:08:30.000
ただし、モデルパッケージは、特に小さなテキスト変更の場合、はるかに効率的で作業が簡単です。

00:08:30.000 --> 00:08:34.000
Xcodeは、モデルパッケージとモデルファイルの両方を均等にサポートしています。

00:08:34.000 --> 00:08:39.000
たとえば、プレビュータブを使用してモデルパッケージをテストできます。

00:08:39.000 --> 00:08:46.000
2匹のクマの画像を持ち込むと、クマごとに1つずつ、2つのバウンディングボックスが手に入ることがわかります。

00:08:46.000 --> 00:08:57.000
同様に、[ユーティリティ]タブに移動して、MLモデルファイルと同じように、モデルパッケージの暗号化キーまたはMLアーカイブを生成できます。

00:08:57.000 --> 00:08:59.000
それがXcodeのモデルパッケージです。

00:08:59.000 --> 00:09:04.000
パッケージは、モデルメタデータの編集など、モデルファイルができることすべてを行うことができます。

00:09:04.000 --> 00:09:10.000
最後に見せたいのは、プロジェクトに追加するモデルごとにXcodeが自動的に生成するコードです。

00:09:10.000 --> 00:09:14.000
このアイコンをクリックすると、生成されたコードが表示されます。

00:09:14.000 --> 00:09:19.000
先ほど、MLMultiArrayとその新しいSwiftのカウンターパートであるMLShapedArrayを見てみました。

00:09:19.000 --> 00:09:25.000
Xcodeは、ラッパークラスのMultiArray出力ごとに新しい形状の配列プロパティを追加するようになりました。

00:09:25.000 --> 00:09:31.000
たとえば、生成されたクラスは、モデルの出力に対して confidenceShapedArray プロパティを持つようになりました。

00:09:31.000 --> 00:09:37.000
必要に応じて、元の信頼度MLMultiArrayプロパティを引き続き使用できます。

00:09:37.000 --> 00:09:46.000
新しい形状の配列プロパティを利用するには、プロジェクトの展開ターゲットがmacOS 12やiOS 15など、これらのOSバージョンの1つである必要があることに注意してください。

00:09:46.000 --> 00:09:51.000
これらすべてを実際に見たので、MLモデルとMLパッケージを並べて見てみましょう。

00:09:51.000 --> 00:09:59.000
MLパッケージは、ツリー、SVM、ニューラルネットワークなど、MLモデルファイルがサポートするすべての同じタイプをサポートしています。

00:09:59.000 --> 00:10:05.000
これらのタイプに加えて、MLパッケージはMLプログラムと呼ばれる強力な新しいモデルタイプもサポートしています。

00:10:05.000 --> 00:10:10.000
MLプログラムは、よりコード指向の形式でニューラルネットワークを表すモデルタイプです。

00:10:10.000 --> 00:10:15.000
MLプログラムとそれらが有効にする新機能について詳しくお伝えするために、ブライアンに渡します。

00:10:15.000 --> 00:10:16.000
ありがとう、ジョン。

00:10:16.000 --> 00:10:24.000
私の名前はブライアン・キーンで、MLプログラムと、型付き実行がどのように精度とより良いモデルパフォーマンスをより制御できるかについて話すことに興奮しています。

00:10:24.000 --> 00:10:28.000
機械学習モデルがあなたに提示されたかもしれないさまざまな方法があります。

00:10:28.000 --> 00:10:36.000
機械学習コースを受講したり、論文を読んだりしている場合は、その数学的または統計的定式化に関して記述されたモデルに遭遇する可能性があります。

00:10:36.000 --> 00:10:45.000
しかし、これらの数学的表現はしばしば抽象化され、代わりに計算グラフまたはネットワークの形で提示されます。

00:10:45.000 --> 00:10:54.000
中央の2つの図に描かれているこのグラフィカルな表現は、データが一連のレイヤーをどのように流れるかを説明し、それぞれが独自の特定の変換を適用します。

00:10:54.000 --> 00:10:59.000
機械学習ソフトウェアライブラリでは、モデルは代わりにコード内の操作として表現されます。

00:10:59.000 --> 00:11:07.000
機械学習エンジニアは、ブロック、機能、および制御フローで構成されるこのより一般的なプログラム構造をますます活用しています。

00:11:07.000 --> 00:11:13.000
Core MLの新しいMLプログラムモデルタイプは、この最後の表現と一致します。

00:11:13.000 --> 00:11:16.000
これは代表的なMLプログラムです。

00:11:16.000 --> 00:11:21.000
それは人間が読めるテキスト形式ですが、意図は自分で書く必要がないということです。

00:11:21.000 --> 00:11:25.000
MLプログラムは、Core MLのコンバーターによって自動的に生成されます。

00:11:25.000 --> 00:11:28.000
MLプログラムはメイン機能で構成されています。

00:11:28.000 --> 00:11:32.000
この主な機能は、一連の操作、または操作で構成されています。

00:11:32.000 --> 00:11:37.000
各opは変数を生成し、この変数は強く型付けされます。

00:11:37.000 --> 00:11:46.000
線形操作や畳み込み操作などの重みを持つ操作の場合、重みは通常、別のバイナリファイルにシリアル化されます。

00:11:46.000 --> 00:11:52.000
これは、MLプログラムとニューラルネットワークの比較の簡単な要約です。

00:11:52.000 --> 00:11:57.000
ニューラルネットワークにはレイヤーがあり、MLプログラムにはオペレーションがあります。

00:11:57.000 --> 00:12:04.000
ニューラルネットワークモデルの重みはレイヤーの説明に埋め込まれていますが、MLプログラムは重みを別々にシリアル化します。

00:12:04.000 --> 00:12:07.000
また、ニューラルネットワークは中間テンソル型を指定していません。

00:12:07.000 --> 00:12:11.000
代わりに、計算ユニットは実行時にこれらのタイプを決定します。

00:12:11.000 --> 00:12:15.000
一方、MLプログラムにはテンソルが強く型付けされています。

00:12:15.000 --> 00:12:24.000
今日は、MLプログラムの強く型付けされた構文と、型付けされた中間テンソルがMLプログラムを使用したデバイス上の機械学習に与える影響に焦点を当てます。

00:12:24.000 --> 00:12:28.000
しかし、まず、どのようにMLプログラムを取得しますか?

00:12:28.000 --> 00:12:32.000
Core MLは以前、統一されたコンバータAPIを導入しました。

00:12:32.000 --> 00:12:42.000
この統合コンバータAPIは、単一の関数呼び出しでTensorflowまたはPyTorchからCore MLニューラルネットワークモデルにモデルを取得する便利な手段を提供します。

00:12:42.000 --> 00:12:51.000
最小展開ターゲットとしてiOS 15を選択することで、同じAPIを使用してMLプログラムに変換できるようになりました。

00:12:51.000 --> 00:12:57.000
ボンネットの下で、Core MLコンバータは、変換時にモデルのディスク上の表現を選択します。

00:12:57.000 --> 00:13:05.000
MLプログラムの場合、ディスク上の中間表現は、WWDC 2020で導入された機能であるModel Intermediate Languageによって提供されます。

00:13:05.000 --> 00:13:12.000
統合コンバータAPIは、モデルをMLプログラムとして展開することをオプトインできる場所です。

00:13:12.000 --> 00:13:16.000
今後、MLプログラムはニューラルネットワークよりも好ましい形式になります。

00:13:16.000 --> 00:13:21.000
そして、MLプログラムはiOS15とmacOS Montereyから利用可能です。

00:13:21.000 --> 00:13:31.000
Core MLは、ニューラルネットワークモデルのMLモデルとMLパッケージフォーマットの両方をサポートしていますが、MLプログラムは、その重みをアーキテクチャとは別に保存するためのMLパッケージでなければなりません。

00:13:31.000 --> 00:13:35.000
コアMLは、将来の基盤としてMLプログラムに投資しています。

00:13:35.000 --> 00:13:40.000
ニューラルネットワークは引き続きサポートされますが、MLプログラムは新機能の中心になります。

00:13:40.000 --> 00:13:45.000
では、MLプログラムが未来であるならば、今日MLプログラムを採用することの利点は何ですか?

00:13:45.000 --> 00:13:47.000
これにより、型付き実行につながります。

00:13:47.000 --> 00:13:54.000
MLプログラムで型付き実行の利点を強調するために、まずニューラルネットワークで何が起こるかについて話し合いましょう。

00:13:54.000 --> 00:14:04.000
ここに示されているのは、入出力テンソルにFloat32を指定するCore MLニューラルネットワークモデルへの入出力の例です。

00:14:04.000 --> 00:14:09.000
入力と出力は、ダブルまたは32ビットの整数型にすることもできます。

00:14:09.000 --> 00:14:13.000
したがって、ニューラルネットワークモデルは、これらの入力テンソルと出力テンソルを強くタイプします。

00:14:13.000 --> 00:14:17.000
中間テンソルの種類はどうですか?

00:14:17.000 --> 00:14:21.000
ニューラルネットワークは、その中間テンソルを強く入力しません。

00:14:21.000 --> 00:14:25.000
ディスク上のモデルには、これらのテンソルの種類に関する情報はありません。

00:14:25.000 --> 00:14:33.000
代わりに、モデルを実行する計算ユニットは、Core MLがモデルをロードした後にテンソルのタイプを推測します。

00:14:33.000 --> 00:14:43.000
Core MLランタイムがニューラルネットワークをロードすると、ネットワークグラフがAppleニューラルエンジンフレンドリー、GPUフレンドリー、CPUのセクションに自動的かつ動的に分割されます。

00:14:43.000 --> 00:14:51.000
各コンピューティングユニットは、そのパフォーマンスとモデルの全体的なパフォーマンスを最大化するために、ネイティブタイプを使用してネットワークのセクションを実行します。

00:14:51.000 --> 00:14:57.000
GPUとニューラルエンジンはどちらもFloat16を使用し、CPUはFloat32を使用します。

00:14:57.000 --> 00:15:07.000
開発者として、モデルの computeUnits プロパティで .all、.cpuAndGPU、または .cpuOnly を選択することで、この実行スキームをある程度制御できます。

00:15:07.000 --> 00:15:18.000
このプロパティのデフォルトは.allで、Core MLに実行時にニューラルエンジン、GPU、CPU全体でモデルを分割して、アプリに可能な限り最高のパフォーマンスを提供するように指示します。

00:15:18.000 --> 00:15:28.000
また、CPUのみに設定すると、Core MLはニューラルエンジンもGPUも使用せず、モデルがCPU上でFloat32精度のみを実行していることを保証します。

00:15:28.000 --> 00:15:36.000
要約すると、ニューラルネットワークには中間テンソルがあり、生成を担当するコンピューティングユニットによって実行時に自動的に入力されます。

00:15:36.000 --> 00:15:46.000
許可された計算ユニットのセットを設定することで、それらの精度をある程度制御できますが、そうすることはモデルのグローバル設定であり、テーブルにいくつかのパフォーマンスを残す可能性があります。

00:15:46.000 --> 00:15:49.000
MLプログラムはどうですか?

00:15:49.000 --> 00:15:58.000
ここに示されているMLプログラムでは、入力テンソルと出力テンソルは強く型付けされており、プログラムのすべての中間テンソルもそうである。

00:15:58.000 --> 00:16:07.000
CPUやGPUなど、単一のコンピューティングユニット内で精度サポートを混在させることさえでき、これらのタイプはモデル変換時に明確に定義されています。

00:16:07.000 --> 00:16:13.000
これは、Core MLを使用して展開シナリオでモデルをロードして実行するずっと前です。

00:16:13.000 --> 00:16:20.000
MLプログラムは、ニューラルエンジン、GPU、およびCPUに作業を分配するのと同じ自動分割スキームを使用します。

00:16:20.000 --> 00:16:22.000
ただし、型制約が追加されます。

00:16:22.000 --> 00:16:32.000
Core MLはテンソルをより高い精度に昇格させる能力を保持しますが、Core MLランタイムは、MLプログラムで指定された精度よりも低い精度に中間テンソルをキャストすることはありません。

00:16:32.000 --> 00:16:45.000
型付き実行のためのこの新しいサポートは、特にGPU上のFloat32 opsとCPU上のFloat16で選択されたopsのために、GPUとCPUの両方で拡張されたopサポートによって可能になりました。

00:16:45.000 --> 00:16:54.000
この拡張サポートにより、MLプログラムがFloat32精度を指定しても、GPUのパフォーマンス上の利点を確認できます。

00:16:54.000 --> 00:17:00.000
異なる精度でMLプログラムを生成するために、統一されたコンバータAPIを試してみましょう。

00:17:00.000 --> 00:17:07.000
さて、私は今、インタラクティブな方法でPythonコードを実行するための便利なツールであるJupyterノートブックにいます。

00:17:07.000 --> 00:17:11.000
モデルを新しいMLプログラム形式に変換するプロセスについて確認します。

00:17:11.000 --> 00:17:14.000
今日使うモデルはスタイル転送モデルです。

00:17:14.000 --> 00:17:18.000
私はすでにオープンソースから事前訓練されたTensorflowモデルをダウンロードしました。

00:17:18.000 --> 00:17:22.000
このモデルは画像を取り込み、様式化された画像を生成します。

00:17:22.000 --> 00:17:24.000
最初に必要なのは、いくつかのインポートステートメントです。

00:17:24.000 --> 00:17:38.000
coremltools、Pythonイメージライブラリ、およびここで使用するコードを簡潔に保つために書いたいくつかのヘルパーライブラリと簡単なヘルパー関数をインポートします。

00:17:38.000 --> 00:17:43.000
次に、スタイル転送モデルのパスと、スタイル化する画像へのパスを指定します。

00:17:43.000 --> 00:17:46.000
変換の入力タイプも設定します。

00:17:46.000 --> 00:17:52.000
この場合、モデルがトレーニングされた画像の寸法を指定する画像入力タイプになります。

00:17:52.000 --> 00:18:00.000
最後に、Core MLモデルのポストコンバージョンを実行するために使用できる入力辞書を準備するための追加のセットアップがあります。

00:18:00.000 --> 00:18:03.000
したがって、入力がロードされ、ソースモデルが利用可能です。

00:18:03.000 --> 00:18:11.000
この時点で、すべての外部リソースはMLプログラムに変換する準備ができています。

00:18:11.000 --> 00:18:14.000
変換には、Unified Converter APIを使用します。

00:18:14.000 --> 00:18:17.000
最初の引数はソースモデルパスです。

00:18:17.000 --> 00:18:19.000
次に、入力タイプの配列を渡します。

00:18:19.000 --> 00:18:21.000
ここに1つだけあります。

00:18:21.000 --> 00:18:28.000
最後に、最小展開ターゲット引数は、Core ML ToolsがニューラルネットワークまたはMLプログラムを生成するかどうかを決定します。

00:18:28.000 --> 00:18:32.000
デフォルトはiOS 13で、ニューラルネットワークを生成します。

00:18:32.000 --> 00:18:37.000
今、私はMLプログラムを手に入れたいので、展開目標をiOS 15に設定します。

00:18:37.000 --> 00:18:40.000
最終的にはこのモデルをiOSアプリにデプロイしたい。

00:18:40.000 --> 00:18:47.000
ターゲットデバイスがMacであれば、macOS 12の展開ターゲットを指定することもできます。

00:18:47.000 --> 00:18:50.000
ShiftとEnterキーを押してモデルを変換します。

00:18:50.000 --> 00:18:53.000
そして、変換が完了しました。

00:18:53.000 --> 00:18:57.000
変換中にMLプログラムに対して自動的に行われるグラフ変換があります。

00:18:57.000 --> 00:19:02.000
それはFP16ComputePrecisionパスと呼ばれています。

00:19:02.000 --> 00:19:09.000
このグラフパスは、元のTensorflowグラフのすべてのFloat32テンソルをMLプログラムのFloat16テンソルにキャストします。

00:19:09.000 --> 00:19:15.000
さて、変換が終わったので、次のステップはMLプログラムの正確性を確認することです。

00:19:15.000 --> 00:19:23.000
両方のモデルで同じ画像で予測を呼び出すことで、出力数値を元のTensorflowモデルと比較することができます。

00:19:23.000 --> 00:19:32.000
MLプログラムには注目に値します。予測、モデル保存、その他のユーティリティには、前年とまったく同じCore ML Tools APIを使用しています。

00:19:32.000 --> 00:19:37.000
比較を行うために、私はすでに_get_coreml_tensorflow_outputというユーティリティメソッドを書きました。

00:19:37.000 --> 00:19:45.000
Tensorflowからの出力とCore MLからの出力を評価するために、複数のエラーメトリックを出力します。

00:19:45.000 --> 00:19:51.000
したがって、これは画像であるため、最も適切な誤差メトリックは、信号対雑音比、またはSNRである可能性があります。

00:19:51.000 --> 00:19:55.000
実際には、20または30を超えるSNRは通常、良い結果を示しています。

00:19:55.000 --> 00:19:59.000
ここで私は71のSNRを持っていますが、それはかなり素晴らしいです。

00:19:59.000 --> 00:20:04.000
他にもいくつかの指標があります：最大絶対誤差、平均絶対誤差。

00:20:04.000 --> 00:20:08.000
しかし、Float16を使用する際の精度コストはいくらですか？

00:20:08.000 --> 00:20:10.000
私は何を失いましたか?

00:20:10.000 --> 00:20:16.000
調べるには、Float16変換を無効にして再度変換できます。

00:20:16.000 --> 00:20:22.000
同じconvertコマンドを使用しますが、今回はcompute_precision引数を指定してFloat32に設定します。

00:20:22.000 --> 00:20:32.000
これにより、コンバータにFloat16キャストを注入しないように指示されるため、Core ML ToolsコンバータはFloat32 MLプログラムを生成します。

00:20:32.000 --> 00:20:41.000
さて、このFloat32 MLプログラムを元のTensorflowプログラムと比較します。

00:20:41.000 --> 00:20:50.000
そして、SNRは100以上に増加し、最大絶対誤差は約1から0.02に減少しました。

00:20:50.000 --> 00:20:55.000
Float16モデルで以前に発生したエラーが識別可能な影響を与えたかどうかはまだ答えていません。

00:20:55.000 --> 00:21:03.000
これはスタイル転送モデルであるため、出力画像の単純なプロットに基づいて評決を下すことができます。

00:21:03.000 --> 00:21:14.000
Float16 MLプログラム、Float32 MLプログラム、Tensorflowモデルの3つのモデルすべてから、ソース画像と様式化されたバージョンをプロットします。

00:21:14.000 --> 00:21:17.000
そして、3つのモデル出力の間に違いは見当たりません。

00:21:17.000 --> 00:21:24.000
もちろん、いくつかの指標と目視検査で一度、単一の画像のこの評価は、実際には単なる煙のテストです。

00:21:24.000 --> 00:21:26.000
物事は大丈夫に見えます。

00:21:26.000 --> 00:21:35.000
実際には、大規模なデータセット全体でより多くのエラーメトリックで評価し、機械学習モデルで使用されるパイプライン内の障害ケースを評価し、それらをトリアージします。

00:21:35.000 --> 00:21:44.000
小さなデータセットが手元にあり、この例をさらに一歩進めるために、データセット内の各画像の2つのMLプログラムとTensorflowモデルを比較することができます。

00:21:44.000 --> 00:21:53.000
Float32 MLプログラムとTensorflowのSNRはXsの赤い線として描かれ、Float16 MLプログラムは円の青い線です。

00:21:53.000 --> 00:22:00.000
Float32 MLプログラムは平均SNRが約100のようで、Float16 MLプログラムは約70のままです。

00:22:00.000 --> 00:22:05.000
Float16の精度は数値に少し影響しますが、このユースケースでは重要ではないようです。

00:22:05.000 --> 00:22:09.000
ただし、131の画像のこの小さなデータセットでも、いくつかの外れ値があります。

00:22:09.000 --> 00:22:13.000
全体として、モデルは期待されていることをかなりうまくやっています。

00:22:13.000 --> 00:22:16.000
そして、これは大部分の深層学習モデルの場合です。

00:22:16.000 --> 00:22:19.000
彼らは通常、Float16の精度でもうまく機能する傾向があります。

00:22:19.000 --> 00:22:24.000
そのため、Core MLコンバーターでデフォルトでFloat16変換をオンにしました。

00:22:24.000 --> 00:22:33.000
Float16型MLプログラムは、ニューラルエンジンで実行でき、パフォーマンスが大幅に向上し、消費電力を削減することができます。

00:22:33.000 --> 00:22:43.000
ランタイムは実行中にテンソルのタイプを最小精度として扱うため、Float32 MLプログラムはGPUとCPUのみの組み合わせで実行されます。

00:22:43.000 --> 00:22:51.000
このデモは、MLプログラムが変換時に正しく実行される最小精度を制御することがいかに簡単であるかを示しました。

00:22:51.000 --> 00:23:01.000
また、ニューラルネットワークCore MLモデルとは異なり、モデルがより高い精度を必要とする場合、それを達成するためにアプリコードでコンピューティングユニットの設定をcpuOnlyに変更する必要はありません。

00:23:01.000 --> 00:23:07.000
そして最後に、このデモノートブックは、Core ML Toolsのドキュメントサイトで例として入手できます。

00:23:07.000 --> 00:23:18.000
要約すると、MLプログラムを取得するには、変換関数を使用し、追加の引数を渡して展開ターゲットを指定し、少なくともiOS 15またはmacOS 12に設定します。

00:23:18.000 --> 00:23:25.000
デフォルトでは、Core MLコンバータは、ニューラルエンジンで実行できる最適化されたFloat16モデルを生成します。

00:23:25.000 --> 00:23:34.000
場合によっては発生する可能性があるため、モデルがFloat16精度に敏感な場合は、代わりに精度をFloat32に設定するのは簡単です。

00:23:34.000 --> 00:23:45.000
実際、Core ML Tools APIには、より高度なオプションがあります。これにより、Float32で実行する特定の操作を選択し、残りをFloat16に保持して混合タイプのMLプログラムを作成できます。

00:23:45.000 --> 00:23:48.000
これらの例については、ドキュメントをご覧ください。

00:23:48.000 --> 00:23:54.000
要約すると、Core MLには、モデルの調整と作業を容易にするいくつかの新しい機能強化があります。

00:23:54.000 --> 00:23:59.000
新しいMLShapedArrayタイプにより、多次元データの操作が容易になります。

00:23:59.000 --> 00:24:03.000
MLパッケージ形式を使用すると、Xcodeでメタデータを直接編集できます。

00:24:03.000 --> 00:24:14.000
新しいMLプログラムモデルタイプのMLパッケージは、GPUでFloat32をサポートするタイプ実行をサポートし、モデルのパフォーマンスと精度を調整する際により多くのオプションを提供します。

00:24:14.000 --> 00:24:18.000
モデルをMLパッケージにアップグレードし、MLプログラムを使用することをお勧めします。

00:24:18.000 --> 00:24:22.000
私たちのセッションを見てくれてありがとう、そしてWWDCの残りの部分を楽しんでください。

00:24:22.000 --> 23:59:59.000
[音楽]。

