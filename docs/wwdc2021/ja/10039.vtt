WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:13.000
こんにちは、「Create MLで手のポーズとアクションを分類する」へようこそ。

00:00:13.000 --> 00:00:15.000
私はネイサン・ワートマンです。

00:00:15.000 --> 00:00:18.000
そして今日、同僚のBrittany WeinertとGeppy Parzialeが参加します。

00:00:18.000 --> 00:00:22.000
今日は、ハンドポジションの分類について話します。

00:00:22.000 --> 00:00:25.000
しかし、それを掘り下げる前に、手そのものについて話しましょう。

00:00:25.000 --> 00:00:30.000
2ダース以上の骨、関節、筋肉で、手はエンジニアリングの驚異です。

00:00:30.000 --> 00:00:36.000
この複雑さにもかかわらず、手は幼児が周囲の世界と交流するために使用する最初のツールの1つです。

00:00:36.000 --> 00:00:42.000
赤ちゃんは、話すことができるようになる前に、簡単な手の動きを使ってコミュニケーションの基礎を学びます。

00:00:42.000 --> 00:00:45.000
話すことを学ぶと、私たちの手はコミュニケーションの役割を果たし続けます。

00:00:45.000 --> 00:00:49.000
彼らは強調と表現を加えることに移行します。

00:00:49.000 --> 00:00:55.000
昨年、私たちの手は人々を近づけるためにこれまで以上に重要になりました。

00:00:55.000 --> 00:01:05.000
2020年、ビジョンフレームワークは、開発者がフレーム内の手と手に存在する21の識別可能な関節のそれぞれを識別できるハンドポーズ検出を導入しました。

00:01:05.000 --> 00:01:15.000
手が存在するかどうか、または手がフレーム内のどこにあるかを特定しようとしている場合、これは素晴らしいツールですが、手が何をしているかを分類しようとすると、課題になる可能性があります。

00:01:15.000 --> 00:01:30.000
手の表現力は無限ですが、このセッションの残りの部分では、ストップ、静か、平和のような短い片手ポーズ、そしてバックアップ、離れて、ここに来るような短い片手の行動に焦点を当てたいと思います。

00:01:30.000 --> 00:01:33.000
私はちょうど手のポーズと行動について言及しました。

00:01:33.000 --> 00:01:36.000
もう少し具体的な定義はいかがですか?

00:01:36.000 --> 00:01:41.000
さて、一方では、静止画として意味のあるポーズがあります。

00:01:41.000 --> 00:01:46.000
これら2つのビデオは一時停止されていますが、主題の意図は明確に表現されています。

00:01:46.000 --> 00:01:48.000
ポーズをイメージのように考えてください。

00:01:48.000 --> 00:01:54.000
一方、私たちには、意味を完全に表現するために動きを必要とする行動があります。

00:01:54.000 --> 00:01:56.000
これら2つの行動の意味は不明です。

00:01:56.000 --> 00:01:59.000
単一のフレームを見るだけでは不十分です。

00:01:59.000 --> 00:02:06.000
しかし、ビデオやライブ写真など、時間の経過とともに一連のフレームでは、アクションの意味は明らかです。

00:02:06.000 --> 00:02:08.000
フレンドリーな「こんにちは」と「ここに来て」。

00:02:08.000 --> 00:02:17.000
それが明確になったので、今年は2つの新しいCreate MLテンプレート、Hand Pose ClassificationとHand Action Classificationを紹介することに興奮しています。

00:02:17.000 --> 00:02:25.000
これらの新しいテンプレートを使用すると、Create MLアプリまたはCreate MLフレームワークを使用して、ハンドポーズとアクションモデルをトレーニングできます。

00:02:25.000 --> 00:02:33.000
これらのモデルは、macOS Big Sur以降、iOSおよびiPadOS 14以降と互換性があります。

00:02:33.000 --> 00:02:38.000
そして、今年の新機能は、Create MLフレームワークを使用してiOSデバイスでモデルをトレーニングする機能を追加しました。

00:02:38.000 --> 00:02:45.000
詳細については、「Create ML Frameworkを使用して動的iOSアプリを構築する」セッションで詳しく知ることができます。

00:02:45.000 --> 00:02:54.000
まず、ビジョンフレームワークによって検出された手の位置を分類するために機械学習モデルを簡単にトレーニングできるハンドポーズ分類について話したいと思います。

00:02:54.000 --> 00:03:02.000
あなたはモデルのトレーニングを担当しているので、アプリがそのニーズに最も合うように分類すべきポーズを定義します。

00:03:02.000 --> 00:03:04.000
訓練されたモデルの簡単なデモをしましょう。

00:03:04.000 --> 00:03:09.000
シンプルなプロトタイプアプリから始めて、Hand Pose Classifierモデルを簡単に統合することができました。

00:03:09.000 --> 00:03:16.000
私のアプリは、手のポーズを分類し、対応する絵文字と分類されたポーズの自信を表示できるようになりました。

00:03:16.000 --> 00:03:20.000
ハンドポーズを1と2に分類します。

00:03:20.000 --> 00:03:25.000
しかし、認識していないすべてのポジションが背景の一部として分類されていることに気付くでしょう。

00:03:25.000 --> 00:03:29.000
これには、サポートを追加したいオープンパームポーズが含まれます。

00:03:29.000 --> 00:03:36.000
すぐにこのモデルを同僚のブリタニーに手渡し、ハンドポーズ分類モデルをアプリに統合する方法を紹介します。

00:03:36.000 --> 00:03:40.000
その前に、オープンパームポーズのサポートを追加したい。

00:03:40.000 --> 00:03:44.000
それは本当に簡単ですが、最初にモデルがどのように訓練されるかについて話すべきです。

00:03:44.000 --> 00:03:50.000
他のすべてのCreate MLプロジェクトと同様に、Hand Pose Classifierをアプリに統合するのは非常に簡単です。

00:03:50.000 --> 00:03:52.000
このプロセスには3つのステップがあります。

00:03:52.000 --> 00:03:58.000
トレーニングデータを収集して分類し、モデルをトレーニングし、モデルをアプリケーションに統合します。

00:03:58.000 --> 00:04:02.000
トレーニングデータの収集について話しましょう。

00:04:02.000 --> 00:04:05.000
ハンドポーズ分類器の場合、画像が必要になります。

00:04:05.000 --> 00:04:08.000
ポーズは画像として完全に表現力豊かであることを覚えておいてください。

00:04:08.000 --> 00:04:13.000
これらの画像は、画像に存在するポーズと一致する名前のフォルダに分類する必要があります。

00:04:13.000 --> 00:04:20.000
ここでは、特定したい2つのポーズがあります。1つ、2つ、そしてバックグラウンドクラスです。

00:04:20.000 --> 00:04:26.000
バックグラウンドクラスは、アプリが正しく識別することを気にしないポーズのためのキャッチオールカテゴリです。

00:04:26.000 --> 00:04:30.000
私のデモでは、これには1つまたは2つではない多くのハンドポジションが含まれています。

00:04:30.000 --> 00:04:36.000
明確に定義されたバックグラウンドクラスは、ユーザーが重要なポーズをとっていないときにアプリが知るのに役立ちます。

00:04:36.000 --> 00:04:41.000
背景クラスを構成する画像には2つのタイプがあります。

00:04:41.000 --> 00:04:47.000
まず、アプリに分類したい重要なポーズではないハンドポーズのランダムな品揃えがあります。

00:04:47.000 --> 00:04:53.000
これらのポーズには、肌のトーン、年齢、性別、照明条件の多様なセットを含める必要があります。

00:04:53.000 --> 00:04:58.000
第二に、アプリに分類したい表現と非常によく似た一連のポジションがあります。

00:04:58.000 --> 00:05:05.000
これらの過渡的なポーズは、ユーザーがアプリが気にかけている表現の1つに向かって手を動かしているときに頻繁に発生します。

00:05:05.000 --> 00:05:16.000
オープンパームのポーズをとるために手を挙げると、アプリがオープンパームを検討してほしいものと似ているが、そうではないいくつかの位置を移行していることに気付きます。

00:05:16.000 --> 00:05:19.000
これらの位置は、その後も腕を下げるときに発生します。

00:05:19.000 --> 00:05:21.000
これはオープンパームに特有のものではありません。

00:05:21.000 --> 00:05:28.000
腕を上げて2つのポーズをとるときも、それを下げるときも同じタイプの過渡的なポーズが起こります。

00:05:28.000 --> 00:05:34.000
これらの過渡的なポーズはすべて、ランダムなポーズとともにバックグラウンドクラスに追加する必要があります。

00:05:34.000 --> 00:05:43.000
この組み合わせにより、モデルはアプリが気にしているポーズと他のすべての背景のポーズを適切に区別できます。

00:05:43.000 --> 00:05:48.000
トレーニングデータが収集され、分類されたので、Create MLアプリを使用してモデルをトレーニングする時が来ました。

00:05:48.000 --> 00:05:50.000
だから、手を汚しましょう。

00:05:50.000 --> 00:05:56.000
前回のデモでモデルをトレーニングするために使用した既存のCreate MLプロジェクトから始めます。

00:05:56.000 --> 00:06:01.000
トレーニングの結果は良さそうだったので、このモデルはかなり良いパフォーマンスを発揮することを期待しています。

00:06:01.000 --> 00:06:07.000
幸いなことに、Create MLアプリでは、アプリに統合する前にモデルをプレビューできます。

00:06:07.000 --> 00:06:14.000
プレビュータブでは、ハンドポーズ分類器のために、このリリースでライブプレビュー機能が追加されていることがわかります。

00:06:14.000 --> 00:06:19.000
ライブプレビューは、FaceTimeカメラを利用して、リアルタイムで予測を表示します。

00:06:19.000 --> 00:06:25.000
ライブプレビューを使用して、このモデルがポーズ1と2を正しく分類していることを確認できます。

00:06:25.000 --> 00:06:32.000
また、Open Palmも正しく分類したいのですが、現在、そのポーズをバックグラウンドクラスの一部として分類しています。

00:06:32.000 --> 00:06:42.000
このモデルをトレーニングするために使用したデータソースでは、Open Palmクラスは含まれておらず、1、Two、およびBackgroundのクラスのみが含まれていることに注意してください。

00:06:42.000 --> 00:06:45.000
今すぐOpen Palmをサポートする新しいモデルをトレーニングしましょう。

00:06:45.000 --> 00:06:54.000
まず、これのための新しいモデルソースを作成します。

00:06:54.000 --> 00:06:59.000
このトレーニングに使用したいOpen Palmクラスを含むデータセットがあります。

00:06:59.000 --> 00:07:05.000
このデータセットを選択します。

00:07:05.000 --> 00:07:13.000
この新しいデータソースに飛び込むと、Open Palmのエントリと以前のデータセットのクラスが含まれていることがわかりました。

00:07:13.000 --> 00:07:23.000
モデルソースに戻って、トレーニングデータを拡張し、モデルをより堅牢にするために、いくつかの拡張を追加したいと思います。

00:07:23.000 --> 00:07:28.000
それでおそれ。電車に乗る時間です。

00:07:28.000 --> 00:07:34.000
トレーニングが始まる前に、Create MLは予備的な画像処理と特徴抽出を行う必要があります。

00:07:34.000 --> 00:07:37.000
私たちはCreate MLに80回の反復のために訓練するように言いました。

00:07:37.000 --> 00:07:41.000
これは良い出発点ですが、データセットに基づいてその数を微調整する必要があるかもしれません。

00:07:41.000 --> 00:07:43.000
このプロセスにはしばらく時間がかかります。

00:07:43.000 --> 00:07:46.000
幸いなことに、私はすでにモデルを訓練しました。

00:07:46.000 --> 00:07:48.000
今それをつかませてください。

00:07:48.000 --> 00:07:54.000
ライブプレビューは、新しく訓練されたモデルがオープンパームのポーズを正しく識別するようになったことを示しています。

00:07:54.000 --> 00:08:03.000
そして、念のため、私はそれが1つと2つのポーズを識別し続けることを確認するつもりです。

00:08:03.000 --> 00:08:05.000
簡単じゃなかった？

00:08:05.000 --> 00:08:10.000
私はこのモデルを同僚のブリタニーに送るつもりです、そして彼女はそれを彼女のアプリに統合することについて話します。

00:08:10.000 --> 00:08:12.000
ネイサン、モデルをありがとう。

00:08:12.000 --> 00:08:14.000
こんにちは。私はブリタニー・ワイナートです。

00:08:14.000 --> 00:08:17.000
そして、私はビジョンフレームワークチームのメンバーです。

00:08:17.000 --> 00:08:24.000
最初にハンドポーズの分類について学んだとき、私はすぐにこれを使って手で特殊効果を作成できると思いました。

00:08:24.000 --> 00:08:33.000
CoreMLを使用して手のポーズを分類し、ビジョンを使用して手を検出して追跡することは、一緒に使用するのに最適な技術であることを知っています。

00:08:33.000 --> 00:08:35.000
自分自身に超大国を与えることができるかどうか見てみましょう。

00:08:35.000 --> 00:08:40.000
私はすでにそれを行うことができるデモのためのパイプラインの最初のドラフトを作成しました。

00:08:40.000 --> 00:08:43.000
それを見直しましょう。

00:08:43.000 --> 00:08:54.000
まず、フレームのストリームを提供するカメラを用意し、各フレームをビジョンリクエストに使用して、フレーム内の手の位置とキーポイントを検出します。

00:08:54.000 --> 00:08:58.000
DetectHumanHandPoseRequestは、私たちが使用しているリクエストになります。

00:08:58.000 --> 00:09:03.000
フレーム内で見つかった各ハンドのHumanHandPoseObservationを返します。

00:09:03.000 --> 00:09:15.000
CoreMLハンドポーズ分類モデルに送信するデータは、MLMultiArrayとkeypointsMultiArrayと呼ばれるHumanHandPoseObservationのプロパティです。

00:09:15.000 --> 00:09:26.000
ハンドポーズ分類器は、信頼スコアでトップ推定ハンドアクションラベルを返送し、アプリ内のアクションを決定するために使用できます。

00:09:26.000 --> 00:09:30.000
アプリの高レベルの詳細を詳しく確認したので、コードを見てみましょう。

00:09:30.000 --> 00:09:36.000
ビジョンを使ってフレーム内の手を検出する方法から始めましょう。

00:09:36.000 --> 00:09:48.000
私たちがやりたいことは、VNDetectHumanHandPoseRequestのインスタンスが1つだけ必要で、片手を検出するだけでよいので、maximumHandCountを1つに設定します。

00:09:48.000 --> 00:09:56.000
maximumHandCountを設定し、フレームで指定されたよりも多くのハンドがある場合、アルゴリズムは代わりにフレーム内の最も目立つ中央の手を検出します。

00:09:56.000 --> 00:09:59.000
maximumHandCountのデフォルト値は2です。

00:09:59.000 --> 00:10:04.000
後でリクエストの更新に驚かないように、ここでリビジョンを設定することをお勧めします。

00:10:04.000 --> 00:10:13.000
しかし、リンクされているSDKでサポートされている最新のアルゴリズムを常にオプトインしたい場合は、設定する必要はありません。

00:10:13.000 --> 00:10:24.000
また、メモとして、ARKitを介してARSessionによって取得されたすべてのフレームの検出を行いますが、これはカメラフィードからフレームを取得する方法の1つにすぎません。

00:10:24.000 --> 00:10:27.000
どちらの方法でもかからかおうか。

00:10:27.000 --> 00:10:30.000
AVCaptureOutputも有用な代替手段になります。

00:10:30.000 --> 00:10:38.000
受信したフレームごとに、特定の画像のすべての要求を処理するVNImageRequestHandlerを作成する必要があります。

00:10:38.000 --> 00:10:49.000
ハンドポーズリクエストの結果プロパティには、以前のリクエストで指定したように、最大ハンド番号1までのVNHumanHandPoseObservationsが入力されます。

00:10:49.000 --> 00:10:55.000
リクエストで手のポーズが検出されない場合は、現在表示されているエフェクトをクリアすることをお勧めします。

00:10:55.000 --> 00:11:00.000
そうでなければ、片手で観察します。

00:11:00.000 --> 00:11:05.000
次に、CoreMLモデルを使用して、ハンドポーズが何であるかを予測します。

00:11:05.000 --> 00:11:12.000
効果のレンダリングを不安にしたくないので、すべてのフレームで予測したくありません。

00:11:12.000 --> 00:11:16.000
間隔をあけて予測を行うと、よりスムーズなユーザーエクスペリエンスが得られます。

00:11:16.000 --> 00:11:27.000
予測をしたいときは、MLMultiArrayをHand Pose CoreMLモデルに渡すことから始め、返された単一の予測からトップラベルと信頼を取得します。

00:11:27.000 --> 00:11:34.000
ラベルが高レベルの自信を持って予測された場合にのみ、表示される効果の変更をトリガーしたい。

00:11:34.000 --> 00:11:41.000
これはまた、効果があまりにも早くオンとオフになり、不安になる可能性のある行動から保護するための鍵です。

00:11:41.000 --> 00:11:49.000
ここでは、背景分類は、信頼のしきい値を非常に高く保つことで、私たちを助けています。

00:11:49.000 --> 00:11:53.000
非常に自信を持って予測されている場合は、レンダリングするeffectNodeを設定できます。

00:11:53.000 --> 00:12:00.000
自信を持って予測されない場合は、私の手がしていることに合わせて画面上の効果を止めたいです。

00:12:00.000 --> 00:12:02.000
私たちが持っているものを試してみましょう。

00:12:02.000 --> 00:12:08.000
私がワンポーズに手を入れると、それは単一のエネルギービーム効果をトリガーするはずです。

00:12:08.000 --> 00:12:09.000
とてもかっこいい！

00:12:09.000 --> 00:12:14.000
モデルは、私がポーズワンを作り、その効果を引き起こしたことがわかります。

00:12:14.000 --> 00:12:18.000
それが私の指に従えば、さらに涼しいでしょうが。

00:12:18.000 --> 00:12:22.000
それが私の指の特定のポイントでレンダリングされたら、さらに良いです。

00:12:22.000 --> 00:12:23.000
コードに戻って変更しましょう。 では変更しましょう。

00:12:23.000 --> 00:12:35.000
私たちがする必要があるのは、手の重要なポイントの位置をグラフィックアセットに供給することです。これは、ビューを使用して正規化されたキーポイントをカメラビュースペースに変換することを意味します。

00:12:35.000 --> 00:12:41.000
また、自信スコアを見て、保存するキーポイントの剪定を検討することもできます。

00:12:41.000 --> 00:12:44.000
ここでは、人差し指の指先しか気にしません。

00:12:44.000 --> 00:12:50.000
ビジョンは正規化された座標を使用するため、キーポイントを座標空間に変換する必要があります。

00:12:50.000 --> 00:12:57.000
また、ビジョンの原点は画像の左下隅にあるので、変換を行うときは覚えておいてください。

00:12:57.000 --> 00:13:03.000
最後に、インデックスの場所を保存し、キーポイントが見つからない場合、デフォルトはnilです。

00:13:03.000 --> 00:13:08.000
効果のレンダリングを担当するコードと、指をたどるように調整する方法を見てみましょう。

00:13:08.000 --> 00:13:13.000
グラフィカルオブジェクトの場所が設定されている場所を見つけたいです。

00:13:13.000 --> 00:13:18.000
setLocationForEffectsは、すべてのフレームを非同期に呼び出されています。

00:13:18.000 --> 00:13:23.000
デフォルトでは、ビューの中央に表示される効果を設定します。

00:13:23.000 --> 00:13:37.000
以前からindexFingerTipLocation CGPointに切り替えると、意図した効果が得られます。

00:13:37.000 --> 00:13:38.000
すごい！

00:13:38.000 --> 00:13:40.000
これはかっこよく見え始めています。

00:13:40.000 --> 00:13:43.000
もう一歩踏み出しましょう。

00:13:43.000 --> 00:13:52.000
超大国を取り巻くより興味深いグラフィカルなストーリーを作成するには、私たちのアプリケーションでハンドポーズ分類のいくつかを利用するのが良いでしょう。

00:13:52.000 --> 00:13:56.000
この場合、分類2とオープンパームを選択します。

00:13:56.000 --> 00:14:00.000
これらのポーズの両方が検出されたときに行動を起こすために、私はすでにアプリケーションを拡張しました。

00:14:00.000 --> 00:14:08.000
ここでは、前に示したように、ポーズワンのために、人差し指の先端に表示されるようにエネルギービームを集中させています。

00:14:08.000 --> 00:14:14.000
ポーズ2のために私の中指と人差し指の先端に2つのエネルギービーム。

00:14:14.000 --> 00:14:27.000
そして、最後のエネルギービームはハンドポーズオープンパームによってトリガーされ、中指の下部にあるキーポイントと手首のキーポイントの間に固定されています。

00:14:27.000 --> 00:14:28.000
わかった。

00:14:28.000 --> 00:14:35.000
ネイサンと私が紹介したものはすべて、あなた自身のハンドポーズ分類モデルを完全に統合する手順をカバーしています。

00:14:35.000 --> 00:14:44.000
ビジョンには、役に立つかもしれないもう1つの新機能がありますので、このアプリの機能のトリガーと制御に役立つAPIを紹介します。

00:14:44.000 --> 00:14:54.000
ビジョンは、ユーザーがHumanHand- PoseObservation: chiralityで左手と右手を区別できる新しいプロパティを導入しています。

00:14:54.000 --> 00:15:04.000
これは、HumanHandPoseObservationがどの手であるかを示す列挙型であり、左手、右手、不明の3つの値の1つである可能性があります。

00:15:04.000 --> 00:15:16.000
おそらく最初の2つの値の背後にある意味を推測できますが、未知の値は、古いバージョンのHumanHandPoseObservationがデシリアライズされ、プロパティが設定されていない場合にのみ表示されます。

00:15:16.000 --> 00:15:27.000
ネイサンが先に述べたように、WWDC 2020セッション「視覚で身体と手のポーズを検出する」を参照することで、ビジョンハンドポーズ検出に関する詳細情報を得ることができます。

00:15:27.000 --> 00:15:36.000
余談ですが、フレーム内で検出された各ハンドについて、基礎となるアルゴリズムは各ハンドのキラリティを別々に予測しようとします。

00:15:36.000 --> 00:15:42.000
これは、片手の予測がフレーム内の他の手の予測に影響を与えないことを意味します。

00:15:42.000 --> 00:15:47.000
キラリティを使用するコードがどのように見えるかの例をお見せしましょう。

00:15:47.000 --> 00:15:52.000
VNDetectHumanHandPoseRequestを作成して実行するためのセットアップについては、すでに説明しました。

00:15:52.000 --> 00:16:03.000
リクエストを実行した後、オブザベーションはEumプロパティのキラリティを持ち、それを使用してアクションを実行したり、ビジョンハンドポーズオブザベーションをソートしたりできます。

00:16:03.000 --> 00:16:07.000
これまでのすべては、ハンドポーズ分類の使い方についてでした。

00:16:07.000 --> 00:16:14.000
しかし、ネイサンが先に述べたように、ハンドアクション分類は今年のもう一つの新技術です。

00:16:14.000 --> 00:16:16.000
それについてあなたに話すためのゲッピーです。

00:16:16.000 --> 00:16:18.000
ありがとう、ブリタニー。

00:16:18.000 --> 00:16:23.000
こんにちは、私の名前はGeppy Parzialeで、Create MLチームの機械学習エンジニアです。

00:16:23.000 --> 00:16:36.000
ハンドポーズ分類に加えて、今年、Create MLはハンドアクション分類を実行するための新しいテンプレートを導入し、アプリでの使用方法を紹介します。

00:16:36.000 --> 00:16:47.000
このため、私はいくつかのハンドアクションでブルターニュの超大国のデモを拡張し、ハンドポーズとハンドアクションのいくつかの重要な違いを強調します。

00:16:47.000 --> 00:17:00.000
ハンドアクションとボディアクションは2つの非常によく似たタスクであるため、追加情報と比較については、WWDC 2020のセッション「Create MLでアクション分類器を構築する」を参照してください。

00:17:00.000 --> 00:17:05.000
しかし、今、ハンドアクションとは何かを説明しましょう。

00:17:05.000 --> 00:17:13.000
ハンドアクションは、MLモデルが手の動き中に分析する必要がある一連のハンドポーズで構成されています。

00:17:13.000 --> 00:17:20.000
シーケンス内のポーズの数は、最初から最後までハンドアクション全体をキャプチャするのに十分な大きさでなければなりません。

00:17:20.000 --> 00:17:23.000
ビデオを使用してハンドアクションをキャプチャします。

00:17:23.000 --> 00:17:33.000
ネイサンが先に示したように、ハンドアクション分類器のトレーニングは、ハンドポーズ分類器のトレーニングと同じですが、いくつかの小さな違いがあります。

00:17:33.000 --> 00:17:40.000
静止画像は手のポーズを表しますが、ビデオは手の動きをキャプチャして表現するために使用されます。

00:17:40.000 --> 00:17:48.000
したがって、ハンドアクション分類器を訓練するには、各ビデオがハンドアクションを表す短いビデオを使用します。

00:17:48.000 --> 00:17:54.000
これらのビデオは、各フォルダ名がアクションクラスを表すフォルダに整理できます。

00:17:54.000 --> 00:18:03.000
また、分類器に認識させたいアクションとは異なるアクションを持つビデオを含む背景クラスを含めることを忘れないでください。

00:18:03.000 --> 00:18:10.000
代替表現として、すべてのサンプルビデオファイルを1つのフォルダに追加できます。

00:18:10.000 --> 00:18:17.000
次に、CSVまたはJSON形式を使用して注釈ファイルを追加します。

00:18:17.000 --> 00:18:27.000
注釈ファイルの各エントリは、ビデオファイルの名前、関連するクラス、ハンドアクションの開始時刻と終了時刻を表します。

00:18:27.000 --> 00:18:31.000
また、この場合、バックグラウンドクラスを含めることを忘れないでください。

00:18:31.000 --> 00:18:36.000
多かれ少なかれ、同じ長さのビデオでモデルを訓練することを忘れないでください。

00:18:36.000 --> 00:18:48.000
実際、アクション期間をトレーニングパラメータとして指定し、Create MLは、提供した値に従って連続した数のフレームをランダムにサンプリングします。

00:18:48.000 --> 00:18:53.000
また、ビデオフレームレートとトレーニングの反復を提供することもできます。

00:18:53.000 --> 00:19:03.000
それに加えて、このアプリは、モデルがよりよく一般化し、精度を高めるのに役立つさまざまな種類のデータ拡張を提供します。

00:19:03.000 --> 00:19:15.000
特に、タイム補間とフレームドロップは、実際のユースケースに近いビデオバリエーションを提供するために、ハンドアクション分類に追加された2つの拡張です。

00:19:15.000 --> 00:19:20.000
だから、私はすでに私のデモのためにハンドアクション分類器を訓練しました--それを実際に見てみましょう。

00:19:20.000 --> 00:19:25.000
さて、私はスーパーヒーローなので、エネルギー源が必要です。

00:19:25.000 --> 00:19:27.000
これが私のです。

00:19:27.000 --> 00:19:31.000
ここでは、手のポーズを使ってエネルギー源を視覚化します。

00:19:31.000 --> 00:19:37.000
しかし、今、私の超能力を使ってそれを活性化させてください。

00:19:37.000 --> 00:19:40.000
この場合、私はハンドアクションを使っています。

00:19:40.000 --> 00:19:44.000
これはかっこいい。

00:19:44.000 --> 00:19:50.000
そして今、ハンドポーズとハンドアクション分類器が同時に実行されています。

00:19:50.000 --> 00:20:03.000
私はビジョンの新しいキラリティ機能を活用し、左手を手のポーズに、右手を手のアクションに使っています。

00:20:03.000 --> 00:20:05.000
これは超かっこいい。

00:20:05.000 --> 00:20:15.000
したがって、これはCreate MLが適用される最適化、Apple Neural Engineのすべてのパワーを解き放つためのすべてのモデルへのトレーニング時間のために可能です。

00:20:15.000 --> 00:20:25.000
そして今、私の現実の世界に戻って、Create ML Hand Action Classifierを私のデモに統合する方法を説明します。

00:20:25.000 --> 00:20:28.000
まず、モデルの入力を見てみましょう。 

00:20:28.000 --> 00:20:37.000
ハンドアクション分類器をアプリに統合するときは、モデルに正しい数の予想されるハンドポーズを提供する必要があります。

00:20:37.000 --> 00:20:44.000
私のモデルは、XCodeプレビューで検査できるので、サイズ45 x 3 x 21のマルチアレイを期待しています。

00:20:44.000 --> 00:20:52.000
ここでは、45は分類器がアクションを認識するために分析する必要があるポーズの数です。

00:20:52.000 --> 00:20:57.000
21は、各ハンドのビジョンフレームワークによって提供されるジョイントの数です。

00:20:57.000 --> 00:21:03.000
最後に、3はx座標とy座標と各ジョイントの信頼値です。

00:21:03.000 --> 00:21:06.000
45はどこから来たのですか?

00:21:06.000 --> 00:21:14.000
これは予測ウィンドウのサイズであり、トレーニング時に使用されたビデオの長さとフレームレートに依存します。

00:21:14.000 --> 00:21:22.000
私の場合、私は30fpsと1.5秒の長さで録画されたビデオでモデルを訓練することにしました。

00:21:22.000 --> 00:21:32.000
これは、モデルがハンドアクションごとに45のビデオフレームで訓練されたことを意味するので、推論中、モデルは同じ数のハンドポーズを期待しています。

00:21:32.000 --> 00:21:41.000
推論時に到着する手のポーズの頻度に関して、追加の考慮を考慮する必要があります。

00:21:41.000 --> 00:21:50.000
推論中にモデルに提示されたハンドポーズのレートが、モデルをトレーニングするために使用されるポーズのレートと一致することが非常に重要です。

00:21:50.000 --> 00:21:53.000
私のデモでは、ARKitを使いました。

00:21:53.000 --> 00:22:05.000
ARKitは60fpsでフレームを提供し、私の分類器は30fpsのビデオで訓練されたので、私は毎秒ごとに到着するポーズの数を半分にしなければなりませんでした。

00:22:05.000 --> 00:22:10.000
そうでなければ、分類器は間違った予測を提供する可能性があります。

00:22:10.000 --> 00:22:15.000
ソースコードに飛び込んで、これを実装する方法を紹介しましょう。

00:22:15.000 --> 00:22:30.000
まず、カウンターを使用して、ビジョンから到着するポーズのレートを60fpsから30fpsに減らし、モデルが正常に動作することを期待しているフレームレートに一致させます。

00:22:30.000 --> 00:22:37.000
次に、シーン内の各手の関節とキラリティを含む配列を取得します。

00:22:37.000 --> 00:22:48.000
次に、デモでは右手を使ってハンドアクションで効果の一部をアクティブにするので、左手からキーポイントを破棄します。

00:22:48.000 --> 00:22:52.000
さて、今、私は分類器のために手のポーズを蓄積する必要があります。

00:22:52.000 --> 00:23:03.000
これを行うには、FIFOキューを使用して45のハンドポーズを蓄積し、キューに常に最後の45のポーズが含まれていることを確認します。

00:23:03.000 --> 00:23:05.000
キューは最初は空です。

00:23:05.000 --> 00:23:12.000
新しいハンドポーズが到着したら、それをキューに追加し、キューがいっぱいになるまでこのステップを繰り返します。

00:23:12.000 --> 00:23:16.000
キューがいっぱいになったら、そのコンテンツ全体を読み始めることができます。

00:23:16.000 --> 00:23:21.000
ビジョンから新しい手のポーズを受け取るたびにキューを読むことができました。

00:23:21.000 --> 00:23:25.000
しかし、覚えておいてください、今、私は毎秒30フレームを処理しています。

00:23:25.000 --> 00:23:31.000
そして、ユースケースによっては、これはリソースの無駄になる可能性があります。

00:23:31.000 --> 00:23:37.000
そのため、フレーム数を定義した後、別のカウンターを使用してキューを読み取ります。

00:23:37.000 --> 00:23:49.000
アプリケーションの応答性と取得したい1秒あたりの予測数の間のトレードオフとして、キューサンプリングレートを選択する必要があります。

00:23:49.000 --> 00:24:02.000
この時点で、MLMultiArrayで整理された45のハンドポーズのシーケンス全体を読み、それを分類器に入力してハンドアクションを予測します。

00:24:02.000 --> 00:24:07.000
次に、予測ラベルと信頼値を抽出します。

00:24:07.000 --> 00:24:15.000
最後に、信頼値が定義されたしきい値よりも大きい場合は、パーティクル効果をシーンに追加します。

00:24:15.000 --> 00:24:27.000
したがって、Create ML Hand Action Classifierをアプリに統合するときは、モデルが期待するフレームレートでハンドポーズのシーケンスを必ず入力してください。

00:24:27.000 --> 00:24:32.000
分類器のトレーニングに使用されたビデオと同じフレームレートを一致させます。

00:24:32.000 --> 00:24:39.000
ファーストインファーストアウトキューを使用して、モデル予測へのハンドポーズを収集します。

00:24:39.000 --> 00:24:42.000
適切なフレームレートでキューを読み取ります。

00:24:42.000 --> 00:24:49.000
Create MLで訓練されたハンドアクションモデルで構築するすべてのクールなアプリケーションを見るのを楽しみにしています。

00:24:49.000 --> 00:24:54.000
そして今、最終的な検討と要約のためにネイサンに戻ります。

00:24:54.000 --> 00:24:55.000
ありがとう、ジェッピー。

00:24:55.000 --> 00:24:57.000
あなたとブリタニーはあなたのアプリで素晴らしい仕事をしました。

00:24:57.000 --> 00:24:59.000
試してみるのが楽しみです。

00:24:59.000 --> 00:25:05.000
しかし、物事が手に負えなくなる前に、ユーザーに高品質の体験を確実にするために心に留めておくべきことがいくつかあります。

00:25:05.000 --> 00:25:08.000
手がカメラからどれだけ離れているかに注意してください。

00:25:08.000 --> 00:25:12.000
最良の結果を得るには、距離を11フィート、または3 1/2メートル以下に保つ必要があります。

00:25:12.000 --> 00:25:17.000
また、暗すぎるか明るすぎるか、極端な照明条件を避けるのが最善です。

00:25:17.000 --> 00:25:25.000
かさばる、緩い、またはカラフルな手袋は、手のポーズを正確に検出することが困難になり、分類品質に影響を与える可能性があります。

00:25:25.000 --> 00:25:29.000
すべての機械学習タスクと同様に、トレーニングデータの質と量が重要です。

00:25:29.000 --> 00:25:34.000
このセッションで示されたハンドポーズ分類器では、クラスごとに500枚の画像を使用しました。

00:25:34.000 --> 00:25:41.000
ハンドアクション分類器では、クラスごとに100本のビデオを使用しましたが、ユースケースのデータ要件は異なる場合があります。

00:25:41.000 --> 00:25:48.000
最も重要なことは、モデルがアプリに表示される予想されるバリエーションをキャプチャするのに十分なトレーニングデータを収集することです。

00:25:48.000 --> 00:25:50.000
今、要約するのに良い時期のように感じます。

00:25:50.000 --> 00:25:52.000
それで、私たちは何を学びましたか?

00:25:52.000 --> 00:25:57.000
さて、2021年から、人間の手の表情を解釈するアプリを構築できます。

00:25:57.000 --> 00:26:02.000
私たちは、手の表情、ポーズ、行動の2つのカテゴリーの違いについて議論しました。

00:26:02.000 --> 00:26:08.000
モデルをトレーニングするためにCreate MLアプリで使用するために、バックグラウンドクラスを含むトレーニングデータを準備する方法について話しました。

00:26:08.000 --> 00:26:12.000
訓練を受けたモデルをアプリに統合する方法について話しました。

00:26:12.000 --> 00:26:19.000
そして最後に、複数のモデルを1つのアプリに組み込み、キラリティを使用して手を区別することについて話しました。

00:26:19.000 --> 00:26:22.000
明らかに、今日のデモは表面を傷つけるだけです。

00:26:22.000 --> 00:26:28.000
ビジョンフレームワークは、手の存在、ポーズ、位置、キラリティを検出するための強力な技術です。

00:26:28.000 --> 00:26:33.000
Create MLは、手のポーズと手のアクションを訓練し、分類する楽しくて簡単な方法です。

00:26:33.000 --> 00:26:41.000
一緒に使用すると、人類の最も強力で表現力豊かなツールの1つに深い洞察を提供し、あなたが彼らと何をするかを見るのが待ちきれません。

00:26:41.000 --> 00:26:43.000
さようなら。

00:26:43.000 --> 23:59:59.000
[明るい音楽]。

