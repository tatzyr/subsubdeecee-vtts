WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:11.000
こんにちは。

00:00:11.000 --> 00:00:13.000
WWDCのサウンド分析セッションへようこそ。

00:00:13.000 --> 00:00:15.000
私の名前はジョン・フアンです。

00:00:15.000 --> 00:00:17.000
私はオーディオチームの研究者です。

00:00:17.000 --> 00:00:25.000
今日、同僚のケビンと私は、SoundAnalysisフレームワークとCreateMLを通じて利用可能なサウンド分類の強化を紹介します。

00:00:25.000 --> 00:00:31.000
2019年には、CreateMLを使用してサウンド分類モデルをトレーニングできるようにしました。

00:00:31.000 --> 00:00:37.000
サウンド分類モデルを作成し、Appleデバイスに展開するのが簡単であることを示しました。

00:00:37.000 --> 00:00:45.000
このフレームワークを使用すると、すべての計算がハードウェアアクセラレーション用に最適化され、ローカルでデバイス上で行われます。

00:00:45.000 --> 00:00:50.000
これは、オーディオがクラウドに送信されないため、ユーザーのプライバシーを保護するのに役立ちます。

00:00:50.000 --> 00:00:56.000
サウンド分析フレームワークを活用して、サウンド認識と呼ばれるアクセシビリティ機能を導入しました。

00:00:56.000 --> 00:01:06.000
この機能は、アラーム、ペット、その他の家庭の音などの環境で特定の音が聞こえたときにユーザーに通知を提供できます。

00:01:06.000 --> 00:01:09.000
これは音の分類の1つのアプリケーションにすぎません。

00:01:09.000 --> 00:01:12.000
それで他に何ができるか見てみましょう。

00:01:12.000 --> 00:01:17.000
デモアプリは、私のMacの内蔵マイクを使って、環境の音を聴いています。

00:01:17.000 --> 00:01:24.000
音声分類器にオーディオを渡し、分類結果をUIに表示しています。

00:01:24.000 --> 00:01:27.000
だから、私があなたとチャットしているとき、音声が検出されます。

00:01:27.000 --> 00:01:30.000
少し私と一緒に座って、何が起こるか見てください。

00:01:30.000 --> 00:01:32.000
快適に過ごしてください。

00:01:32.000 --> 00:01:34.000
音楽から始めましょう。

00:01:34.000 --> 00:01:38.000
ねえSiri、KarunとMombruの「Catch a Vibe」をプレイして。

00:01:38.000 --> 00:01:41.000
現在、KarunとMombruの「Catch a Vibe」を演奏しています。

00:01:41.000 --> 00:01:46.000
♪ ♪

00:01:46.000 --> 00:01:49.000
♪ああ、ああ、ああ♪

00:01:49.000 --> 00:01:53.000
分類器が音楽と歌の音の両方を拾っていることに注意してください。

00:01:53.000 --> 00:01:56.000
♪私は誰も知りません♪

00:01:56.000 --> 00:01:59.000
♪誰がそんなに正しいと感じているのか♪

00:01:59.000 --> 00:02:01.000
♪私を作る人を誰も知らない♪

00:02:01.000 --> 00:02:03.000
♪雰囲気をキャッチし、周波数を感じる...♪

00:02:03.000 --> 00:02:05.000
さあ、私と一緒にお茶を飲んでください。

00:02:05.000 --> 00:02:07.000
♪大丈夫だよ、ベイビー♪

00:02:07.000 --> 00:02:08.000
♪私たちは見ることができました♪

00:02:08.000 --> 00:02:14.000
♪私たちが正しければ♪

00:02:14.000 --> 00:02:18.000
♪あなたの笑顔の仕方について何かあるかどうか確かめてください♪

00:02:18.000 --> 00:02:20.000
♪息を止めて、ベイビー、私たちはできるよ♪

00:02:20.000 --> 00:02:23.000
♪雰囲気をキャッチし、周波数を感じる♪

00:02:23.000 --> 00:02:25.000
♪雰囲気をつかんで、あなたと私だけ♪

00:02:25.000 --> 00:02:27.000
♪大丈夫、ベイビー、私たちは見ることができた♪

00:02:27.000 --> 00:02:29.000
♪もし私たちがなれたら♪

00:02:29.000 --> 00:02:33.000
♪私は言った、私は言った♪

00:02:33.000 --> 00:02:35.000
♪それは私の一部です♪

00:02:35.000 --> 00:02:37.000
♪あなたが見るすべての部分♪

00:02:37.000 --> 00:02:38.000
♪あなたは誇りに思うべきです...♪

00:02:38.000 --> 00:02:42.000
これはおいしいお茶です。

00:02:42.000 --> 00:02:45.000
♪あなたは私の赤ちゃんです、ジー♪

00:02:45.000 --> 00:02:47.000
♪あなたはカートです、なるほど...♪

00:02:47.000 --> 00:02:50.000
♪どうしてこうなるの？♪

00:02:50.000 --> 00:02:51.000
♪どうしてこうなるの?...♪

00:02:51.000 --> 00:02:54.000
ねえ、Siri。止まる。

00:02:54.000 --> 00:03:03.000
さて、これらのサウンドカテゴリごとにいくつかのデータを収集し、CreateMLを使用してカスタムモデルをトレーニングしたと仮定するのが妥当です。

00:03:03.000 --> 00:03:09.000
はい、それはできましたが、実際には、使用される分類器が組み込まれています。

00:03:09.000 --> 00:03:15.000
今年は初めてですが、サウンド分析フレームワークにサウンド分類器が組み込まれています。

00:03:15.000 --> 00:03:19.000
アプリでサウンド分類を有効にするのは、かつてないほど簡単になりました。

00:03:19.000 --> 00:03:24.000
すべてのプラットフォームでサポートされています。

00:03:24.000 --> 00:03:28.000
組み込みの分類器は、開発者のエクスペリエンスを簡素化するためのものです。

00:03:28.000 --> 00:03:40.000
高精度モデルを開発するために、大量のデータ、専門的な機械学習とオーディオの専門知識、および多くの計算能力を収集し、ラベル付けする必要がなくなります。

00:03:40.000 --> 00:03:48.000
これらの詳細を心配する時間が少なければ少ないほど、アプリのユーザーエクスペリエンスを豊かにするのにより多くの時間を費やすことができます。

00:03:48.000 --> 00:03:52.000
健全な分類を可能にするには、数行のコードしかかかりません。

00:03:52.000 --> 00:03:54.000
この分類器で何ができるかをお見せします。

00:03:54.000 --> 00:03:58.000
あなたが使用できる300以上のカテゴリがあります。

00:03:58.000 --> 00:04:00.000
詳しく見てみましょう。 

00:04:00.000 --> 00:04:09.000
家畜、家畜、野生動物の音を分類できます。

00:04:09.000 --> 00:04:18.000
音楽については、キーボード楽器、打楽器、弦楽器、管楽器など、多くの楽器を認識できます。

00:04:18.000 --> 00:04:27.000
グループ活動、呼吸音、発声など、さまざまな人間の音を検出できます。

00:04:27.000 --> 00:04:33.000
それから、車、アラーム、道具、液体などの音があります。

00:04:33.000 --> 00:04:37.000
これらのすぐに使用できるカテゴリは、あなたが試すことができます。

00:04:37.000 --> 00:04:42.000
ケビンに渡して、このサウンド分類器の使い方を説明します。

00:04:42.000 --> 00:04:44.000
ありがとう、ジョン。

00:04:44.000 --> 00:04:49.000
こんにちは、私はオーディオチームのソフトウェアエンジニアのケビンです。

00:04:49.000 --> 00:04:56.000
私が構築した小さなアプリケーションを見て、新しい内蔵のサウンド分類器の使い方をお見せしたいと思います。

00:04:56.000 --> 00:05:04.000
ジョンが分類器がカウベルでどれほどうまく機能するかを示すことを望んでいましたが、彼のデモに収まらなかったので、私は別のアイデアを思いつきました。

00:05:04.000 --> 00:05:13.000
このセッションの準備中に収集したメディアがMacにいくつかあり、カウベルを含む古い映像があると確信しています。

00:05:13.000 --> 00:05:18.000
あなたに見せたいのですが、まずそれを見つけなければなりません。

00:05:18.000 --> 00:05:26.000
これは、適切なファイルを見つけなければならず、適切な部分を見つけるために中を見なければならないことを意味します。

00:05:26.000 --> 00:05:29.000
では、どうすればいいのでしょうか?

00:05:29.000 --> 00:05:38.000
内蔵のサウンド分類器を使用してファイルを読み取り、特定の音が中にあるかどうか教えてくれる簡単なプログラムを作成します。

00:05:38.000 --> 00:05:47.000
音が見つかった場合、プログラムは分類器を使用して、それが発生した時間を伝えることができます。

00:05:47.000 --> 00:05:57.000
その後、macOSで利用可能なショートカットを使用して、多くのファイルでプログラムを実行するワークフローを作成できます。

00:05:57.000 --> 00:06:07.000
私のプログラムが音を見つけると、ワークフローは自動的に報告された検出時間を使用して、音を含むビデオクリップを抽出することができます。

00:06:07.000 --> 00:06:11.000
これはカウベルクリップを見つけるのに最適です。

00:06:11.000 --> 00:06:15.000
だから、それを実際に見てみましょう。

00:06:15.000 --> 00:06:19.000
ここには、ビデオでいっぱいのフォルダがあります。

00:06:19.000 --> 00:06:27.000
すべてのファイルを選択し、クイックアクションメニューを使用してショートカットを開始します。

00:06:27.000 --> 00:06:35.000
見つけたい音を選択するように頼まれたので、オプションのリストからカウベルを選択します。

00:06:35.000 --> 00:06:45.000
今、私のショートカットは実行されており、ほんの数分後、それは私が探している音を見つけ、ファインダーウィンドウで私にそれを表示します。

00:06:45.000 --> 00:06:47.000
見てみましょう。 

00:06:47.000 --> 00:06:54.000
元気そうだね、ジョン。

00:06:54.000 --> 00:06:59.000
私が使ったショートカットを詳しく見てみましょう。 

00:06:59.000 --> 00:07:06.000
ショートカットが始まると、認識できるすべての音のリストが収集され、1つを選ぶように求められます。

00:07:06.000 --> 00:07:15.000
私の選択を使用して、Finderで選択した各ファイルにアクセスし、内部の音を探します。

00:07:15.000 --> 00:07:25.000
音が検出されると、音が発生した頃にビデオから数秒を抽出し、結果のクリップを表示します。

00:07:25.000 --> 00:07:31.000
これらのステップのうち、そのうちの2つは組み込みの分類器を使用しています。

00:07:31.000 --> 00:07:41.000
これらは、ショートカットがオンデマンドで使用するために、私自身のカスタムアプリケーションに実装した手順です。

00:07:41.000 --> 00:07:53.000
ショートカットについては詳しく説明しませんが、もっと知りたい場合は、「Meet Shortcuts for macOS」というタイトルのWWDCセッションを参照してください。

00:07:53.000 --> 00:07:59.000
それでは、私のアプリの2つのカスタムアクションの実装を見てみましょう。

00:07:59.000 --> 00:08:04.000
最初のアクションは、アプリが認識できるすべてのサウンドを報告します。

00:08:04.000 --> 00:08:11.000
アプリは内蔵のサウンド分類器を使用しているため、数百の音を認識できます。

00:08:11.000 --> 00:08:15.000
これは私がこれらの音を得るために書いた関数です。

00:08:15.000 --> 00:08:24.000
組み込みの分類器を選択できる新しい初期化子を使用してSNClassifySoundRequestを作成します。

00:08:24.000 --> 00:08:31.000
このリクエストを受け取ったら、それを使用して、分類器がサポートするサウンドのリストを照会できます。

00:08:31.000 --> 00:08:37.000
アプリの2番目のアクションは、ファイル内で音が聞こえたときにショートカットに伝えます。

00:08:37.000 --> 00:08:46.000
これを実装するために、私は音の分類を実行し、音がまったく検出された場合、音が最初に検出されたときに報告します。

00:08:46.000 --> 00:08:51.000
音の分類を実行するには、3つのオブジェクトを準備する必要があります。

00:08:51.000 --> 00:08:59.000
まず、サウンド分類の設定に使用できるSNClassifySoundRequestが必要です。

00:08:59.000 --> 00:09:08.000
次に、特定のファイルに分類をターゲットにできるSNAudioFileAnalyzerが必要です。

00:09:08.000 --> 00:09:12.000
3番目のオブジェクトには、少し特別な注意が必要です。

00:09:12.000 --> 00:09:18.000
分類の結果を処理する独自のオブザーバータイプを定義する必要があります。

00:09:18.000 --> 00:09:25.000
今のところオブザーバーをスキップして、ここにこれらのオブジェクトの最初の2つを準備するためのいくつかのコードがあります。

00:09:25.000 --> 00:09:38.000
組み込みの分類器を使用してSNClassifySoundRequestを作成し、分類したいファイルのURLを使用してSNAudioFileAnalyzerを作成できます。

00:09:38.000 --> 00:09:50.000
この時点で、オブザーバーの準備ができたら、音の分類を開始するのは簡単ですが、オブザーバーを定義することは欠けている部分です。

00:09:50.000 --> 00:09:52.000
だから、そうしましょう。

00:09:52.000 --> 00:10:01.000
私はここで、NSObjectを継承し、SNResultsObservingプロトコルに準拠した裸のObserverから始めます。

00:10:01.000 --> 00:10:11.000
検索したいサウンドラベルでインスタンスを初期化し、CMTimeメンバー変数を追加して、サウンドを検出した時間を格納します。

00:10:11.000 --> 00:10:18.000
リクエストを実装するだけです：didProduce結果メソッド。

00:10:18.000 --> 00:10:29.000
このメソッドは、健全な分類によって結果が生成されたときに呼び出されるので、SNClassificationResultのインスタンスを受け取ることを期待しています。

00:10:29.000 --> 00:10:39.000
結果のclassificationForIdentifierメソッドを使用して、探しているラベルに関する情報を抽出できます。

00:10:39.000 --> 00:10:49.000
ラベルに関連する信頼度スコアを照会し、そのスコアが特定のしきい値を超えた場合は、音が検出されたとみなします。

00:10:49.000 --> 00:10:59.000
初めて検出に気付いたら、後でショートカットに提供できるように、音が発生した時間を節約します。

00:10:59.000 --> 00:11:07.000
それだけで、私のオブザーバーは完成し、ファイル内でいつ音が鳴るかを判断するために必要なすべてのピースを持っています。

00:11:07.000 --> 00:11:12.000
この例では、さらに議論したい2つの重要なトピックに触れています。

00:11:12.000 --> 00:11:20.000
まず、検出時間について話し、次に検出しきい値について話します。

00:11:20.000 --> 00:11:24.000
検出時間から始めましょう。

00:11:24.000 --> 00:11:30.000
オーディオを分類すると、信号は重なり合うウィンドウに分割されます。

00:11:30.000 --> 00:11:38.000
これらの各ウィンドウについて、どの音が検出され、どの程度自信を持って検出されたかを示す結果が得られます。

00:11:38.000 --> 00:11:45.000
また、オーディオのどの部分が分類されたかを示す時間範囲も取得されます。

00:11:45.000 --> 00:11:52.000
私のアプリでは、音を検出すると、結果の時間範囲を使用して、音がいつ発生したかを判断します。

00:11:52.000 --> 00:11:57.000
しかし、ウィンドウの期間が変わると、時間範囲が影響を受ける可能性があります。

00:11:57.000 --> 00:12:06.000
ウィンドウの期間をカスタマイズして、ユースケースに基づいて大きくまたは小さくすることができます。

00:12:06.000 --> 00:12:12.000
短いウィンドウは、ドラムタップのような短い音で作業するときにうまく機能します。

00:12:12.000 --> 00:12:19.000
これは、小さな時間枠でその音の重要な機能をすべて捉えることができるからです。

00:12:19.000 --> 00:12:23.000
小さな窓は重要な情報を切り取らない。

00:12:23.000 --> 00:12:33.000
小さなウィンドウの持続時間を使用する利点は、音が発生した瞬間をより密接に特定できることです。

00:12:33.000 --> 00:12:38.000
しかし、より長い音で作業する場合、小さなウィンドウの持続時間は適切ではないかもしれません。

00:12:38.000 --> 00:12:46.000
たとえば、サイレンには、長期間にわたって上昇ピッチと下降ピッチの両方が含まれている場合があります。

00:12:46.000 --> 00:12:54.000
これらすべてのピッチを1つのウィンドウにまとめると、音の分類が音を正しく検出するのに役立ちます。

00:12:54.000 --> 00:13:02.000
一般的に、興味のあるサウンドの重要な部分をすべてキャプチャするのに十分な長さのウィンドウ期間を使用するのは良いことです。

00:13:02.000 --> 00:13:10.000
ウィンドウの期間を編集したい場合は、SNClassifySoundRequestのwindowDurationプロパティを設定できます。

00:13:10.000 --> 00:13:16.000
ただし、すべてのウィンドウ期間がサポートされているわけではないことに注意してください。

00:13:16.000 --> 00:13:20.000
異なる分類器は、異なるウィンドウ期間をサポートする場合があります。

00:13:20.000 --> 00:13:28.000
SNClassifySoundRequestのwindowDurationConstraintプロパティを読むことで、サポートされているウィンドウ期間を確認できます。

00:13:28.000 --> 00:13:34.000
内蔵の分類器は、1/2秒から15秒のウィンドウ持続時間をサポートします。

00:13:34.000 --> 00:13:43.000
1秒以上の持続時間は、アプリで分類器を採用する際の素晴らしい出発点です。

00:13:43.000 --> 00:13:47.000
次に、信頼のしきい値について話しましょう。

00:13:47.000 --> 00:13:56.000
私のアプリでは、その音に対する自信が固定されたしきい値を超えると、いつでも音が検出されると考えました。

00:13:56.000 --> 00:14:05.000
私はしきい値に0.5の値を選択しましたが、自分のアプリのしきい値を選択する際に考慮すべきことがいくつかあります。

00:14:05.000 --> 00:14:10.000
分類器は同時に複数の音を検出できます。

00:14:10.000 --> 00:14:17.000
これが起こると、いくつかのラベルが自信を持って採点していることに気付くかもしれません。

00:14:17.000 --> 00:14:26.000
CreateMLを使用して訓練されたカスタムモデルを使用する場合とは異なり、ラベルスコアは1の値に加算されません。

00:14:26.000 --> 00:14:32.000
信頼は独立しており、互いに比較すべきではありません。

00:14:32.000 --> 00:14:42.000
信頼スコアは独立しているため、異なるサウンドに対して異なる信頼しきい値を選択すると便利かもしれません。

00:14:42.000 --> 00:14:44.000
しきい値を選択するにはトレードオフが必要です。

00:14:44.000 --> 00:14:58.000
信頼しきい値が高いと、音が誤って検出される確率が低下しますが、十分に強くなかったため、真の検出が見逃される可能性も高まります。

00:14:58.000 --> 00:15:06.000
アプリケーションのしきい値を選択するときは、ユースケースでこれらの要因の適切なバランスを達成する値を見つける必要があります。

00:15:06.000 --> 00:15:15.000
カスタムウィンドウの期間を設定すると、信頼スコアが変化する可能性があるため、しきい値にも影響する可能性があることに注意してください。

00:15:15.000 --> 00:15:21.000
組み込みの分類器を使用する際に覚えておくべき最後のことの1つは、いくつかの音が似ているということです。

00:15:21.000 --> 00:15:32.000
分類器が識別できる多数の音の中には、人間であっても、オーディオ単独で区別するのが難しい音のグループがいくつかあります。

00:15:32.000 --> 00:15:38.000
可能であれば、あなたが注意を払う音について選択するのが最善です。

00:15:38.000 --> 00:15:46.000
アプリが使用されるコンテキストで発生する可能性のあるサウンドのみを監視するようにしてください。

00:15:46.000 --> 00:15:53.000
では、ジョンに戻って、サウンドの分類に関するCreateMLの新機能について学びましょう。

00:15:53.000 --> 00:15:55.000
ケビン、その素晴らしい例をありがとう。

00:15:55.000 --> 00:15:58.000
カウベルのビデオを楽しんでくれてうれしいです。

00:15:58.000 --> 00:16:08.000
それでは、CreateMLの新機能、具体的には、組み込みの分類器の力を活用してカスタムモデルを改善する方法を紹介します。

00:16:08.000 --> 00:16:13.000
組み込みの分類器は、膨大な数のカテゴリにわたる大量のデータで訓練されました。

00:16:13.000 --> 00:16:18.000
したがって、このモデルには、実際には健全な分類に関する多くの知識が含まれています。

00:16:18.000 --> 00:16:24.000
この知識はすべて、CreateMLを使用したカスタムモデルのトレーニングに活用できます。

00:16:24.000 --> 00:16:26.000
これがどのように機能するかをお見せします。

00:16:26.000 --> 00:16:30.000
サウンド分類器は2つの異なるネットワークに分けることができます。

00:16:30.000 --> 00:16:36.000
最初の部分は特徴抽出器で、2番目の部分は分類器モデルです。

00:16:36.000 --> 00:16:41.000
フィーチャーエクストラクタは、埋め込みモデルとも呼ばれ、ネットワークのバックボーンです。

00:16:41.000 --> 00:16:47.000
それはオーディオ波形を取り、それを低次元空間に変換します。

00:16:47.000 --> 00:16:55.000
よく訓練された特徴抽出器は、音響的に似た音を空間の近くの場所に整理します。

00:16:55.000 --> 00:17:04.000
例えば、ギターの音は一緒に集まりますが、ドラムや車の音から離れて配置されます。

00:17:04.000 --> 00:17:07.000
さて、このパイプラインの2番目の部分は分類器モデルです。

00:17:07.000 --> 00:17:13.000
特徴抽出器の出力を受け取り、クラスの確率を計算します。

00:17:13.000 --> 00:17:22.000
分類器は、組み込みの分類器に埋め込んだような、優れた機能抽出器とペアになっていることから利益を得ます。

00:17:22.000 --> 00:17:26.000
組み込みの分類器の機能抽出器を利用できるようにしています。

00:17:26.000 --> 00:17:28.000
それはオーディオフィーチャープリントと呼ばれています。

00:17:28.000 --> 00:17:35.000
CreateMLで独自のカスタムモデルをトレーニングすると、モデルはオーディオ機能プリントとペアリングされます。

00:17:35.000 --> 00:17:40.000
これにより、モデルは組み込みの分類器に含まれるすべての知識の恩恵を受けます。

00:17:40.000 --> 00:17:47.000
前世代のフィーチャーエクストラクタと比較して、オーディオフィーチャープリントは全面的に改善されています。

00:17:47.000 --> 00:17:56.000
このネットワークは小さくて高速ですが、比較したすべてのベンチマークデータセットでより高い精度を実現します。

00:17:56.000 --> 00:18:02.000
また、組み込みの分類器と同様に、オーディオ機能印刷を使用するモデルは、柔軟なウィンドウ期間をサポートします。

00:18:02.000 --> 00:18:11.000
長いウィンドウ時間を選択して、サイレンのような音に最適化するか、ドラムタップのような音に短いウィンドウ時間を選択できます。

00:18:11.000 --> 00:18:18.000
オーディオ機能印刷は、CreateMLを使用してカスタムモデルをトレーニングするときの新しいデフォルトの機能抽出器です。

00:18:18.000 --> 00:18:24.000
ウィンドウの持続時間は、トレーニング中に単一の機能を生成するために使用されるオーディオの長さです。

00:18:24.000 --> 00:18:29.000
デフォルトは3秒ですが、ニーズに合わせて調整できます。

00:18:29.000 --> 00:18:35.000
CreateMLは、1/2秒から15秒の間のウィンドウの持続時間を選択するオプションを提供します。

00:18:35.000 --> 00:18:44.000
カスタムモデルのトレーニングのより詳細な例については、「CreateMLでのサウンド分類モデルのトレーニング」に関する2019年のセッションをご覧ください。

00:18:44.000 --> 00:18:49.000
また、サウンド分析フレームワークを使用してカスタムモデルを実行する方法も示します。

00:18:49.000 --> 00:18:52.000
サウンド分析のセッションにご参加いただきありがとうございます。

00:18:52.000 --> 00:18:56.000
本日、OSに組み込まれた強力な新しいサウンド分類器を導入しました。

00:18:56.000 --> 00:19:01.000
それに伴い、CreateMLの機能抽出器をアップグレードしました。

00:19:01.000 --> 00:19:07.000
これらは新しい可能性を解き放ち、あなたのアプリでそれらをどうするかを見るのが待ちきれません。

00:19:07.000 --> 00:19:10.000
WWDCの残りの部分を楽しんでください。

00:19:10.000 --> 23:59:59.000
[明るい音楽]。

