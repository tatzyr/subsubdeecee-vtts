WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:13.000
Xin chào, và chào mừng đến với "Phân loại các tư thế và hành động tay với Create ML."

00:00:13.000 --> 00:00:15.000
Tôi là Nathan Wertman.

00:00:15.000 --> 00:00:18.000
Và hôm nay, tôi sẽ được tham gia bởi các đồng nghiệp của tôi, Brittany Weinert và Geppy Parziale.

00:00:18.000 --> 00:00:22.000
Hôm nay, chúng ta sẽ nói về việc phân loại các vị trí tay.

00:00:22.000 --> 00:00:25.000
Nhưng trước khi chúng ta đào sâu vào điều đó, hãy nói về chính bàn tay.

00:00:25.000 --> 00:00:30.000
Với hơn hai chục xương, khớp và cơ bắp, bàn tay là một kỳ quan kỹ thuật.

00:00:30.000 --> 00:00:36.000
Bất chấp sự phức tạp này, bàn tay là một trong những công cụ đầu tiên trẻ sơ sinh sử dụng để tương tác với thế giới xung quanh.

00:00:36.000 --> 00:00:42.000
Trẻ sơ sinh học những điều cơ bản về giao tiếp bằng cách sử dụng các động tác tay đơn giản trước khi chúng có thể nói.

00:00:42.000 --> 00:00:45.000
Một khi chúng ta học nói, bàn tay của chúng ta tiếp tục đóng một vai trò trong giao tiếp.

00:00:45.000 --> 00:00:49.000
Họ chuyển sang thêm sự nhấn mạnh và biểu hiện.

00:00:49.000 --> 00:00:55.000
Trong năm qua, bàn tay của chúng ta đã trở nên quan trọng hơn bao giờ hết để đưa mọi người đến gần nhau hơn.

00:00:55.000 --> 00:01:05.000
Vào năm 2020, Khung Tầm nhìn đã giới thiệu Phát hiện Tư thế Tay, cho phép các nhà phát triển xác định các bàn tay trong khung, cũng như mỗi trong số 21 khớp có thể nhận dạng có trong tay.

00:01:05.000 --> 00:01:15.000
Đây là một công cụ tuyệt vời nếu bạn đang cố gắng xác định xem một bàn tay có tồn tại hay một bàn tay ở đâu trong khung, nhưng nó có thể là một thách thức nếu bạn đang cố gắng phân loại những gì bàn tay đang làm.

00:01:15.000 --> 00:01:30.000
Mặc dù khả năng biểu cảm của bàn tay là vô hạn, nhưng trong phần còn lại của phiên này, tôi muốn tập trung vào các tư thế ngắn, một tay, như dừng lại, và yên tĩnh, và hòa bình, và các hành động ngắn, một tay, như lùi lại, biến mất và đến đây.

00:01:30.000 --> 00:01:33.000
Tôi vừa đề cập đến các tư thế và hành động bằng tay.

00:01:33.000 --> 00:01:36.000
Làm thế nào về một số định nghĩa cụ thể hơn?

00:01:36.000 --> 00:01:41.000
Chà, một mặt, chúng ta có những tư thế, có ý nghĩa như một hình ảnh tĩnh.

00:01:41.000 --> 00:01:46.000
Mặc dù hai video này bị tạm dừng, ý định của chủ đề được thể hiện rõ ràng.

00:01:46.000 --> 00:01:48.000
Hãy nghĩ về một tư thế như một hình ảnh.

00:01:48.000 --> 00:01:54.000
Và mặt khác, chúng ta có một hành động, đòi hỏi chuyển động để thể hiện đầy đủ ý nghĩa.

00:01:54.000 --> 00:01:56.000
Ý nghĩa của hai hành động này không rõ ràng.

00:01:56.000 --> 00:01:59.000
Nhìn vào một khung hình đơn giản là không đủ.

00:01:59.000 --> 00:02:06.000
Nhưng với một loạt các khung hình theo thời gian, như video hoặc ảnh trực tiếp, ý nghĩa của hành động là rõ ràng.

00:02:06.000 --> 00:02:08.000
Một lời "xin chào" thân thiện và "đến đây."

00:02:08.000 --> 00:02:17.000
Với điều đó đã được làm sáng tỏ, tôi rất vui mừng được giới thiệu hai mẫu Tạo ML mới trong năm nay, Phân loại tư thế bằng tay và Phân loại hành động bằng tay.

00:02:17.000 --> 00:02:25.000
Những mẫu mới này cho phép bạn đào tạo các mô hình Hand Pose và Action bằng ứng dụng Create ML hoặc khung Create ML.

00:02:25.000 --> 00:02:33.000
Các mô hình này tương thích với macOS Big Sur trở lên, cũng như iOS và iPadOS 14 trở lên.

00:02:33.000 --> 00:02:38.000
Và mới cho năm nay, chúng tôi đã thêm khả năng đào tạo các mô hình trên các thiết bị iOS bằng cách sử dụng khung Tạo ML.

00:02:38.000 --> 00:02:45.000
Bạn có thể tìm hiểu thêm về điều này trong phiên "Xây dựng ứng dụng iOS động với Tạo khung ML".

00:02:45.000 --> 00:02:54.000
Đầu tiên, tôi muốn nói về Phân loại tư thế tay, cho phép bạn dễ dàng đào tạo các mô hình học máy để phân loại các vị trí tay được phát hiện bởi Khung tầm nhìn.

00:02:54.000 --> 00:03:02.000
Vì bạn chịu trách nhiệm đào tạo mô hình, bạn xác định tư thế nào mà ứng dụng của bạn nên phân loại để phù hợp nhất với nhu cầu của nó.

00:03:02.000 --> 00:03:04.000
Hãy để tôi cung cấp cho bạn một bản demo ngắn gọn về một mô hình được đào tạo đang hoạt động.

00:03:04.000 --> 00:03:09.000
Bắt đầu từ một ứng dụng nguyên mẫu đơn giản, tôi có thể dễ dàng tích hợp mô hình Hand Pose Classifier.

00:03:09.000 --> 00:03:16.000
Ứng dụng của tôi bây giờ có thể phân loại các tư thế tay và hiển thị biểu tượng cảm xúc tương ứng và độ tin cậy của tư thế được phân loại.

00:03:16.000 --> 00:03:20.000
Nó phân loại tư thế bàn tay Một cũng như Hai.

00:03:20.000 --> 00:03:25.000
Nhưng bạn sẽ nhận thấy rằng tất cả các vị trí mà nó không nhận ra đều được phân loại là một phần của nền.

00:03:25.000 --> 00:03:29.000
Điều này bao gồm tư thế Open Palm, mà tôi muốn thêm hỗ trợ.

00:03:29.000 --> 00:03:36.000
Tôi sẽ trao mô hình này cho đồng nghiệp Brittany của tôi trong giây lát, để chỉ cho bạn cách tích hợp mô hình Hand Pose Classifier vào ứng dụng của bạn.

00:03:36.000 --> 00:03:40.000
Trước khi tôi làm vậy, tôi muốn thêm hỗ trợ cho tư thế Open Palm.

00:03:40.000 --> 00:03:44.000
Nó sẽ thực sự dễ dàng, nhưng chúng ta nên nói về cách một người mẫu được đào tạo trước.

00:03:44.000 --> 00:03:50.000
Cũng giống như tất cả các dự án Tạo ML khác, việc tích hợp Hand Pose Classifier vào ứng dụng của bạn rất đơn giản.

00:03:50.000 --> 00:03:52.000
Quá trình này có ba bước.

00:03:52.000 --> 00:03:58.000
Thu thập và phân loại dữ liệu đào tạo, đào tạo mô hình, tích hợp mô hình vào ứng dụng của bạn.

00:03:58.000 --> 00:04:02.000
Hãy bắt đầu bằng cách nói về việc thu thập dữ liệu đào tạo.

00:04:02.000 --> 00:04:05.000
Đối với Hand Pose Classifier, bạn sẽ cần hình ảnh.

00:04:05.000 --> 00:04:08.000
Hãy nhớ rằng, các tư thế hoàn toàn biểu cảm như hình ảnh.

00:04:08.000 --> 00:04:13.000
Những hình ảnh này nên được phân loại thành các thư mục có tên khớp với tư thế có trong hình ảnh.

00:04:13.000 --> 00:04:20.000
Ở đây chúng tôi có hai tư thế mà chúng tôi muốn xác định: Một, Hai, cũng như một lớp Nền.

00:04:20.000 --> 00:04:26.000
Lớp Nền là một danh mục bắt tất cả cho các tư thế mà ứng dụng của bạn không quan tâm đến việc xác định chính xác.

00:04:26.000 --> 00:04:30.000
Trong bản demo của tôi, điều này bao gồm nhiều vị trí tay không phải là Một hoặc Hai.

00:04:30.000 --> 00:04:36.000
Lớp Nền được xác định rõ ràng giúp ứng dụng của bạn biết khi nào người dùng của bạn không tạo ra một tư thế quan trọng.

00:04:36.000 --> 00:04:41.000
Có hai loại hình ảnh tạo nên một lớp Nền.

00:04:41.000 --> 00:04:47.000
Đầu tiên, chúng tôi có một loại tư thế tay ngẫu nhiên không phải là tư thế quan trọng mà bạn muốn ứng dụng của mình phân loại.

00:04:47.000 --> 00:04:53.000
Những tư thế này nên bao gồm một tập hợp đa dạng các tông màu da, độ tuổi, giới tính và điều kiện ánh sáng.

00:04:53.000 --> 00:04:58.000
Thứ hai, chúng tôi có một tập hợp các vị trí rất giống với các biểu thức mà bạn muốn ứng dụng của mình phân loại.

00:04:58.000 --> 00:05:05.000
Những tư thế chuyển tiếp này thường xảy ra khi người dùng đang di chuyển bàn tay của họ về phía một trong những biểu hiện mà ứng dụng của bạn quan tâm.

00:05:05.000 --> 00:05:16.000
Khi tôi giơ tay để tạo tư thế Open Palm, hãy chú ý rằng tôi chuyển qua một số vị trí tương tự nhưng không hoàn toàn giống như những gì tôi muốn ứng dụng của mình xem xét Open Palm.

00:05:16.000 --> 00:05:19.000
Những tư thế này cũng xảy ra khi tôi hạ cánh tay của mình sau đó.

00:05:19.000 --> 00:05:21.000
Điều này không phải là duy nhất đối với Open Palm.

00:05:21.000 --> 00:05:28.000
Cùng một loại tư thế chuyển tiếp xảy ra khi tôi giơ cánh tay lên để tạo tư thế Hai, cũng như khi tôi hạ thấp nó xuống.

00:05:28.000 --> 00:05:34.000
Tất cả các tư thế chuyển tiếp này nên được thêm vào lớp Nền, cùng với các tư thế ngẫu nhiên.

00:05:34.000 --> 00:05:43.000
Sự kết hợp này cho phép người mẫu phân biệt chính xác các tư thế mà ứng dụng của bạn quan tâm và tất cả các tư thế nền khác.

00:05:43.000 --> 00:05:48.000
Với dữ liệu đào tạo được thu thập và phân loại, bây giờ là lúc để đào tạo mô hình của chúng tôi bằng ứng dụng Tạo ML.

00:05:48.000 --> 00:05:50.000
Vậy chúng ta hãy làm bẩn tay chúng ta.

00:05:50.000 --> 00:05:56.000
Tôi sẽ bắt đầu với dự án Create ML hiện tại mà tôi đã sử dụng để đào tạo mô hình trong bản demo trước đây của mình.

00:05:56.000 --> 00:06:01.000
Kết quả đào tạo có vẻ tốt, vì vậy tôi hy vọng mô hình này sẽ hoạt động khá tốt.

00:06:01.000 --> 00:06:07.000
May mắn thay, ứng dụng Create ML cho phép bạn xem trước mô hình của mình trước khi tích hợp nó vào ứng dụng của mình.

00:06:07.000 --> 00:06:14.000
Trên tab Xem trước, bạn sẽ thấy rằng, đối với Hand Pose Classifiers, chúng tôi đã thêm khả năng Xem trước Trực tiếp vào bản phát hành này.

00:06:14.000 --> 00:06:19.000
Live Preview tận dụng lợi thế của camera FaceTime để hiển thị cho bạn các dự đoán trong thời gian thực.

00:06:19.000 --> 00:06:25.000
Sử dụng Live Preview, chúng tôi có thể xác minh rằng mô hình này phân loại chính xác các tư thế Một và Hai.

00:06:25.000 --> 00:06:32.000
Và tôi cũng muốn nó phân loại chính xác Open Palm, nhưng nó hiện đang phân loại tư thế đó như một phần của lớp Nền.

00:06:32.000 --> 00:06:42.000
Trong Nguồn Dữ liệu mà tôi đã sử dụng để đào tạo mô hình này, lưu ý rằng nó không bao gồm lớp Open Palm, chỉ bao gồm các lớp cho Một, Hai và Nền.

00:06:42.000 --> 00:06:45.000
Hãy đào tạo một mô hình mới hỗ trợ Open Palm ngay bây giờ.

00:06:45.000 --> 00:06:54.000
Đầu tiên, tôi sẽ tạo một nguồn mô hình mới cho việc này.

00:06:54.000 --> 00:06:59.000
Tôi có một bộ dữ liệu bao gồm một lớp Open Palm mà tôi muốn sử dụng cho khóa đào tạo này.

00:06:59.000 --> 00:07:05.000
Tôi sẽ chọn tập dữ liệu này.

00:07:05.000 --> 00:07:13.000
Nhảy vào nguồn dữ liệu mới này, chúng tôi thấy rằng bây giờ nó bao gồm một mục nhập cho Open Palm cũng như các lớp từ tập dữ liệu trước đó.

00:07:13.000 --> 00:07:23.000
Quay lại nguồn mô hình, tôi muốn thêm một vài bổ sung để mở rộng dữ liệu đào tạo và làm cho mô hình của tôi mạnh mẽ hơn.

00:07:23.000 --> 00:07:28.000
Thế là xong. Đã đến lúc lên tàu.

00:07:28.000 --> 00:07:34.000
Trước khi khóa đào tạo có thể bắt đầu, Create ML cần thực hiện một số xử lý hình ảnh sơ bộ, cũng như trích xuất tính năng.

00:07:34.000 --> 00:07:37.000
Chúng tôi đã nói với Create ML để đào tạo cho 80 lần lặp lại.

00:07:37.000 --> 00:07:41.000
Đây là một điểm khởi đầu tốt, nhưng bạn có thể cần điều chỉnh con số đó dựa trên tập dữ liệu của mình.

00:07:41.000 --> 00:07:43.000
Quá trình này sẽ mất một thời gian.

00:07:43.000 --> 00:07:46.000
May mắn thay, tôi đã đào tạo một người mẫu.

00:07:46.000 --> 00:07:48.000
Hãy để tôi lấy nó ngay bây giờ.

00:07:48.000 --> 00:07:54.000
Xem trước trực tiếp cho thấy mô hình mới được đào tạo của chúng tôi hiện xác định chính xác tư thế Open Palm.

00:07:54.000 --> 00:08:03.000
Và chỉ để chắc chắn, tôi sẽ xác minh rằng nó tiếp tục xác định tư thế Một và Hai.

00:08:03.000 --> 00:08:05.000
Điều đó không dễ sao?

00:08:05.000 --> 00:08:10.000
Tôi sẽ gửi mô hình này cho đồng nghiệp của tôi, Brittany, và cô ấy sẽ nói về việc tích hợp nó vào ứng dụng của mình.

00:08:10.000 --> 00:08:12.000
Cảm ơn, Nathan, vì người mẫu.

00:08:12.000 --> 00:08:14.000
Xin chào. Tôi là Brittany Weinert.

00:08:14.000 --> 00:08:17.000
Và tôi là thành viên của nhóm Vision Framework.

00:08:17.000 --> 00:08:24.000
Khi tôi lần đầu tiên biết về Phân loại Tư thế Tay, tôi ngay lập tức nghĩ rằng, tôi có thể sử dụng điều này để tạo ra các hiệu ứng đặc biệt bằng tay của mình.

00:08:24.000 --> 00:08:33.000
Tôi biết rằng sử dụng CoreML để phân loại tư thế tay và Vision để phát hiện và theo dõi bàn tay sẽ là công nghệ hoàn hảo để sử dụng cùng nhau.

00:08:33.000 --> 00:08:35.000
Hãy xem liệu chúng ta có thể tự cho mình siêu năng lực hay không.

00:08:35.000 --> 00:08:40.000
Tôi đã tạo bản nháp đầu tiên của đường ống cho một bản demo có thể làm được điều đó.

00:08:40.000 --> 00:08:43.000
Hãy xem lại nó.

00:08:43.000 --> 00:08:54.000
Đầu tiên, chúng ta sẽ có một máy ảnh cung cấp một luồng khung hình và chúng ta sẽ sử dụng từng khung hình cho một yêu cầu Tầm nhìn để phát hiện vị trí và các điểm chính của bàn tay trong khung hình.

00:08:54.000 --> 00:08:58.000
DetectHumanHandPoseRequest sẽ là yêu cầu mà chúng tôi đang sử dụng.

00:08:58.000 --> 00:09:03.000
Nó sẽ trả về một HumanHandPoseObservation cho mỗi bàn tay mà nó tìm thấy trong khung.

00:09:03.000 --> 00:09:15.000
Dữ liệu chúng tôi sẽ gửi đến mô hình Phân loại Tư thế Tay CoreML là MLMultiArray và một thuộc tính trên HumanHandPoseObservation được gọi là keypointsMultiArray.

00:09:15.000 --> 00:09:26.000
Bộ phân loại tư thế tay của chúng tôi sau đó sẽ trả lại cho chúng tôi nhãn hành động tay ước tính hàng đầu với điểm tin cậy của nó, sau đó chúng tôi có thể sử dụng để xác định hành động trong ứng dụng.

00:09:26.000 --> 00:09:30.000
Bây giờ chúng ta đã xem qua các chi tiết cấp cao của ứng dụng, hãy xem mã.

00:09:30.000 --> 00:09:36.000
Hãy bắt đầu bằng cách xem cách sử dụng Vision để phát hiện bàn tay trong khung hình.

00:09:36.000 --> 00:09:48.000
Đối với những gì chúng tôi muốn làm, chúng tôi chỉ cần một phiên bản của VNDetectHumanHandPoseRequest, và chúng tôi chỉ cần phát hiện một tay, vì vậy chúng tôi đặt maximumHandCount thành một.

00:09:48.000 --> 00:09:56.000
Nếu bạn đặt Số tay tối đa và có nhiều tay hơn được chỉ định trong khung, thay vào đó, thuật toán sẽ phát hiện các kim trung tâm và nổi bật nhất trong khung.

00:09:56.000 --> 00:09:59.000
Giá trị mặc định cho maximumHandCount là hai.

00:09:59.000 --> 00:10:04.000
Chúng tôi khuyên bạn nên đặt bản sửa đổi ở đây, vì vậy bạn không ngạc nhiên khi cập nhật yêu cầu sau này.

00:10:04.000 --> 00:10:13.000
Nhưng nếu bạn luôn muốn chọn tham gia thuật toán mới nhất được hỗ trợ bởi SDK mà bạn được liên kết, bạn không cần phải đặt nó.

00:10:13.000 --> 00:10:24.000
Ngoài ra, như một lưu ý, chúng tôi sẽ thực hiện phát hiện trên mọi khung hình được ARSession truy xuất thông qua ARKit, nhưng đây chỉ là một cách để lấy khung hình từ nguồn cấp dữ liệu máy ảnh.

00:10:24.000 --> 00:10:27.000
Bạn có thể sử dụng bất kỳ phương pháp nào bạn thích.

00:10:27.000 --> 00:10:30.000
AVCaptureOutput cũng sẽ là một giải pháp thay thế hữu ích.

00:10:30.000 --> 00:10:38.000
Đối với mỗi khung hình nhận được, chúng ta cần tạo một VNImageRequestHandler, xử lý tất cả các yêu cầu trên một hình ảnh nhất định.

00:10:38.000 --> 00:10:49.000
Thuộc tính kết quả trên yêu cầu tư thế tay sẽ được điền với VNHumanHandPoseObservations, tối đa số tay là một, như chúng tôi đã chỉ định trên yêu cầu trước đó.

00:10:49.000 --> 00:10:55.000
Nếu yêu cầu phát hiện không có tư thế tay, chúng tôi có thể muốn xóa bất kỳ hiệu ứng nào hiện đang được hiển thị.

00:10:55.000 --> 00:11:00.000
Nếu không, chúng ta sẽ có một quan sát bằng một tay.

00:11:00.000 --> 00:11:05.000
Tiếp theo, chúng tôi muốn dự đoán tư thế tay của chúng tôi đang sử dụng mô hình CoreML của chúng tôi.

00:11:05.000 --> 00:11:12.000
Chúng tôi không muốn đưa ra dự đoán từng khung hình, vì chúng tôi không muốn việc hiển thị hiệu ứng bị rung chuyển.

00:11:12.000 --> 00:11:16.000
Thực hiện dự đoán theo các khoảng thời gian sẽ tạo ra trải nghiệm người dùng mượt mà hơn.

00:11:16.000 --> 00:11:27.000
Khi chúng tôi muốn đưa ra dự đoán, chúng tôi bắt đầu bằng cách chuyển MLMultiArray sang mô hình Hand Pose CoreML và chúng tôi lấy lại nhãn hàng đầu và sự tự tin từ dự đoán duy nhất được trả về.

00:11:27.000 --> 00:11:34.000
Tôi muốn kích hoạt các thay đổi đối với các hiệu ứng chỉ được hiển thị khi nhãn được dự đoán với mức độ tin cậy cao.

00:11:34.000 --> 00:11:41.000
Đây cũng là chìa khóa để bảo vệ chống lại hành vi trong đó hiệu ứng có thể bật và tắt quá nhanh và trở nên bồn chồn.

00:11:41.000 --> 00:11:49.000
Ở đây, phân loại nền tảng đang giúp chúng tôi bằng cách cho phép chúng tôi giữ ngưỡng tin cậy rất cao.

00:11:49.000 --> 00:11:53.000
Nếu One được dự đoán với sự tự tin cao, chúng ta có thể đặt effectNode để hiển thị.

00:11:53.000 --> 00:12:00.000
Nếu One không được dự đoán với sự tự tin cao, tôi muốn dừng hiệu ứng trên màn hình để phù hợp với những gì tay tôi đang làm.

00:12:00.000 --> 00:12:02.000
Hãy kiểm tra xem chúng ta có gì.

00:12:02.000 --> 00:12:08.000
Nếu tôi thực hiện tư thế Một, nó sẽ kích hoạt một hiệu ứng chùm năng lượng duy nhất.

00:12:08.000 --> 00:12:09.000
Rất tuyệt!

00:12:09.000 --> 00:12:14.000
Người mẫu có thể nói rằng tôi đã tạo ra tư thế Một và kích hoạt hiệu ứng.

00:12:14.000 --> 00:12:18.000
Mặc dù nó sẽ còn tuyệt hơn nếu nó đi theo ngón tay của tôi.

00:12:18.000 --> 00:12:22.000
Thậm chí tốt hơn nếu nó hiển thị tại một điểm cụ thể trên ngón tay của tôi.

00:12:22.000 --> 00:12:23.000
Hãy quay lại mã và thay đổi nó.

00:12:23.000 --> 00:12:35.000
Những gì chúng ta cần làm là đưa vị trí điểm chính của bàn tay vào nội dung đồ họa, có nghĩa là sử dụng chế độ xem để dịch các điểm chính chuẩn hóa vào không gian chế độ xem của máy ảnh.

00:12:35.000 --> 00:12:41.000
Bạn cũng có thể cân nhắc việc cắt tỉa những điểm chính mà bạn tiết kiệm được bằng cách nhìn vào điểm tin cậy.

00:12:41.000 --> 00:12:44.000
Ở đây, tôi chỉ quan tâm đến đầu ngón trỏ.

00:12:44.000 --> 00:12:50.000
Chúng ta cần dịch điểm chính sang không gian tọa độ, vì Vision sử dụng tọa độ chuẩn hóa.

00:12:50.000 --> 00:12:57.000
Ngoài ra, điểm gốc của Vision nằm ở góc dưới bên trái của hình ảnh, vì vậy hãy ghi nhớ điều đó khi bạn thực hiện chuyển đổi.

00:12:57.000 --> 00:13:03.000
Cuối cùng, hãy lưu vị trí chỉ mục và nếu không tìm thấy điểm chính nào, chúng tôi mặc định là số không.

00:13:03.000 --> 00:13:08.000
Hãy xem mã chịu trách nhiệm hiển thị hiệu ứng và cách tôi có thể điều chỉnh nó theo ngón tay của mình.

00:13:08.000 --> 00:13:13.000
Chúng tôi muốn tìm vị trí mà vị trí của đối tượng đồ họa đang được thiết lập.

00:13:13.000 --> 00:13:18.000
setLocationForEffects đang được gọi không đồng bộ là mọi khung hình.

00:13:18.000 --> 00:13:23.000
Theo mặc định, chúng tôi đặt hiệu ứng xuất hiện ở trung tâm của chế độ xem.

00:13:23.000 --> 00:13:37.000
Chuyển nó sang indexFingerTipLocation CGPoint từ trước đó, chúng ta có thể có được hiệu quả như mong muốn.

00:13:37.000 --> 00:13:38.000
Tuyệt vời!

00:13:38.000 --> 00:13:40.000
Điều này đang bắt đầu trông thật ngầu.

00:13:40.000 --> 00:13:43.000
Hãy thực hiện thêm một bước nữa.

00:13:43.000 --> 00:13:52.000
Để tạo ra một câu chuyện đồ họa thú vị hơn xung quanh các siêu năng lực, sẽ rất tốt nếu sử dụng thêm một vài Phân loại Tư thế Tay trong ứng dụng của chúng tôi.

00:13:52.000 --> 00:13:56.000
Trong trường hợp này, chúng tôi sẽ chọn phân loại Two và Open Palm.

00:13:56.000 --> 00:14:00.000
Tôi đã mở rộng ứng dụng của mình để thực hiện hành động khi cả hai tư thế này được phát hiện.

00:14:00.000 --> 00:14:08.000
Ở đây, tôi đang căn giữa chùm năng lượng xuất hiện ở đầu ngón trỏ của mình, như được hiển thị trước đây, cho tư thế Một.

00:14:08.000 --> 00:14:14.000
Hai chùm năng lượng ở đầu ngón giữa và ngón trỏ của tôi cho tư thế Hai.

00:14:14.000 --> 00:14:27.000
Và chùm năng lượng cuối cùng được kích hoạt bởi Hand Pose Open Palm và được neo giữa một điểm chính ở dưới cùng của ngón giữa của tôi và điểm chính cổ tay.

00:14:27.000 --> 00:14:28.000
Được rồi.

00:14:28.000 --> 00:14:35.000
Mọi thứ Nathan và tôi đã giới thiệu bao gồm các bước tích hợp đầy đủ mô hình Phân loại Tư thế Tay của riêng bạn.

00:14:35.000 --> 00:14:44.000
Có một tính năng mới nữa trong Vision mà bạn có thể thấy hữu ích, vì vậy hãy để tôi giới thiệu cho bạn một API có thể giúp kích hoạt và kiểm soát chức năng của ứng dụng này.

00:14:44.000 --> 00:14:54.000
Vision đang giới thiệu một thuộc tính mới cho phép người dùng phân biệt giữa tay trái và tay phải trên HumanHand- PoseObservation: chirality.

00:14:54.000 --> 00:15:04.000
Đây là một Enum chỉ ra bàn tay nào mà HumanHandPoseObservation rất có thể là và có thể là một trong ba giá trị: tay trái, tay phải và không xác định.

00:15:04.000 --> 00:15:16.000
Bạn có thể đoán được ý nghĩa đằng sau hai giá trị đầu tiên, nhưng giá trị chưa biết sẽ chỉ xuất hiện nếu phiên bản cũ hơn của HumanHandPoseObservation bị hủy nối tiếp và thuộc tính chưa bao giờ được đặt.

00:15:16.000 --> 00:15:27.000
Như Nathan đã đề cập trước đó, bạn có thể nhận thêm thông tin về phát hiện tư thế tay Vision bằng cách tham khảo lại phiên WWDC 2020, "Phát hiện tư thế cơ thể và tay với Vision."

00:15:27.000 --> 00:15:36.000
Như một lưu ý phụ, đối với mỗi bàn tay được phát hiện trong một khung, thuật toán cơ bản sẽ cố gắng dự đoán chirality của từng bàn tay riêng biệt.

00:15:36.000 --> 00:15:42.000
Điều này có nghĩa là dự đoán của một tay không ảnh hưởng đến dự đoán của các tay khác trong khung hình.

00:15:42.000 --> 00:15:47.000
Hãy để tôi chỉ cho bạn một ví dụ về mã sử dụng chirality có thể trông như thế nào.

00:15:47.000 --> 00:15:52.000
Chúng tôi đã đề cập đến việc thiết lập để tạo và chạy một VNDetectHumanHandPoseRequest.

00:15:52.000 --> 00:16:03.000
Sau khi thực hiện yêu cầu, các quan sát sẽ có tính chirality thuộc tính Enum và bạn có thể sử dụng nó để thực hiện hành động hoặc sắp xếp thông qua các quan sát tư thế tay Vision như vậy.

00:16:03.000 --> 00:16:07.000
Mọi thứ cho đến nay đều là về cách sử dụng Phân loại Tư thế Tay.

00:16:07.000 --> 00:16:14.000
Nhưng như Nathan đã đề cập trước đó, Phân loại Hành động Tay là một công nghệ mới khác trong năm nay.

00:16:14.000 --> 00:16:16.000
Đây là Geppy để nói chuyện với bạn về nó.

00:16:16.000 --> 00:16:18.000
Cảm ơn, Brittany.

00:16:18.000 --> 00:16:23.000
Xin chào, tên tôi là Geppy Parziale, và tôi là một kỹ sư học máy từ nhóm Create ML.

00:16:23.000 --> 00:16:36.000
Ngoài Phân loại Tư thế Tay, năm nay, Create ML giới thiệu một mẫu mới để thực hiện Phân loại Hành động Tay và tôi sẽ chỉ cho bạn cách sử dụng nó trong các ứng dụng của bạn.

00:16:36.000 --> 00:16:47.000
Vì lý do này, tôi sẽ mở rộng bản demo siêu năng lực của Brittany với một số hành động tay và làm nổi bật một số khác biệt quan trọng giữa tư thế tay và hành động tay.

00:16:47.000 --> 00:17:00.000
Vui lòng tham khảo phiên "Xây dựng Phân loại Hành động với Tạo ML" từ WWDC 2020 để biết thêm thông tin và so sánh vì Hành động Tay và Hành động Cơ thể là hai nhiệm vụ rất giống nhau.

00:17:00.000 --> 00:17:05.000
Nhưng bây giờ, hãy để tôi giải thích Hand Action là gì.

00:17:05.000 --> 00:17:13.000
Hành động tay bao gồm một chuỗi các tư thế tay mà mô hình ML cần phân tích trong quá trình chuyển động của bàn tay.

00:17:13.000 --> 00:17:20.000
Số lượng tư thế trong một chuỗi phải đủ lớn để ghi lại toàn bộ hành động tay từ đầu đến cuối.

00:17:20.000 --> 00:17:23.000
Bạn sử dụng video để ghi lại các hành động bằng tay.

00:17:23.000 --> 00:17:33.000
Đào tạo Phân loại Hành động Tay giống hệt với đào tạo Phân loại Tư thế Tay, như Nathan đã chỉ cho chúng ta trước đó, với một số khác biệt nhỏ.

00:17:33.000 --> 00:17:40.000
Trong khi một hình ảnh tĩnh đại diện cho tư thế tay, các video được sử dụng để chụp và đại diện cho các hành động tay.

00:17:40.000 --> 00:17:48.000
Vì vậy, để đào tạo Trình phân loại hành động bằng tay, bạn sử dụng các video ngắn, trong đó mỗi video đại diện cho hành động bằng tay.

00:17:48.000 --> 00:17:54.000
Những video này có thể được sắp xếp thành các thư mục, trong đó mỗi tên thư mục đại diện cho một lớp hành động.

00:17:54.000 --> 00:18:03.000
Và hãy nhớ bao gồm một lớp Nền chứa các video có hành động không giống với hành động mà bạn muốn trình phân loại nhận ra.

00:18:03.000 --> 00:18:10.000
Để đại diện thay thế, bạn có thể thêm tất cả các tệp video ví dụ của mình vào một thư mục duy nhất.

00:18:10.000 --> 00:18:17.000
Sau đó, bạn thêm một tệp chú thích, sử dụng định dạng CSV hoặc JSON.

00:18:17.000 --> 00:18:27.000
Mỗi mục nhập trong tệp chú thích đại diện cho tên của tệp video, lớp liên quan, thời gian bắt đầu và kết thúc của hành động tay.

00:18:27.000 --> 00:18:31.000
Ngoài ra, trong trường hợp này, hãy nhớ bao gồm một lớp Nền.

00:18:31.000 --> 00:18:36.000
Hãy nhớ rằng, bạn đào tạo người mẫu với các video có cùng độ dài, ít nhiều.

00:18:36.000 --> 00:18:48.000
Thật vậy, bạn cung cấp thời lượng hành động như một tham số đào tạo và sau đó Tạo ML lấy mẫu ngẫu nhiên một số khung hình liên tiếp theo giá trị bạn cung cấp.

00:18:48.000 --> 00:18:53.000
Bạn cũng có thể cung cấp tốc độ khung hình video và các lần lặp lại đào tạo.

00:18:53.000 --> 00:19:03.000
Thêm vào đó, ứng dụng cung cấp các loại tăng cường dữ liệu khác nhau sẽ giúp mô hình khái quát hóa tốt hơn và tăng độ chính xác của nó.

00:19:03.000 --> 00:19:15.000
Đặc biệt, Nội suy thời gian và Giảm khung hình là hai phần tăng cường được thêm vào Phân loại hành động bằng tay để cung cấp biến thể video gần hơn với các trường hợp sử dụng thực tế.

00:19:15.000 --> 00:19:20.000
Vì vậy, tôi đã đào tạo một Hand Action Classifier cho bản demo của mình - hãy xem nó hoạt động.

00:19:20.000 --> 00:19:25.000
Chà, vì tôi là một siêu anh hùng, tôi cần một nguồn năng lượng.

00:19:25.000 --> 00:19:27.000
Đây là của tôi.

00:19:27.000 --> 00:19:31.000
Ở đây, tôi sử dụng tư thế tay để hình dung nguồn năng lượng của mình.

00:19:31.000 --> 00:19:37.000
Nhưng bây giờ, hãy để tôi sử dụng siêu năng lực của mình để kích hoạt nó.

00:19:37.000 --> 00:19:40.000
Trong trường hợp này, tôi đang sử dụng hành động tay.

00:19:40.000 --> 00:19:44.000
Điều này thật tuyệt.

00:19:44.000 --> 00:19:50.000
Và bây giờ, Hand Pose và Hand Action Classifier đang thực hiện đồng thời.

00:19:50.000 --> 00:20:03.000
Tôi đang tận dụng tính năng chirality mới từ Vision và sử dụng tay trái để tạo dáng tay và tay phải để thực hiện hành động tay.

00:20:03.000 --> 00:20:05.000
Điều này thật tuyệt.

00:20:05.000 --> 00:20:15.000
Vì vậy, điều này có thể thực hiện được nhờ sự tối ưu hóa mà Create ML áp dụng, thời gian đào tạo cho mọi mô hình để giải phóng tất cả sức mạnh của Apple Neural Engine.

00:20:15.000 --> 00:20:25.000
Và bây giờ, hãy để tôi quay trở lại thế giới thực của mình và giải thích cho bạn cách tích hợp Create ML Hand Action Classifier vào bản demo của tôi.

00:20:25.000 --> 00:20:28.000
Trước tiên hãy xem xét đầu vào của mô hình.

00:20:28.000 --> 00:20:37.000
Khi bạn tích hợp Hand Action Classifier vào ứng dụng của mình, bạn cần đảm bảo cung cấp cho mô hình số lượng tư thế hand dự kiến chính xác.

00:20:37.000 --> 00:20:44.000
Mô hình của tôi đang mong đợi một MultiArray có kích thước 45 x 3 x 21, vì tôi có thể kiểm tra trong Bản xem trước XCode.

00:20:44.000 --> 00:20:52.000
Ở đây, 45 là số lượng tư thế mà người phân loại cần phân tích để nhận ra hành động.

00:20:52.000 --> 00:20:57.000
21 là số lượng khớp được cung cấp bởi Vision Framework cho mỗi bàn tay.

00:20:57.000 --> 00:21:03.000
Cuối cùng, 3 là tọa độ x và y và giá trị tin cậy cho mỗi khớp.

00:21:03.000 --> 00:21:06.000
45 đến từ đâu?

00:21:06.000 --> 00:21:14.000
Đó là kích thước cửa sổ dự đoán và phụ thuộc vào độ dài video và tốc độ khung hình của các video được sử dụng tại thời điểm đào tạo.

00:21:14.000 --> 00:21:22.000
Trong trường hợp của tôi, tôi quyết định đào tạo mô hình của mình với các video được quay ở tốc độ 30 khung hình / giây và dài 1,5 giây.

00:21:22.000 --> 00:21:32.000
Điều này có nghĩa là người mẫu đã được đào tạo với 45 khung hình video cho mỗi hành động tay, vì vậy trong quá trình suy luận, người mẫu đang mong đợi cùng một số lượng tư thế tay.

00:21:32.000 --> 00:21:41.000
Một sự cân nhắc bổ sung cần được tính đến liên quan đến tần suất các tư thế tay đến tại thời điểm suy luận.

00:21:41.000 --> 00:21:50.000
Điều rất quan trọng là tỷ lệ của các tư thế tay được trình bày cho người mẫu trong quá trình suy luận phải khớp với tỷ lệ của các tư thế được sử dụng để đào tạo người mẫu.

00:21:50.000 --> 00:21:53.000
Trong bản demo của tôi, tôi đã sử dụng ARKit.

00:21:53.000 --> 00:22:05.000
Vì vậy, tôi đã phải giảm một nửa số lượng tư thế đến mỗi giây, vì ARKit cung cấp khung hình ở tốc độ 60 khung hình / giây và trình phân loại của tôi được đào tạo với video ở tốc độ 30 khung hình / giây.

00:22:05.000 --> 00:22:10.000
Nếu làm khác đi, trình phân loại có thể đưa ra những dự đoán sai.

00:22:10.000 --> 00:22:15.000
Bây giờ chúng ta hãy nhảy vào mã nguồn để chỉ cho bạn cách triển khai điều này.

00:22:15.000 --> 00:22:30.000
Đầu tiên, tôi sử dụng bộ đếm để giảm tốc độ của các tư thế đến từ Vision từ 60 khung hình / giây xuống 30 khung hình / giây, phù hợp để tốc độ khung hình mà mô hình của tôi mong đợi hoạt động bình thường.

00:22:30.000 --> 00:22:37.000
Sau đó, tôi nhận được mảng chứa các khớp và chirality cho mỗi bàn tay trong cảnh.

00:22:37.000 --> 00:22:48.000
Tiếp theo, tôi loại bỏ các điểm chính khỏi tay trái của mình vì, trong bản demo của mình, tôi sử dụng tay phải để kích hoạt một số hiệu ứng bằng hành động tay.

00:22:48.000 --> 00:22:52.000
Được rồi, bây giờ tôi cần tích lũy các tư thế tay cho bộ phân loại.

00:22:52.000 --> 00:23:03.000
Để làm như vậy, tôi sử dụng hàng đợi FIFO và tích lũy 45 tư thế tay và đảm bảo hàng đợi luôn chứa 45 tư thế cuối cùng.

00:23:03.000 --> 00:23:05.000
Hàng đợi ban đầu trống rỗng.

00:23:05.000 --> 00:23:12.000
Khi một tư thế tay mới đến, tôi thêm nó vào hàng đợi và tôi lặp lại bước này cho đến khi hàng đợi đầy.

00:23:12.000 --> 00:23:16.000
Khi hàng đợi đầy, tôi có thể bắt đầu đọc toàn bộ nội dung của nó.

00:23:16.000 --> 00:23:21.000
Tôi có thể đọc hàng đợi mỗi khi tôi nhận được tư thế tay mới từ Vision.

00:23:21.000 --> 00:23:25.000
Nhưng hãy nhớ rằng, bây giờ tôi đang xử lý 30 khung hình mỗi giây.

00:23:25.000 --> 00:23:31.000
Và tùy thuộc vào trường hợp sử dụng, điều này có thể là một sự lãng phí tài nguyên.

00:23:31.000 --> 00:23:37.000
Vì vậy, tôi sử dụng một bộ đếm khác để đọc hàng đợi sau khi tôi xác định số lượng khung hình.

00:23:37.000 --> 00:23:49.000
Bạn nên chọn tỷ lệ lấy mẫu hàng đợi như một sự đánh đổi giữa khả năng phản hồi của ứng dụng và số lượng dự đoán mỗi giây bạn muốn có được.

00:23:49.000 --> 00:24:02.000
Tại thời điểm này, tôi đã đọc toàn bộ chuỗi 45 tư thế tay, được sắp xếp trong MLMultiArray và nhập nó vào một bộ phân loại để dự đoán hành động của tay.

00:24:02.000 --> 00:24:07.000
Sau đó, tôi trích xuất nhãn dự đoán và giá trị tin cậy.

00:24:07.000 --> 00:24:15.000
Cuối cùng, nếu giá trị tin cậy lớn hơn ngưỡng xác định của chúng tôi, tôi sẽ thêm các hiệu ứng hạt vào cảnh.

00:24:15.000 --> 00:24:27.000
Vì vậy, hãy nhớ rằng, khi bạn tích hợp Create ML Hand Action Classifier trong ứng dụng của mình, hãy đảm bảo nhập chuỗi các tư thế tay ở tốc độ khung hình mà người mẫu đang mong đợi.

00:24:27.000 --> 00:24:32.000
Khớp với cùng tốc độ khung hình của video được sử dụng để đào tạo trình phân loại.

00:24:32.000 --> 00:24:39.000
Sử dụng hàng đợi vào trước ra trước để thu thập các tư thế tay cho dự đoán mô hình.

00:24:39.000 --> 00:24:42.000
Đọc hàng đợi ở tốc độ khung hình phù hợp.

00:24:42.000 --> 00:24:49.000
Tôi rất mong được xem tất cả các ứng dụng thú vị mà bạn sẽ xây dựng với các mô hình hành động tay được đào tạo với Create ML.

00:24:49.000 --> 00:24:54.000
Và bây giờ, quay lại với Nathan để xem xét và tóm tắt lần cuối.

00:24:54.000 --> 00:24:55.000
Cảm ơn, Geppy.

00:24:55.000 --> 00:24:57.000
Bạn và Brittany đã làm rất tốt trên ứng dụng của bạn.

00:24:57.000 --> 00:24:59.000
Tôi rất hào hứng để thử nó.

00:24:59.000 --> 00:25:05.000
Nhưng trước khi mọi thứ vượt khỏi tầm kiểm soát, đây là một vài điều bạn nên ghi nhớ để đảm bảo trải nghiệm chất lượng cao cho người dùng của mình.

00:25:05.000 --> 00:25:08.000
Hãy chú ý bàn tay cách máy ảnh bao xa.

00:25:08.000 --> 00:25:12.000
Khoảng cách nên được giữ dưới 11 feet, hoặc 3 1/2 mét, để có kết quả tốt nhất.

00:25:12.000 --> 00:25:17.000
Tốt nhất là tránh các điều kiện ánh sáng khắc nghiệt, quá tối hoặc quá sáng.

00:25:17.000 --> 00:25:25.000
Găng tay cồng kềnh, lỏng lẻo hoặc nhiều màu sắc có thể gây khó khăn cho việc phát hiện chính xác tư thế tay, điều này có thể ảnh hưởng đến chất lượng phân loại.

00:25:25.000 --> 00:25:29.000
Giống như tất cả các nhiệm vụ học máy, chất lượng và số lượng dữ liệu đào tạo của bạn là chìa khóa.

00:25:29.000 --> 00:25:34.000
Đối với Trình phân loại tư thế bằng tay được hiển thị trong phiên này, chúng tôi đã sử dụng 500 hình ảnh cho mỗi lớp.

00:25:34.000 --> 00:25:41.000
Hand Action Classifier, chúng tôi đã sử dụng 100 video cho mỗi lớp, nhưng các yêu cầu dữ liệu cho trường hợp sử dụng của bạn có thể khác nhau.

00:25:41.000 --> 00:25:48.000
Điều quan trọng nhất là bạn thu thập đủ dữ liệu đào tạo để nắm bắt biến thể dự kiến mà mô hình của bạn sẽ thấy trong ứng dụng của bạn.

00:25:48.000 --> 00:25:50.000
Bây giờ cảm thấy như là thời điểm tốt để tóm tắt lại.

00:25:50.000 --> 00:25:52.000
Vậy chúng ta đã học được gì?

00:25:52.000 --> 00:25:57.000
Chà, bắt đầu từ năm 2021, bạn có thể xây dựng các ứng dụng diễn giải biểu cảm của bàn tay con người.

00:25:57.000 --> 00:26:02.000
Chúng tôi đã thảo luận về sự khác biệt giữa hai loại biểu cảm tay, tư thế và hành động.

00:26:02.000 --> 00:26:08.000
Chúng tôi đã nói về cách chuẩn bị dữ liệu đào tạo, bao gồm cả lớp Nền, để sử dụng trong ứng dụng Tạo ML để đào tạo một mô hình.

00:26:08.000 --> 00:26:12.000
Chúng tôi đã nói về cách tích hợp một mô hình được đào tạo vào một ứng dụng.

00:26:12.000 --> 00:26:19.000
Và cuối cùng, chúng tôi đã nói về việc kết hợp nhiều mô hình vào một ứng dụng duy nhất và sử dụng chirality để phân biệt bàn tay.

00:26:19.000 --> 00:26:22.000
Rõ ràng, bản demo hôm nay chỉ làm trầy xước bề mặt.

00:26:22.000 --> 00:26:28.000
Khung Tầm nhìn là một công nghệ mạnh mẽ để phát hiện sự hiện diện của bàn tay, tư thế, vị trí và tính chirality.

00:26:28.000 --> 00:26:33.000
Tạo ML là một cách thú vị và dễ dàng để rèn luyện và phân loại các tư thế tay và các hành động tay.

00:26:33.000 --> 00:26:41.000
Khi được sử dụng cùng nhau, chúng cung cấp những hiểu biết sâu sắc về một trong những công cụ mạnh mẽ và biểu cảm nhất của loài người, và chúng tôi nóng lòng muốn xem bạn làm gì với chúng.

00:26:41.000 --> 00:26:43.000
Tạm biệt.

00:26:43.000 --> 23:59:59.000
[Nhạc lạc quan].

