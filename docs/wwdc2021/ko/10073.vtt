WEBVTT

00:00:02.000 --> 00:00:10.000
안녕.

00:00:10.000 --> 00:00:13.000
저는 ARKit 팀의 엔지니어인 데이비드입니다.

00:00:13.000 --> 00:00:17.000
오늘 크리스토퍼와 나는 ARKit 5에 대한 광범위한 개선 사항을 공유할 것이다.

00:00:17.000 --> 00:00:21.000
우리는 iOS 15의 변경 사항에 대해 논의하게 되어 기쁩니다.

00:00:21.000 --> 00:00:27.000
올해, 우리는 전반적으로 많은 업그레이드를 했고, 여러 기능에 대해 논의할 것입니다.

00:00:27.000 --> 00:00:32.000
그렇게 하기 전에, 우리는 여러분 모두가 LiDAR로 구축해온 경험을 선보이고 싶습니다.

00:00:32.000 --> 00:00:43.000
우리는 장면 재구성 및 깊이 API를 사용하는 다양한 LiDAR 지원 앱을 보았습니다: 생산성, 사진 필터 효과, 엔터테인먼트, 심지어 거실에서 플레이할 수 있는 게임까지.

00:00:43.000 --> 00:00:48.000
우리는 ARKit 커뮤니티가 보여준 창의성과 수완을 보게 되어 정말 기쁩니다.

00:00:48.000 --> 00:00:56.000
당신이 이 앱을 만드는 동안, 우리는 당신에게 세계 최고의 AR 프레임워크를 제공하고 가능한 것의 경계를 넓히기 위해 열심히 노력하고 있습니다.

00:00:56.000 --> 00:01:00.000
ARKit 5의 변경 사항을 살펴봅시다.

00:01:00.000 --> 00:01:10.000
먼저, 우리는 실제 야외 장소에서 AR 경험을 가능하게 하는 위치 앵커에 대한 몇 가지 업데이트와 모범 사례를 공유할 것입니다.

00:01:10.000 --> 00:01:18.000
다음으로 우리는 앱 클립을 발견하고 AR에서 콘텐츠를 배치할 수 있는 좋은 방법인 앱 클립 코드를 다룰 것입니다.

00:01:18.000 --> 00:01:26.000
우리는 새로운 iPad Pro에서 울트라 와이드 전면 카메라를 사용하여 얼굴 추적의 몇 가지 개선 사항을 강조할 것입니다.

00:01:26.000 --> 00:01:31.000
그리고 우리는 ARKit 모션 캡처에 대한 몇 가지 개선 사항으로 마무리할 것입니다.

00:01:31.000 --> 00:01:38.000
우리는 지역 지원을 확장하고 삶의 질 개선을 제공하기 위해 노력한 위치 앵커로 시작할 것입니다.

00:01:38.000 --> 00:01:42.000
우리는 또한 애플리케이션을 만들기 위한 몇 가지 모범 사례를 추천할 것입니다.

00:01:42.000 --> 00:01:50.000
위치 앵커는 특정 위도와 경도에 AR 콘텐츠를 배치할 수 있도록 작년에 도입되었다.

00:01:50.000 --> 00:01:56.000
그들의 목적은 지리적 위치와 연결된 AR 경험을 만들 수 있도록 하는 것이다.

00:01:56.000 --> 00:01:58.000
예를 들어 봅시다.

00:01:58.000 --> 00:02:05.000
이것은 위치 앵커 API를 사용하여 구축된 ScavengAR 애플리케이션의 새로운 자연 경험입니다.

00:02:05.000 --> 00:02:13.000
ScavengAR는 실제 위치에서 AR 콘텐츠를 호스팅하고 가상 공공 예술 설치 및 활동을 만들 수 있습니다.

00:02:13.000 --> 00:02:19.000
그것은 세계가 다시 열리면서 위치 앵커가 어떻게 야외 경험을 강화할 수 있는지를 보여주는 좋은 예이다.

00:02:19.000 --> 00:02:24.000
지도 앱은 또한 iOS 15에서 API를 사용하는 새로운 AR 기능을 도입하고 있다.

00:02:24.000 --> 00:02:26.000
한 번 보자.

00:02:26.000 --> 00:02:32.000
올해 지도는 위치 앵커 API를 사용하여 AR에 표시된 턴 바이 턴 보행 방향을 추가하고 있다.

00:02:32.000 --> 00:02:35.000
그들은 우리가 추천하는 몇 가지 관행을 통합한다.

00:02:35.000 --> 00:02:39.000
우리는 당신이 훌륭한 애플리케이션을 어떻게 만들 수 있는지 보여주기 위해 나중에 이것들을 다룰 것입니다.

00:02:39.000 --> 00:02:49.000
이제 몇 가지 샘플을 보았으니, GeoTrackingConfiguration을 설정하는 데 필요한 단계부터 시작하여 위치 앵커를 만드는 데 어떻게 사용할 수 있는지 요약해 봅시다.

00:02:49.000 --> 00:02:53.000
먼저, 그 기능이 장치에서 지원되는지 확인하세요.

00:02:53.000 --> 00:02:59.000
위치 앵커는 A12 칩 또는 최신의 셀룰러 및 GPS 지원이 필요합니다.

00:02:59.000 --> 00:03:04.000
다음으로, 시작하기 전에 해당 위치에서 기능을 사용할 수 있는지 확인하세요.

00:03:04.000 --> 00:03:08.000
카메라와 위치 권한은 장치 소유자의 승인을 받아야 합니다.

00:03:08.000 --> 00:03:11.000
ARKit은 필요한 경우 권한을 요청할 것이다.

00:03:11.000 --> 00:03:23.000
ARKit 4와 샘플 프로젝트인 "AR의 지리적 위치 추적"을 소개하는 작년 프레젠테이션은 이러한 모든 주제와 API 사용을 더 깊이 있게 다룹니다.

00:03:23.000 --> 00:03:28.000
우리는 이 두 가지 출처에 익숙해지는 것을 강력히 추천합니다.

00:03:28.000 --> 00:03:32.000
이 코드 샘플은 이전 슬라이드에서 검사를 수행하는 방법을 보여줍니다.

00:03:32.000 --> 00:03:42.000
장치 지원을 쿼리한 다음 GeoTrackingConfiguration을 실행하기 전에 현재 위치에서 기능을 사용할 수 있는지 확인합니다.

00:03:42.000 --> 00:03:47.000
그런 다음 GeoAnchors는 다른 유형의 앵커와 마찬가지로 ARSession에 추가될 수 있습니다.

00:03:47.000 --> 00:03:52.000
그것들은 위도-경도 좌표와 선택적으로 고도로 지정됩니다.

00:03:52.000 --> 00:04:01.000
GeoTrackingConfiguration의 상태를 모니터링하여 기능이 현지화되었는지 그리고 어떤 문제가 해결되어야 하는지 확인하는 것이 중요합니다.

00:04:01.000 --> 00:04:07.000
개발자 샘플에는 상태 업데이트를 받는 방법을 구현하는 방법에 대한 예가 포함되어 있습니다.

00:04:07.000 --> 00:04:13.000
장치 위치 근처의 가용성을 확인하는 것은 지리적 추적으로 애플리케이션을 시작하는 데 중요합니다.

00:04:13.000 --> 00:04:16.000
우리는 더 많은 지역을 지원하기 위해 끊임없이 노력하고 있습니다.

00:04:16.000 --> 00:04:27.000
위치 앵커는 초기 출시를 위해 5개의 대도시 지역으로 제한되었고, 그 이후로 지원은 미국 전역의 25개 이상의 도시로 확대되었다.

00:04:27.000 --> 00:04:31.000
우리는 또한 전 세계 도시에 위치 앵커를 가져오기 위해 열심히 노력하고 있습니다.

00:04:31.000 --> 00:04:36.000
처음으로, 우리는 미국 이외의 시장을 발표하게 되어 기쁩니다.

00:04:36.000 --> 00:04:39.000
로케이션 앵커들이 런던으로 오고 있다.

00:04:39.000 --> 00:04:43.000
우리는 시간이 지남에 따라 새로운 지역을 추가하기 위해 계속 노력할 것이다.

00:04:43.000 --> 00:04:52.000
지원되는 대도시 지역에 살지 않는다면, 녹음 및 재생을 사용하여 위치 앵커 실험을 시작할 수도 있으며, 이 세션의 뒷부분에서 다룰 것입니다.

00:04:52.000 --> 00:04:59.000
지원되는 지역 목록은 언제든지 ARGeoTrackingConfiguration에 대한 온라인 문서를 참조하십시오.

00:04:59.000 --> 00:05:06.000
더 많은 지역에서 위치 앵커를 사용할 수 있게 됨에 따라, 우리는 사람들을 안내하는 공통의 시각적 언어가 필요하다는 것을 인식하고 있다.

00:05:06.000 --> 00:05:14.000
일관된 온보딩 프로세스를 지원하기 위해, 우리는 ARCoachingOverlayView와 함께 사용할 새로운 .geoTracking 목표를 추가하고 있습니다.

00:05:14.000 --> 00:05:21.000
세계 추적을 위한 기존 오버레이와 유사하게, 사람들이 좋은 경험을 할 수 있도록 애니메이션을 표시합니다.

00:05:21.000 --> 00:05:30.000
코칭 오버레이는 지도를 포함한 다양한 AR 앱에서 사용되기 때문에, 사람들은 이미 그것에 대해 어느 정도 익숙하고 대응 방법을 알게 될 것이다.

00:05:30.000 --> 00:05:35.000
이 기능의 학습 곡선을 쉽게 하기 위해 코칭 오버레이를 포함하는 것이 좋습니다.

00:05:35.000 --> 00:05:45.000
코칭 오버레이를 사용하는 동안에도, 추적 상태에 대한 더 자세한 정보가 포함된 .geoTracking 상태 업데이트를 모니터링하는 것이 좋습니다.

00:05:45.000 --> 00:05:48.000
.geoTracking 코칭 오버레이는 다음과 같습니다.

00:05:48.000 --> 00:05:55.000
UI는 장치를 지상에서 멀어지게 한 다음 건물 외관을 향하도록 지시한다.

00:05:55.000 --> 00:06:01.000
몇 초 후, 추적이 성공하고, 앱은 지리적으로 추적된 콘텐츠를 배치할 수 있습니다.

00:06:01.000 --> 00:06:06.000
이 애니메이션을 표시하는 코드는 다른 코칭 오버레이에 사용되는 것과 매우 유사합니다.

00:06:06.000 --> 00:06:10.000
독특한 것은 오버레이에 대한 .geoTracking 목표의 도입이다.

00:06:10.000 --> 00:06:14.000
올바른 가이드를 표시하기 위해 이 목표를 설정하세요.

00:06:14.000 --> 00:06:18.000
우리는 코칭 오버레이가 어떻게 균일한 온보딩 프로세스를 만들 수 있는지 보았다.

00:06:18.000 --> 00:06:24.000
이제 우리는 당신이 지리적으로 추적된 AR 경험을 만드는 데 도움이 될 몇 가지 다른 모범 사례를 검토할 것입니다.

00:06:24.000 --> 00:06:29.000
우리의 첫 번째 권장 사항은 더 빠른 개발을 위해 녹음과 재생을 사용하는 것입니다.

00:06:29.000 --> 00:06:36.000
ARKit 세션은 App Store에서 사용할 수 있는 Reality Composer를 사용하여 장치에 기록할 수 있습니다.

00:06:36.000 --> 00:06:42.000
이것은 위치 앵커에게 특히 유용하므로 테스트하기 위해 자주 밖에 나갈 필요가 없습니다.

00:06:42.000 --> 00:06:46.000
그것은 또한 원격으로 위치한 제작자와의 협업을 가능하게 한다.

00:06:46.000 --> 00:06:49.000
녹음은 Xcode를 사용하여 장치에서 재생할 수 있습니다.

00:06:49.000 --> 00:06:56.000
비호환성 문제를 피하려면, 동일한 장치와 iOS 버전을 사용하는 것이 좋습니다.

00:06:56.000 --> 00:06:59.000
이것은 다른 유형의 ARKit 애플리케이션에서도 작동합니다.

00:06:59.000 --> 00:07:03.000
리플레이는 위치 앵커에만 국한되지 않습니다.

00:07:03.000 --> 00:07:07.000
녹음을 캡처하는 과정을 살펴봅시다.

00:07:07.000 --> 00:07:13.000
녹화하려면, Reality Composer를 열고 오른쪽 상단의 더 많은 옵션을 탭하세요.

00:07:13.000 --> 00:07:17.000
그런 다음 개발자 창을 열고 AR 세션 녹화를 선택하세요.

00:07:17.000 --> 00:07:20.000
위치 서비스가 활성화되어 있는지 확인하세요.

00:07:20.000 --> 00:07:25.000
녹음을 시작하고 중지하려면 빨간색 버튼을 누르세요.

00:07:25.000 --> 00:07:29.000
녹음을 재생하려면, 장치를 Xcode를 실행하는 컴퓨터에 연결하세요.

00:07:29.000 --> 00:07:35.000
Scheme 편집을 클릭하고 실행 구성에 대한 ARKit Replay 데이터 옵션을 설정하십시오.

00:07:35.000 --> 00:07:37.000
그런 다음 애플리케이션을 실행하세요.

00:07:37.000 --> 00:07:43.000
녹화와 재생은 개발 속도를 높이는 데 도움이 될 수 있지만, 콘텐츠 배치에 권장하는 다른 관행이 있습니다.

00:07:43.000 --> 00:07:46.000
여기 이것들을 보여주는 비디오가 있습니다.

00:07:46.000 --> 00:07:55.000
AR 콘텐츠가 어떻게 크고 명확하게 보이는지 주목하세요, 그리고 환경의 구조와 오버레이할 필요 없이 정보가 전달됩니다.

00:07:55.000 --> 00:08:05.000
개발 시간과 배치 정밀도 사이의 절충안으로, 실제 물체를 밀접하게 겹치려고 하기보다는 공중에 떠다니는 콘텐츠를 만드는 것을 고려하십시오.

00:08:05.000 --> 00:08:09.000
콘텐츠를 배치하기 위한 몇 가지 다른 권장 사항이 있습니다.

00:08:09.000 --> 00:08:18.000
물체를 배치하기 위한 위도와 경도 좌표를 얻으려면, Apple Maps 앱을 사용하고 최소 6자리의 정밀도로 좌표를 복사하십시오.

00:08:18.000 --> 00:08:25.000
이에 대한 단계는 ARKit 4를 소개하는 비디오에 표시되므로, 자세한 내용은 여기를 참조하십시오.

00:08:25.000 --> 00:08:35.000
애플리케이션을 만들 때, 좋은 경험을 제공하기 위해 필요에 따라 위치 앵커에 대한 콘텐츠의 고도를 조정하는 것도 중요합니다.

00:08:35.000 --> 00:08:44.000
앱에 더 정확한 콘텐츠 배치가 필요한 경우, 장치가 위치에서 50미터 이내에 있을 때 지오 앵커를 추가하십시오.

00:08:44.000 --> 00:08:52.000
ARKit이 정확한 고도로 앵커를 배치하면, 이를 나타내기 위해 앵커의 고도 소스 필드를 업데이트할 것이다.

00:08:52.000 --> 00:08:58.000
CLLocation 클래스에는 두 점 사이의 거리를 미터 단위로 계산하는 데 사용할 수 있는 방법이 있습니다.

00:08:58.000 --> 00:09:03.000
이것은 앵커를 추가하기 전에 누군가가 위치에 가까운지 확인하는 데 사용될 수 있다.

00:09:03.000 --> 00:09:06.000
이것으로 위치 앵커에 대한 우리의 세션을 마칩니다.

00:09:06.000 --> 00:09:10.000
ARKit 5를 사용하여 앱에 AR 콘텐츠를 배치하는 더 많은 방법이 있습니다.

00:09:10.000 --> 00:09:13.000
그래서 내가 그걸 크리스토퍼에게 넘겨줄게, 크리스토퍼가 너에게 더 말해줄 거야.

00:09:13.000 --> 00:09:14.000
고마워, 데이비드.

00:09:14.000 --> 00:09:17.000
안녕하세요, 제 이름은 크리스토퍼이고, 저는 ARKit 팀의 엔지니어입니다.

00:09:17.000 --> 00:09:20.000
ARKit 5의 다른 훌륭한 새로운 기능에 대해 더 알려드리게 되어 기쁩니다.

00:09:20.000 --> 00:09:23.000
ARKit의 앱 클립 코드부터 시작하겠습니다.

00:09:23.000 --> 00:09:26.000
당신은 아마 우리가 작년에 WWDC에서 앱 클립을 도입했다는 것을 기억할 것입니다.

00:09:26.000 --> 00:09:33.000
앱 클립은 전체 앱을 설치할 필요 없이 앱의 하나의 상황에 맞는 워크플로우를 통해 사람들을 안내하는 앱의 작은 조각입니다.

00:09:33.000 --> 00:09:42.000
파일 크기가 작기 때문에 앱 클립은 다운로드 시간을 절약하고 사람들을 현재 컨텍스트와 매우 관련이 있는 앱의 특정 부분으로 직접 안내합니다.

00:09:42.000 --> 00:09:47.000
우리는 또한 사람들이 앱 클립을 시각적으로 발견하고 실행할 수 있는 좋은 방법인 앱 클립 코드를 도입했습니다.

00:09:47.000 --> 00:09:50.000
App Store로의 여행은 필요하지 않습니다.

00:09:50.000 --> 00:09:51.000
이것이 앱 클립 코드의 모습이다.

00:09:51.000 --> 00:09:54.000
그것들은 다양한 모양과 색상으로 나올 수 있다.

00:09:54.000 --> 00:09:58.000
개발자로서, 당신은 당신의 시나리오에 가장 적합한 모습을 만들 수 있습니다.

00:09:58.000 --> 00:10:05.000
또한 앱 클립 코드에서 인코딩할 데이터와 어떤 앱 클립이 어떤 코드와 관련이 있는지 결정합니다.

00:10:05.000 --> 00:10:15.000
모든 앱 클립 코드에는 시각적으로 스캔 가능한 패턴이 포함되어 있으며 여기에 표시된 빨간색, 파란색 및 주황색 코드와 같은 일부에는 사용자의 편의를 위해 NFC 태그가 포함되어 있습니다.

00:10:15.000 --> 00:10:21.000
사람들은 카메라로 코드를 스캔하거나 휴대폰을 내장된 NFC 태그에 대고 관련 앱 클립을 실행할 수 있습니다.

00:10:21.000 --> 00:10:26.000
그리고 이제, 당신은 또한 AR 경험에서 앱 클립 코드를 인식하고 추적할 수 있습니다.

00:10:26.000 --> 00:10:29.000
우리는 이 세션의 뒷부분에서 그것이 어떻게 이루어지는지 살펴볼 것이다.

00:10:29.000 --> 00:10:36.000
하지만 먼저, App Clip Code를 사용하여 AR 경험을 시작하는 Primer가 개발한 이 앱 클립을 살펴봅시다.

00:10:36.000 --> 00:10:43.000
프라이머는 클레 타일과 협력하여 사람들에게 앱 클립 코드의 도움으로 AR에서 샘플이 어떻게 보일지 보여주었다.

00:10:43.000 --> 00:10:47.000
iPhone과 iPad를 앱 클립 코드 위에 놓기만 하면 AR 경험을 불러올 수 있습니다.

00:10:47.000 --> 00:10:53.000
이제 사람들은 앱을 다운로드하지 않고도 벽에 있는 타일 견본을 미리 볼 수 있습니다.

00:10:53.000 --> 00:10:54.000
그거 꽤 멋지네, 그렇지?

00:10:54.000 --> 00:11:00.000
따라서 iOS 및 iPad 14.3부터 AR 경험에서 앱 클립 코드를 감지하고 추적할 수 있습니다.

00:11:00.000 --> 00:11:07.000
앱 클립 코드 추적에는 iPhone XS와 같은 A12 바이오닉 프로세서 이상이 설치된 장치가 필요합니다.

00:11:07.000 --> 00:11:10.000
ARKit에서 앱 클립 코드를 사용하는 방법을 자세히 살펴봅시다.

00:11:10.000 --> 00:11:16.000
iOS 14.3에서, 우리는 새로운 유형의 ARAnchor인 ARAppClipCodeAnchor를 도입했습니다.

00:11:16.000 --> 00:11:26.000
이 앵커에는 세 가지 새로운 속성이 있습니다: 앱 클립 코드에 포함된 URL, URL 디코딩 상태 및 미터 단위의 앱 클립 코드의 반경.

00:11:26.000 --> 00:11:29.000
내가 설명할게.

00:11:29.000 --> 00:11:33.000
각 앱 클립 코드에는 올바른 콘텐츠를 표시하기 위해 디코딩된 URL이 포함되어 있습니다.

00:11:33.000 --> 00:11:36.000
URL을 디코딩하는 것은 즉각적이지 않다.

00:11:36.000 --> 00:11:39.000
ARKit은 앱 클립 코드의 존재를 빠르게 감지할 수 있다.

00:11:39.000 --> 00:11:48.000
하지만 ARKit이 코드에 대한 사용자의 거리와 조명과 같은 다른 요인에 따라 URL을 디코딩하는 데 조금 더 오래 걸릴 수 있습니다.

00:11:48.000 --> 00:11:56.000
이것이 앱 클립 코드 앵커가 .decoding 상태 속성을 포함하는 이유이며, 세 가지 상태 중 하나에 있을 수 있습니다.

00:11:56.000 --> 00:12:00.000
초기 상태 .decoding은 ARKit이 여전히 URL을 디코딩하고 있음을 나타냅니다.

00:12:00.000 --> 00:12:05.000
ARKit이 URL을 성공적으로 디코딩하자마자, 상태는 .decoded로 전환됩니다.

00:12:05.000 --> 00:12:10.000
URL을 디코딩할 수 없을 때, 상태는 대신 .failed로 전환될 것이다.

00:12:10.000 --> 00:12:16.000
예를 들어, 이것은 누군가가 앱 클립과 관련이 없는 앱 클립 코드를 스캔할 때 발생할 수 있습니다.

00:12:16.000 --> 00:12:21.000
앱 클립 코드 추적을 사용하려면, 먼저 장치에서 지원되는지 확인해야 합니다.

00:12:21.000 --> 00:12:26.000
앱 클립 코드 추적은 A12 바이오닉 프로세서 이상이 설치된 장치에서만 지원된다는 것을 기억하십시오.

00:12:26.000 --> 00:12:34.000
그런 다음 구성에서 appClipCodeTrackingEnabled 속성을 true로 설정하고 세션을 실행하십시오.

00:12:34.000 --> 00:12:44.000
앱 클립 코드의 URL을 읽으려면, AR 세션을 모니터링하고 앵커 콜백을 업데이트하고 감지된 앱 클립 코드 앵커의 디코딩 상태를 확인하십시오.

00:12:44.000 --> 00:12:57.000
ARKit이 앱 클립 코드를 디코딩하는 동안, 앱 클립 코드 위에 자리 표시자 시각화를 표시하여 사용자에게 앱 클립 코드가 감지되었지만 여전히 디코딩해야 한다는 즉각적인 피드백을 제공할 수 있습니다.

00:12:57.000 --> 00:13:06.000
앞서 언급했듯이, 앱 클립 코드 디코딩도 실패할 수 있습니다. 예를 들어, 누군가가 당신의 앱 클립에 속하지 않는 앱 클립 코드에서 휴대폰을 가리킬 때.

00:13:06.000 --> 00:13:10.000
우리는 당신이 그 경우에 피드백을 주는 것을 추천합니다.

00:13:10.000 --> 00:13:17.000
앱 클립 코드가 디코딩되면, 마침내 URL에 액세스하고 이 앱 클립 코드에 적합한 콘텐츠를 표시하기 시작할 수 있습니다.

00:13:17.000 --> 00:13:25.000
예를 들어, 이전에 본 프라이머 앱 클립의 경우, URL에는 표시할 타일 견본에 대한 정보가 포함되어 있습니다.

00:13:25.000 --> 00:13:31.000
앱 클립 코드가 디코딩되면, 질문은 이 코드와 관련된 콘텐츠를 어디에 표시해야 합니까?

00:13:31.000 --> 00:13:34.000
한 가지 옵션은 앱 클립 코드 앵커 위에 직접 표시하는 것입니다.

00:13:34.000 --> 00:13:40.000
그러나, 사용 사례에 따라, 앱 클립 코드 자체는 콘텐츠를 표시하기에 가장 좋은 장소가 아닐 수 있습니다.

00:13:40.000 --> 00:13:46.000
예를 들어, 고정된 상대 위치로 앱 클립 코드 근처에 콘텐츠를 배치할 수 있습니다.

00:13:46.000 --> 00:13:57.000
이것은 앱 클립 코드가 커피 메이커와 같은 물체에 인쇄될 때 잘 작동하며, 기계의 버튼 위에 작동하는 방법에 대한 가상 지침을 표시하고 싶을 때 잘 작동합니다.

00:13:57.000 --> 00:14:02.000
또는 앱 클립 코드 추적을 ARKit에서 지원하는 다른 추적 기술과 결합할 수 있습니다.

00:14:02.000 --> 00:14:04.000
예를 들어, 이미지 추적.

00:14:04.000 --> 00:14:07.000
그것의 구현을 살펴봅시다.

00:14:07.000 --> 00:14:16.000
다음에 볼 수 있는 비디오와 코드는 developer.apple.com에서 다운로드할 수 있는 "AR에서 앱 클립 코드와 상호 작용" 샘플 코드를 기반으로 합니다.

00:14:16.000 --> 00:14:20.000
당신이 지금 보는 것은 샘플의 AR 경험에 대한 기록입니다.

00:14:20.000 --> 00:14:24.000
먼저, 저는 카메라 앱에서 해바라기 씨 패키지를 스캔하는 것을 시작합니다.

00:14:24.000 --> 00:14:28.000
아마 나는 어떤 식물 씨앗을 살지 결정하려고 원예 가게에서 쇼핑하고 있을 거야.

00:14:28.000 --> 00:14:34.000
iOS는 패키지의 앱 클립 코드를 인식하고 관련 Seed Shop 앱 클립을 시작합니다.

00:14:34.000 --> 00:14:40.000
여기서, 저는 앱 클립 코드를 두 번째로 스캔하고 있으며, 자란 해바라기가 씨앗 패키지에 나타납니다.

00:14:40.000 --> 00:14:45.000
앱 클립은 전체 씨앗 패키지의 이미지 추적을 사용하고 그 위에 해바라기를 배치한다는 점에 유의하십시오.

00:14:45.000 --> 00:14:54.000
이 접근 방식은 이 사용 사례에서 의미가 있습니다. 왜냐하면 그 사람의 관심은 오른쪽 상단의 작은 앱 클립 코드가 아닌 전체 시드 패키지에 있을 가능성이 높기 때문입니다.

00:14:54.000 --> 00:14:57.000
하지만 누군가가 그들의 정원에서 식물이 자라는 것을 보고 싶다면 어떨까요?

00:14:57.000 --> 00:14:59.000
이게 그렇게 보일 수 있는 거야.

00:14:59.000 --> 00:15:04.000
여기서 우리는 코드가 처음으로 스캔될 때 앱 클립 다운로드를 호출한다는 것을 알 수 있습니다.

00:15:04.000 --> 00:15:13.000
그런 다음 앱 클립 내에서 동일한 코드를 다시 스캔하면 코드를 해바라기 씨앗 상자와 연결한 다음 잔디밭을 두드리면 해바라기가 나타납니다.

00:15:13.000 --> 00:15:18.000
대신, 앱 클립이 장미 씨앗 상자의 코드를 보았다면, 잔디밭에 장미 식물을 낳았을 것이다.

00:15:18.000 --> 00:15:22.000
앱 클립은 하나의 워크플로우만 포함해야 한다는 점에 유의하십시오.

00:15:22.000 --> 00:15:28.000
하지만 앱 클립은 전체 Seed Shop 앱을 다운로드하여 공간에서 미리 볼 수 있는 다른 식물을 경험할 수 있는 버튼을 제공할 수 있습니다.

00:15:28.000 --> 00:15:32.000
앱 클립 코드 추적은 앱 클립의 상위 앱에서도 작동한다는 것을 기억하세요.

00:15:32.000 --> 00:15:37.000
잔디밭에 해바라기를 놓는 데 필요한 코드를 살펴봅시다.

00:15:37.000 --> 00:15:42.000
먼저, 화면의 탭을 감지하기 위해 tapGestureRecognizer를 보기에 추가합니다.

00:15:42.000 --> 00:15:51.000
사람이 화면을 탭하면 광선을 세상에 던지고 장치 앞의 수평면에서 결과 위치를 되돌릴 수 있습니다.

00:15:51.000 --> 00:15:55.000
우리의 시나리오에서, 이것은 그 사람의 잔디밭이 될 것이다.

00:15:55.000 --> 00:16:04.000
그런 다음 디코딩된 마지막 앱 클립 코드 URL을 잡고 잔디밭에 새로운 ARAnchor를 추가하세요.

00:16:04.000 --> 00:16:09.000
마지막으로, 해바라기 3D 모델을 다운로드하여 잔디밭에 전시하세요.

00:16:09.000 --> 00:16:14.000
이제, ARKit의 앱 클립 코드에 대한 몇 가지 모범 사례에 대해 이야기해 봅시다.

00:16:14.000 --> 00:16:18.000
앱 클립은 다른 환경과 다른 사용 사례에 사용할 수 있습니다.

00:16:18.000 --> 00:16:22.000
NFC 앱 클립 코드를 만들 수 있는 옵션인지 생각해 보세요.

00:16:22.000 --> 00:16:27.000
사람들이 물리적으로 코드에 접근할 수 있는 환경을 위해 NFC 앱 클립 코드를 추천합니다.

00:16:27.000 --> 00:16:39.000
NFC 앱 클립 코드를 사용할 때, 사람들이 태그를 탭하도록 안내하는 적절한 콜 투 액션 텍스트를 사용하거나, 또는 코드를 스캔할 수 있는 명시적인 이점을 제공합니다.

00:16:39.000 --> 00:16:46.000
마지막으로, 앱 클립 코드가 사용자 환경에 적합한 크기로 인쇄되어 있는지 확인해야 합니다.

00:16:46.000 --> 00:16:57.000
예를 들어, 레스토랑 메뉴는 A4 용지에 인쇄될 수 있으며, 사람들은 최대 50센티미터 거리에서 해당 메뉴의 2.5센티미터 앱 클립 코드를 편안하게 스캔할 수 있습니다.

00:16:57.000 --> 00:17:07.000
그러나 영화 포스터는 보통 훨씬 더 크고 사람들이 최대 2.5미터 떨어진 곳에서 휴대폰으로 스캔할 수 있는 12센티미터 앱 클립 코드를 위한 충분한 공간이 있을 수 있다.

00:17:07.000 --> 00:17:14.000
권장 코드 크기에 대한 자세한 내용은 앱 클립 코드에 대한 휴먼 인터페이스 지침을 확인하세요.

00:17:14.000 --> 00:17:17.000
그래서 그것이 ARKit에서 앱 클립 코드를 사용하는 방법입니다.

00:17:17.000 --> 00:17:26.000
앱 클립과 앱 클립 코드에 대해 더 깊이 파고들고 싶다면, "앱 클립의 새로운 기능"과 "가볍고 빠른 앱 클립 만들기" 세션을 확인하세요.

00:17:26.000 --> 00:17:29.000
이제 얼굴 추적으로 넘어가자.

00:17:29.000 --> 00:17:36.000
얼굴 추적을 사용하면 전면 카메라의 얼굴을 감지하고, 가상 콘텐츠를 오버레이하고, 실시간으로 표정을 애니메이션화할 수 있습니다.

00:17:36.000 --> 00:17:41.000
iPhone X의 출시 이후, ARKit은 얼굴 추적을 활용하는 수많은 훌륭한 앱을 보았다.

00:17:41.000 --> 00:17:50.000
여러 얼굴을 추적하는 것부터 동시 전면 및 후면 카메라 사용 사례에서 얼굴 추적에 이르기까지, 이 API는 수년에 걸쳐 많은 발전을 받았습니다.

00:17:50.000 --> 00:17:57.000
작년에, 우리는 A12 바이오닉 프로세서 이상이 있는 한 TrueDepth 센서가 없는 장치에 얼굴 추적을 도입했습니다.

00:17:57.000 --> 00:18:05.000
그리고 올해 초, 우리는 AR 얼굴 추적 경험을 위한 초광시야 전면 카메라를 제공하는 새로운 iPad Pro를 출시했습니다.

00:18:05.000 --> 00:18:07.000
한 번 보자.

00:18:07.000 --> 00:18:12.000
여기서 당신은 일반 전면 카메라의 시야를 볼 수 있습니다.

00:18:12.000 --> 00:18:15.000
그리고 이것은 새로운 iPad Pro의 새로운 울트라 와이드 시야각이다.

00:18:15.000 --> 00:18:18.000
그건 정말 차이를 만들어, 그렇지 않니?

00:18:18.000 --> 00:18:22.000
기존 앱은 얼굴 추적을 위해 일반 카메라를 계속 사용할 것이라는 점에 유의하십시오.

00:18:22.000 --> 00:18:32.000
새로운 iPad Pro에서 사용자 경험을 울트라 와이드 시야로 업그레이드하려면, 어떤 비디오 형식을 사용할 수 있는지 확인하고 새로운 울트라 와이드 포맷을 선택해야 합니다.

00:18:32.000 --> 00:18:39.000
지원되는 모든 비디오 형식을 반복하고 builtInUltraWideCamera 옵션을 확인하여 이를 수행할 수 있습니다.

00:18:39.000 --> 00:18:45.000
그런 다음 AR 구성에서 이 형식을 설정하고 세션을 실행합니다.

00:18:45.000 --> 00:18:51.000
주목해야 할 한 가지는 새로운 iPad Pro의 울트라 와이드 카메라가 TrueDepth 센서보다 훨씬 더 큰 시야각을 가지고 있다는 것이다.

00:18:51.000 --> 00:18:57.000
따라서 울트라 와이드 비디오 포맷을 사용할 때 ARFrame에서 캡처된DepthData 버퍼를 얻을 수 없습니다.

00:18:57.000 --> 00:18:59.000
마지막으로, 모션 캡처에 대해 이야기해 봅시다.

00:18:59.000 --> 00:19:10.000
2019년에 출시된 이후, 모션 캡처는 2D 및 3D 시뮬레이션에 사용되는 것과 함께 가상 캐릭터를 애니메이션화하는 것과 같은 AR 장면에서 실제 사람들의 강력한 통합을 가능하게 했다.

00:19:10.000 --> 00:19:13.000
iOS 15에서는 모션 캡처가 훨씬 더 좋아지고 있다.

00:19:13.000 --> 00:19:20.000
iPhone 12와 같은 Apple A14 Bionic 프로세서가 장착된 장치에서 모션 캡처는 이제 더 넓은 범위의 바디 포즈를 지원합니다.

00:19:20.000 --> 00:19:23.000
그리고 이것은 코드 변경이 전혀 필요하지 않습니다.

00:19:23.000 --> 00:19:27.000
iOS 15의 모든 모션 캡처 앱은 이것의 이점을 누릴 수 있습니다.

00:19:27.000 --> 00:19:33.000
특히, 회전은 그 어느 때보다 정확하며, 스포츠 행동을 훨씬 더 정확하게 추적할 수 있도록 도와줍니다.

00:19:33.000 --> 00:19:38.000
또 다른 큰 개선은 장치 카메라가 이제 훨씬 더 먼 거리에서 신체 관절을 추적할 수 있다는 것입니다.

00:19:38.000 --> 00:19:43.000
또한 사지 움직임의 범위를 추적하는 것이 크게 증가했다.

00:19:43.000 --> 00:19:45.000
예를 들어 봅시다.

00:19:45.000 --> 00:19:50.000
여기 내 동료 중 한 명인 Ejler가 Driven2win 앱으로 그의 운동을 추적하고 있다.

00:19:50.000 --> 00:19:54.000
iOS 15의 결과는 그 어느 때보다 정확하다.

00:19:54.000 --> 00:19:57.000
요약하자면, ARKit 5는 많은 새로운 기능과 개선 사항을 제공합니다.

00:19:57.000 --> 00:20:01.000
위치 앵커는 새로운 도시에서 사용할 수 있으며 새로운 코칭 오버레이를 특징으로 한다.

00:20:01.000 --> 00:20:08.000
앱 클립 코드 추적은 앱 클립에서 AR을 쉽게 발견하고 사용할 수 있을 뿐만 아니라 가상 콘텐츠의 정확한 위치를 지원합니다.

00:20:08.000 --> 00:20:16.000
얼굴 추적은 새로운 iPad Pro의 새로운 울트라 와이드 시야각과 함께 작동하며, 모션 캡처는 더 나은 정확도와 더 넓은 동작 범위를 추가합니다.

00:20:16.000 --> 00:20:21.000
ARKit 5로 만들 수 있는 모든 놀라운 경험을 보게 되어 매우 기쁩니다.

00:20:21.000 --> 23:59:59.000
[음악].

