WEBVTT

00:00:02.000 --> 00:00:11.000
안녕하세요, WWDC에 오신 것을 환영합니다.

00:00:11.000 --> 00:00:16.000
제 이름은 존이고, 저는 애플의 기계 학습 프레임워크인 코어 ML에서 일하고 있습니다.

00:00:16.000 --> 00:00:22.000
제 동료 브라이언과 함께, 우리는 당신이 기계 학습의 마법을 앱에 가져올 때 모델을 조정하는 방법을 보여드리게 되어 기쁩니다.

00:00:22.000 --> 00:00:27.000
일을 시작하기 위해, 저는 당신에게 우리의 기계 학습 API에 대한 몇 가지 개선 사항을 보여줄 것입니다.

00:00:27.000 --> 00:00:32.000
그 후, 우리는 다양한 새로운 가능성을 열어주는 파일 형식 개선에 뛰어들 것입니다.

00:00:32.000 --> 00:00:42.000
나중에, 브라이언은 우리에게 ML 프로그램을 보여주고 우리를 후드 아래로 데려가서 타이핑된 실행과 모델의 정확성과 성능을 미세 조정하는 데 사용할 수 있는 방법을 안내할 것입니다.

00:00:42.000 --> 00:00:48.000
이러한 개선 사항을 사용하여 워크플로우를 간소화하고 ML 기반 경험을 더욱 발전시킬 수 있습니다.

00:00:48.000 --> 00:00:52.000
API 개선부터 시작합시다.

00:00:52.000 --> 00:00:56.000
Core ML은 사용자 장치의 모델로 작업할 수 있는 간단한 API를 제공합니다.

00:00:56.000 --> 00:01:05.000
이러한 모델은 문자열이나 기본 값 또는 이미지 및 MultiArray와 같은 더 복잡한 입력과 같은 다양한 입력과 출력으로 작동하도록 설계될 수 있습니다.

00:01:05.000 --> 00:01:09.000
이 마지막 유형인 MultiArray에 대해 더 이야기해 봅시다.

00:01:09.000 --> 00:01:14.000
Core ML을 사용하면 MLMultiArray를 사용하여 다차원 데이터로 쉽게 작업할 수 있습니다.

00:01:14.000 --> 00:01:20.000
간단한 API이지만, 데이터를 조작하기 위해 작성해야 하는 코드가 스위프트에서 항상 자연스럽게 느껴지는 것은 아닙니다.

00:01:20.000 --> 00:01:26.000
예를 들어, 많은 정수로 MultiArray를 초기화하려면, 런타임에 유형을 전달해야 합니다.

00:01:26.000 --> 00:01:34.000
게다가 당신은 일반 정수 대신 NSNumber를 사용해야 하며, 그것은 형식이 안전하지 않으며 우아한 스위프트처럼 보이지 않습니다.

00:01:34.000 --> 00:01:39.000
Core ML은 다차원 데이터로 더 쉽게 작업할 수 있도록 MLShapedArray를 소개합니다.

00:01:39.000 --> 00:01:45.000
MLShapedArray는 일반 배열과 비슷하지만 여러 차원을 지원하는 순수한 Swift 유형입니다.

00:01:45.000 --> 00:01:55.000
배열과 마찬가지로, 그것은 쓰기 시 복사 의미와 기존 MLMultiArray 코드와 쉽게 작동하는 풍부한 슬라이싱 구문을 가진 값 유형입니다.

00:01:55.000 --> 00:02:02.000
2차원 MLMultiArray를 초기화하려면, 일반적으로 두 개의 중첩된 "for" 루프를 사용합니다.

00:02:02.000 --> 00:02:08.000
MLShapedArray를 사용하면 한 줄로 동일한 2D 배열을 초기화할 수 있습니다.

00:02:08.000 --> 00:02:14.000
MLShapedArray는 Swift에 자연스럽게 맞으며 코드를 훨씬 쉽게 작성하고 검토할 수 있습니다.

00:02:14.000 --> 00:02:15.000
여기 또 다른 예가 있습니다.

00:02:15.000 --> 00:02:20.000
두 번째 행에 슬라이스로 접근하려면, 이렇게 인덱싱할 수 있습니다.

00:02:20.000 --> 00:02:27.000
여러 행과 열에 슬라이스로 액세스하려면, 각 차원의 범위를 사용할 수 있습니다.

00:02:27.000 --> 00:02:31.000
MLShapedArray와 MLMultiArray는 서로 완벽하게 호환됩니다.

00:02:31.000 --> 00:02:37.000
다른 유형의 인스턴스를 사용하는 이니셜라이저를 사용하여 한 유형을 다른 유형으로 쉽게 변환할 수 있습니다.

00:02:37.000 --> 00:02:41.000
변환 이니셜라이저를 사용하여 데이터 유형을 변환할 수도 있습니다.

00:02:41.000 --> 00:02:47.000
예를 들어, 이 코드는 Doubles의 MultiArray를 Floats의 ShapedArray로 변환합니다.

00:02:47.000 --> 00:02:51.000
모양의 배열은 다차원 데이터로 작업해야 할 때 언제든지 유용합니다.

00:02:51.000 --> 00:02:58.000
예를 들어, YOLO 객체 감지 모델은 이미지에서 객체를 찾은 다음 2차원 배열을 출력합니다.

00:02:58.000 --> 00:03:00.000
그 표는 한 예측의 데이터를 보여준다.

00:03:00.000 --> 00:03:05.000
각 행은 경계 상자를 나타내며, 각 열의 값은 0에서 1 사이입니다.

00:03:05.000 --> 00:03:12.000
각 값은 모델이 경계 상자에 사람, 자전거 또는 자동차 등을 포함한다는 자신감이 있는지를 나타낸다.

00:03:12.000 --> 00:03:17.000
나는 각 경계 상자에 대해 가장 가능성이 높은 라벨을 고르기 위해 코드를 쓰고 싶다.

00:03:17.000 --> 00:03:19.000
여기 그것을 하는 방법의 예가 있습니다.

00:03:19.000 --> 00:03:24.000
코드는 2차원 멀티어레이인 출력의 신뢰 속성으로 시작한다.

00:03:24.000 --> 00:03:29.000
이 함수는 그 행에서 가장 높은 신뢰 점수를 찾기 위해 각 행을 반복한다.

00:03:29.000 --> 00:03:33.000
정수를 NSNumber로 자주 캐스팅해야 합니다.

00:03:33.000 --> 00:03:39.000
이 코드는 대신 MLShapedArray를 사용하며 읽기 쉬운 더 적은 줄에서 동일한 작업을 수행합니다.

00:03:39.000 --> 00:03:44.000
모델의 예측 결과는 우리에게 신뢰 값을 포함하는 ShapedArray 속성을 제공한다는 것을 주목하세요.

00:03:44.000 --> 00:03:50.000
이 코드는 MLShapedArray와 스칼라가 표준 스위프트 컬렉션 프로토콜을 준수하기 때문에 더 간단합니다.

00:03:50.000 --> 00:03:56.000
이것은 더 읽기 쉽고 스위프트에서 작업할 수 있는 좋은 강력한 형식의 경험을 제공합니다.

00:03:56.000 --> 00:04:01.000
다음으로, Core ML 모델과 파일 시스템에서 어떻게 표현되는지에 대해 이야기해 봅시다.

00:04:01.000 --> 00:04:06.000
Core ML을 사용하면 사용자를 위한 풍부한 기계 학습 기반 경험을 쉽게 구축할 수 있습니다.

00:04:06.000 --> 00:04:09.000
ML 모델은 이러한 경험에 생명을 불어넣는 엔진이다.

00:04:09.000 --> 00:04:15.000
.Mlmodel 파일 형식은 모델의 기능을 인코딩하고 추상화하므로 걱정할 필요가 없습니다.

00:04:15.000 --> 00:04:19.000
이 형식은 모델의 모든 구현 세부 사항과 복잡성을 저장합니다.

00:04:19.000 --> 00:04:24.000
개발자로서, 당신은 그것이 트리 앙상블인지 수백만 개의 매개 변수가 있는 신경망인지 신경 쓸 필요가 없습니다.

00:04:24.000 --> 00:04:32.000
ML 모델은 다른 API와 마찬가지로 Xcode 프로젝트에 추가하고 함께 작동하는 코드를 작성하는 단일 파일일 뿐입니다.

00:04:32.000 --> 00:04:36.000
각 코어 ML 모델 파일은 여러 구성 요소로 구성되어 있다.

00:04:36.000 --> 00:04:42.000
메타데이터는 저자, 라이선스, 버전 및 간략한 설명과 같은 정보를 저장합니다.

00:04:42.000 --> 00:04:46.000
인터페이스는 모델의 입력과 출력을 정의한다.

00:04:46.000 --> 00:04:49.000
아키텍처는 모델의 내부 구조를 정의한다.

00:04:49.000 --> 00:04:56.000
예를 들어, 신경망을 사용하면 아키텍처 섹션은 모델의 레이어와 그 사이의 모든 연결을 설명합니다.

00:04:56.000 --> 00:05:03.000
마지막으로, 마지막 섹션은 모델이 훈련 단계에서 배운 엄청난 양의 값을 저장한다.

00:05:03.000 --> 00:05:12.000
ML 모델 파일은 이 모든 섹션을 파일 시스템과 소스 제어 소프트웨어가 단일 바이너리 파일로 보는 protobuf 바이너리 형식으로 인코딩합니다.

00:05:12.000 --> 00:05:18.000
소스 제어 소프트웨어는 이진 모델 파일이 실제로 몇 가지 별개의 구성 요소의 조합이라는 것을 알 수 없다.

00:05:18.000 --> 00:05:27.000
이를 해결하기 위해, Core ML은 macOS의 내장 패키지 기능을 사용하여 이러한 구성 요소를 별도의 파일로 나누는 새로운 모델 형식을 추가하고 있다.

00:05:27.000 --> 00:05:30.000
그것은 우리를 새로운 코어 ML 모델 패키지로 데려온다.

00:05:30.000 --> 00:05:37.000
모델의 각 구성 요소를 자체 파일에 저장하여 아키텍처, 가중치 및 메타데이터를 분리하는 컨테이너입니다.

00:05:37.000 --> 00:05:44.000
이러한 구성 요소를 분리함으로써, 모델 패키지를 사용하면 메타데이터를 쉽게 편집하고 소스 제어로 변경 사항을 추적할 수 있습니다.

00:05:44.000 --> 00:05:50.000
그들은 또한 더 효율적으로 컴파일하고 모델을 읽고 쓰는 도구에 더 많은 유연성을 제공한다.

00:05:50.000 --> 00:05:55.000
Core ML과 Xcode는 여전히 원래의 ML 모델 형식을 완벽하게 지원합니다.

00:05:55.000 --> 00:06:00.000
하지만 모델 패키지로 업데이트하여 더 확장 가능한 형식으로 이동하고 더 효율적으로 컴파일할 수 있습니다.

00:06:00.000 --> 00:06:02.000
Xcode에서 이것을 시도해 봅시다.

00:06:02.000 --> 00:06:07.000
여기 물체 감지 모델을 사용하여 이미지에서 동물을 식별하는 간단한 앱이 있습니다.

00:06:07.000 --> 00:06:10.000
일부 메타데이터 필드가 비어 있다는 것을 주목하세요.

00:06:10.000 --> 00:06:14.000
메타데이터가 채워지지 않은 모델을 만나는 것은 꽤 일반적이다.

00:06:14.000 --> 00:06:17.000
과거에는 Xcode에서 이 필드를 편집할 수 없었습니다.

00:06:17.000 --> 00:06:21.000
하지만 이제 Xcode가 모델 패키지를 지원하므로, 당신은 할 수 있습니다.

00:06:21.000 --> 00:06:30.000
현재 모델의 파일 유형은 ML 모델이지만, 편집 버튼을 클릭하면 Xcode에서 ML 모델 파일을 ML 패키지로 업데이트하라는 메시지가 표시됩니다.

00:06:30.000 --> 00:06:37.000
Xcode는 new.mlpackage를 가리키기 위해 원래 모델 파일에 대한 내 작업 공간의 참조를 업데이트하려고 한다고 말한다.

00:06:37.000 --> 00:06:42.000
내가 가서 업데이트와 편집을 클릭할게.

00:06:42.000 --> 00:06:48.000
Xcode의 UI는 이제 모델이 ML 패키지 형식임을 나타냅니다.

00:06:48.000 --> 00:06:51.000
이제 Xcode에서 누락된 값을 직접 채울 수 있습니다.

00:06:51.000 --> 00:07:01.000
"동물"이라는 단어로 설명을 업데이트하겠습니다. 이 모델은 내 동료 조셉에게서 왔기 때문에, 나는 그의 이름을 저자 필드에 넣을 것이다.

00:07:01.000 --> 00:07:07.000
나는 MIT 라이선스와 버전 2.0이라고 말할 것이다.

00:07:07.000 --> 00:07:11.000
또한 추가 메타데이터 필드를 추가, 수정 및 제거할 수 있습니다.

00:07:11.000 --> 00:07:17.000
WWDC에서 이 모델을 사용한 연도를 나타내는 새로운 메타데이터 항목을 추가하겠습니다.

00:07:17.000 --> 00:07:19.000
그래서 우리는 2021년이라고 말할 거야.

00:07:19.000 --> 00:07:27.000
이제 UI 지원 외에도 런타임에 Core ML의 MLModelDescription API를 사용하여 이 모든 정보에 액세스할 수 있습니다.

00:07:27.000 --> 00:07:32.000
또한 예측 탭에서 모델의 입력 및 출력에 대한 설명을 수정할 수 있습니다.

00:07:32.000 --> 00:07:35.000
여기서 이 입력에 대한 설명을 변경하겠습니다.

00:07:35.000 --> 00:07:38.000
우리는 "동물"을 추가할 것이다.

00:07:38.000 --> 00:07:43.000
그리고 여기, 나는 누락된 하이픈을 추가하여 오타를 고칠 것이다.

00:07:43.000 --> 00:07:46.000
이제, 좋은 메타데이터를 가진 모델은 좋은 주석이 있는 코드와 매우 과 매우 같다.

00:07:46.000 --> 00:07:54.000
그것은 당신과 당신의 팀이 모델의 의도를 이해하는 데 도움이 되므로, 모델의 입력과 출력에 대한 좋은 설명을 작성하는 것이 특히 중요합니다.

00:07:54.000 --> 00:07:57.000
변경 사항을 저장하려면 완료를 클릭하겠습니다.

00:07:57.000 --> 00:08:07.000
이제 소스 컨트롤을 클릭한 다음 커밋을 클릭하면, Xcode는 다른 보기에서 변경 사항을 보여줍니다.

00:08:07.000 --> 00:08:12.000
메타데이터는 이제 own.json 파일에 있어 변경 사항을 쉽게 확인할 수 있습니다.

00:08:12.000 --> 00:08:17.000
마찬가지로, 기능 설명에는 자체 별도의.json 파일이 있습니다.

00:08:17.000 --> 00:08:24.000
만약 우리가 62메가바이트 이진 ML 모델 파일의 몇 바이트를 변경했다면, 우리는 62메가바이트 이진 차이를 가졌을 것이다.

00:08:24.000 --> 00:08:30.000
그러나 모델 패키지는 특히 작은 텍스트 변경을 위해 훨씬 더 효율적이고 작업하기 쉽다.

00:08:30.000 --> 00:08:34.000
Xcode는 모델 패키지와 모델 파일을 모두 동등하게 지원합니다.

00:08:34.000 --> 00:08:39.000
예를 들어, 미리보기 탭을 사용하여 모델 패키지를 테스트할 수 있습니다.

00:08:39.000 --> 00:08:46.000
내가 두 마리의 곰의 이미지를 가져오면, 우리는 곰 한 마리당 한 마리씩 두 개의 경계 상자를 얻는 것을 보게 될 것이다.

00:08:46.000 --> 00:08:57.000
마찬가지로, ML 모델 파일과 마찬가지로 모델 패키지에 대한 암호화 키 또는 ML 아카이브를 생성할 수 있는 유틸리티 탭으로 갈 수 있습니다.

00:08:57.000 --> 00:08:59.000
그래서 그것은 Xcode의 모델 패키지이다.

00:08:59.000 --> 00:09:04.000
패키지는 모델 메타데이터 편집과 같이 모델 파일이 할 수 있는 모든 것을 할 수 있다.

00:09:04.000 --> 00:09:10.000
마지막으로 보여주고 싶은 것은 Xcode가 프로젝트에 추가하는 각 모델에 대해 자동으로 생성하는 코드입니다.

00:09:10.000 --> 00:09:14.000
생성된 코드를 보려면 이 아이콘을 클릭할 것입니다.

00:09:14.000 --> 00:09:19.000
이전에, 우리는 MLMultiArray와 새로운 Swift 대응인 MLShapedArray를 살펴보았습니다.

00:09:19.000 --> 00:09:25.000
Xcode는 이제 래퍼 클래스의 각 MultiArray 출력에 대해 새로운 모양의 배열 속성을 추가합니다.

00:09:25.000 --> 00:09:31.000
예를 들어, 생성된 클래스는 이제 모델의 출력에 대한 confidenceShapedArray 속성을 가지고 있다.

00:09:31.000 --> 00:09:37.000
원한다면 여전히 원래의 신뢰 MLMultiArray 속성을 사용할 수 있습니다.

00:09:37.000 --> 00:09:46.000
새로운 모양의 배열 속성을 활용하려면 프로젝트의 배포 대상은 macOS 12 또는 iOS 15와 같은 OS 버전 중 하나여야 합니다.

00:09:46.000 --> 00:09:51.000
이제 이 모든 것을 보았으니, ML 모델과 ML 패키지를 나란히 살펴봅시다.

00:09:51.000 --> 00:09:59.000
ML 패키지는 트리, SVM, 신경망 등을 포함하여 ML 모델 파일이 지원하는 모든 유형을 지원합니다.

00:09:59.000 --> 00:10:05.000
이러한 유형 외에도, ML 패키지는 ML 프로그램이라는 강력한 새로운 모델 유형을 지원합니다.

00:10:05.000 --> 00:10:10.000
ML 프로그램은 더 코드 지향적인 형식으로 신경망을 나타내는 모델 유형이다.

00:10:10.000 --> 00:10:15.000
ML 프로그램과 그들이 가능하게 하는 새로운 기능에 대해 더 자세히 알려드리기 위해, 저는 그것을 브라이언에게 넘길 것입니다.

00:10:15.000 --> 00:10:16.000
고마워, 존.

00:10:16.000 --> 00:10:24.000
제 이름은 브라이언 킨이고, ML 프로그램과 타이핑된 실행이 어떻게 정확성과 더 나은 모델 성능을 더 잘 제어할 수 있는지에 대해 이야기하게 되어 기쁩니다.

00:10:24.000 --> 00:10:28.000
기계 학습 모델이 당신에게 제시될 수 있는 다양한 방법이 있습니다.

00:10:28.000 --> 00:10:36.000
기계 학습 과정을 수강하거나 논문을 읽는 경우, 수학적 또는 통계적 공식과 관련하여 설명된 모델을 접할 수 있습니다.

00:10:36.000 --> 00:10:45.000
그러나, 이러한 수학적 표현은 종종 추상화되고 계산 그래프나 네트워크의 형태로 당신에게 제공됩니다.

00:10:45.000 --> 00:10:54.000
가운데 두 그림에 묘사된 이 그래픽 표현은 데이터가 일련의 레이어를 통해 어떻게 흐르는지 설명하며, 각 레이어는 고유한 변형을 적용한다.

00:10:54.000 --> 00:10:59.000
기계 학습 소프트웨어 라이브러리에서, 모델은 대신 코드의 작업으로 표현된다.

00:10:59.000 --> 00:11:07.000
기계 학습 엔지니어들은 블록, 기능 및 제어 흐름으로 구성된 이 보다 일반적인 프로그램 구조를 점점 더 활용하고 있다.

00:11:07.000 --> 00:11:13.000
Core ML의 새로운 ML 프로그램 모델 유형은 이 마지막 표현과 일치한다.

00:11:13.000 --> 00:11:16.000
이것은 대표적인 ML 프로그램입니다.

00:11:16.000 --> 00:11:21.000
그것은 사람이 읽을 수 있는 텍스트 형식이지만, 의도는 당신이 직접 쓸 필요가 없다는 것입니다.

00:11:21.000 --> 00:11:25.000
ML 프로그램은 Core ML의 변환기에 의해 자동으로 생성됩니다.

00:11:25.000 --> 00:11:28.000
ML 프로그램은 주요 기능으로 구성되어 있다.

00:11:28.000 --> 00:11:32.000
이 주요 기능은 일련의 작업 또는 운영으로 구성되어 있다.

00:11:32.000 --> 00:11:37.000
각 op는 변수를 생성하며, 이 변수는 강하게 입력된다.

00:11:37.000 --> 00:11:46.000
선형 또는 컨벌루션 작업과 같은 가중치가 있는 작업의 경우, 가중치는 일반적으로 별도의 이진 파일로 직렬화됩니다.

00:11:46.000 --> 00:11:52.000
이것은 ML 프로그램이 신경망과 어떻게 비교되는지에 대한 간략한 요약이다.

00:11:52.000 --> 00:11:57.000
신경망에는 레이어가 있는 반면, ML 프로그램에는 작전이 있다.

00:11:57.000 --> 00:12:04.000
신경망 모델의 가중치는 레이어 설명에 포함되어 있는 반면, ML 프로그램은 가중치를 별도로 직렬화한다.

00:12:04.000 --> 00:12:07.000
그리고 신경망은 중간 텐서 유형을 지정하지 않는다.

00:12:07.000 --> 00:12:11.000
대신, 컴퓨팅 유닛은 런타임에 이러한 유형을 결정한다.

00:12:11.000 --> 00:12:15.000
반면에, ML 프로그램은 텐서를 강하게 입력했다.

00:12:15.000 --> 00:12:24.000
오늘 저는 ML 프로그램의 강력한 형식 구문과 형식화된 중간 텐서가 ML 프로그램을 통한 온디바이스 기계 학습에 미치는 영향에 초점을 맞출 것입니다.

00:12:24.000 --> 00:12:28.000
하지만 먼저, ML 프로그램을 어떻게 받나요?

00:12:28.000 --> 00:12:32.000
코어 ML은 이전에 통합 변환기 API를 도입했다.

00:12:32.000 --> 00:12:42.000
이 통합 변환기 API는 단일 함수 호출로 Tensorflow 또는 PyTorch에서 Core ML 신경망 모델로 모델을 가져올 수 있는 편리한 방법을 제공합니다.

00:12:42.000 --> 00:12:51.000
이제 iOS 15를 최소 배포 대상으로 선택하여 동일한 API를 사용하여 ML 프로그램으로 변환할 수 있습니다.

00:12:51.000 --> 00:12:57.000
후드 아래에서, 코어 ML 컨버터는 변환 시간에 모델에 대한 온디스크 표현을 선택합니다.

00:12:57.000 --> 00:13:05.000
ML 프로그램의 경우, 디스크 내 중간 표현은 WWDC 2020에서 도입된 기능인 모델 중급 언어에 의해 제공됩니다.

00:13:05.000 --> 00:13:12.000
통합 컨버터 API는 모델을 ML 프로그램으로 배포하도록 선택할 수 있는 곳입니다.

00:13:12.000 --> 00:13:16.000
앞으로, ML 프로그램은 신경망보다 선호되는 형식이 될 것이다.

00:13:16.000 --> 00:13:21.000
그리고 ML 프로그램은 iOS15와 macOS Monterey부터 사용할 수 있습니다.

00:13:21.000 --> 00:13:31.000
코어 ML은 신경망 모델에 대한 ML 모델과 ML 패키지 형식을 모두 지원하지만, ML 프로그램은 아키텍처와 별도로 가중치를 저장하기 위한 ML 패키지여야 합니다.

00:13:31.000 --> 00:13:35.000
코어 ML은 미래를 위한 기초로 ML 프로그램에 투자하고 있다.

00:13:35.000 --> 00:13:40.000
신경망에 대한 지속적인 지원이 있을 것이지만, ML 프로그램은 새로운 기능의 중심이 될 것이다.

00:13:40.000 --> 00:13:45.000
그래서 ML 프로그램이 미래라면, 오늘날 ML 프로그램을 채택하면 어떤 이점이 있나요?

00:13:45.000 --> 00:13:47.000
이것은 우리를 타이핑된 실행으로 이끈다.

00:13:47.000 --> 00:13:54.000
ML 프로그램으로 입력된 실행의 이점을 강조하기 위해, 먼저 신경망에서 어떤 일이 일어나는지 논의해 봅시다.

00:13:54.000 --> 00:14:04.000
여기에 표시된 것은 입력 및 출력 텐서에 대해 Float32를 지정하는 Core ML Neural Network 모델에 대한 입력 및 출력의 예입니다.

00:14:04.000 --> 00:14:09.000
입력과 출력은 또한 이중 또는 32비트 정수 유형일 수 있다.

00:14:09.000 --> 00:14:13.000
그래서 신경망 모델은 이러한 입력 및 출력 텐서를 강력하게 입력한다.

00:14:13.000 --> 00:14:17.000
중간 텐서의 종류는 어떤가요?

00:14:17.000 --> 00:14:21.000
신경망은 중간 텐서를 강하게 입력하지 않는다.

00:14:21.000 --> 00:14:25.000
온디스크 모델에는 이러한 텐서의 유형에 대한 정보가 없습니다.

00:14:25.000 --> 00:14:33.000
대신, 모델을 실행하는 컴퓨팅 유닛은 코어 ML이 모델을 로드한 후 텐서의 유형을 추론한다.

00:14:33.000 --> 00:14:43.000
코어 ML 런타임이 신경망을 로드할 때, 네트워크 그래프를 자동으로 동적으로 섹션으로 분할합니다: 애플 뉴럴 엔진 친화적, GPU 친화적, CPU.

00:14:43.000 --> 00:14:51.000
각 컴퓨팅 유닛은 성능과 모델의 전반적인 성능을 극대화하기 위해 기본 유형을 사용하여 네트워크 섹션을 실행합니다.

00:14:51.000 --> 00:14:57.000
GPU와 신경 엔진은 모두 Float16을 사용하고, CPU는 Float32를 사용한다.

00:14:57.000 --> 00:15:07.000
개발자로서, 모델의 computeUnits 속성으로 .all, .cpuAndGPU 또는 .cpuOnly를 선택하여 이 실행 계획을 제어할 수 있습니다.

00:15:07.000 --> 00:15:18.000
이 속성의 기본값은 .all이며, Core ML이 런타임에 신경 엔진, GPU 및 CPU를 통해 모델을 분할하여 앱에 최상의 성능을 제공하도록 지시합니다.

00:15:18.000 --> 00:15:28.000
그리고 cpuOnly로 설정하면 Core ML은 신경 엔진이나 GPU를 사용하지 않아 모델이 CPU에서 Float32 정밀도만 실행되도록 합니다.

00:15:28.000 --> 00:15:36.000
요약하자면, 신경망은 중간 텐서를 가지고 있으며, 이는 그것들을 생산하는 컴퓨팅 유닛에 의해 런타임에 자동으로 입력된다.

00:15:36.000 --> 00:15:46.000
허용된 컴퓨팅 단위 세트를 구성하여 정밀도를 제어할 수 있지만, 그렇게 하는 것은 모델에 대한 글로벌 설정이며 테이블에 약간의 성능을 남길 수 있습니다.

00:15:46.000 --> 00:15:49.000
ML 프로그램은 어때?

00:15:49.000 --> 00:15:58.000
여기에 묘사된 ML 프로그램에서, 입력 및 출력 텐서는 강하게 입력되어 있으며, 프로그램의 모든 중간 텐서도 마찬가지입니다.

00:15:58.000 --> 00:16:07.000
CPU 또는 GPU와 같은 단일 컴퓨팅 단위 내에서 정밀 지원을 혼합하고 일치시킬 수도 있으며, 이러한 유형은 모델 변환 시 잘 정의되어 있습니다.

00:16:07.000 --> 00:16:13.000
Core ML을 사용하여 배포 시나리오에서 모델을 로드하고 실행하기까지는 오래 전입니다.

00:16:13.000 --> 00:16:20.000
ML 프로그램은 신경 엔진, GPU 및 CPU에 작업을 배포하는 것과 동일한 자동 파티 구성표를 사용합니다.

00:16:20.000 --> 00:16:22.000
그러나, 그것은 유형 제약을 추가한다.

00:16:22.000 --> 00:16:32.000
코어 ML은 텐서를 더 높은 정밀도로 높일 수 있는 능력을 유지하지만, 코어 ML 런타임은 중간 텐서를 ML 프로그램에 지정된 것보다 낮은 정밀도로 캐스팅하지 않습니다.

00:16:32.000 --> 00:16:45.000
형식 실행에 대한 이 새로운 지원은 특히 GPU의 Float32 ops와 CPU의 Float16의 선택된 ops에 대해 GPU와 CPU 모두에서 확장된 op 지원을 통해 가능해졌습니다.

00:16:45.000 --> 00:16:54.000
이 확장된 지원을 통해 ML 프로그램이 Float32 정밀도를 지정할 때 GPU의 성능 이점을 여전히 볼 수 있습니다.

00:16:54.000 --> 00:17:00.000
다른 정밀도로 ML 프로그램을 생성하기 위해 통합 변환기 API를 사용해 봅시다.

00:17:00.000 --> 00:17:07.000
좋아요, 저는 지금 대화형 방식으로 파이썬 코드를 실행할 수 있는 편리한 도구인 Jupyter 노트북에 있습니다.

00:17:07.000 --> 00:17:11.000
나는 모델을 새로운 ML 프로그램 형식으로 변환하는 과정을 검토할 것이다.

00:17:11.000 --> 00:17:14.000
내가 오늘 사용할 모델은 스타일 전송 모델이다.

00:17:14.000 --> 00:17:18.000
저는 이미 오픈 소스에서 사전 훈련된 Tensorflow 모델을 다운로드했습니다.

00:17:18.000 --> 00:17:22.000
이 모델은 이미지를 가져와 양식화된 이미지를 생성한다.

00:17:22.000 --> 00:17:24.000
가장 먼저 필요한 것은 몇 가지 수입 진술이다.

00:17:24.000 --> 00:17:38.000
나는 coremltools, 파이썬 이미지 라이브러리, 그리고 내가 여기서 사용하는 코드를 간결하게 유지하기 위해 작성한 몇 개의 도우미 라이브러리와 간단한 도우미 함수를 가져올 것이다.

00:17:38.000 --> 00:17:43.000
이제 스타일 전송 모델의 경로와 스타일화할 이미지의 경로를 지정하겠습니다.

00:17:43.000 --> 00:17:46.000
나는 또한 변환을 위한 입력 유형을 설정할 것이다.

00:17:46.000 --> 00:17:52.000
이 경우, 모델이 훈련된 이미지의 크기를 지정하는 이미지 입력 유형이 될 것이다.

00:17:52.000 --> 00:18:00.000
마지막으로, Core ML 모델 사후 변환을 실행하는 데 사용할 수 있는 입력 사전을 준비하기 위한 몇 가지 추가 설정이 있습니다.

00:18:00.000 --> 00:18:03.000
그래서 입력이 로드되었고, 소스 모델을 사용할 수 있습니다.

00:18:03.000 --> 00:18:11.000
이 시점에서, 모든 외부 자원은 ML 프로그램으로 전환할 준비가 되어 있다.

00:18:11.000 --> 00:18:14.000
변환을 위해, 나는 통합 변환기 API를 사용할 것이다.

00:18:14.000 --> 00:18:17.000
첫 번째 인수는 소스 모델 경로이다.

00:18:17.000 --> 00:18:19.000
다음으로, 입력 유형의 배열을 전달하세요.

00:18:19.000 --> 00:18:21.000
여기, 딱 하나 있어.

00:18:21.000 --> 00:18:28.000
마지막으로, 최소 배포 대상 인수는 Core ML Tools가 신경망 또는 ML 프로그램을 생성하는지 여부를 결정할 것이다.

00:18:28.000 --> 00:18:32.000
기본값은 iOS 13이며 신경망을 생성합니다.

00:18:32.000 --> 00:18:37.000
지금 당장 ML 프로그램을 받고 싶기 때문에, 배포 목표를 iOS 15로 설정할 것입니다.

00:18:37.000 --> 00:18:40.000
저는 결국 이 모델을 iOS 앱에 배포하고 싶습니다.

00:18:40.000 --> 00:18:47.000
내 대상 장치가 Mac이라면 macOS 12의 배포 대상을 지정할 수 있었다.

00:18:47.000 --> 00:18:50.000
Shift-Enter를 눌러 모델을 변환하겠습니다.

00:18:50.000 --> 00:18:53.000
그리고 전환이 완료되었습니다.

00:18:53.000 --> 00:18:57.000
변환 중에 ML 프로그램에 대해 자동으로 발생하는 그래프 변환이 있습니다.

00:18:57.000 --> 00:19:02.000
그것은 FP16ComputePrecision 패스라고 불린다.

00:19:02.000 --> 00:19:09.000
이 그래프 패스는 원래 Tensorflow 그래프의 모든 Float32 텐서를 ML 프로그램의 Float16 텐서로 캐스팅합니다.

00:19:09.000 --> 00:19:15.000
좋아, 이제 변환이 끝났으니, 다음 단계는 ML 프로그램의 정확성을 확인하는 거야.

00:19:15.000 --> 00:19:23.000
두 모델 모두에서 동일한 이미지로 예측을 호출하여 출력 숫자를 원래 Tensorflow 모델과 비교할 수 있습니다.

00:19:23.000 --> 00:19:32.000
ML 프로그램에 대해 주목할 가치가 있습니다. 저는 예측, 모델 저장 및 기타 유틸리티를 위해 전년과 정확히 동일한 Core ML Tools API를 사용하고 있습니다.

00:19:32.000 --> 00:19:37.000
비교를 하기 위해, 나는 이미 _get_coreml_tensorflow_output이라는 유틸리티 메소드를 작성했다.

00:19:37.000 --> 00:19:45.000
그것은 Tensorflow의 출력과 Core ML의 출력을 평가하기 위해 여러 오류 메트릭을 인쇄할 것이다.

00:19:45.000 --> 00:19:51.000
그래서 이것은 이미지이기 때문에, 가장 적절한 오류 메트릭은 신호 대 노이즈 비율 또는 SNR일 수 있다.

00:19:51.000 --> 00:19:55.000
실제로, 20 또는 30 이상의 SNR은 보통 좋은 결과를 나타낸다.

00:19:55.000 --> 00:19:59.000
여기 나는 71의 SNR을 가지고 있고, 그것은 꽤 훌륭하다.

00:19:59.000 --> 00:20:04.000
몇 가지 다른 지표가 있습니다: 최대 절대 오차, 평균 절대 오차.

00:20:04.000 --> 00:20:08.000
하지만, Float16을 사용하는 정확도 비용은 얼마인가요?

00:20:08.000 --> 00:20:10.000
내가 뭘 잃어버렸어?

00:20:10.000 --> 00:20:16.000
알아내기 위해, 나는 Float16 변환을 비활성화하고 다시 변환할 수 있다.

00:20:16.000 --> 00:20:22.000
나는 같은 변환 명령을 사용할 것이지만, 이번에는 compute_precision 인수를 지정하고 Float32로 설정할 것이다.

00:20:22.000 --> 00:20:32.000
이것은 컨버터에게 Float16 캐스트를 주입하지 말라고 말할 것이며, 따라서 Core ML Tools 컨버터는 Float32 ML 프로그램을 생성할 것이다.

00:20:32.000 --> 00:20:41.000
좋아, 이제 이 Float32 ML 프로그램을 원래의 Tensorflow 프로그램과 비교해 볼게.

00:20:41.000 --> 00:20:50.000
그리고 SNR은 100 이상으로 증가했고, 최대 절대 오차는 약 1에서 0.02로 감소했다.

00:20:50.000 --> 00:20:55.000
나는 Float16 모델에서 이전에 받은 오류가 눈에 띄는 영향을 미쳤는지 아직 대답하지 않았다.

00:20:55.000 --> 00:21:03.000
이것은 스타일 전송 모델이므로, 출력 이미지의 간단한 플롯을 기반으로 평결을 내릴 수 있다.

00:21:03.000 --> 00:21:14.000
나는 내가 가지고 있는 세 가지 모델 모두에서 소스 이미지와 양식화된 버전을 그릴 것이다: Float16 ML 프로그램, Float32 ML 프로그램, 그리고 Tensorflow 모델.

00:21:14.000 --> 00:21:17.000
그리고 나는 세 가지 모델 출력 사이에 어떤 차이도 보이지 않는다.

00:21:17.000 --> 00:21:24.000
물론, 몇 가지 지표와 육안 검사로 한 번 단일 이미지에 대한 이 평가는 실제로 연기 테스트일 뿐입니다.

00:21:24.000 --> 00:21:26.000
괜찮아 보여.

00:21:26.000 --> 00:21:35.000
실제로, 나는 대규모 데이터 세트에서 더 많은 오류 메트릭으로 평가하고, 기계 학습 모델에서 사용하는 파이프라인 내에서 실패 사례를 평가하고, 그것들을 분류할 것이다.

00:21:35.000 --> 00:21:44.000
나는 작은 데이터 세트를 가지고 있으며, 이 예제로 한 걸음 더 나아가기 위해, 두 개의 ML 프로그램을 데이터 세트 내의 각 이미지에 대한 Tensorflow 모델과 비교할 수 있습니다.

00:21:44.000 --> 00:21:53.000
Float32 ML 프로그램 대 Tensorflow의 SNR은 Xs가 있는 빨간색 선으로 묘사되며, Float16 ML 프로그램은 원이 있는 파란색 선이다.

00:21:53.000 --> 00:22:00.000
Float32 ML 프로그램은 평균 SNR이 약 100인 것으로 보이며, Float16 ML 프로그램은 약 70을 유지한다.

00:22:00.000 --> 00:22:05.000
Float16 정밀도는 숫자에 약간 영향을 미치지만, 이 사용 사례에서는 중요하지 않은 것 같다.

00:22:05.000 --> 00:22:09.000
131개의 이미지로 구성된 이 작은 데이터 세트에도, 몇 가지 이상값이 있다.

00:22:09.000 --> 00:22:13.000
전반적으로, 그 모델은 예상되는 일을 꽤 잘 하고 있다.

00:22:13.000 --> 00:22:16.000
그리고 이것은 대부분의 딥 러닝 모델의 경우이다.

00:22:16.000 --> 00:22:19.000
그들은 일반적으로 Float16 정밀도로도 잘 작동하는 경향이 있다.

00:22:19.000 --> 00:22:24.000
그것이 우리가 Core ML 변환기에서 기본적으로 Float16 변환을 켠 이유입니다.

00:22:24.000 --> 00:22:33.000
Float16 형식의 ML 프로그램은 신경 엔진에서 실행할 수 있으며, 이는 상당한 성능 향상과 전력 소비 감소를 제공할 수 있습니다.

00:22:33.000 --> 00:22:43.000
런타임은 실행 중에 텐서의 유형을 최소 정밀도로 취급하기 때문에, Float32 ML 프로그램은 GPU와 CPU의 조합에서만 실행됩니다.

00:22:43.000 --> 00:22:51.000
이 데모는 ML 프로그램이 변환 시간에 바로 실행될 최소 정밀도를 제어하는 것이 얼마나 쉬운지 보여주었다.

00:22:51.000 --> 00:23:01.000
그리고 신경망 코어 ML 모델과 달리, 모델이 더 높은 정밀도가 필요한 경우, 이를 달성하기 위해 앱 코드에서 컴퓨팅 단위의 설정을 cpuOnly로 변경할 필요가 없습니다.

00:23:01.000 --> 00:23:07.000
그리고 마지막으로, 이 데모 노트북은 Core ML Tools 문서 사이트에서 예시로 사용할 수 있습니다.

00:23:07.000 --> 00:23:18.000
요약하자면, ML 프로그램을 얻으려면, 변환 기능을 사용하고 추가 인수를 전달하여 배포 대상을 지정하고, 적어도 iOS 15 또는 macOS 12로 설정하십시오.

00:23:18.000 --> 00:23:25.000
기본적으로, 코어 ML 컨버터는 신경 엔진에서 실행할 수 있는 최적화된 Float16 모델을 생성합니다.

00:23:25.000 --> 00:23:34.000
경우에 따라 모델이 Float16 정밀도에 민감하다면, 대신 정밀도를 Float32로 설정하는 것이 간단합니다.

00:23:34.000 --> 00:23:45.000
사실, Core ML Tools API에는 더 많은 고급 옵션이 있으며, Float32에서 실행할 특정 작업을 선택하고 나머지는 Float16에 유지하여 혼합 유형의 ML 프로그램을 생성할 수 있습니다.

00:23:45.000 --> 00:23:48.000
이 예시들에 대한 우리의 문서를 확인해 주세요.

00:23:48.000 --> 00:23:54.000
요약하자면, Core ML에는 모델을 더 쉽게 조정하고 작업할 수 있는 몇 가지 새로운 개선 사항이 있습니다.

00:23:54.000 --> 00:23:59.000
새로운 MLShapedArray 유형은 다차원 데이터로 쉽게 작업할 수 있게 해준다.

00:23:59.000 --> 00:24:03.000
ML 패키지 형식을 사용하면 Xcode에서 직접 메타데이터를 편집할 수 있습니다.

00:24:03.000 --> 00:24:14.000
새로운 ML 프로그램 모델 유형의 ML 패키지는 GPU에서 Float32를 지원하는 형식 실행을 지원하여 모델의 성능과 정확성을 조정할 때 더 많은 옵션을 제공합니다.

00:24:14.000 --> 00:24:18.000
모델을 ML 패키지로 업그레이드하고 ML 프로그램을 사용하는 것이 좋습니다.

00:24:18.000 --> 00:24:22.000
저희 세션을 시청해 주셔서 감사드리며, 남은 WWDC를 즐기세요.

00:24:22.000 --> 23:59:59.000
[음악].

