WEBVTT

00:00:00.000 --> 00:00:12.000
안녕하세요, 제 이름은 Bharath이고, 저는 Apple의 Core Audio 팀 출신입니다.

00:00:12.000 --> 00:00:17.000
그리고 오늘, 저는 PHASE와 함께 기하학 인식 오디오에 대해 이야기하고 싶습니다.

00:00:17.000 --> 00:00:21.000
우리는 당신이 왜 새로운 PHASE 프레임워크를 사용하고 싶은지에 대해 이야기할 것입니다.

00:00:21.000 --> 00:00:25.000
우리는 프레임워크가 제공하는 몇 가지 기능을 소개할 것입니다.

00:00:25.000 --> 00:00:33.000
그런 다음, 저는 그것을 제 동료인 David Thall에게 넘겨주어 이 API를 사용한 개념과 샘플 사용 사례를 통해 당신을 안내할 것입니다.

00:00:33.000 --> 00:00:35.000
시작하자.

00:00:35.000 --> 00:00:38.000
오디오는 모든 게임 경험의 중요한 측면이다.

00:00:38.000 --> 00:00:44.000
헤드폰의 공간 오디오는 전반적인 게임플레이를 한 단계 끌어올리고 당신이 더 많이 참여하게 느끼게 합니다.

00:00:44.000 --> 00:00:57.000
오늘날 게임에서 물리학, 애니메이션, 시각 효과 등과 같은 엔진의 다양한 하위 시스템은 모두 서로 통신하고 플레이어의 행동에 따라 게임 흐름이나 스토리라인을 앞으로 움직인다.

00:00:57.000 --> 00:01:02.000
그러나, 오디오 서브시스템은 일반적으로 나머지와 별도로 관리되고 구동된다.

00:01:02.000 --> 00:01:07.000
그들은 또한 때때로 시뮬레이션을 항상 인식하지 못하는 미들웨어를 통해 작성된다.

00:01:07.000 --> 00:01:15.000
오디오 자산은 게임 비주얼과 일치하는 오디오 스토리를 전달하기 위해 사후 제작되고, 사전 구워진, 수작업으로 조정됩니다.

00:01:15.000 --> 00:01:24.000
비주얼이 발전함에 따라, 다양한 플랫폼에서 오디오 경험을 응집력 있게 유지하기 위해 오디오 시스템, 사운드 디자인 및 관련 자산을 재생성해야 합니다.

00:01:24.000 --> 00:01:29.000
이 반복적인 개발 과정은 게임 개발 중에 설명되어야 한다.

00:01:29.000 --> 00:01:36.000
이것은 일반적으로 게임플레이의 시각적 측면에 뒤처진 오디오 경험으로 이어진다.

00:01:36.000 --> 00:01:42.000
더 나은 게임 오디오 경험을 제공하기 위해, 우리는 오디오 시스템을 다른 하위 시스템에 더 가깝게 만들고 싶습니다.

00:01:42.000 --> 00:01:51.000
우리는 또한 당신이 지원되는 모든 장치에서 일관된 공간 오디오 경험을 제공할 수 있는 응용 프로그램을 더 쉽게 작성할 수 있도록 하고 싶습니다.

00:01:51.000 --> 00:01:56.000
이제 새로운 오디오 프레임워크 단계와 그 기능을 살펴보겠습니다.

00:01:56.000 --> 00:02:19.000
PHASE는 오디오 엔진에 지오메트리 정보를 제공하고, 사운드 디자인 친화적인 이벤트 기반 오디오 재생 시스템을 구축하는 데 도움이 되며, 지원되는 모든 장치에서 일관된 공간 오디오 경험을 자동으로 제공할 수 있고 기존 저작 솔루션 및 파이프라인과 통합될 수 있는 애플리케이션을 작성할 수 있는 새로운 프레임워크입니다.

00:02:19.000 --> 00:02:26.000
PHASE에 대해 더 배우기 전에, 일반적으로 사용되는 게임 오디오 워크플로우를 검토해 봅시다.

00:02:26.000 --> 00:02:34.000
여기 청취자, 음원, 흐르는 개울, 오클루더, 헛간이 있는 야외 장면의 예가 있습니다.

00:02:34.000 --> 00:02:40.000
오클루더는 소스와 청취자 사이의 소리를 약화시킬 수 있는 장면의 물체이다.

00:02:40.000 --> 00:02:44.000
일반적으로, 당신은 한 지역을 따라 여러 포인트 소스를 배치할 것입니다.

00:02:44.000 --> 00:02:58.000
청취자가 움직일 때, 레이 트레이싱과 같은 다양한 기술을 사용하여 포인트 소스 간의 적절한 필터링 및 믹스 비율을 결정하고 좋은 오디오 경험을 제공하기 위해 수동으로 혼합해야 합니다.

00:02:58.000 --> 00:03:11.000
게임 개발의 자연스러운 과정에서, 예를 들어 예시 장면의 헛간과 같은 시각적 장면이 바뀌면, 시각적 장면 변경과 일치하도록 오디오 경험을 업데이트하고 손으로 조정해야 합니다.

00:03:11.000 --> 00:03:25.000
오디오 소스가 장면을 기반으로 관리하고 믹싱해야 하는 지점이 아니라 오디오 시스템이 자동으로 관리할 수 있는 영역이나 볼륨에서 나오는 사운드로 애플리케이션을 구축할 수 있다고 상상해 보세요.

00:03:25.000 --> 00:03:27.000
단계는 정확히 그렇게 한다.

00:03:27.000 --> 00:03:29.000
체적 소스를 소개합니다.

00:03:29.000 --> 00:03:38.000
새로운 프레임워크는 사운드 소스를 기하학적 모양으로 기본 오디오 엔진에 전달할 수 있는 API를 제공합니다.

00:03:38.000 --> 00:03:44.000
체적 음원 외에도, 장면에서 오클루더를 기하학적 모양으로 통과할 수도 있습니다.

00:03:44.000 --> 00:03:51.000
또한 사전 설정 세트에서 음향 재료 특성을 선택하고 오클루더에 부착할 수 있습니다.

00:03:51.000 --> 00:04:00.000
또한 PHASE 프레임워크를 사용하면 애플리케이션에서 요구하는 포인트 소스에 대한 중간 전파 및 소스 지향성을 설정할 수 있습니다.

00:04:00.000 --> 00:04:04.000
우리는 여기서 예시로 야외 장면을 보고 있다.

00:04:04.000 --> 00:04:12.000
그러나, 애플리케이션에 실내 장면이 있는 경우, 사전 설정 라이브러리에서 초기 반사 및 후기 잔향 속성을 선택할 수 있습니다.

00:04:12.000 --> 00:04:25.000
다양한 음원, 오클루더 및 청취자가 어디에 있는지 프레임워크에 말하면, PHASE는 무거운 일을 돕고 장면에서 다양한 음원의 오클루와 확산 효과를 모델링하는 데 도움이 될 것입니다.

00:04:25.000 --> 00:04:37.000
이제 애플리케이션의 오디오 시스템은 기하학을 인식하고 있으므로, 게임 개발이 발전함에 따라 시각적 장면 변화에 훨씬 더 빨리 적응할 수 있습니다.

00:04:37.000 --> 00:04:43.000
기하학 인식 외에도, PHASE는 이벤트 기반 대화형 재생 시스템을 제공합니다.

00:04:43.000 --> 00:04:48.000
사운드 이벤트는 단계에서 오디오 재생 이벤트를 설명하기 위한 기본 단위입니다.

00:04:48.000 --> 00:04:52.000
그들은 오디오 자산의 선택, 혼합 및 재생을 캡슐화한다.

00:04:52.000 --> 00:05:02.000
사운드 이벤트는 원샷 재생, 루프와 같은 간단한 이벤트부터 재생 이벤트의 하위 트리를 혼합하거나 전환할 수 있는 트리로 구성된 복잡한 시퀀스에 이르기까지 다양합니다.

00:05:02.000 --> 00:05:06.000
발자취를 연주하는 간단한 예를 들어 봅시다.

00:05:06.000 --> 00:05:13.000
여기에 자갈 위의 세 가지 다른 발자취 소리 중에서 자동으로 선택할 수 있는 무작위 노드가 있습니다.

00:05:13.000 --> 00:05:16.000
우리는 천 바스락거리는 소리를 위한 또 다른 사운드 이벤트를 할 수 있다.

00:05:16.000 --> 00:05:24.000
두 이벤트 나무 모두 이 예에서 "가난"의 다른 나무에 접목되어 천 바스락거리는 소리와 발자국 소리의 혼합을 재생할 수 있다.

00:05:24.000 --> 00:05:30.000
우리는 캐릭터가 멀리 떨어져 있을 때 다른 소리를 연주하기 위해 "멀리"라는 또 다른 나무를 가질 수 있다.

00:05:30.000 --> 00:05:37.000
가까운 나무와 먼 나무의 혼합은 이제 게임플레이에 따라 거리로 제어할 수 있다.

00:05:37.000 --> 00:05:51.000
예를 들어, 눈이나 잔디의 발자국과 같은 더 많은 이벤트 트리를 추가하고, 사용자 상호 작용이나 물리학 및 애니메이션과 같은 하위 시스템에 의해 트리거될 수 있는 복잡한 재생 이벤트 시퀀스를 만들 수 있습니다.

00:05:51.000 --> 00:06:02.000
단계를 사용하면, 소리는 간단한 채널 구성이나 방향과 위치가 있는 3D 공간에서, 또는 소리가 방향이 있지만 위치가 없는 주변 침대로 재생할 수 있습니다.

00:06:02.000 --> 00:06:14.000
기본 엔진은 지원되는 iOS, macOS 장치 및 Air Pods 헤드폰 제품군에서 이미 사용할 수 있는 공간 오디오 렌더링 기능을 기반으로 합니다.

00:06:14.000 --> 00:06:22.000
이를 통해 지원되는 모든 장치에서 일관된 공간 오디오 경험을 자동으로 제공하는 애플리케이션을 구축할 수 있습니다.

00:06:22.000 --> 00:06:30.000
다음으로, 저는 David Thall이 PHASE에 대해 더 깊이 파고들고 몇 가지 예시 사용 사례에 대한 개념과 API에 대해 더 많이 이야기하도록 초대하고 싶습니다.

00:06:30.000 --> 00:06:37.000
안녕하세요, 여러분, 제 이름은 David Thall이고, 저는 PHASE의 시스템 아키텍트이자 개발 책임자입니다.

00:06:37.000 --> 00:06:42.000
오늘 저는 당신에게 PHASE API를 안내할 것입니다.

00:06:42.000 --> 00:06:46.000
이 섹션에서는 일반적인 개념을 소개해 드리겠습니다.

00:06:46.000 --> 00:06:51.000
이에 따라, 당신이 시작할 수 있도록 몇 가지 샘플 사용 사례를 살펴보겠습니다.

00:06:51.000 --> 00:06:56.000
단계 API는 세 가지 주요 개념으로 나눌 수 있다.

00:06:56.000 --> 00:06:59.000
엔진은 자산을 관리한다.

00:06:59.000 --> 00:07:01.000
노드는 재생을 제어한다.

00:07:01.000 --> 00:07:04.000
그리고 믹서는 공간화를 제어한다.

00:07:04.000 --> 00:07:09.000
PHASE 엔진은 세 가지 주요 섹션으로 나눌 수 있다.

00:07:09.000 --> 00:07:14.000
자산 레지스트리, 장면 그래프 및 렌더링 상태.

00:07:14.000 --> 00:07:20.000
엔진 수명 주기 전반에 걸쳐 당신은 엔진에 자산을 등록하고 등록을 취소할 것입니다.

00:07:20.000 --> 00:07:26.000
오늘날, PHASE는 건전한 자산과 건전한 이벤트 자산 등록을 지원합니다.

00:07:26.000 --> 00:07:38.000
사운드 자산은 오디오 파일에서 직접 로드하거나 자신의 자산에 원시 오디오 데이터로 포장 및 내장하여 엔진에 직접 로드할 수 있습니다.

00:07:38.000 --> 00:07:50.000
사운드 이벤트 자산은 사운드 재생을 제어하는 하나 이상의 계층적 노드와 공간화를 제어하는 다운스트림 믹서의 모음입니다.

00:07:50.000 --> 00:07:55.000
장면 그래프는 시뮬레이션에 참여하는 물체의 계층 구조이다.

00:07:55.000 --> 00:08:00.000
여기에는 청취자, 출처 및 폐쇄자가 포함됩니다.

00:08:00.000 --> 00:08:07.000
청취자는 시뮬레이션을 듣는 우주의 위치를 나타내는 물체이다.

00:08:07.000 --> 00:08:12.000
소스는 소리가 어디에서 왔는지 나타내는 물체이다.

00:08:12.000 --> 00:08:18.000
Bharath가 앞서 언급했듯이, PHASE는 포인트 소스와 체적 소스를 모두 지원합니다.

00:08:18.000 --> 00:08:28.000
오클루더는 환경을 통해 이동할 때 사운드 전송에 영향을 미치는 시뮬레이션의 기하학을 나타내는 물체이다.

00:08:28.000 --> 00:08:34.000
오클루더는 또한 소리를 흡수하고 전달하는 방식에 영향을 미치는 재료가 할당된다.

00:08:34.000 --> 00:08:46.000
PHASE는 판지 상자에서 유리창, 벽돌 벽에 이르기까지 모든 것을 시뮬레이션하기 위해 occluders에 할당할 수 있는 재료 사전 설정 라이브러리와 함께 제공됩니다.

00:08:46.000 --> 00:08:56.000
장면에 개체를 추가할 때, 계층 구조로 구성하고 직접 또는 간접적으로 엔진의 루트 개체에 연결할 수 있습니다.

00:08:56.000 --> 00:09:01.000
이것은 그들이 프레임에서 프레임으로 시뮬레이션에 참여할 수 있도록 할 것이다.

00:09:01.000 --> 00:09:07.000
렌더링 상태는 사운드 이벤트와 오디오 IO 재생을 관리합니다.

00:09:07.000 --> 00:09:13.000
엔진을 처음 만들 때, 오디오 IO가 비활성화됩니다.

00:09:13.000 --> 00:09:26.000
이를 통해 오디오 IO를 실행하지 않고도 자산을 등록하고, 장면 그래프를 만들고, 사운드 이벤트를 구성하고, 다른 엔진 작업을 수행할 수 있습니다.

00:09:26.000 --> 00:09:33.000
사운드 이벤트를 재생할 준비가 되면, 내부적으로 오디오 IO를 시작하는 엔진을 시작할 수 있습니다.

00:09:33.000 --> 00:09:39.000
마찬가지로, 사운드 이벤트 재생을 마치면, 엔진을 멈출 수 있습니다.

00:09:39.000 --> 00:09:45.000
이것은 오디오 IO를 멈추고 사운드 이벤트 재생을 중지할 것이다.

00:09:45.000 --> 00:09:50.000
PHASE의 노드는 오디오 콘텐츠의 재생을 제어한다.

00:09:50.000 --> 00:09:59.000
노드는 오디오 재생을 생성하거나 제어하는 계층적 객체 모음이다.

00:09:59.000 --> 00:10:02.000
발전기 노드는 오디오를 생성한다.

00:10:02.000 --> 00:10:06.000
그것들은 항상 노드 계층 구조의 리프 노드이다.

00:10:06.000 --> 00:10:15.000
제어 노드는 공간화 전에 발전기가 어떻게 선택되고, 혼합되고, 매개 변수화되는지에 대한 논리를 설정합니다.

00:10:15.000 --> 00:10:24.000
제어 노드는 항상 부모 노드이며 복잡한 사운드 설계 시나리오를 위한 계층 구조로 구성될 수 있습니다.

00:10:24.000 --> 00:10:28.000
샘플러 노드는 발전기 노드의 일종이다.

00:10:28.000 --> 00:10:31.000
샘플러는 등록된 사운드 자산을 재생합니다.

00:10:31.000 --> 00:10:38.000
일단 구성되면, 샘플러 노드에서 몇 가지 기본 속성을 설정하여 올바르게 재생할 수 있습니다.

00:10:38.000 --> 00:10:43.000
재생 모드는 오디오 파일이 재생되는 방식을 결정합니다.

00:10:43.000 --> 00:10:51.000
재생 모드를 OneShot으로 설정하면 오디오 파일이 한 번 재생되고 자동으로 중지됩니다.

00:10:51.000 --> 00:10:56.000
이것은 음향 효과를 유발하는 것과 같은 "불과 잊기" 시나리오에서 사용될 수 있다.

00:10:56.000 --> 00:11:05.000
재생 모드를 루프로 설정하면, 샘플러를 명시적으로 멈출 때까지 오디오 파일이 무기한으로 재생됩니다.

00:11:05.000 --> 00:11:10.000
컬 옵션은 소리가 들리지 않을 때 무엇을 해야 하는지 단계적으로 알려줍니다.

00:11:10.000 --> 00:11:17.000
컬 옵션을 종료하도록 설정하면, 소리가 들리지 않을 때 자동으로 멈춥니다.

00:11:17.000 --> 00:11:26.000
컬 옵션을 절전 모드로 설정하면, 소리가 들리지 않을 때 렌더링이 중지되고 들릴 때 다시 렌더링을 시작합니다.

00:11:26.000 --> 00:11:33.000
이렇게 하면 엔진에 의해 도태될 때 수동으로 소리를 시작하고 멈출 필요가 없습니다.

00:11:33.000 --> 00:11:40.000
교정 수준은 데시벨 SPL에서 소리의 실제 수준을 설정합니다.

00:11:40.000 --> 00:11:44.000
단계는 또한 네 가지 유형의 제어 노드를 지원합니다.

00:11:44.000 --> 00:11:51.000
여기에는 랜덤, 스위치, 블렌드 및 컨테이너 노드가 포함됩니다.

00:11:51.000 --> 00:11:58.000
무작위 노드는 가중 무작위 선택에 따라 자식 중 하나를 선택합니다.

00:11:58.000 --> 00:12:08.000
예를 들어, 이 경우, 왼쪽 샘플러는 다음에 사운드 이벤트가 트리거될 때 오른쪽 샘플러에 대해 4:1의 확률을 가지고 있습니다.

00:12:08.000 --> 00:12:14.000
스위치 노드는 매개 변수 이름에 따라 자식 사이를 전환합니다.

00:12:14.000 --> 00:12:20.000
예를 들어, 지형 스위치를 "삐는 나무"에서 "부드러운 자갈"로 바꿀 수 있습니다.

00:12:20.000 --> 00:12:26.000
다음에 사운드 이벤트가 트리거되면, 매개 변수 이름과 일치하는 샘플러를 선택합니다.

00:12:26.000 --> 00:12:32.000
블렌드 노드는 매개 변수 값을 기반으로 자식 간에 혼합됩니다.

00:12:32.000 --> 00:12:46.000
예를 들어, 블렌드 노드에 습기 매개 변수를 할당할 수 있으며, 이는 건조한 끝의 시끄러운 발걸음과 조용한 스플래시와 젖은 끝의 조용한 발걸음과 시끄러운 스플래시를 혼합할 수 있습니다.

00:12:46.000 --> 00:12:51.000
컨테이너 노드는 모든 자식을 한 번에 재생한다.

00:12:51.000 --> 00:13:02.000
예를 들어, 발자국을 재생하는 하나의 샘플러와 ruffling Gor-Tex 재킷의 소리와 같은 옷의 소리를 재생하는 또 다른 샘플러를 가질 수 있습니다.

00:13:02.000 --> 00:13:11.000
컨테이너가 트리거될 때마다, 두 샘플러 모두 동시에 재생됩니다.

00:13:11.000 --> 00:13:17.000
오디오 콘텐츠의 위상 제어 공간화의 믹서.

00:13:17.000 --> 00:13:23.000
PHASE는 현재 채널, 주변 및 공간 믹서를 지원합니다.

00:13:23.000 --> 00:13:29.000
채널 믹서는 공간화와 환경 효과 없이 오디오를 렌더링한다.

00:13:29.000 --> 00:13:42.000
스테레오 음악이나 중앙 채널 내러티브 대화와 같이 출력 장치로 직접 렌더링해야 하는 일반 스템 기반 콘텐츠에 채널 믹서를 사용하세요.

00:13:42.000 --> 00:13:49.000
주변 믹서는 거리 모델링이나 환경 영향 없이 외부화로 오디오를 렌더링합니다.

00:13:49.000 --> 00:13:57.000
청취자가 머리를 회전시키면서, 소리는 우주에서 같은 상대적인 위치에서 계속 나올 것이다.

00:13:57.000 --> 00:14:12.000
환경에서 시뮬레이션되지 않지만 여전히 우주의 어딘가에서 오는 것처럼 들리는 멀티채널 콘텐츠에 앰비언트 믹서를 사용하세요. 예를 들어, 큰 숲에서 짹짹거리는 귀뚜라미의 배경.

00:14:12.000 --> 00:14:16.000
공간 믹서는 완전한 공간화를 수행한다.

00:14:16.000 --> 00:14:31.000
음원이 청취자를 기준으로 이동함에 따라, 패닝, 거리 모델링 및 지향성 모델링 알고리즘을 기반으로 인식된 위치, 레벨 및 주파수 응답의 변화를 들을 수 있습니다.

00:14:31.000 --> 00:14:39.000
이 외에도, 기하학 인식 환경 효과는 소스와 청취자 사이의 경로에 적용된다.

00:14:39.000 --> 00:14:45.000
헤드폰을 착용하고 있다면, 바이노럴 필터를 적용하여 외부화도 받을 수 있습니다.

00:14:45.000 --> 00:14:52.000
전체 환경 시뮬레이션에 참여해야 하는 소리를 위해 공간 믹서를 사용하세요.

00:14:52.000 --> 00:14:58.000
공간 믹서는 두 가지 독특한 거리 모델링 알고리즘을 지원합니다.

00:14:58.000 --> 00:15:04.000
거리에 대한 자연 감쇠를 위한 표준 기하학적 확산 손실을 설정할 수 있습니다.

00:15:04.000 --> 00:15:10.000
당신은 또한 당신의 취향에 따라 효과를 늘리거나 줄일 수 있습니다.

00:15:10.000 --> 00:15:17.000
예를 들어, 거리를 두고 대화를 하고 싶다면 가치를 낮추는 것이 유용할 수 있다.

00:15:17.000 --> 00:15:25.000
스펙트럼의 다른 쪽 끝에서, 당신은 거리에 대한 감쇠의 전체 조각별 곡선 세그먼트를 추가할 수 있습니다.

00:15:25.000 --> 00:15:40.000
예를 들어, 범위의 시작과 끝에서 자연 거리 감쇠가 있는 세그먼트 세트를 구성할 수 있지만, 증가된 거리에서 중요한 대화를 들을 수 있도록 중간에 감쇠를 줄일 수 있습니다.

00:15:40.000 --> 00:15:47.000
포인트 소스의 경우, 공간 믹서는 두 가지 다른 지향성 모델링 알고리즘을 지원합니다.

00:15:47.000 --> 00:15:52.000
공간 믹스에 유산소 지향성 모델링을 추가할 수 있습니다.

00:15:52.000 --> 00:16:03.000
몇 가지 간단한 수정을 사용하여, 당신은 카디오이드 지향성 패턴이나 하이퍼 카디오이드 패턴의 어쿠스틱 현악기의 소리로 인간 스피커를 모델링할 수 있습니다.

00:16:03.000 --> 00:16:06.000
원뿔 지향성 모델링을 추가할 수도 있습니다.

00:16:06.000 --> 00:16:13.000
이 클래식 모드를 사용하면 지향성 필터링을 특정 회전 범위 내에서 제한할 수 있습니다.

00:16:13.000 --> 00:16:21.000
공간 믹서는 또한 공간 파이프라인을 기반으로 한 기하학 인식 환경 효과를 지원합니다.

00:16:21.000 --> 00:16:29.000
공간 파이프라인은 활성화하거나 비활성화할 환경 효과와 각각에 대한 전송 레벨을 선택합니다.

00:16:29.000 --> 00:16:37.000
단계는 현재 직접 경로 전송, 초기 반사 및 후기 잔향을 지원합니다.

00:16:37.000 --> 00:16:44.000
직접 경로 전송은 소스와 리스너 사이의 직접 및 차단된 경로를 렌더링합니다.

00:16:44.000 --> 00:16:54.000
막힌 소리로, 일부 에너지는 물질에 의해 흡수되는 반면, 다른 에너지는 물체의 반대편으로 전달된다는 점에 유의하십시오.

00:16:54.000 --> 00:17:01.000
초기 반사는 직접 경로에 강도 수정과 채색을 모두 제공한다.

00:17:01.000 --> 00:17:07.000
이것들은 보통 벽과 바닥의 반사 반사로 만들어진다.

00:17:07.000 --> 00:17:12.000
더 큰 공간에서, 그들은 또한 경험에 눈에 띄는 메아리를 더한다.

00:17:12.000 --> 00:17:17.000
늦은 잔향은 환경의 소리를 제공한다.

00:17:17.000 --> 00:17:26.000
그것은 공간의 최종 가청 표현으로 합쳐진 확산된 흩어진 에너지의 조밀한 축적이다.

00:17:26.000 --> 00:17:34.000
방의 크기와 모양에 대한 단서를 제공하는 것 외에도, 그것은 또한 당신에게 포위감을 줄 책임이 있습니다.

00:17:34.000 --> 00:17:45.000
이제 PHASE 엔진, 노드 및 믹서의 개념을 검토했으므로, 이러한 개념을 몇 가지 샘플 사용 사례와 함께 가져올 때입니다.

00:17:45.000 --> 00:17:55.000
이 섹션에서는 오디오 파일을 재생하고, 공간 오디오 경험을 구축하고, 행동 사운드 이벤트를 구축하는 과정을 안내해 드리겠습니다.

00:17:55.000 --> 00:18:08.000
이 세 가지 주요 영역은 처음에 부드러운 소개와 중간과 끝을 향한 더 흥미로운 기능에 대한 깊은 다이빙과 함께 PHASE의 기능에 대한 광범위한 개요를 제공할 것입니다.

00:18:08.000 --> 00:18:13.000
일을 시작하기 위해, 오디오 파일을 재생하는 방법을 보여드리겠습니다.

00:18:13.000 --> 00:18:17.000
먼저, PHASE 엔진 인스턴스를 만들어 봅시다.

00:18:17.000 --> 00:18:24.000
다음으로, 오디오 파일의 URL을 검색하고 PHASE에 사운드 자산을 등록할 것입니다.

00:18:24.000 --> 00:18:29.000
나중에 참조할 수 있도록 "드럼"이라는 이름을 붙일게.

00:18:29.000 --> 00:18:34.000
여기서 나는 엔진을 만들고 코드에 사운드 자산을 등록할 것이다.

00:18:34.000 --> 00:18:39.000
먼저, 자동 업데이트 모드에서 PHASE 엔진 인스턴스를 만들 것입니다.

00:18:39.000 --> 00:18:45.000
이것은 일을 시작하고 실행하는 데 선호되는 모드이므로, 간단한 재생을 시연하기 위해 여기에서 사용하고 있습니다.

00:18:45.000 --> 00:18:51.000
게임이 프레임 업데이트와 더 정확한 동기화가 필요할 때, 수동 모드가 가장 좋은 방법이라는 점에 유의하십시오.

00:18:51.000 --> 00:18:54.000
자세한 내용은 문서를 확인하세요.

00:18:54.000 --> 00:19:00.000
다음으로, 애플리케이션 번들에 저장된 오디오 파일의 URL을 검색하겠습니다.

00:19:00.000 --> 00:19:07.000
이것은 준비된 드럼 루프 샘플이 있는 모노 24비트, 48kHz WAV 파일입니다.

00:19:07.000 --> 00:19:13.000
사운드 자산을 엔진에 등록할 때, 나는 몇 가지 추가 주장을 제공할 것이다.

00:19:13.000 --> 00:19:17.000
나중에 참조할 수 있도록 사운드 자산에 고유한 이름을 부여하겠습니다.

00:19:17.000 --> 00:19:26.000
사운드 자산 내의 오디오 데이터는 실시간으로 메모리로 스트리밍하는 것과는 달리 상주 메모리에 사전 로드되어야 한다고 명시할 것이다.

00:19:26.000 --> 00:19:34.000
드럼 루프가 상당히 짧기 때문에 이것은 괜찮을 것이며, 짧은 연속으로 여러 번 재생하고 싶을 수도 있습니다.

00:19:34.000 --> 00:19:40.000
나는 또한 출력 장치의 보정된 음량을 위해 사운드 자산을 정상화하기로 선택하고 있다.

00:19:40.000 --> 00:19:44.000
일반적으로, 입력을 정상화하는 것이 좋습니다.

00:19:44.000 --> 00:19:50.000
이렇게 하면 샘플러에 할당하고 목표 출력 수준을 설정하면 콘텐츠를 더 쉽게 혼합할 수 있습니다.

00:19:50.000 --> 00:19:58.000
이제 엔진에 사운드 자산을 등록했으니, 사운드 이벤트 자산을 구성할 것이다.

00:19:58.000 --> 00:20:02.000
먼저 채널 레이아웃에서 채널 믹서를 만들겠습니다.

00:20:02.000 --> 00:20:05.000
그럼 내가 샘플러 노드를 만들게.

00:20:05.000 --> 00:20:11.000
샘플러 노드는 등록된 사운드 자산의 이름과 다운스트림 채널 믹서에 대한 참조를 취한다.

00:20:11.000 --> 00:20:17.000
다음으로 올바르게 재생될 수 있도록 샘플러 노드에 몇 가지 기본 속성을 설정하겠습니다.

00:20:17.000 --> 00:20:27.000
재생 모드는 샘플러가 오디오 파일을 루프할지 여부를 설정하고, 보정 수준은 믹스 내에서 샘플러의 인지된 음량을 설정합니다.

00:20:27.000 --> 00:20:39.000
이제 샘플러 노드의 출력을 채널 믹서의 입력에 연결하고 몇 가지 기본 매개 변수를 설정했으므로, 사운드 이벤트 자산을 엔진에 등록할 때입니다.

00:20:39.000 --> 00:20:46.000
이 경우, 나는 사운드 이벤트 자산을 drumEvent라는 이름으로 등록할 것이며, 나중에 참조하는 데 사용할 것이다.

00:20:46.000 --> 00:20:49.000
여기서 나는 코드로 사운드 이벤트 자산을 등록할 것이다.

00:20:49.000 --> 00:20:54.000
나는 모노 ChannelLayoutTag에서 channelLayout을 만들 것이다.

00:20:54.000 --> 00:20:59.000
그런 다음 모노 채널 레이아웃으로 채널 믹서를 초기화하겠습니다.

00:20:59.000 --> 00:21:09.000
다음으로 나는 샘플러 노드를 만들고 이전에 엔진에 등록한 모노 드럼 자산을 나타내는 드럼이라는 이름을 전달할 것이다.

00:21:09.000 --> 00:21:13.000
샘플러 노드는 다운스트림 채널 믹서로 라우팅될 것이다.

00:21:13.000 --> 00:21:15.000
재생 모드를 루프로 설정하겠습니다.

00:21:15.000 --> 00:21:21.000
이것은 내가 코드에서 명시적으로 멈출 때까지 소리가 계속 재생되도록 할 것이다.

00:21:21.000 --> 00:21:27.000
CalibrationMode를 relativeSpl로 설정하고 레벨을 0 데시벨로 설정하겠습니다.

00:21:27.000 --> 00:21:41.000
이것은 경험을 위한 편안한 청취 수준을 보장할 것이다. 마지막으로 soundEventAsset을 엔진에 등록하고 drumEvent라는 이름을 전달하여 나중에 재생을 위한 사운드 이벤트를 만들기 시작할 때 참조할 수 있습니다.

00:21:41.000 --> 00:21:47.000
사운드 이벤트 자산이 등록되면, 인스턴스를 만들고 재생을 시작할 수 있습니다.

00:21:47.000 --> 00:21:55.000
내가 가장 먼저 할 일은 drumEvent라는 등록된 사운드 이벤트 자산에서 사운드 이벤트를 만드는 것이다.

00:21:55.000 --> 00:21:59.000
이제 사운드 이벤트가 있으니, 엔진 시동을 걸겠습니다.

00:21:59.000 --> 00:22:06.000
이것은 내가 출력 장치를 공급하는 오디오를 들을 수 있도록 오디오 IO를 시작할 것이다.

00:22:06.000 --> 00:22:08.000
마침내 나는 사운드 이벤트를 시작할 거야.

00:22:08.000 --> 00:22:21.000
이 시점에서, 로드된 사운드 자산은 샘플러를 통해 재생되고, 채널 믹서로 라우팅되고, 현재 출력 형식으로 다시 매핑되고, 출력 장치를 통해 재생됩니다.

00:22:21.000 --> 00:22:25.000
여기서 나는 코드로 사운드이벤트를 시작할 것이다.

00:22:25.000 --> 00:22:30.000
사운드 이벤트 자산은 등록된 사운드 자산의 이름으로 구성된다.

00:22:30.000 --> 00:22:33.000
여기, 나는 드럼 이벤트를 지나가고 있어.

00:22:33.000 --> 00:22:40.000
나는 계속해서 엔진을 시작할 것이고, 오디오 IO를 시작하고 사운드 이벤트를 시작할 것이다.

00:22:40.000 --> 00:22:45.000
사운드 이벤트 재생이 끝나면, 엔진을 정리할 수 있습니다.

00:22:45.000 --> 00:22:48.000
먼저 사운드 이벤트를 중단하겠습니다.

00:22:48.000 --> 00:22:50.000
그럼 내가 엔진을 멈출게.

00:22:50.000 --> 00:22:55.000
이것은 오디오 IO를 멈추고 사운드 이벤트 재생을 중지할 것이다.

00:22:55.000 --> 00:23:02.000
다음으로 나는 drumEvent라는 이름의 사운드 이벤트 자산의 등록을 취소하고 드럼이라는 이름의 사운드 자산의 등록을 취소할 것이다.

00:23:02.000 --> 00:23:05.000
마침내 나는 엔진을 파괴할 거야.

00:23:05.000 --> 00:23:07.000
여기서 나는 코드로 정리를 할 것이다.

00:23:07.000 --> 00:23:12.000
먼저 드럼 루프를 다 들으면 사운드 이벤트를 중단할 거야.

00:23:12.000 --> 00:23:17.000
그러면 나는 내부적으로 오디오 IO를 멈출 엔진을 멈출 것이다.

00:23:17.000 --> 00:23:24.000
다음으로 나는 drumEvent라는 이름의 사운드 이벤트 자산의 등록을 취소하고 드럼이라는 이름의 사운드 자산의 등록을 취소할 것이다.

00:23:24.000 --> 00:23:28.000
마침내 나는 엔진을 파괴할 거야.

00:23:28.000 --> 00:23:35.000
이제 기본 사항을 다뤘으니, PHASE에서 간단한 공간 오디오 경험을 구축하는 방법을 보여드리겠습니다.

00:23:35.000 --> 00:23:42.000
우리는 공간 믹서, 체적 음원 및 오클루더를 포함한 주제를 다룰 것입니다.

00:23:42.000 --> 00:23:47.000
내가 가장 먼저 할 일은 엔진에 사운드 이벤트 자산을 등록하는 것이다.

00:23:47.000 --> 00:23:54.000
이 예를 들어, 나는 이미 드럼 사운드 이벤트가 등록된 엔진으로 시작할 것이다.

00:23:54.000 --> 00:24:00.000
여기서부터, 나는 간단한 채널 기반 재생에서 완전한 공간화로 믹스를 업그레이드할 것이다.

00:24:00.000 --> 00:24:09.000
내가 가장 먼저 할 일은 내 음원에 다른 환경 효과를 선택적으로 적용하기 위해 공간 파이프라인을 구성하는 것이다.

00:24:09.000 --> 00:24:13.000
그러면 나는 공간 파이프라인에서 공간 믹서를 만들 것이다.

00:24:13.000 --> 00:24:20.000
일단 구성되면, 올바르게 재생될 수 있도록 공간 믹서에 몇 가지 기본 속성을 설정할 것입니다.

00:24:20.000 --> 00:24:26.000
이 예에서, 나는 거리의 레벨 감쇠를 제어하기 위해 거리 모델을 설정할 것이다.

00:24:26.000 --> 00:24:35.000
나는 또한 청취자와 상대적인 소스 사이의 각도에 따라 레벨 감쇠를 제어하기 위해 지향성 모델을 설정할 것이다.

00:24:35.000 --> 00:24:37.000
그럼 내가 샘플러 노드를 만들게.

00:24:37.000 --> 00:24:43.000
샘플러 노드는 등록된 사운드 자산의 이름과 다운스트림 공간 믹서에 대한 참조를 취한다.

00:24:43.000 --> 00:24:49.000
다음으로 올바르게 재생될 수 있도록 샘플러 노드에 몇 가지 기본 속성을 설정하겠습니다.

00:24:49.000 --> 00:24:55.000
재생 모드와 보정 수준 외에도, 여기서 컬 옵션을 설정할 것입니다.

00:24:55.000 --> 00:25:00.000
이것은 샘플러가 들리지 않을 때 무엇을 해야 하는지 단계를 알려줄 것이다.

00:25:00.000 --> 00:25:11.000
이제 샘플러 노드의 출력을 공간 믹서의 입력에 연결하고 몇 가지 기본 매개 변수를 설정했으므로 사운드 이벤트 자산을 엔진에 등록할 때입니다.

00:25:11.000 --> 00:25:13.000
나는 이전과 같은 이름을 사용할 거야.

00:25:13.000 --> 00:25:17.000
여기서 나는 코드로 건전한 이벤트 자산을 만들 것이다.

00:25:17.000 --> 00:25:34.000
먼저 .directPathTransmission과 .lateReverb를 렌더링할 spatialPipeline을 만들 것입니다. 나는 또한 .lateReverb .sendLevel을 설정하여 직접 대 잔향 비율을 제어하고 늦은 잔향 시뮬레이션을 위한 .mediumRoom 프리셋을 선택할 것이다.

00:25:34.000 --> 00:25:38.000
그런 다음 나는 spatialPipeline으로 공간 믹서를 만들 것이다.

00:25:38.000 --> 00:25:46.000
다음으로 나는 자연스럽게 들리는 GeometricSpreadingDistanceModel을 만들어 공간 믹서에 할당할 것이다.

00:25:46.000 --> 00:25:49.000
내가 컬어디스턴을 10미터로 설정할게.

00:25:49.000 --> 00:25:55.000
만약 소스가 이 거리를 넘어선다면, 나는 그것을 믹스에서 자동으로 뽑아주고 싶다.

00:25:55.000 --> 00:26:00.000
그리고 나는 거리 감쇠 효과를 덜 강조하기 위해 롤오프팩터를 조금 조정할 것이다.

00:26:00.000 --> 00:26:11.000
그런 다음 샘플러 노드를 만들고 이전에 엔진에 등록한 모노 드럼 사운드 자산을 나타내는 "드럼"이라는 이름을 전달할 것입니다.

00:26:11.000 --> 00:26:13.000
재생 모드를 .looping으로 설정하겠습니다.

00:26:13.000 --> 00:26:21.000
캘리브레이션 모드를 .relativeSpl로 설정하고 레벨을 +12 데시벨로 설정하여 샘플러의 출력 레벨을 높일 것입니다.

00:26:21.000 --> 00:26:23.000
그리고 나는 컬옵션을 자도록 설정할 거야.

00:26:23.000 --> 00:26:33.000
마지막으로, soundEventAsset을 엔진에 등록하고 drumEvent라는 이름을 전달하여 나중에 사운드 이벤트를 만들기 시작할 때 참조할 수 있습니다.

00:26:33.000 --> 00:26:40.000
이제 엔진에 등록된 사운드 이벤트 자산이 있으니 시뮬레이션을 위한 장면을 만들어야 합니다.

00:26:40.000 --> 00:26:45.000
이것은 리스너, 소스 및 오클루더를 만드는 것을 포함한다.

00:26:45.000 --> 00:26:50.000
이 예에서, 나는 소스와 리스너 사이에 오클루더를 배치할 것이다.

00:26:50.000 --> 00:26:53.000
먼저 나는 청취자를 만들 거야.

00:26:53.000 --> 00:26:56.000
그럼 내가 변환을 설정할게.

00:26:56.000 --> 00:27:04.000
장면 그래프 내에서 청취자를 활성화할 준비가 되면, 엔진의 루트 객체 또는 자식 중 하나에 첨부할 것입니다.

00:27:04.000 --> 00:27:07.000
여기서 나는 리스너를 코드로 설정할 것이다.

00:27:07.000 --> 00:27:11.000
먼저 나는 청취자를 만들 거야. 그럼 내가 변환을 설정할게.

00:27:11.000 --> 00:27:15.000
이 예에서, 나는 청취자를 회전 없이 원점으로 설정할 것이다.

00:27:15.000 --> 00:27:20.000
마지막으로 나는 리스너를 엔진의 루트 객체에 연결할 것이다.

00:27:20.000 --> 00:27:23.000
이제 체적 소스를 설정해 봅시다.

00:27:23.000 --> 00:27:27.000
먼저 나는 메쉬에서 소스 모양을 만들 것이다.

00:27:27.000 --> 00:27:31.000
그러면 나는 그 모양에서 소스를 만들 것이다.

00:27:31.000 --> 00:27:35.000
이것은 본질적으로 체적 소스를 구성한다.

00:27:35.000 --> 00:27:38.000
다음으로 나는 그것의 변형을 설정할 것이다.

00:27:38.000 --> 00:27:46.000
그리고 장면 그래프 내에서 소스를 활성화할 준비가 되면, 엔진의 루트 객체 또는 자식 중 하나에 첨부할 것입니다.

00:27:46.000 --> 00:27:50.000
여기서 나는 코드에 부피 측정 소스를 설정할 것이다.

00:27:50.000 --> 00:27:57.000
먼저 나는 Icosahedron 메쉬를 만들고 HomePod Mini의 크기로 확장할 것이다.

00:27:57.000 --> 00:27:59.000
그러면 나는 메시에서 모양을 만들 것이다.

00:27:59.000 --> 00:28:05.000
이 모양은 체적 소스의 여러 인스턴스를 구성하기 위해 재사용될 수 있다.

00:28:05.000 --> 00:28:14.000
예를 들어, 나는 같은 메쉬를 공유하는 시뮬레이션에 여러 개의 HomePod Mini를 배치할 수 있다.

00:28:14.000 --> 00:28:18.000
다음으로 나는 그 모양에서 체적 소스를 만들 것이다.

00:28:18.000 --> 00:28:26.000
입력으로 모양을 취하지 않는 이니셜라이저 버전을 사용하여 간단한 포인트 소스를 만들 수도 있습니다.

00:28:26.000 --> 00:28:28.000
그럼 내가 변환을 설정할게.

00:28:28.000 --> 00:28:36.000
나는 청취자 앞에서 2미터 떨어진 곳에서 소스를 번역하고 그들이 서로 마주보고 있도록 청취자를 향해 다시 회전시킬 것이다.

00:28:36.000 --> 00:28:42.000
마지막으로 엔진의 루트 객체에 소스를 첨부하겠습니다.

00:28:42.000 --> 00:28:44.000
이제 occluder를 설치합시다.

00:28:44.000 --> 00:28:50.000
먼저 나는 메쉬로 모양을 만들 것이다.

00:28:50.000 --> 00:28:55.000
그런 다음 나는 판지 재료를 만들고 그것을 모양에 할당할 것이다.

00:28:55.000 --> 00:29:00.000
이제 그 모양은 기하학과 관련 재료를 가지고 있다.

00:29:00.000 --> 00:29:03.000
다음으로 나는 그 모양에서 오클루더를 만들 것이다.

00:29:03.000 --> 00:29:06.000
그럼 내가 변환을 설정할게.

00:29:06.000 --> 00:29:15.000
그리고 장면 그래프 내에서 오클루더를 활성화할 준비가 되면, 엔진의 루트 객체나 자식 중 하나에 부착할 것이다.

00:29:15.000 --> 00:29:17.000
여기서 나는 코드로 occluder를 설정할 것이다.

00:29:17.000 --> 00:29:22.000
먼저 나는 boxMesh를 만들고 그에 따라 치수를 조정할 것이다.

00:29:22.000 --> 00:29:25.000
그러면 나는 메시에서 모양을 만들 것이다.

00:29:25.000 --> 00:29:30.000
이 모양은 오클루더의 여러 인스턴스를 구성하기 위해 재사용할 수 있다.

00:29:30.000 --> 00:29:35.000
예를 들어, 나는 같은 메쉬를 공유하는 시뮬레이션에 여러 상자를 배치할 수 있다.

00:29:35.000 --> 00:29:44.000
다음으로, 나는 판지 상자 프리셋에서 재료를 만들고 그 재료를 모양에 할당할 것이다.

00:29:44.000 --> 00:29:48.000
다음으로 나는 그 모양에서 오클루더를 만들 것이다.

00:29:48.000 --> 00:29:50.000
그럼 내가 변환을 설정할게.

00:29:50.000 --> 00:29:58.000
나는 청취자 앞에서 occluder를 1미터 번역하고 청취자를 향해 다시 돌려서 그들이 서로 마주보게 할 것이다.

00:29:58.000 --> 00:30:02.000
이것은 occluder를 소스와 청취자 사이의 중간에 둔다.

00:30:02.000 --> 00:30:07.000
마지막으로 나는 엔진의 루트 객체에 오클루더를 부착할 것이다.

00:30:07.000 --> 00:30:14.000
이 시점에서, 나는 소스와 청취자 사이의 중간에 오클루더가 있는 장면이 있다.

00:30:14.000 --> 00:30:22.000
다음으로 등록된 사운드 이벤트 자산에서 사운드 이벤트를 만들고 장면 그래프의 소스 및 청취자와 연결할 것입니다.

00:30:22.000 --> 00:30:32.000
내가 사운드 이벤트를 시작할 때, 나는 판지 상자의 반대편에 있는 작은 체적 소스에서 막린 드럼 루프가 연주하는 것을 들을 것이다.

00:30:32.000 --> 00:30:35.000
여기서 나는 코드로 사운드 이벤트를 시작할 것이다.

00:30:35.000 --> 00:30:40.000
먼저 나는 사운드 이벤트에서 소스와 청취자를 공간 믹서와 연관시킬 것이다.

00:30:40.000 --> 00:30:45.000
그런 다음 drumEvent라는 등록된 soundEvent 자산에서 soundEvent를 만들 것입니다.

00:30:45.000 --> 00:30:47.000
나머지는 이전과 같다.

00:30:47.000 --> 00:30:51.000
엔진이 작동 중인지 확인한 다음, 사운드 이벤트를 시작하세요.

00:30:51.000 --> 00:30:58.000
이제 공간 오디오를 다루었으니, 복잡한 사운드 이벤트를 만드는 방법을 보여드리겠습니다.

00:30:58.000 --> 00:31:04.000
사운드 이벤트는 인터랙티브 사운드 디자인을 위한 행동 계층으로 구성될 수 있다.

00:31:04.000 --> 00:31:13.000
이 섹션에서는 최종 사운드 이벤트를 만들기 위해 각 유형의 사운드 이벤트 노드를 기반으로 하는 연속적인 예제를 안내해 드리겠습니다.

00:31:13.000 --> 00:31:23.000
여기서 우리는 시끄러운 고어텍스 재킷을 입고 다양한 유형의 지형을 걷는 배우를 모델링할 것이다.

00:31:23.000 --> 00:31:27.000
먼저 나는 삐걱거리는 나무에서 발자국을 재생하는 샘플러 노드를 만들 것이다.

00:31:27.000 --> 00:31:43.000
코드에서, 나는 "footstep_wood_clip_1"이라는 등록된 사운드 자산으로 샘플러 노드를 만들 것이다. 이 예에서, 이 노드와 다른 모든 노드는 사전 구성된 단일 채널 믹서에서 재생됩니다.

00:31:43.000 --> 00:31:45.000
이제 나는 무작위성을 추가할 것이다.

00:31:45.000 --> 00:31:54.000
나는 삐걱거리는 나무 샘플에서 약간 다른 발자국을 연주하는 두 개의 자식 샘플러 노드로 무작위 노드를 만들 것이다.

00:31:54.000 --> 00:31:58.000
코드에서, 나는 두 개의 샘플러 노드를 만들 것이다.

00:31:58.000 --> 00:32:08.000
첫 번째는 "footstep_wood_clip_1"이라는 등록된 사운드 자산을 사용하고 두 번째는 "footstep_wood_clip_2"라는 등록된 사운드 자산을 사용합니다.

00:32:08.000 --> 00:32:13.000
그런 다음, 나는 무작위 노드를 만들고 샘플러 노드를 자식으로 추가할 것이다.

00:32:13.000 --> 00:32:22.000
연속적인 반복을 통해 그 아이가 선택될 가능성을 제어하기 위해 가중치 요인이 각 자식 노드에 적용된다는 점에 유의하십시오.

00:32:22.000 --> 00:32:29.000
이 경우, 첫 번째 아이는 두 번째 아이보다 선택될 확률이 두 배이다.

00:32:29.000 --> 00:32:31.000
다음으로 나는 지형 스위치를 추가할 것이다.

00:32:31.000 --> 00:32:35.000
나는 스위치 노드와 두 개의 무작위 노드를 자식으로 만들 것이다.

00:32:35.000 --> 00:32:43.000
이 경우, 두 번째 무작위 노드는 나무의 무작위 발자국이 아닌 자갈에서 무작위 발자국을 재생한다.

00:32:43.000 --> 00:32:46.000
나는 스위치를 제어하기 위해 지형 매개 변수를 사용할 것이다.

00:32:46.000 --> 00:32:50.000
코드에서, 나는 두 개의 샘플러 노드를 만들 것이다.

00:32:50.000 --> 00:33:00.000
첫 번째는 "footstep_gravel_clip_1"이라는 등록된 사운드 자산을 사용하고 두 번째는 "footstep_gravel_clip_2"라는 등록된 사운드 자산을 사용합니다.

00:33:00.000 --> 00:33:06.000
그런 다음 임의의 노드를 만들고 샘플러 노드를 자식으로 추가할 것입니다.

00:33:06.000 --> 00:33:09.000
다음으로 나는 지형 매개 변수를 만들 것이다.

00:33:09.000 --> 00:33:11.000
기본값은 "creaky_wood"입니다.

00:33:11.000 --> 00:33:16.000
그런 다음 지형 매개 변수에 의해 제어되는 스위치 노드를 만들 것입니다.

00:33:16.000 --> 00:33:21.000
나무 랜덤 노드와 자갈 랜덤 노드라는 두 아이를 추가할 것이다.

00:33:21.000 --> 00:33:25.000
매개 변수를 "creaky_wood"로 설정하면 우드 랜덤 노드가 선택됩니다.

00:33:25.000 --> 00:33:31.000
마찬가지로, 매개 변수를 "soft_gravel"로 설정하면 자갈 랜덤 노드가 선택됩니다.

00:33:31.000 --> 00:33:33.000
다음으로 나는 습기 블렌드를 추가할 것이다.

00:33:33.000 --> 00:33:40.000
나는 어렸을 때 지형 스위치 노드와 랜덤 스플래시 노드로 블렌드 노드를 만들 것이다.

00:33:40.000 --> 00:33:53.000
새로운 랜덤 스플래시 노드는 배우가 발걸음을 내딛을 때 무작위 스플래시 소음을 재생하는 반면, 지형 스위치는 배우의 발이 삐걱거리는 나무나 부드러운 자갈 위를 걷고 있는지 결정합니다.

00:33:53.000 --> 00:34:08.000
건조한 발걸음 소리와 튀는 소리 사이의 혼합은 완전히 건조한--큰 발자국과 튀지 않는--에서 완전히 젖은--조용한 발자국과 큰 튀는 소리에 이르기까지 습기 매개 변수에 달려 있습니다.

00:34:08.000 --> 00:34:12.000
코드에서, 나는 두 개의 샘플러 노드를 만들 것이다.

00:34:12.000 --> 00:34:21.000
첫 번째는 "splash_clip_1"이라는 등록된 사운드 자산을 사용하고 두 번째는 "splash_clip_2"라는 등록된 사운드 자산을 사용합니다.

00:34:21.000 --> 00:34:26.000
그런 다음 임의의 노드를 만들고 샘플러 노드를 자식으로 추가할 것입니다.

00:34:26.000 --> 00:34:30.000
다음으로 나는 습기 매개 변수를 만들 것이다.

00:34:30.000 --> 00:34:35.000
범위는 0에서 1이 될 것이며, 기본값은 0.5가 될 것이다.

00:34:35.000 --> 00:34:41.000
내 게임에서 지원하는 모든 값과 범위로 매개 변수를 설정할 수 있습니다.

00:34:41.000 --> 00:34:45.000
그런 다음 습기 매개 변수에 의해 제어되는 블렌드 노드를 만들 것입니다.

00:34:45.000 --> 00:34:51.000
나는 두 아이, 지형 스위치 노드와 랜덤 스플래시 노드를 추가할 것이다.

00:34:51.000 --> 00:34:58.000
매개 변수를 0으로 설정하면, 지형에 따라 삐걱거리는 나무나 자갈에서만 건조한 발자국 소리가 들릴 것이다.

00:34:58.000 --> 00:35:10.000
습기를 0에서 1로 높이면서, 나는 젖은 지형을 시뮬레이션하여 각 발자국에 수반되는 스플래시 소음의 음량을 증가시킬 것이다.

00:35:10.000 --> 00:35:17.000
마지막으로 나는 어렸을 때 습윤 블렌드 노드와 무작위로 시끄러운 의류 노드가 있는 컨테이너 노드를 만들 것이다.

00:35:17.000 --> 00:35:27.000
새로운 시끄러운 의류 노드는 배우가 다양한 습기로 지형을 바꾸는 단계를 밟을 때마다 고어텍스 재킷의 울퉁불퉁한 소리를 재생한다.

00:35:27.000 --> 00:35:34.000
이 최종 노드 계층 구조가 제자리에 있으면, 나는 장면에서 걷는 배우의 완전한 표현을 가지고 있다.

00:35:34.000 --> 00:35:46.000
배우가 한 걸음을 내딛을 때마다, 나는 지형 매개 변수에 따라 재킷의 울퉁불퉁한 소리와 삐걱거리는 나무나 부드러운 자갈길의 발자국 소리를 들을 것이다.

00:35:46.000 --> 00:35:52.000
이 외에도, 나는 습기 매개 변수에 따라 각 발자국마다 튀는 소리를 다소 들을 것이다.

00:35:52.000 --> 00:35:56.000
코드에서, 나는 두 개의 샘플러 노드를 만들 것이다.

00:35:56.000 --> 00:36:06.000
첫 번째는 "gortex_clip_1"이라는 등록된 사운드 자산을 사용하고 두 번째는 "gortex_clip_2"라는 등록된 사운드 자산을 사용합니다.

00:36:06.000 --> 00:36:11.000
그런 다음 임의의 노드를 만들고 샘플러 노드를 자식으로 추가할 것입니다.

00:36:11.000 --> 00:36:15.000
마지막으로 나는 actor_container 노드를 만들 것이다.

00:36:15.000 --> 00:36:20.000
나는 두 개의 자식, wetness_blend 노드와 noisy_clothing_random 노드를 추가할 것이다.

00:36:20.000 --> 00:36:25.000
함께, 그들은 배우의 완전한 소리를 나타낸다.

00:36:25.000 --> 00:36:29.000
검토에서, 우리는 오디오 파일을 재생하는 방법을 배웠다.

00:36:29.000 --> 00:36:38.000
이에 따라, 우리는 간단하면서도 효과적인 공간 오디오 경험을 구축하기 위해 먼저 뛰어들어 지식을 확장했습니다.

00:36:38.000 --> 00:36:44.000
여기서 우리는 청취자, 체적 소스 및 occluders에 대해 배웠습니다.

00:36:44.000 --> 00:36:50.000
마지막으로, 우리는 인터랙티브 사운드 디자인을 위한 행동 사운드 이벤트를 만드는 것에 대해 배웠다.

00:36:50.000 --> 00:37:01.000
여기서 우리는 계층적, 대화형 사운드 이벤트를 형성하기 위해 무작위, 스위치, 혼합 및 컨테이너 노드를 함께 접목하는 것에 대해 배웠습니다.

00:37:01.000 --> 00:37:17.000
종합하면, 이제 PHASE의 내부 작동에 대한 폭 넓은 이해를 가져야 하며, 다음 기하학 인식 게임 오디오 경험을 구축할 준비가 되었을 때 기본 시스템 구성 요소에 대해 더 깊이 파고들 수 있어야 합니다.

00:37:17.000 --> 00:37:22.000
고마워. 멋진 WWDC21 되세요.

00:37:22.000 --> 23:59:59.000
[쾌활한 음악].

