WEBVTT

00:00:00.000 --> 00:00:05.000
♪ 베이스 음악 연주 ♪

00:00:05.000 --> 00:00:09.000
♪

00:00:09.000 --> 00:00:12.000
세르게이 카멘스키: 안녕하세요, WWDC에 오신 것을 환영합니다.

00:00:12.000 --> 00:00:15.000
제 이름은 세르게이 카멘스키이고 비전 프레임워크 팀의 소프트웨어 엔지니어입니다.

00:00:15.000 --> 00:00:20.000
오늘 세션의 주제는 비전 프레임워크가 사람들의 분석에 어떻게 도움이 될 수 있는지 보여주는 것이다.

00:00:20.000 --> 00:00:23.000
오늘 우리의 의제는 두 가지 주요 항목으로 구성되어 있다.

00:00:23.000 --> 00:00:27.000
먼저 우리는 비전 프레임워크에서 사람 분석 기술에 대한 개요를 가질 것이다.

00:00:27.000 --> 00:00:31.000
그렇게 하는 동안, 우리는 특히 새로운 추가 사항에 집중할 것이다.

00:00:31.000 --> 00:00:37.000
그리고 둘째, 우리는 새로운 사람 세분화 기능에 대한 심층적인 검토를 할 것이다.

00:00:37.000 --> 00:00:40.000
먼저 사람 분석 기술부터 시작합시다.

00:00:40.000 --> 00:00:44.000
비전에서 사람 분석의 초석은 사람 얼굴 분석이다.

00:00:44.000 --> 00:00:49.000
비전 프레임워크가 시작된 이래로, 우리는 인간의 얼굴 분석 능력을 추가하고 향상시켜 왔습니다.

00:00:49.000 --> 00:00:56.000
우리는 현재 얼굴 감지, 얼굴 랜드마크 감지 및 얼굴 캡처 품질 감지를 제공합니다.

00:00:56.000 --> 00:01:02.000
비전 프레임워크의 얼굴 감지 기능은 DetectFaceRectanglesRequest를 통해 노출됩니다.

00:01:02.000 --> 00:01:11.000
우리의 얼굴 탐지기는 높은 정밀도와 리콜 메트릭을 제공하며, 임의의 방향, 다른 크기, 그리고 부분적으로 차단된 얼굴을 찾을 수 있습니다.

00:01:11.000 --> 00:01:15.000
지금까지 우리는 안경과 모자와 같은 폐색을 지원했다.

00:01:15.000 --> 00:01:26.000
이제 우리는 얼굴 탐지기를 기존의 모든 훌륭한 품질을 개선하는 것 외에도 마스크로 덮인 얼굴을 감지할 수 있는 개정 3번으로 업그레이드하고 있습니다.

00:01:26.000 --> 00:01:33.000
얼굴 검출기의 주요 기능은 물론 얼굴 경계 상자를 찾는 것이지만, 얼굴 포즈 메트릭도 감지할 수 있습니다.

00:01:33.000 --> 00:01:36.000
이전에, 우리는 롤과 요 메트릭스만 제공했다.

00:01:36.000 --> 00:01:41.000
대부분의 메트릭은 라디안으로 보고되며 그 값은 개별 쓰레기통으로 반환됩니다.

00:01:41.000 --> 00:01:47.000
새로운 개정판 소개와 함께, 우리는 또한 피치 메트릭을 추가하여 전체 그림을 완성하고 있습니다.

00:01:47.000 --> 00:01:49.000
하지만 우리는 거기서 멈추지 않았어.

00:01:49.000 --> 00:01:54.000
우리는 또한 지속적인 공간에서 보고된 세 가지 지표를 모두 만들고 있다.

00:01:54.000 --> 00:02:03.000
모든 얼굴 포즈 메트릭은 DetectFaceRectanglesRequest를 실행한 결과인 FaceObservation 객체의 속성으로 반환됩니다.

00:02:03.000 --> 00:02:08.000
얼굴 포즈 감지 기능을 보여주기 위해 설계된 데모 앱을 살펴봅시다.

00:02:08.000 --> 00:02:16.000
이 앱은 비전 얼굴 검출기를 실행하여 카메라 피드를 처리하고 결과를 라디안에서 각도로 변환한 후 사용자에게 얼굴 포즈 메트릭을 제공합니다.

00:02:16.000 --> 00:02:28.000
메트릭 변화를 더 잘 추적하기 위해, 이 앱은 빨간색 그라디언트를 사용하여 얼굴 포즈 메트릭이 긍정적인 방향으로 증가할 때를 보여주고 파란색 그라디언트를 사용하여 메트릭이 부정적인 방향으로 증가할 때를 보여줍니다.

00:02:28.000 --> 00:02:33.000
두 경우 모두, 색이 밝을수록, 미터법이 0 위치에 더 가깝다.

00:02:33.000 --> 00:02:41.000
각 메트릭의 제로 위치는 사람이 똑바로 보고 있을 때 인간 머리의 중립적인 위치라고 부르는 것입니다.

00:02:41.000 --> 00:02:46.000
우리가 이미 논의했듯이, 우리는 세 가지 얼굴 포즈 지표가 있습니다: 롤, 요, 그리고 피치.

00:02:46.000 --> 00:02:53.000
이 용어들은 비행 역학에서 비롯되며 항공기의 무게 중심과 관련하여 항공기 원리 축을 설명한다.

00:02:53.000 --> 00:02:57.000
인간의 머리 포즈를 묘사하기 위해 같은 용어가 채택되었다.

00:02:57.000 --> 00:03:03.000
머리 포즈에 적용될 때 - 또는 우리가 그것을 얼굴 포즈라고 부를 때 - 그들은 다음과 같이 인간의 머리 움직임을 추적한다.

00:03:03.000 --> 00:03:09.000
롤은 이 방향으로 머리의 움직임을 추적하고 있다.

00:03:09.000 --> 00:03:25.000
내가 롤의 가장 부정적인 값에서 가장 긍정적인 값으로 갈 때, 배경색이 진한 파란색에서 밝은 파란색, 중성, 밝은 빨간색, 그리고 마지막으로 진한 빨간색으로 변하는 것을 볼 수 있습니다.

00:03:25.000 --> 00:03:33.000
머리가 오른쪽이나 왼쪽으로 돌 때 각도를 추적하는 요 메트릭에서도 비슷한 색상 변화가 일어나고 있다.

00:03:33.000 --> 00:03:40.000
그리고 마지막으로, 피치 메트릭은 내 머리가 위아래로 고개를 끄덕일 때 내 머리 움직임을 추적하고 있다.

00:03:40.000 --> 00:03:50.000
여기서 당신은 내가 스펙트럼의 가장 부정적인 끝에서 가장 긍정적인 끝으로 갈 때 비슷한 색상 변화를 다시 볼 수 있습니다.

00:03:50.000 --> 00:03:55.000
얼굴 랜드마크 감지는 얼굴 분석 제품군의 또 다른 중요한 기능입니다.

00:03:55.000 --> 00:04:02.000
얼굴 랜드마크 감지는 DetectFaceLandmarksRequest를 통해 제공되며, 최신 개정판은 3번 개정판입니다.

00:04:02.000 --> 00:04:12.000
이 개정판은 주요 얼굴 영역을 더 잘 나타내고 정확한 눈동자 감지를 제공하기 위해 76점 별자리를 제공합니다.

00:04:12.000 --> 00:04:16.000
얼굴 분석 제품군에는 얼굴 캡처 품질 감지도 포함되어 있습니다.

00:04:16.000 --> 00:04:25.000
이 전체적인 측정은 인간의 얼굴 표정, 조명, 폐색, 흐림, 집중 등과 같은 속성을 고려합니다.

00:04:25.000 --> 00:04:33.000
그것은 DetectFaceCaptureQualityRequest API를 통해 노출되며 이 요청의 최신 개정판은 2번 개정판입니다.

00:04:33.000 --> 00:04:38.000
얼굴 캡처 품질이 같은 주제의 비교 척도라는 것을 기억하는 것이 중요하다.

00:04:38.000 --> 00:04:46.000
예를 들어, 이 기능은 사진 버스트 시리즈에서 최고의 사진을 고르거나 사진 라이브러리에서 사람을 대표하는 최고의 사진을 고르는 데 잘 작동합니다.

00:04:46.000 --> 00:04:52.000
이 기능은 다른 사람들의 얼굴을 비교하도록 설계되지 않았다.

00:04:52.000 --> 00:04:57.000
인체 분석은 비전 프레임워크가 제공하는 사람 분석 기술의 또 다른 큰 부분이다.

00:04:57.000 --> 00:05:07.000
비전은 이 분야에서 인체 감지, 인간의 자세 감지, 그리고 마지막으로 인간의 손 자세 감지를 포함한 몇 가지 기능을 제공한다.

00:05:07.000 --> 00:05:10.000
먼저 인체 감지를 살펴봅시다.

00:05:10.000 --> 00:05:17.000
이 기능은 DetectHumanRectanglesRequest를 통해 제공되며, 현재 인간의 상체만 감지합니다.

00:05:17.000 --> 00:05:25.000
우리는 이 요청에 새로운 기능을 추가하고 있으며, 따라서 이 개정판을 2번 개정판으로 업그레이드하고 있습니다.

00:05:25.000 --> 00:05:31.000
새로운 개정으로, 상체 감지 외에도, 우리는 또한 전신 감지를 제공합니다.

00:05:31.000 --> 00:05:39.000
상체와 전신 감지 사이의 선택은 DetectHumanRectanglesRequest의 새로운 upperBodyOnly 속성을 통해 제어됩니다.

00:05:39.000 --> 00:05:45.000
이 속성의 기본값은 이전 버전과의 호환성을 유지하기 위해 true로 설정됩니다.

00:05:45.000 --> 00:05:50.000
인체 자세 감지는 DetectHumanBodyPoseRequest를 통해 비전 프레임워크에서 제공됩니다.

00:05:50.000 --> 00:05:55.000
이 요청을 처리하는 것은 인체 관절 위치의 모음을 제공한다.

00:05:55.000 --> 00:06:02.000
1번 개정판은 이 요청의 최신이자 유일하게 사용 가능한 개정판이다.

00:06:02.000 --> 00:06:07.000
비전 프레임워크는 또한 DetectHumanHandPoseRequest로 인간의 손 포즈 감지를 제공합니다.

00:06:07.000 --> 00:06:16.000
인체 포즈 감지와 유사하게, 손 포즈 요청의 처리는 인간의 손 관절 위치 모음을 반환합니다.

00:06:16.000 --> 00:06:23.000
우리는 결과 관찰, 손 chirality에 중요한 속성을 추가하여 이 요청의 기능을 업그레이드하고 있습니다.

00:06:23.000 --> 00:06:30.000
HumanHandPoseObservation의 새로운 키랄리티 속성은 감지된 손이 왼쪽인지 오른쪽인지에 대한 정보를 포함할 것이다.

00:06:30.000 --> 00:06:41.000
손 포즈 감지에 대해 더 자세히 알고 싶다면, "Create ML로 손 포즈와 동작 분류" 세션을 보는 것이 좋습니다.

00:06:41.000 --> 00:06:46.000
이것으로 사람 분석 기술 제품군에 대한 새로운 업그레이드에 대한 개요를 마칩니다.

00:06:46.000 --> 00:06:53.000
이제 우리 세션의 두 번째 주제인 사람 세분화로 넘어갈 시간이다.

00:06:53.000 --> 00:06:55.000
사람 세분화가 뭐야?

00:06:55.000 --> 00:07:00.000
아주 간단한 용어로, 그것은 사람들을 현장에서 분리하는 능력이다.

00:07:00.000 --> 00:07:04.000
요즘에는 사람 세분화 기술의 수많은 응용이 있다.

00:07:04.000 --> 00:07:09.000
예를 들어, 여러분은 모두 화상 회의 앱의 가상 배경 기능에 익숙합니다.

00:07:09.000 --> 00:07:13.000
그것은 또한 라이브 스포츠 분석, 자율주행 및 더 많은 장소에서 사용된다.

00:07:13.000 --> 00:07:19.000
사람 세분화는 또한 우리의 유명한 초상화 모드를 강화한다.

00:07:19.000 --> 00:07:23.000
비전 프레임워크의 사람 세분화는 단일 프레임으로 작동하도록 설계된 기능입니다.

00:07:23.000 --> 00:07:28.000
스트리밍 파이프라인에서 사용할 수 있으며, 오프라인 처리에도 적합합니다.

00:07:28.000 --> 00:07:38.000
이 기능은 macOS, iOS, iPadOS 및 tvOS와 같은 여러 플랫폼에서 지원됩니다.

00:07:38.000 --> 00:07:48.000
비전 프레임워크는 의미론적 인격 세분화를 구현하며, 이는 프레임의 모든 사람들에게 단일 마스크를 반환한다는 것을 의미합니다.

00:07:48.000 --> 00:07:56.000
개인 세분화를 위한 비전 API는 GeneratePersonSegmentationRequest를 통해 구현되며, 이는 상태 저장 요청입니다.

00:07:56.000 --> 00:08:03.000
비전 프레임워크의 전통적인 요청과는 달리, 상태 저장 요청 객체는 전체 프레임 시퀀스에 걸쳐 재사용됩니다.

00:08:03.000 --> 00:08:10.000
우리의 특별한 경우, 요청 객체를 사용하면 빠른 품질 수준의 모델에서 프레임 간의 시간적 변화를 부드럽게 하는 데 도움이 됩니다.

00:08:10.000 --> 00:08:15.000
비전 프레임워크에서 제공하는 Person Segmentation API를 살펴봅시다.

00:08:15.000 --> 00:08:18.000
이 API는 이미 친숙하고 확립된 패턴을 따른다.

00:08:18.000 --> 00:08:27.000
요청을 만들고, 요청 핸들러를 만들고, 요청 핸들러로 요청을 처리하고, 마지막으로 결과를 검토하십시오.

00:08:27.000 --> 00:08:37.000
GeneratePersonSegmentationRequest 객체의 기본 초기화는 수정, 품질 수준 및 outputPixelFormat 속성을 기본값으로 설정하는 것과 같습니다.

00:08:37.000 --> 00:08:41.000
모든 부동산을 하나씩 검토해 봅시다.

00:08:41.000 --> 00:08:43.000
첫 번째는 개정 재산이다.

00:08:43.000 --> 00:08:46.000
여기서 우리는 개정판을 1번 개정판으로 설정했다.

00:08:46.000 --> 00:08:51.000
우리가 새로운 요청 유형을 다루고 있기 때문에, 이것은 기본값이자 사용 가능한 유일한 개정판입니다.

00:08:51.000 --> 00:08:58.000
오늘날에는 기술적으로 선택의 여지가 없지만, 우리는 항상 미래에 결정적인 행동을 보장하기 위해 명시적으로 설정할 것을 권장합니다.

00:08:58.000 --> 00:09:06.000
새로운 개정판이 도입되면, 기본값도 사용 가능한 최신 개정판을 나타내기 위해 변경되기 때문입니다.

00:09:06.000 --> 00:09:09.000
두 번째는 품질 수준 속성이다.

00:09:09.000 --> 00:09:16.000
비전 API는 세 가지 다른 수준을 제공합니다: 정확하며, 이는 기본 수준이기도 합니다; 균형 잡힌; 그리고 빠릅니다.

00:09:16.000 --> 00:09:22.000
사용 사례에 관한 한, 전산 사진 촬영을 위해 정확한 수준을 사용하는 것이 좋습니다.

00:09:22.000 --> 00:09:29.000
이것은 당신이 가능한 최고의 품질을 달성하고자 하는 사용 사례이며 일반적으로 시간에 제한되지 않습니다.

00:09:29.000 --> 00:09:37.000
비슷한 논리를 사용하여, 비디오 프레임별 세분화와 스트리밍 처리를 위해 균형 잡힌 수준이 권장됩니다.

00:09:37.000 --> 00:09:40.000
세 번째 속성은 출력 마스크 형식이다.

00:09:40.000 --> 00:09:50.000
우리는 결과 마스크를 자세히 검토할 것이지만, 여기서 저는 고객으로서 결과 마스크가 반환될 형식을 지정할 수 있다는 것을 언급하고 싶습니다.

00:09:50.000 --> 00:10:02.000
여기에는 세 가지 선택이 있습니다: 일반적인 0 ~ 255 양자화 범위의 부호 없는 8비트 정수 마스크와 두 개의 부동 소수점 마스크 형식 - 하나는 32비트 완전 정밀도이고 다른 하나는 16비트 반 정밀도입니다.

00:10:02.000 --> 00:10:14.000
16비트 절반 정밀도는 Metal을 사용한 추가 GPU 기반 처리에 직접 삽입할 수 있는 감소된 메모리 부동 소수점 형식을 제공하기 위한 것입니다.

00:10:14.000 --> 00:10:19.000
지금까지 우리는 개인 세분화 요청을 생성, 구성 및 실행하는 방법을 배웠습니다.

00:10:19.000 --> 00:10:22.000
이제 결과를 볼 시간이다.

00:10:22.000 --> 00:10:28.000
사람 세분화 요청을 처리한 결과는 PixelBufferObservation 객체의 형태로 제공됩니다.

00:10:28.000 --> 00:10:35.000
PixelBufferObservation은 관찰에서 파생되며 중요한 pixelBuffer 속성을 추가합니다.

00:10:35.000 --> 00:10:45.000
이 속성에 저장된 실제 CVPixelBuffer 객체는 개인 세분화 요청이 구성된 것과 동일한 픽셀 형식을 가지고 있습니다.

00:10:45.000 --> 00:10:49.000
사람 세분화 요청을 처리하면 세분화 마스크가 생성됩니다.

00:10:49.000 --> 00:10:57.000
원본 이미지와 사람 세분화 요청을 실행하여 생성된 세 가지 품질 수준 마스크를 살펴봅시다.

00:10:57.000 --> 00:11:02.000
빠르고, 균형 잡히고, 정확하다.

00:11:02.000 --> 00:11:05.000
각 마스크의 세부 사항을 보려면 확대해 봅시다.

00:11:05.000 --> 00:11:15.000
예상대로, 우리가 빠른 것에서 균형 잡힌 것, 그리고 결국 정확한 것까지 갈 때, 마스크의 품질이 증가하고 우리는 점점 더 많은 세부 사항을 보기 시작할 것이다.

00:11:15.000 --> 00:11:21.000
이제 품질 대 성능의 기능으로 다양한 마스크 레벨을 살펴봅시다.

00:11:21.000 --> 00:11:30.000
우리가 빠른 것에서 균형 잡힌 것으로, 그리고 결국 정확한 것으로 이동할 때, 마스크의 품질은 올라가지만 자원 사용도 올라간다.

00:11:30.000 --> 00:11:36.000
다이내믹 레인지, 마스크 해상도, 메모리 소비, 처리 시간은 모두 마스크 품질이 향상되면 증가합니다.

00:11:36.000 --> 00:11:46.000
이것은 세분화 마스크의 품질과 마스크를 계산하는 데 필요한 자원 소비 사이의 절충을 나타낸다.

00:11:46.000 --> 00:11:50.000
그래서 당신은 이미 마스크 생성과 그 특성에 대한 모든 것을 알고 있습니다.

00:11:50.000 --> 00:11:54.000
마스크로 실제로 무엇을 할 수 있나요?

00:11:54.000 --> 00:11:56.000
세 개의 이미지로 시작합시다.

00:11:56.000 --> 00:12:03.000
입력 이미지, 입력 이미지를 처리하여 얻은 세분화 마스크 및 배경 이미지.

00:12:03.000 --> 00:12:12.000
우리가 하고 싶은 것은 마스크 영역 외부 영역의 원본 이미지의 배경을 다른 이미지의 배경으로 바꾸는 것입니다.

00:12:12.000 --> 00:12:22.000
당신이 그러한 혼합 작업을 수행할 때, 우리는 원래 이미지의 젊은이가 해변 산책로에서 숲으로 이송되는 것으로 끝납니다.

00:12:22.000 --> 00:12:26.000
이 블렌딩 시퀀스는 코드에서 어떻게 생겼나요?

00:12:26.000 --> 00:12:35.000
먼저 우리가 모든 관련 처리를 완료했고 이미 입력 이미지, 마스크 및 배경의 세 가지 이미지를 가지고 있다고 가정해 봅시다.

00:12:35.000 --> 00:12:40.000
이제 우리는 마스크와 배경을 원본 이미지의 크기로 조정해야 합니다.

00:12:40.000 --> 00:12:45.000
그런 다음 우리는 코어 이미지 블렌딩 필터를 만들고 초기화할 것입니다.

00:12:45.000 --> 00:12:48.000
넌 아마 내가 빨간 마스크로 블렌딩 필터를 만들었다는 걸 알아챘을 거야.

00:12:48.000 --> 00:12:58.000
이것은 CIImage가 하나의 구성 요소인 PixelBuffer로 초기화될 때 - 우리의 모든 마스크와 마찬가지로 - 기본적으로 빨간색 채널로 객체를 만들기 때문입니다.

00:12:58.000 --> 00:13:04.000
마지막으로, 우리는 결과를 얻기 위해 블렌딩 작업을 수행합니다.

00:13:04.000 --> 00:13:09.000
비전 프레임워크에서 사람 세분화 기능을 어떻게 사용할 수 있는지 살펴봅시다.

00:13:09.000 --> 00:13:16.000
다운로드할 수 있는 두 번째 데모 앱은 얼굴 포즈 메트릭 감지와 새로운 사람 세분화 기능을 결합합니다.

00:13:16.000 --> 00:13:20.000
이 앱은 얼굴 감지와 사람 세분화를 실행하여 카메라 피드를 처리합니다.

00:13:20.000 --> 00:13:28.000
그런 다음 그것은 최종 세분화 마스크를 차지하여 마스크 픽셀 외부 영역의 배경을 다른 색상으로 대체하는 데 사용합니다.

00:13:28.000 --> 00:13:36.000
어떤 배경색을 사용할지에 대한 결정은 주어진 시점에서 롤, 요, 피치에 대한 값의 조합에서 나온다.

00:13:36.000 --> 00:13:46.000
저는 현재 테이블과 의자가 있는 방에 있으며, 데모 앱은 내 머리 위치에 해당하는 색상 믹스인 새로운 배경 위에 분할된 실루엣을 보여줍니다.

00:13:46.000 --> 00:13:49.000
그것이 롤, 요, 피치 변화를 추적하는지 봅시다.

00:13:49.000 --> 00:13:58.000
내가 이렇게 고개를 돌릴 때, 롤은 배경색 믹스 결정에 큰 기여를 한다.

00:13:58.000 --> 00:14:04.000
내가 머리를 좌우로 돌릴 때, 요우가 주요 기여자가 된다.

00:14:04.000 --> 00:14:12.000
그리고 마지막으로, 고개를 끄덕이는 것은 피치가 주요 기여자가 된다.

00:14:12.000 --> 00:14:15.000
비전 프레임워크는 사람 세분화 API를 제공하는 유일한 장소가 아니다.

00:14:15.000 --> 00:14:20.000
같은 기술로 구동되는 유사한 기능을 제공하는 몇 가지 다른 프레임워크가 있다.

00:14:20.000 --> 00:14:23.000
그들 각각을 간략하게 살펴봅시다.

00:14:23.000 --> 00:14:26.000
첫 번째는 AVFoundation이다.

00:14:26.000 --> 00:14:33.000
AVFoundation은 사진 캡처 세션 중에 일부 최신 장치에서 사람 세분화 마스크를 반환할 수 있습니다.

00:14:33.000 --> 00:14:39.000
세분화 마스크는 AVCapturePhoto의 PortraitEffectsMatte 속성을 통해 반환됩니다.

00:14:39.000 --> 00:14:45.000
그것을 얻으려면, 먼저 그것이 지원되는지 확인해야 합니다; 그리고 만약 그렇다면, 배송을 활성화하세요.

00:14:45.000 --> 00:14:50.000
사람 세분화 API를 제공하는 두 번째 프레임워크는 ARKit이다.

00:14:50.000 --> 00:14:56.000
이 기능은 A12 Bionic 및 이후 장치에서 지원되며, 카메라 피드를 처리할 때 생성됩니다.

00:14:56.000 --> 00:15:01.000
세분화 마스크는 ARFrame의 segmentationBuffer 속성을 통해 반환됩니다.

00:15:01.000 --> 00:15:12.000
검색을 시도하기 전에, ARWorldTrackingConfiguration 클래스의 supportsFrameSemantics 속성을 검토하여 지원되는지 확인해야 합니다.

00:15:12.000 --> 00:15:14.000
세 번째 프레임워크는 코어 이미지이다.

00:15:14.000 --> 00:15:22.000
Core Image는 Vision 사람 세분화 API 위에 얇은 래퍼를 제공하므로 Core Image 도메인 내에서 전체 사용 사례를 수행할 수 있습니다.

00:15:22.000 --> 00:15:28.000
이제 Core Image API를 사용하여 개인 세분화를 어떻게 구현할 수 있는지 살펴봅시다.

00:15:28.000 --> 00:15:32.000
우리는 세분화를 수행하기 위해 이미지를 기록하는 것으로 시작할 것이다.

00:15:32.000 --> 00:15:43.000
그런 다음 우리는 사람 세분화 CIFilter를 만들고, inputImage를 할당하고, 필터를 실행하여 세분화 마스크를 얻을 것입니다.

00:15:43.000 --> 00:15:47.000
우리는 방금 여러 버전의 개인 세분화 API와 Apple SDK를 검토했습니다.

00:15:47.000 --> 00:15:51.000
각각을 어디에 사용할 수 있는지 요약해 봅시다.

00:15:51.000 --> 00:15:56.000
AVFoundation은 AVCaptureSession이 있는 일부 iOS 기기에서 사용할 수 있습니다.

00:15:56.000 --> 00:16:00.000
캡처 세션이 실행 중이라면, 이것은 당신의 선택이 될 것입니다.

00:16:00.000 --> 00:16:06.000
ARKit 앱을 개발하고 있다면, 이미 세분화 마스크를 받을 수 있는 AR 세션이 있어야 합니다.

00:16:06.000 --> 00:16:10.000
이 경우, ARKit API를 사용하는 것이 좋습니다.

00:16:10.000 --> 00:16:16.000
Vision API는 온라인 및 오프라인 단일 프레임 처리를 위해 여러 플랫폼에서 사용할 수 있습니다.

00:16:16.000 --> 00:16:24.000
그리고 마지막으로, Core Image는 Vision API에 대한 얇은 래퍼를 제공하며, 이는 Core Image 도메인 내에 머물고 싶다면 편리한 옵션입니다.

00:16:24.000 --> 00:16:31.000
다른 알고리즘과 마찬가지로, 사람 세분화에는 모범 사례가 있습니다. 즉, 가장 잘 작동하는 조건의 집합입니다.

00:16:31.000 --> 00:16:37.000
개인 세분화 기능을 사용할 계획이라면, 이러한 규칙을 따르려고 하면 앱이 더 잘 수행될 것입니다.

00:16:37.000 --> 00:16:45.000
먼저, 자연 폐색이 있는 동안 모든 사람들이 대부분 보이는 장면에서 최대 네 명까지 분할하려고 노력해야 합니다.

00:16:45.000 --> 00:16:53.000
둘째, 각 사람의 높이는 이미지 높이의 절반 이상이어야 하며, 이상적으로는 배경에 비해 대비가 좋다.

00:16:53.000 --> 00:17:02.000
그리고 셋째, 우리는 또한 동상, 사람들의 사진, 멀리 있는 사람들과 같은 모호함을 피하는 것이 좋습니다.

00:17:02.000 --> 00:17:03.000
이것으로 우리의 세션을 마칩니다.

00:17:03.000 --> 00:17:07.000
오늘 우리가 배운 것을 간략하게 살펴봅시다.

00:17:07.000 --> 00:17:19.000
첫째, 우리는 마스크된 얼굴 감지와 같은 업그레이드에 초점을 맞추고, 얼굴 피치 메트릭을 추가하고, 모든 얼굴 포즈 메트릭을 연속적인 공간에서 보고하는 것과 같은 업그레이드에 초점을 맞추면서 비전 프레임워크의 사람 분석 기술에 대한 개요를 가지고 있었습니다.

00:17:19.000 --> 00:17:25.000
우리는 또한 인간의 손 포즈 감지에 새로운 손 키랄리티 메트릭을 도입했다.

00:17:25.000 --> 00:17:30.000
두 번째 부분에서, 우리는 비전 프레임워크에 추가된 새로운 사람 세분화 API에 대해 깊이 파고들었다.

00:17:30.000 --> 00:17:37.000
우리는 또한 유사한 기능을 제공하는 다른 API를 살펴보고 각각을 사용할 수 있는 지침을 제공했습니다.

00:17:37.000 --> 00:17:44.000
이 세션을 보면서 앱 개발을 위한 새로운 도구를 배웠고 바로 시도해보고 싶어 주셨으면 합니다.

00:17:44.000 --> 00:17:50.000
오늘 끝내기 전에, 시청해 주셔서 감사드리며, 행운을 빕니다, 그리고 남은 WWDC에서 좋은 시간 보내시길 바랍니다.

00:17:50.000 --> 23:59:59.000
♪

