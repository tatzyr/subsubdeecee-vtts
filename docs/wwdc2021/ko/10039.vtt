WEBVTT

00:00:00.000 --> 00:00:09.000
♪ ♪

00:00:09.000 --> 00:00:13.000
안녕하세요, "Create ML로 손 포즈와 동작을 분류하세요"에 오신 것을 환영합니다.

00:00:13.000 --> 00:00:15.000
난 네이선 워트먼이야.

00:00:15.000 --> 00:00:18.000
그리고 오늘, 저는 제 동료인 Brittany Weinert와 Geppy Parziale과 합류할 것입니다.

00:00:18.000 --> 00:00:22.000
오늘, 우리는 손 위치를 분류하는 것에 대해 이야기할 것이다.

00:00:22.000 --> 00:00:25.000
하지만 우리가 그것을 파고들기 전에, 손 자체에 대해 이야기해 봅시다.

00:00:25.000 --> 00:00:30.000
24개 이상의 뼈, 관절, 근육이 있는 손은 공학적으로 경이로움이다.

00:00:30.000 --> 00:00:36.000
이러한 복잡성에도 불구하고, 손은 유아들이 주변 세계와 상호 작용하기 위해 사용하는 첫 번째 도구 중 하나이다.

00:00:36.000 --> 00:00:42.000
아기들은 말하기 전에 간단한 손 움직임을 사용하여 의사소통의 기초를 배운다.

00:00:42.000 --> 00:00:45.000
일단 우리가 말하는 법을 배우면, 우리의 손은 계속해서 의사소통에 중요한 역할을 한다.

00:00:45.000 --> 00:00:49.000
그들은 강조와 표현을 추가하는 것으로 바꾼다.

00:00:49.000 --> 00:00:55.000
지난 한 해 동안, 우리의 손은 사람들을 더 가깝게 하기 위해 그 어느 때보다 중요해졌다.

00:00:55.000 --> 00:01:05.000
2020년에 비전 프레임워크는 개발자가 프레임의 손과 손에 존재하는 21개의 식별 가능한 관절을 식별할 수 있는 손 포즈 감지를 도입했다.

00:01:05.000 --> 00:01:15.000
이것은 손이 존재하는지 또는 손이 프레임에 어디에 있는지 식별하려고 한다면 훌륭한 도구이지만, 손이 무엇을 하고 있는지 분류하려고 한다면 도전이 될 수 있습니다.

00:01:15.000 --> 00:01:30.000
손의 표현 능력은 무한하지만, 이 세션의 나머지 부분에서, 나는 멈추고, 조용하고, 평화, 그리고 뒤로 물러나고, 떠나고, 여기로 오는 것과 같은 짧고, 한 손으로 하는 포즈에 집중하고 싶습니다.

00:01:30.000 --> 00:01:33.000
나는 방금 손 포즈와 행동을 언급했다.

00:01:33.000 --> 00:01:36.000
좀 더 구체적인 정의는 어때?

00:01:36.000 --> 00:01:41.000
음, 한편으로는, 우리는 정지 이미지로서 의미 있는 포즈를 가지고 있다.

00:01:41.000 --> 00:01:46.000
이 두 비디오가 일시 중지되었지만, 주제의 의도는 명확하게 표현되었다.

00:01:46.000 --> 00:01:48.000
포즈를 이미지처럼 생각해봐.

00:01:48.000 --> 00:01:54.000
그리고 다른 한편으로, 우리는 의미를 완전히 표현하기 위해 움직임이 필요한 행동을 가지고 있다.

00:01:54.000 --> 00:01:56.000
이 두 행동의 의미는 불분명하다.

00:01:56.000 --> 00:01:59.000
하나의 프레임을 보는 것만으로는 충분하지 않다.

00:01:59.000 --> 00:02:06.000
하지만 비디오나 라이브 사진과 같이 시간이 지남에 따라 일련의 프레임으로, 행동의 의미는 분명하다.

00:02:06.000 --> 00:02:08.000
친절한 "안녕하세요" 그리고 "이리와"

00:02:08.000 --> 00:02:17.000
그것이 정리되면서, 저는 올해 두 개의 새로운 Create ML 템플릿인 핸드 포즈 분류와 핸드 액션 분류를 소개하게 되어 기쁩니다.

00:02:17.000 --> 00:02:25.000
이 새로운 템플릿을 사용하면 Create ML 앱 또는 Create ML 프레임워크를 사용하여 핸드 포즈와 액션 모델을 훈련할 수 있습니다.

00:02:25.000 --> 00:02:33.000
이 모델들은 macOS Big Sur 이상, iOS 및 iPadOS 14 이상과 호환됩니다.

00:02:33.000 --> 00:02:38.000
그리고 올해의 새로운 기능, 우리는 Create ML 프레임워크를 사용하여 iOS 장치에서 모델을 훈련시킬 수 있는 기능을 추가했습니다.

00:02:38.000 --> 00:02:45.000
"ML 프레임워크 만들기로 동적 iOS 앱 구축" 세션에서 이에 대해 자세히 알아볼 수 있습니다.

00:02:45.000 --> 00:02:54.000
먼저, 비전 프레임워크에서 감지된 손 위치를 분류하기 위해 기계 학습 모델을 쉽게 훈련할 수 있는 손 포즈 분류에 대해 이야기하고 싶습니다.

00:02:54.000 --> 00:03:02.000
모델 교육을 담당하고 있기 때문에, 앱이 필요에 가장 잘 맞게 분류해야 하는 포즈를 정의합니다.

00:03:02.000 --> 00:03:04.000
훈련된 모델에 대한 간략한 데모를 드리겠습니다.

00:03:04.000 --> 00:03:09.000
간단한 프로토타입 앱에서 시작하여, 나는 핸드 포즈 분류기 모델을 쉽게 통합할 수 있었다.

00:03:09.000 --> 00:03:16.000
내 앱은 이제 손 포즈를 분류하고 해당 이모티콘과 분류된 포즈의 자신감을 보여줄 수 있다.

00:03:16.000 --> 00:03:20.000
그것은 손 포즈를 하나와 둘로 분류한다.

00:03:20.000 --> 00:03:25.000
하지만 인식하지 못하는 모든 위치가 배경의 일부로 분류된다는 것을 알게 될 것입니다.

00:03:25.000 --> 00:03:29.000
여기에는 내가 지원을 추가하고 싶은 오픈 팜 포즈가 포함되어 있다.

00:03:29.000 --> 00:03:36.000
손 포즈 분류기 모델을 앱에 통합하는 방법을 보여주기 위해 잠시 후에 이 모델을 제 동료 브리트니에게 넘겨주려고 합니다.

00:03:36.000 --> 00:03:40.000
내가 하기 전에, 나는 오픈 팜 포즈에 대한 지원을 추가하고 싶다.

00:03:40.000 --> 00:03:44.000
그것은 정말 쉬울 것이지만, 우리는 모델이 어떻게 훈련되는지에 대해 먼저 이야기해야 한다.

00:03:44.000 --> 00:03:50.000
다른 모든 Create ML 프로젝트와 마찬가지로, Hand Pose Classifier를 앱에 통합하는 것은 매우 간단합니다.

00:03:50.000 --> 00:03:52.000
그 과정에는 세 단계가 있다.

00:03:52.000 --> 00:03:58.000
훈련 데이터를 수집하고 분류하고, 모델을 훈련시키고, 모델을 애플리케이션에 통합하세요.

00:03:58.000 --> 00:04:02.000
훈련 데이터를 수집하는 것에 대해 이야기하는 것으로 시작합시다.

00:04:02.000 --> 00:04:05.000
핸드 포즈 분류기의 경우, 이미지가 필요합니다.

00:04:05.000 --> 00:04:08.000
기억하세요, 포즈는 이미지로 완전히 표현됩니다.

00:04:08.000 --> 00:04:13.000
이 이미지들은 이름이 이미지에 있는 포즈와 일치하는 폴더로 분류되어야 합니다.

00:04:13.000 --> 00:04:20.000
여기 우리가 확인하고 싶은 두 가지 포즈가 있습니다: 하나, 둘, 그리고 배경 수업.

00:04:20.000 --> 00:04:26.000
백그라운드 클래스는 앱이 올바르게 식별하는 데 신경 쓰지 않는 포즈를 위한 포괄적인 범주입니다.

00:04:26.000 --> 00:04:30.000
내 데모에서, 이것은 하나 또는 둘이 아닌 많은 손 위치를 포함한다.

00:04:30.000 --> 00:04:36.000
잘 정의된 배경 클래스는 사용자가 중요한 포즈를 취하지 않을 때 앱을 알 수 있도록 도와줍니다.

00:04:36.000 --> 00:04:41.000
배경 클래스를 구성하는 두 가지 유형의 이미지가 있습니다.

00:04:41.000 --> 00:04:47.000
첫째, 우리는 당신의 앱이 분류하고 싶은 중요한 포즈가 아닌 무작위 손 포즈를 가지고 있습니다.

00:04:47.000 --> 00:04:53.000
이러한 포즈에는 다양한 피부색, 나이, 성별 및 조명 조건이 포함되어야 합니다.

00:04:53.000 --> 00:04:58.000
둘째, 우리는 당신의 앱이 분류하고 싶은 표현과 매우 유사한 일련의 위치를 가지고 있습니다.

00:04:58.000 --> 00:05:05.000
이러한 과도기 포즈는 사용자가 앱이 신경 쓰는 표현 중 하나를 향해 손을 움직일 때 자주 발생합니다.

00:05:05.000 --> 00:05:16.000
내가 오픈 팜 포즈를 하기 위해 손을 들 때, 나는 내 앱이 오픈 팜을 고려하기를 원하는 것과 비슷하지만 그렇지 않은 몇 가지 위치를 통해 전환한다는 것을 알아차린다.

00:05:16.000 --> 00:05:19.000
이 위치들은 내가 나중에 팔을 내릴 때 발생한다.

00:05:19.000 --> 00:05:21.000
이것은 오픈 팜에만 있는 것이 아니다.

00:05:21.000 --> 00:05:28.000
같은 유형의 과도기 포즈는 내가 팔을 들어 두 개의 포즈를 취할 때뿐만 아니라 그것을 낮출 때도 발생한다.

00:05:28.000 --> 00:05:34.000
이러한 모든 과도기 포즈는 무작위 포즈와 함께 배경 클래스에 추가되어야 합니다.

00:05:34.000 --> 00:05:43.000
이 조합을 통해 모델은 앱이 신경 쓰는 포즈와 다른 모든 배경 포즈를 적절하게 구별할 수 있습니다.

00:05:43.000 --> 00:05:48.000
훈련 데이터가 수집되고 분류됨에 따라, 이제 Create ML 앱을 사용하여 모델을 훈련시킬 때입니다.

00:05:48.000 --> 00:05:50.000
그러니 우리 손을 더럽히자.

00:05:50.000 --> 00:05:56.000
이전 데모에서 모델을 훈련하는 데 사용한 기존 Create ML 프로젝트부터 시작하겠습니다.

00:05:56.000 --> 00:06:01.000
훈련 결과가 좋아 보였기 때문에, 나는 이 모델이 꽤 잘 수행되기를 기대하고 있다.

00:06:01.000 --> 00:06:07.000
다행히도, Create ML 앱을 사용하면 앱에 통합하기 전에 모델을 미리 볼 수 있습니다.

00:06:07.000 --> 00:06:14.000
미리보기 탭에서 핸드 포즈 분류기의 경우 이번 릴리스에 라이브 미리보기 기능을 추가했다는 것을 알 수 있습니다.

00:06:14.000 --> 00:06:19.000
라이브 프리뷰는 FaceTime 카메라를 활용하여 실시간으로 예측을 보여줍니다.

00:06:19.000 --> 00:06:25.000
라이브 미리보기를 사용하여, 우리는 이 모델이 포즈 1과 2를 올바르게 분류하는지 확인할 수 있습니다.

00:06:25.000 --> 00:06:32.000
그리고 오픈 팜도 올바르게 분류하고 싶지만, 현재 그 포즈를 배경 수업의 일부로 분류하고 있다.

00:06:32.000 --> 00:06:42.000
내가 이 모델을 훈련시키는 데 사용한 데이터 소스에서, 오픈 팜 클래스가 포함되어 있지 않으며, 하나, 둘, 배경에 대한 클래스만 포함되어 있다는 것을 주목하세요.

00:06:42.000 --> 00:06:45.000
이제 오픈 팜을 지원하는 새로운 모델을 훈련합시다.

00:06:45.000 --> 00:06:54.000
먼저, 나는 이것을 위한 새로운 모델 소스를 만들 것이다.

00:06:54.000 --> 00:06:59.000
저는 이 교육에 사용하고 싶은 오픈 팜 수업이 포함된 데이터 세트를 가지고 있습니다.

00:06:59.000 --> 00:07:05.000
나는 이 데이터 세트를 선택할 것이다.

00:07:05.000 --> 00:07:13.000
이 새로운 데이터 소스로 뛰어들면서, 우리는 이제 Open Palm에 대한 항목과 이전 데이터 세트의 클래스가 포함되어 있다는 것을 알게 되었습니다.

00:07:13.000 --> 00:07:23.000
모델 소스로 돌아가서, 훈련 데이터를 확장하고 모델을 더 견고하게 만들기 위해 몇 가지 보강을 추가하고 싶습니다.

00:07:23.000 --> 00:07:28.000
그게 다야. 기차를 탈 시간이야.

00:07:28.000 --> 00:07:34.000
교육이 시작되기 전에, Create ML은 기능 추출뿐만 아니라 예비 이미지 처리를 해야 합니다.

00:07:34.000 --> 00:07:37.000
우리는 Create ML에게 80번의 반복을 훈련하라고 말했다.

00:07:37.000 --> 00:07:41.000
이것은 좋은 출발점이지만, 데이터 세트에 따라 그 숫자를 조정해야 할 수도 있습니다.

00:07:41.000 --> 00:07:43.000
이 과정은 시간이 좀 걸릴 것이다.

00:07:43.000 --> 00:07:46.000
다행히도, 나는 이미 모델을 훈련시켰다.

00:07:46.000 --> 00:07:48.000
내가 지금 그걸 잡을게.

00:07:48.000 --> 00:07:54.000
라이브 프리뷰는 새로 훈련된 모델이 이제 오픈 팜 포즈를 올바르게 식별한다는 것을 보여준다.

00:07:54.000 --> 00:08:03.000
그리고 확실히 하기 위해, 나는 그것이 하나와 둘의 포즈를 계속 식별하는지 확인할 것이다.

00:08:03.000 --> 00:08:05.000
그렇게 쉽지 않았어?

00:08:05.000 --> 00:08:10.000
나는 이 모델을 내 동료 브리트니에게 보낼 것이고, 그녀는 그것을 그녀의 앱에 통합하는 것에 대해 이야기할 것이다.

00:08:10.000 --> 00:08:12.000
고마워, 네이선, 모델 고마워.

00:08:12.000 --> 00:08:14.000
안녕. 저는 브리트니 와이너트입니다.

00:08:14.000 --> 00:08:17.000
그리고 저는 비전 프레임워크 팀의 일원입니다.

00:08:17.000 --> 00:08:24.000
손 포즈 분류에 대해 처음 배웠을 때, 나는 즉시 이것을 사용하여 내 손으로 특수 효과를 만들 수 있다고 생각했다.

00:08:24.000 --> 00:08:33.000
나는 CoreML을 사용하여 손 포즈를 분류하고 시력을 사용하여 손을 감지하고 추적하는 것이 함께 사용하기에 완벽한 기술이 될 것이라는 것을 알고 있다.

00:08:33.000 --> 00:08:35.000
우리가 우리 자신에게 초능력을 줄 수 있는지 보자.

00:08:35.000 --> 00:08:40.000
나는 이미 그것을 할 수 있는 데모를 위한 파이프라인의 첫 번째 초안을 만들었다.

00:08:40.000 --> 00:08:43.000
그걸 검토해 보자.

00:08:43.000 --> 00:08:54.000
먼저, 우리는 프레임 스트림을 제공하는 카메라를 갖게 될 것이며, 프레임에서 손의 위치와 핵심 지점을 감지하기 위해 비전 요청에 각 프레임을 사용할 것입니다.

00:08:54.000 --> 00:08:58.000
DetectHumanHandPoseRequest는 우리가 사용하는 요청이 될 것입니다.

00:08:58.000 --> 00:09:03.000
그것은 프레임에서 찾은 각 손에 대해 HumanHandPoseObservation을 반환할 것이다.

00:09:03.000 --> 00:09:15.000
우리가 CoreML 핸드 포즈 분류 모델로 보낼 데이터는 MLMultiArray와 keypointsMultiArray라고 불리는 HumanHandPoseObservation의 속성입니다.

00:09:15.000 --> 00:09:26.000
그런 다음 우리의 핸드 포즈 분류기는 신뢰 점수와 함께 최고 예상 핸드 액션 라벨을 우리에게 돌려줄 것이며, 그런 다음 앱 내에서 액션을 결정하는 데 사용할 수 있습니다.

00:09:26.000 --> 00:09:30.000
이제 앱의 높은 수준의 세부 사항을 살펴보았으니, 코드를 살펴봅시다.

00:09:30.000 --> 00:09:36.000
프레임에서 손을 감지하기 위해 비전을 사용하는 방법을 살펴보는 것으로 시작합시다.

00:09:36.000 --> 00:09:48.000
우리가 하고 싶은 것을 위해, 우리는 VNDetectHumanHandPoseRequest의 하나의 인스턴스만 필요하며, 한 손만 감지하면 되므로, maximumHandCount를 하나로 설정합니다.

00:09:48.000 --> 00:09:56.000
maximumHandCount를 설정하고 프레임에 지정된 것보다 더 많은 핸드가 있다면, 알고리즘은 대신 프레임에서 가장 눈에 띄는 중앙 핸드를 감지합니다.

00:09:56.000 --> 00:09:59.000
maximumHandCount의 기본값은 2입니다.

00:09:59.000 --> 00:10:04.000
나중에 요청에 대한 업데이트에 놀라지 않도록 여기에 수정본을 설정하는 것이 좋습니다.

00:10:04.000 --> 00:10:13.000
하지만 항상 연결된 SDK에서 지원하는 최신 알고리즘을 선택하고 싶다면, 설정할 필요가 없습니다.

00:10:13.000 --> 00:10:24.000
또한, 참고로, 우리는 ARKit을 통해 ARSession에 의해 검색된 모든 프레임에서 감지를 할 것이지만, 이것은 카메라 피드에서 프레임을 잡는 한 가지 방법일 뿐입니다.

00:10:24.000 --> 00:10:27.000
원하는 방법을 사용할 수 있습니다.

00:10:27.000 --> 00:10:30.000
AVCaptureOutput 또한 유용한 대안이 될 것이다.

00:10:30.000 --> 00:10:38.000
수신된 모든 프레임에 대해, 우리는 주어진 이미지의 모든 요청을 처리하는 VNImageRequestHandler를 만들어야 합니다.

00:10:38.000 --> 00:10:49.000
핸드 포즈 요청의 결과 속성은 이전에 요청에서 지정한 대로 최대 핸드 수 1개까지 VNHumanHandPoseObservations로 채워집니다.

00:10:49.000 --> 00:10:55.000
요청이 손 포즈를 감지하지 못하면, 현재 표시되는 효과를 지우고 싶을 수도 있습니다.

00:10:55.000 --> 00:11:00.000
그렇지 않으면, 우리는 한 손으로 관찰할 것이다.

00:11:00.000 --> 00:11:05.000
다음으로, 우리는 CoreML 모델을 사용하여 손 포즈가 무엇인지 예측하고 싶습니다.

00:11:05.000 --> 00:11:12.000
우리는 효과의 렌더링이 불안한 것을 원하지 않기 때문에 모든 프레임을 예측하고 싶지 않습니다.

00:11:12.000 --> 00:11:16.000
간격을 두고 예측을 하는 것은 더 원활한 사용자 경험을 만든다.

00:11:16.000 --> 00:11:27.000
예측을 하고 싶을 때, 우리는 MLMultiArray를 Hand Pose CoreML 모델로 전달하는 것으로 시작하고, 반환된 단일 예측에서 상단 라벨과 신뢰를 검색합니다.

00:11:27.000 --> 00:11:34.000
나는 라벨이 높은 수준의 자신감으로 예측될 때만 표시되는 효과에 대한 변화를 촉발하고 싶다.

00:11:34.000 --> 00:11:41.000
이것은 또한 효과가 너무 빨리 켜지고 꺼지고 불안해질 수 있는 행동으로부터 보호하는 열쇠이다.

00:11:41.000 --> 00:11:49.000
여기서, 배경 분류는 우리가 신뢰 임계값을 매우 높게 유지할 수 있게 함으로써 우리를 돕고 있다.

00:11:49.000 --> 00:11:53.000
만약 하나가 큰 자신감을 가지고 예측된다면, 우리는 렌더링할 effectNode를 설정할 수 있다.

00:11:53.000 --> 00:12:00.000
만약 하나가 큰 자신감으로 예측되지 않는다면, 나는 내 손이 하는 것과 일치하도록 화면의 효과를 멈추고 싶다.

00:12:00.000 --> 00:12:02.000
우리가 가진 것을 시험해 보자.

00:12:02.000 --> 00:12:08.000
만약 내가 내 손을 하나의 포즈로 만들면, 그것은 단일 에너지 빔 효과를 촉발할 것이다.

00:12:08.000 --> 00:12:09.000
정말 멋져!

00:12:09.000 --> 00:12:14.000
그 모델은 내가 포즈 하나를 만들고 효과를 촉발시켰다는 것을 알 수 있었다.

00:12:14.000 --> 00:12:18.000
내 손가락을 따라가면 더 시원하겠지만.

00:12:18.000 --> 00:12:22.000
내 손가락의 특정 지점에서 렌더링된다면 더 좋을 것이다.

00:12:22.000 --> 00:12:23.000
코드로 돌아가서 바꾸자.

00:12:23.000 --> 00:12:35.000
우리가 해야 할 일은 손의 키 포인트 위치를 그래픽 자산에 공급하는 것입니다. 즉, 뷰를 사용하여 정규화된 키 포인트를 카메라 뷰 공간으로 변환하는 것을 의미합니다.

00:12:35.000 --> 00:12:41.000
자신감 점수를 보고 어떤 요점을 저장하는지 정리하는 것을 고려할 수도 있습니다.

00:12:41.000 --> 00:12:44.000
여기서, 나는 집게 손가락 끝만 신경 쓴다.

00:12:44.000 --> 00:12:50.000
비전이 정규화된 좌표를 사용하기 때문에, 우리는 핵심 지점을 좌표 공간으로 변환해야 한다.

00:12:50.000 --> 00:12:57.000
또한, 비전의 원점은 이미지의 왼쪽 하단 모서리에 있으므로, 변환을 할 때 명심하세요.

00:12:57.000 --> 00:13:03.000
마지막으로, 인덱스 위치를 저장하고, 키 포인트가 발견되지 않으면 기본값은 nil입니다.

00:13:03.000 --> 00:13:08.000
효과를 렌더링하는 코드와 내 손가락을 따라 어떻게 조정할 수 있는지 살펴봅시다.

00:13:08.000 --> 00:13:13.000
우리는 그래픽 객체의 위치가 설정되는 장소를 찾고 싶습니다.

00:13:13.000 --> 00:13:18.000
setLocationForEffects는 비동기적으로 모든 프레임이라고 불린다.

00:13:18.000 --> 00:13:23.000
기본값으로, 우리는 뷰의 중앙에 나타나도록 효과를 설정했습니다.

00:13:23.000 --> 00:13:37.000
이전부터 indexFingerTipLocation CGPoint로 전환하면 의도한 효과를 얻을 수 있습니다.

00:13:37.000 --> 00:13:38.000
멋져!

00:13:38.000 --> 00:13:40.000
이건 멋져 보이기 시작했어.

00:13:40.000 --> 00:13:43.000
한 걸음 더 나아가자.

00:13:43.000 --> 00:13:52.000
초강대국을 둘러싼 더 흥미로운 그래픽 이야기를 만들기 위해, 우리의 응용 프로그램에서 손 포즈 분류를 몇 가지 더 활용하는 것이 좋을 것이다.

00:13:52.000 --> 00:13:56.000
이 경우, 우리는 두 번째 분류와 오픈 팜을 선택할 것이다.

00:13:56.000 --> 00:14:00.000
나는 이미 이 두 가지 포즈가 모두 감지되었을 때 조치를 취하기 위해 신청서를 확장했다.

00:14:00.000 --> 00:14:08.000
여기서, 나는 포즈 원을 위해 전에 표시된 것처럼 집게 손가락 끝에 나타나도록 에너지 빔을 중심에 두고 있다.

00:14:08.000 --> 00:14:14.000
두 번째 포즈를 위해 내 가운데와 집게손가락 끝에 있는 두 개의 에너지 빔.

00:14:14.000 --> 00:14:27.000
그리고 마지막 에너지 빔은 핸드 포즈 오픈 팜에 의해 트리거되며 가운데 손가락 하단의 키 포인트와 손목 키 포인트 사이에 고정됩니다.

00:14:27.000 --> 00:14:28.000
알았어.

00:14:28.000 --> 00:14:35.000
네이선과 내가 소개한 모든 것은 당신의 손 포즈 분류 모델을 완전히 통합하는 단계를 다룹니다.

00:14:35.000 --> 00:14:44.000
Vision에는 도움이 될 수 있는 새로운 기능이 하나 더 있으므로, 이 앱의 기능을 트리거하고 제어하는 데 도움이 될 수 있는 API를 소개합니다.

00:14:44.000 --> 00:14:54.000
비전은 사용자가 HumanHand- PoseObservation: chirality에서 왼손과 오른손을 구별할 수 있는 새로운 속성을 도입하고 있다.

00:14:54.000 --> 00:15:04.000
이것은 HumanHandPoseObservation이 왼손, 오른손, 알 수 없는 세 가지 값 중 하나일 수 있는 손을 나타내는 열거형이다.

00:15:04.000 --> 00:15:16.000
처음 두 값의 의미를 추측할 수 있지만, 알려지지 않은 값은 이전 버전의 HumanHandPoseObservation이 역직렬화되고 속성이 설정되지 않은 경우에만 나타날 것입니다.

00:15:16.000 --> 00:15:27.000
Nathan이 앞서 언급했듯이, WWDC 2020 세션인 "Detect Body and Hand Pose with Vision"을 참조하여 비전 손 포즈 감지에 대한 자세한 정보를 얻을 수 있습니다.

00:15:27.000 --> 00:15:36.000
참고로, 프레임에서 감지된 각 손에 대해, 기본 알고리즘은 각 손의 키랄성을 별도로 예측하려고 시도할 것이다.

00:15:36.000 --> 00:15:42.000
이것은 한 손의 예측이 프레임에 있는 다른 손의 예측에 영향을 미치지 않는다는 것을 의미한다.

00:15:42.000 --> 00:15:47.000
키랄리티를 사용하는 코드가 어떻게 보일 수 있는지에 대한 예를 보여드리겠습니다.

00:15:47.000 --> 00:15:52.000
우리는 이미 VNDetectHumanHandPoseRequest를 만들고 실행하는 설정을 다루었습니다.

00:15:52.000 --> 00:16:03.000
요청을 수행한 후, 관찰은 Enum 속성 chirality를 가질 것이며, 당신은 그것을 사용하여 행동을 취하거나 Vision 손 포즈 관찰을 정렬할 수 있습니다.

00:16:03.000 --> 00:16:07.000
지금까지 모든 것은 손 포즈 분류를 사용하는 방법에 관한 것이었다.

00:16:07.000 --> 00:16:14.000
하지만 Nathan이 앞서 언급했듯이, 손 행동 분류는 올해 또 다른 새로운 기술이다.

00:16:14.000 --> 00:16:16.000
여기 그것에 대해 당신과 이야기할 Geppy가 있습니다.

00:16:16.000 --> 00:16:18.000
고마워, 브리트니.

00:16:18.000 --> 00:16:23.000
안녕하세요, 제 이름은 Geppy Parziale이고, 저는 Create ML 팀의 기계 학습 엔지니어입니다.

00:16:23.000 --> 00:16:36.000
핸드 포즈 분류 외에도, 올해 Create ML은 핸드 액션 분류를 수행하기 위한 새로운 템플릿을 도입하며, 앱에서 사용하는 방법을 보여드리겠습니다.

00:16:36.000 --> 00:16:47.000
이러한 이유로, 나는 브리트니의 초능력 데모를 손 동작으로 확장하고 손 포즈와 손 동작 사이의 몇 가지 중요한 차이점을 강조할 것이다.

00:16:47.000 --> 00:17:00.000
핸드 액션과 바디 액션은 매우 유사한 두 가지 작업이기 때문에 추가 정보와 비교를 위해 WWDC 2020의 "Create ML로 액션 분류기 구축" 세션을 참조하십시오.

00:17:00.000 --> 00:17:05.000
하지만 이제, 핸드 액션이 무엇인지 설명하겠습니다.

00:17:05.000 --> 00:17:13.000
손 동작은 ML 모델이 손이 움직이는 동안 분석해야 하는 일련의 손 포즈로 구성되어 있다.

00:17:13.000 --> 00:17:20.000
시퀀스 내의 포즈 수는 처음부터 끝까지 전체 손 동작을 포착할 수 있을 만큼 커야 한다.

00:17:20.000 --> 00:17:23.000
당신은 손을 잡기 위해 비디오를 사용합니다.

00:17:23.000 --> 00:17:33.000
네이선이 이전에 우리에게 보여준 것처럼, 핸드 액션 분류기를 훈련시키는 것은 손 포즈 분류기를 훈련하는 것과 동일하며, 몇 가지 사소한 차이가 있다.

00:17:33.000 --> 00:17:40.000
정적 이미지는 손 포즈를 나타내지만, 비디오는 손 동작을 포착하고 나타내는 데 사용됩니다.

00:17:40.000 --> 00:17:48.000
따라서 핸드 액션 분류기를 훈련시키기 위해, 각 비디오가 핸드 액션을 나타내는 짧은 비디오를 사용합니다.

00:17:48.000 --> 00:17:54.000
이 비디오는 각 폴더 이름이 액션 클래스를 나타내는 폴더로 구성할 수 있습니다.

00:17:54.000 --> 00:18:03.000
그리고 분류자가 인식하기를 원하는 동작과 다른 동작이 있는 비디오를 포함하는 배경 클래스를 포함하는 것을 잊지 마세요.

00:18:03.000 --> 00:18:10.000
대체 표현으로, 모든 예제 비디오 파일을 하나의 폴더에 추가할 수 있습니다.

00:18:10.000 --> 00:18:17.000
그런 다음, CSV 또는 JSON 형식을 사용하여 주석 파일을 추가합니다.

00:18:17.000 --> 00:18:27.000
주석 파일의 각 항목은 비디오 파일의 이름, 관련 클래스, 손 동작의 시작 및 종료 시간을 나타냅니다.

00:18:27.000 --> 00:18:31.000
또한, 이 경우, 배경 수업을 포함하는 것을 잊지 마세요.

00:18:31.000 --> 00:18:36.000
기억하세요, 당신은 거의 같은 길이의 비디오로 모델을 훈련시킵니다.

00:18:36.000 --> 00:18:48.000
실제로, 작업 기간을 훈련 매개 변수로 제공한 다음 ML을 생성하면 제공한 값에 따라 연속적인 수의 프레임을 무작위로 샘플링합니다.

00:18:48.000 --> 00:18:53.000
또한 비디오 프레임 속도와 훈련 반복을 제공할 수 있습니다.

00:18:53.000 --> 00:19:03.000
그 외에도, 이 앱은 모델이 더 잘 일반화하고 정확성을 높이는 데 도움이 되는 다양한 유형의 데이터 증강을 제공합니다.

00:19:03.000 --> 00:19:15.000
특히, 시간 보간과 프레임 드롭은 실제 사용 사례에 더 가까운 비디오 변형을 제공하기 위해 핸드 액션 분류에 추가된 두 가지 보강이다.

00:19:15.000 --> 00:19:20.000
그래서 나는 이미 내 데모를 위해 Hand Action Classifier를 훈련시켰어. 실제로 실행되는지 보자.

00:19:20.000 --> 00:19:25.000
음, 나는 슈퍼히어로이기 때문에, 에너지원이 필요해.

00:19:25.000 --> 00:19:27.000
여기 내 거야.

00:19:27.000 --> 00:19:31.000
여기서, 나는 내 에너지원을 시각화하기 위해 손 포즈를 사용한다.

00:19:31.000 --> 00:19:37.000
하지만 이제, 내 초능력을 사용하여 활성화할게.

00:19:37.000 --> 00:19:40.000
이 경우, 나는 핸드 액션을 사용하고 있다.

00:19:40.000 --> 00:19:44.000
이거 멋지다.

00:19:44.000 --> 00:19:50.000
그리고 이제, 핸드 포즈와 핸드 액션 분류기가 동시에 실행되고 있다.

00:19:50.000 --> 00:20:03.000
저는 비전의 새로운 키랄리티 기능을 활용하고 있으며 손 포즈를 위해 왼손을 사용하고 손 동작을 위해 오른손을 사용합니다.

00:20:03.000 --> 00:20:05.000
이건 정말 멋져.

00:20:05.000 --> 00:20:15.000
그래서 이것은 Create ML이 적용되는 최적화, Apple Neural Engine의 모든 힘을 발휘하기 위한 모든 모델에 대한 훈련 시간 때문에 가능합니다.

00:20:15.000 --> 00:20:25.000
그리고 이제, 제 현실 세계로 돌아가서 Create ML Hand Action Classifier를 제 데모에 통합하는 방법을 설명하겠습니다.

00:20:25.000 --> 00:20:28.000
먼저 모델의 입력을 살펴봅시다.

00:20:28.000 --> 00:20:37.000
핸드 액션 분류기를 앱에 통합할 때, 모델에 예상되는 손 포즈의 정확한 수를 제공해야 합니다.

00:20:37.000 --> 00:20:44.000
내 모델은 XCode Preview에서 검사할 수 있기 때문에 45 x 3 x 21 크기의 MultiArray를 기대하고 있다.

00:20:44.000 --> 00:20:52.000
여기서, 45는 분류자가 행동을 인식하기 위해 분석해야 하는 포즈의 수이다.

00:20:52.000 --> 00:20:57.000
21은 각 손에 대한 비전 프레임워크가 제공하는 관절의 수이다.

00:20:57.000 --> 00:21:03.000
마지막으로, 3은 x와 y 좌표와 각 관절의 신뢰 값이다.

00:21:03.000 --> 00:21:06.000
45는 어디에서 왔나요?

00:21:06.000 --> 00:21:14.000
그것은 예측 창 크기이며 비디오 길이와 훈련 시간에 사용된 비디오의 프레임 속도에 따라 다릅니다.

00:21:14.000 --> 00:21:22.000
내 경우, 나는 30fps와 1.5초 길이로 녹화된 비디오로 모델을 훈련시키기로 결정했다.

00:21:22.000 --> 00:21:32.000
이것은 모델이 손 동작당 45개의 비디오 프레임으로 훈련되었다는 것을 의미하므로, 추론하는 동안 모델은 같은 수의 손 포즈를 기대하고 있다.

00:21:32.000 --> 00:21:41.000
추론 시간에 도착하는 손 포즈의 빈도와 관련하여 추가적인 고려 사항이 고려되어야 한다.

00:21:41.000 --> 00:21:50.000
추론 중에 모델에 제시된 손 포즈의 비율이 모델을 훈련시키는 데 사용된 포즈의 속도와 일치하는 것은 매우 중요하다.

00:21:50.000 --> 00:21:53.000
내 데모에서, 나는 ARKit을 사용했다.

00:21:53.000 --> 00:22:05.000
그래서 나는 ARKit이 60fps의 프레임을 제공하고 분류기가 30fps의 비디오로 훈련되었기 때문에 초당 도착 포즈의 수를 절반으로 줄이려고 했다.

00:22:05.000 --> 00:22:10.000
그렇지 않으면, 분류기는 잘못된 예측을 제공할 수 있다.

00:22:10.000 --> 00:22:15.000
이제 소스 코드로 뛰어들어 이것을 구현하는 방법을 보여드리겠습니다.

00:22:15.000 --> 00:22:30.000
먼저, 나는 카운터를 사용하여 비전에서 도착하는 포즈의 속도를 60fps에서 30fps로 줄이고, 내 모델이 제대로 작동할 것으로 기대하는 프레임 속도를 일치시킨다.

00:22:30.000 --> 00:22:37.000
그런 다음, 나는 장면의 각 손에 대한 관절과 키랄리티를 포함하는 배열을 얻는다.

00:22:37.000 --> 00:22:48.000
다음으로, 나는 데모에서 오른손을 사용하여 손 동작으로 일부 효과를 활성화하기 때문에 왼손의 요점을 버린다.

00:22:48.000 --> 00:22:52.000
좋아, 이제 나는 분류자를 위해 손 포즈를 모아야 해.

00:22:52.000 --> 00:23:03.000
그렇게 하기 위해, 저는 FIFO 대기열을 사용하고 45개의 손 포즈를 축적하고 대기열에 항상 마지막 45개의 포즈가 포함되어 있는지 확인합니다.

00:23:03.000 --> 00:23:05.000
대기열은 처음에는 비어 있다.

00:23:05.000 --> 00:23:12.000
새로운 손 포즈가 도착하면, 나는 그것을 대기열에 추가하고, 대기열이 가득 찰 때까지 이 단계를 반복한다.

00:23:12.000 --> 00:23:16.000
대기열이 가득 차면, 나는 전체 내용을 읽을 수 있다.

00:23:16.000 --> 00:23:21.000
나는 비전에서 새로운 손 포즈를 받을 때마다 대기열을 읽을 수 있었다.

00:23:21.000 --> 00:23:25.000
하지만 기억하세요, 저는 지금 초당 30프레임을 처리하고 있습니다.

00:23:25.000 --> 00:23:31.000
그리고 사용 사례에 따라, 이것은 자원 낭비일 수 있다.

00:23:31.000 --> 00:23:37.000
그래서 나는 프레임 수를 정의한 후 대기열을 읽기 위해 다른 카운터를 사용한다.

00:23:37.000 --> 00:23:49.000
애플리케이션의 반응성과 얻고자 하는 초당 예측 수 사이의 절충안으로 대기열 샘플링 속도를 선택해야 합니다.

00:23:49.000 --> 00:24:02.000
이 시점에서, 나는 MLMultiArray로 구성된 45개의 손 포즈의 전체 시퀀스를 읽고, 손 동작을 예측하기 위해 분류기에 입력했다.

00:24:02.000 --> 00:24:07.000
그런 다음, 나는 예측 라벨과 신뢰 값을 추출한다.

00:24:07.000 --> 00:24:15.000
마지막으로, 신뢰 값이 정의된 임계값보다 크면, 나는 장면에 입자 효과를 추가한다.

00:24:15.000 --> 00:24:27.000
따라서 앱에 Create ML Hand Action Classifier를 통합할 때, 모델이 기대하는 프레임 속도로 손 포즈의 순서를 입력해야 합니다.

00:24:27.000 --> 00:24:32.000
분류기를 훈련시키는 데 사용된 비디오의 동일한 프레임 속도를 일치시키세요.

00:24:32.000 --> 00:24:39.000
선입선출 대기열을 사용하여 모델 예측에 대한 손 포즈를 수집하세요.

00:24:39.000 --> 00:24:42.000
올바른 프레임 속도로 대기열을 읽으세요.

00:24:42.000 --> 00:24:49.000
Create ML로 훈련된 핸드 액션 모델로 구축할 모든 멋진 응용 프로그램을 보기를 고대하고 있습니다.

00:24:49.000 --> 00:24:54.000
그리고 이제, 최종 고려와 요약을 위해 Nathan에게 돌아갑니다.

00:24:54.000 --> 00:24:55.000
고마워, 제피.

00:24:55.000 --> 00:24:57.000
당신과 브리트니는 당신의 앱에서 훌륭한 일을 했습니다.

00:24:57.000 --> 00:24:59.000
그걸 시도해 볼 수 있어서 신나.

00:24:59.000 --> 00:25:05.000
하지만 일이 통제에서 벗어나기 전에, 사용자에게 고품질의 경험을 보장하기 위해 명심해야 할 몇 가지 사항이 있습니다.

00:25:05.000 --> 00:25:08.000
손이 카메라에서 얼마나 멀리 떨어져 있는지 염두에 두세요.

00:25:08.000 --> 00:25:12.000
최상의 결과를 얻으려면 거리를 11피트 또는 3 1/2미터 이하로 유지해야 합니다.

00:25:12.000 --> 00:25:17.000
또한 너무 어둡거나 너무 밝은 극한의 조명 조건을 피하는 것이 가장 좋습니다.

00:25:17.000 --> 00:25:25.000
부피가 크고 느슨하거나 다채로운 장갑은 손 포즈를 정확하게 감지하기 어렵게 만들 수 있으며, 이는 분류 품질에 영향을 미칠 수 있습니다.

00:25:25.000 --> 00:25:29.000
모든 기계 학습 작업과 마찬가지로, 훈련 데이터의 품질과 양이 중요합니다.

00:25:29.000 --> 00:25:34.000
이 세션에 표시된 손 포즈 분류기의 경우, 우리는 수업당 500개의 이미지를 사용했습니다.

00:25:34.000 --> 00:25:41.000
핸드 액션 분류기, 우리는 수업당 100개의 비디오를 사용했지만, 사용 사례에 대한 데이터 요구 사항은 다를 수 있습니다.

00:25:41.000 --> 00:25:48.000
가장 중요한 것은 모델이 앱에서 볼 수 있는 예상 변화를 포착할 수 있는 충분한 훈련 데이터를 수집하는 것입니다.

00:25:48.000 --> 00:25:50.000
이제 요약하기에 좋은 시간처럼 느껴진다.

00:25:50.000 --> 00:25:52.000
그래서 우리는 무엇을 배웠나요?

00:25:52.000 --> 00:25:57.000
음, 2021년부터, 당신은 인간의 손의 표현을 해석하는 앱을 만들 수 있습니다.

00:25:57.000 --> 00:26:02.000
우리는 두 가지 범주의 손 표현, 포즈 및 행동의 차이점에 대해 논의했다.

00:26:02.000 --> 00:26:08.000
우리는 모델을 훈련시키기 위해 Create ML 앱에서 사용하기 위해 백그라운드 클래스를 포함한 훈련 데이터를 준비하는 방법에 대해 이야기했습니다.

00:26:08.000 --> 00:26:12.000
우리는 훈련된 모델을 앱에 통합하는 방법에 대해 이야기했다.

00:26:12.000 --> 00:26:19.000
그리고 마지막으로, 우리는 여러 모델을 단일 앱에 통합하고 chirality를 사용하여 손을 차별화하는 것에 대해 이야기했습니다.

00:26:19.000 --> 00:26:22.000
분명히, 오늘의 데모는 표면만 긁는다.

00:26:22.000 --> 00:26:28.000
비전 프레임워크는 손의 존재, 포즈, 위치 및 키랄성을 감지하는 강력한 기술이다.

00:26:28.000 --> 00:26:33.000
Create ML은 손 포즈와 손 동작을 훈련하고 분류하는 재미있고 쉬운 방법입니다.

00:26:33.000 --> 00:26:41.000
함께 사용될 때, 그들은 인류의 가장 강력하고 표현력 있는 도구 중 하나에 대한 깊은 통찰력을 제공하며, 우리는 당신이 그것들로 무엇을 하는지 빨리 보고 싶습니다.

00:26:41.000 --> 00:26:43.000
안녕.

00:26:43.000 --> 23:59:59.000
[쾌활한 음악].

